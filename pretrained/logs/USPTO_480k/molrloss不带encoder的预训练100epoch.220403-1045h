Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = **
**** embed_size = *512*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *256*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *256*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *256*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *100*
**** max_steps = *300000*
**** warmup_steps = *100*
**** lr = *0.0001*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=361, out_features=256, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=370, out_features=256, bias=True)
        (W_r): Linear(in_features=114, out_features=256, bias=False)
        (U_r): Linear(in_features=256, out_features=256, bias=True)
        (W_h): Linear(in_features=370, out_features=256, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=256, bias=True)
        (attn_W_k): Linear(in_features=256, out_features=256, bias=True)
        (attn_W_v): Linear(in_features=256, out_features=256, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=256, bias=True)
      (attn_W_k): Linear(in_features=256, out_features=256, bias=True)
      (attn_W_v): Linear(in_features=256, out_features=256, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (attention_encoder): AttnEncoderXL(
    (dropout): Dropout(p=0.3, inplace=False)
    (attention_layers): ModuleList(
      (0): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (1): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (2): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (3): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (4): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (5): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (g): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=False)
    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 8707216
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 100 Loss: 7.2764, lr 0.0001
Step 200 Loss: 5.2699, lr 0.0001
Step 300 Loss: 4.4005, lr 0.0001
Step 400 Loss: 3.8656, lr 0.0001
Step 500 Loss: 3.5022, lr 0.0001
Step 600 Loss: 3.2322, lr 0.0001
Step 700 Loss: 3.0146, lr 0.0001
Step 800 Loss: 2.8360, lr 0.0001
Step 900 Loss: 2.6882, lr 0.0001
Step 1000 Loss: 2.5600, lr 0.0001
Step 1100 Loss: 2.4493, lr 0.0001
Step 1200 Loss: 2.3518, lr 0.0001
Step 1300 Loss: 2.2682, lr 0.0001
Step 1400 Loss: 2.1940, lr 0.0001
Step 1500 Loss: 2.1274, lr 0.0001
Step 1600 Loss: 2.0690, lr 0.0001
Step 1700 Loss: 2.0159, lr 0.0001
Step 1800 Loss: 1.9670, lr 0.0001
Step 1900 Loss: 1.9230, lr 0.0001
Step 2000 Loss: 1.8843, lr 0.0001
Step 2100 Loss: 1.8459, lr 0.0001
Step 2200 Loss: 1.8102, lr 0.0001
Step 2300 Loss: 1.7775, lr 0.0001
Step 2400 Loss: 1.7478, lr 0.0001
Step 2500 Loss: 1.7190, lr 0.0001
Step 2600 Loss: 1.6920, lr 0.0001
Step 2700 Loss: 1.6672, lr 0.0001
Step 2800 Loss: 1.6438, lr 0.0001
Step 2900 Loss: 1.6210, lr 0.0001
Step 3000 Loss: 1.5998, lr 0.0001
Step 3100 Loss: 1.5800, lr 0.0001
Step 3200 Loss: 1.5611, lr 0.0001
Step 3300 Loss: 1.5419, lr 0.0001
Step 3400 Loss: 1.5244, lr 0.0001
Step 3500 Loss: 1.5069, lr 0.0001
Step 3600 Loss: 1.4910, lr 0.0001
Step 3700 Loss: 1.4760, lr 0.0001
Step 3800 Loss: 1.4615, lr 0.0001
Step 3900 Loss: 1.4473, lr 0.0001
Step 4000 Loss: 1.4347, lr 0.0001
Step 4100 Loss: 1.4225, lr 0.0001
Step 4200 Loss: 1.4113, lr 0.0001
Step 4300 Loss: 1.4006, lr 0.0001
Step 4400 Loss: 1.3902, lr 0.0001
Step 4500 Loss: 1.3810, lr 0.0001
Step 4600 Loss: 1.3715, lr 0.0001
Step 4700 Loss: 1.3623, lr 0.0001
Step 4800 Loss: 1.3546, lr 0.0001
Step 4900 Loss: 1.3469, lr 0.0001
Step 5000 Loss: 1.3401, lr 0.0001
Step 5100 Loss: 1.3338, lr 0.0001
Step 5200 Loss: 1.3275, lr 0.0001
Step 5300 Loss: 1.3217, lr 0.0001
Step 5400 Loss: 1.3169, lr 0.0001
Step 5500 Loss: 1.3121, lr 0.0001
Step 5600 Loss: 1.3081, lr 0.0001
Step 5700 Loss: 1.3033, lr 0.0001
Step 5800 Loss: 1.2985, lr 0.0001
Step 5900 Loss: 1.2938, lr 0.0001
Step 6000 Loss: 1.2891, lr 0.0001
Step 6100 Loss: 1.2837, lr 0.0001
Step 6200 Loss: 1.2782, lr 0.0001
Step 6300 Loss: 1.2729, lr 0.0001
Step 6400 Loss: 1.2669, lr 0.0001
Step 6500 Loss: 1.2610, lr 0.0001
Step 6600 Loss: 1.2546, lr 0.0001
Step 6700 Loss: 1.2486, lr 0.0001
Train Epoch: [1/100] Loss: 1.2452,lr 0.000100
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Step 6800 Loss: 1.5465, lr 9.997532801828658e-05
Step 6900 Loss: 1.2111, lr 9.997532801828658e-05
Step 7000 Loss: 1.1127, lr 9.997532801828658e-05
Step 7100 Loss: 1.0489, lr 9.997532801828658e-05
Step 7200 Loss: 1.0078, lr 9.997532801828658e-05
Step 7300 Loss: 0.9770, lr 9.997532801828658e-05
Step 7400 Loss: 0.9489, lr 9.997532801828658e-05
Step 7500 Loss: 0.9279, lr 9.997532801828658e-05
Step 7600 Loss: 0.9096, lr 9.997532801828658e-05
Step 7700 Loss: 0.8938, lr 9.997532801828658e-05
Step 7800 Loss: 0.8804, lr 9.997532801828658e-05
Step 7900 Loss: 0.8699, lr 9.997532801828658e-05
Step 8000 Loss: 0.8602, lr 9.997532801828658e-05
Step 8100 Loss: 0.8516, lr 9.997532801828658e-05
Step 8200 Loss: 0.8443, lr 9.997532801828658e-05
Step 8300 Loss: 0.8378, lr 9.997532801828658e-05
Step 8400 Loss: 0.8313, lr 9.997532801828658e-05
Step 8500 Loss: 0.8254, lr 9.997532801828658e-05
Step 8600 Loss: 0.8192, lr 9.997532801828658e-05
Step 8700 Loss: 0.8142, lr 9.997532801828658e-05
Step 8800 Loss: 0.8089, lr 9.997532801828658e-05
Step 8900 Loss: 0.8038, lr 9.997532801828658e-05
Step 9000 Loss: 0.7994, lr 9.997532801828658e-05
Step 9100 Loss: 0.7947, lr 9.997532801828658e-05
Step 9200 Loss: 0.7909, lr 9.997532801828658e-05
Step 9300 Loss: 0.7866, lr 9.997532801828658e-05
Step 9400 Loss: 0.7825, lr 9.997532801828658e-05
Step 9500 Loss: 0.7802, lr 9.997532801828658e-05
Step 9600 Loss: 0.7768, lr 9.997532801828658e-05
Step 9700 Loss: 0.7737, lr 9.997532801828658e-05
Step 9800 Loss: 0.7709, lr 9.997532801828658e-05
Step 9900 Loss: 0.7678, lr 9.997532801828658e-05
Step 10000 Loss: 0.7647, lr 9.997532801828658e-05
Step 10100 Loss: 0.7619, lr 9.997532801828658e-05
Step 10200 Loss: 0.7590, lr 9.997532801828658e-05
Step 10300 Loss: 0.7567, lr 9.997532801828658e-05
Step 10400 Loss: 0.7546, lr 9.997532801828658e-05
Step 10500 Loss: 0.7522, lr 9.997532801828658e-05
Step 10600 Loss: 0.7505, lr 9.997532801828658e-05
Step 10700 Loss: 0.7484, lr 9.997532801828658e-05
Step 10800 Loss: 0.7473, lr 9.997532801828658e-05
Step 10900 Loss: 0.7458, lr 9.997532801828658e-05
Step 11000 Loss: 0.7450, lr 9.997532801828658e-05
Step 11100 Loss: 0.7435, lr 9.997532801828658e-05
Step 11200 Loss: 0.7421, lr 9.997532801828658e-05
Step 11300 Loss: 0.7400, lr 9.997532801828658e-05
Step 11400 Loss: 0.7379, lr 9.997532801828658e-05
Step 11500 Loss: 0.7362, lr 9.997532801828658e-05
Step 11600 Loss: 0.7359, lr 9.997532801828658e-05
Step 11700 Loss: 0.7358, lr 9.997532801828658e-05
Step 11800 Loss: 0.7344, lr 9.997532801828658e-05
Step 11900 Loss: 0.7339, lr 9.997532801828658e-05
Step 12000 Loss: 0.7325, lr 9.997532801828658e-05
Step 12100 Loss: 0.7314, lr 9.997532801828658e-05
Step 12200 Loss: 0.7307, lr 9.997532801828658e-05
Step 12300 Loss: 0.7299, lr 9.997532801828658e-05
Step 12400 Loss: 0.7293, lr 9.997532801828658e-05
Step 12500 Loss: 0.7294, lr 9.997532801828658e-05
Step 12600 Loss: 0.7293, lr 9.997532801828658e-05
Step 12700 Loss: 0.7292, lr 9.997532801828658e-05
Step 12800 Loss: 0.7299, lr 9.997532801828658e-05
Step 12900 Loss: 0.7306, lr 9.997532801828658e-05
Step 13000 Loss: 0.7315, lr 9.997532801828658e-05
Step 13100 Loss: 0.7317, lr 9.997532801828658e-05
Step 13200 Loss: 0.7315, lr 9.997532801828658e-05
Step 13300 Loss: 0.7312, lr 9.997532801828658e-05
Step 13400 Loss: 0.7297, lr 9.997532801828658e-05
Step 13500 Loss: 0.7287, lr 9.997532801828658e-05
Train Epoch: [2/100] Loss: 0.7287,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Step 13600 Loss: 1.2251, lr 9.990133642141359e-05
Step 13700 Loss: 1.0422, lr 9.990133642141359e-05
Step 13800 Loss: 0.9588, lr 9.990133642141359e-05
Step 13900 Loss: 0.9070, lr 9.990133642141359e-05
Step 14000 Loss: 0.8744, lr 9.990133642141359e-05
Step 14100 Loss: 0.8453, lr 9.990133642141359e-05
Step 14200 Loss: 0.8244, lr 9.990133642141359e-05
Step 14300 Loss: 0.8059, lr 9.990133642141359e-05
Step 14400 Loss: 0.7907, lr 9.990133642141359e-05
Step 14500 Loss: 0.7774, lr 9.990133642141359e-05
Step 14600 Loss: 0.7671, lr 9.990133642141359e-05
Step 14700 Loss: 0.7573, lr 9.990133642141359e-05
Step 14800 Loss: 0.7500, lr 9.990133642141359e-05
Step 14900 Loss: 0.7421, lr 9.990133642141359e-05
Step 15000 Loss: 0.7354, lr 9.990133642141359e-05
Step 15100 Loss: 0.7286, lr 9.990133642141359e-05
Step 15200 Loss: 0.7224, lr 9.990133642141359e-05
Step 15300 Loss: 0.7169, lr 9.990133642141359e-05
Step 15400 Loss: 0.7107, lr 9.990133642141359e-05
Step 15500 Loss: 0.7054, lr 9.990133642141359e-05
Step 15600 Loss: 0.6999, lr 9.990133642141359e-05
Step 15700 Loss: 0.6957, lr 9.990133642141359e-05
Step 15800 Loss: 0.6915, lr 9.990133642141359e-05
Step 15900 Loss: 0.6881, lr 9.990133642141359e-05
Step 16000 Loss: 0.6840, lr 9.990133642141359e-05
Step 16100 Loss: 0.6795, lr 9.990133642141359e-05
Step 16200 Loss: 0.6760, lr 9.990133642141359e-05
Step 16300 Loss: 0.6739, lr 9.990133642141359e-05
Step 16400 Loss: 0.6705, lr 9.990133642141359e-05
Step 16500 Loss: 0.6673, lr 9.990133642141359e-05
Step 16600 Loss: 0.6648, lr 9.990133642141359e-05
Step 16700 Loss: 0.6622, lr 9.990133642141359e-05
Step 16800 Loss: 0.6593, lr 9.990133642141359e-05
Step 16900 Loss: 0.6566, lr 9.990133642141359e-05
Step 17000 Loss: 0.6539, lr 9.990133642141359e-05
Step 17100 Loss: 0.6514, lr 9.990133642141359e-05
Step 17200 Loss: 0.6493, lr 9.990133642141359e-05
Step 17300 Loss: 0.6472, lr 9.990133642141359e-05
Step 17400 Loss: 0.6449, lr 9.990133642141359e-05
Step 17500 Loss: 0.6432, lr 9.990133642141359e-05
Step 17600 Loss: 0.6415, lr 9.990133642141359e-05
Step 17700 Loss: 0.6404, lr 9.990133642141359e-05
Step 17800 Loss: 0.6386, lr 9.990133642141359e-05
Step 17900 Loss: 0.6373, lr 9.990133642141359e-05
Step 18000 Loss: 0.6354, lr 9.990133642141359e-05
Step 18100 Loss: 0.6332, lr 9.990133642141359e-05
Step 18200 Loss: 0.6316, lr 9.990133642141359e-05
Step 18300 Loss: 0.6307, lr 9.990133642141359e-05
Step 18400 Loss: 0.6296, lr 9.990133642141359e-05
Step 18500 Loss: 0.6289, lr 9.990133642141359e-05
Step 18600 Loss: 0.6279, lr 9.990133642141359e-05
Step 18700 Loss: 0.6272, lr 9.990133642141359e-05
Step 18800 Loss: 0.6263, lr 9.990133642141359e-05
Step 18900 Loss: 0.6258, lr 9.990133642141359e-05
Step 19000 Loss: 0.6257, lr 9.990133642141359e-05
Step 19100 Loss: 0.6257, lr 9.990133642141359e-05
Step 19200 Loss: 0.6260, lr 9.990133642141359e-05
Step 19300 Loss: 0.6262, lr 9.990133642141359e-05
Step 19400 Loss: 0.6270, lr 9.990133642141359e-05
Step 19500 Loss: 0.6276, lr 9.990133642141359e-05
Step 19600 Loss: 0.6290, lr 9.990133642141359e-05
Step 19700 Loss: 0.6303, lr 9.990133642141359e-05
Step 19800 Loss: 0.6315, lr 9.990133642141359e-05
Step 19900 Loss: 0.6321, lr 9.990133642141359e-05
Step 20000 Loss: 0.6320, lr 9.990133642141359e-05
Step 20100 Loss: 0.6322, lr 9.990133642141359e-05
Step 20200 Loss: 0.6311, lr 9.990133642141359e-05
Train Epoch: [3/100] Loss: 0.6312,lr 0.000100
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Step 20300 Loss: 1.6470, lr 9.977809823015401e-05
Step 20400 Loss: 1.1356, lr 9.977809823015401e-05
Step 20500 Loss: 0.9974, lr 9.977809823015401e-05
Step 20600 Loss: 0.9262, lr 9.977809823015401e-05
Step 20700 Loss: 0.8809, lr 9.977809823015401e-05
Step 20800 Loss: 0.8474, lr 9.977809823015401e-05
Step 20900 Loss: 0.8192, lr 9.977809823015401e-05
Step 21000 Loss: 0.7970, lr 9.977809823015401e-05
Step 21100 Loss: 0.7775, lr 9.977809823015401e-05
Step 21200 Loss: 0.7612, lr 9.977809823015401e-05
Step 21300 Loss: 0.7462, lr 9.977809823015401e-05
Step 21400 Loss: 0.7352, lr 9.977809823015401e-05
Step 21500 Loss: 0.7240, lr 9.977809823015401e-05
Step 21600 Loss: 0.7151, lr 9.977809823015401e-05
Step 21700 Loss: 0.7069, lr 9.977809823015401e-05
Step 21800 Loss: 0.6994, lr 9.977809823015401e-05
Step 21900 Loss: 0.6916, lr 9.977809823015401e-05
Step 22000 Loss: 0.6849, lr 9.977809823015401e-05
Step 22100 Loss: 0.6779, lr 9.977809823015401e-05
Step 22200 Loss: 0.6719, lr 9.977809823015401e-05
Step 22300 Loss: 0.6654, lr 9.977809823015401e-05
Step 22400 Loss: 0.6604, lr 9.977809823015401e-05
Step 22500 Loss: 0.6551, lr 9.977809823015401e-05
Step 22600 Loss: 0.6504, lr 9.977809823015401e-05
Step 22700 Loss: 0.6467, lr 9.977809823015401e-05
Step 22800 Loss: 0.6419, lr 9.977809823015401e-05
Step 22900 Loss: 0.6378, lr 9.977809823015401e-05
Step 23000 Loss: 0.6347, lr 9.977809823015401e-05
Step 23100 Loss: 0.6313, lr 9.977809823015401e-05
Step 23200 Loss: 0.6278, lr 9.977809823015401e-05
Step 23300 Loss: 0.6249, lr 9.977809823015401e-05
Step 23400 Loss: 0.6222, lr 9.977809823015401e-05
Step 23500 Loss: 0.6195, lr 9.977809823015401e-05
Step 23600 Loss: 0.6162, lr 9.977809823015401e-05
Step 23700 Loss: 0.6133, lr 9.977809823015401e-05
Step 23800 Loss: 0.6105, lr 9.977809823015401e-05
Step 23900 Loss: 0.6080, lr 9.977809823015401e-05
Step 24000 Loss: 0.6052, lr 9.977809823015401e-05
Step 24100 Loss: 0.6029, lr 9.977809823015401e-05
Step 24200 Loss: 0.6002, lr 9.977809823015401e-05
Step 24300 Loss: 0.5987, lr 9.977809823015401e-05
Step 24400 Loss: 0.5968, lr 9.977809823015401e-05
Step 24500 Loss: 0.5951, lr 9.977809823015401e-05
Step 24600 Loss: 0.5932, lr 9.977809823015401e-05
Step 24700 Loss: 0.5916, lr 9.977809823015401e-05
Step 24800 Loss: 0.5897, lr 9.977809823015401e-05
Step 24900 Loss: 0.5883, lr 9.977809823015401e-05
Step 25000 Loss: 0.5865, lr 9.977809823015401e-05
Step 25100 Loss: 0.5855, lr 9.977809823015401e-05
Step 25200 Loss: 0.5848, lr 9.977809823015401e-05
Step 25300 Loss: 0.5840, lr 9.977809823015401e-05
Step 25400 Loss: 0.5835, lr 9.977809823015401e-05
Step 25500 Loss: 0.5828, lr 9.977809823015401e-05
Step 25600 Loss: 0.5825, lr 9.977809823015401e-05
Step 25700 Loss: 0.5817, lr 9.977809823015401e-05
Step 25800 Loss: 0.5816, lr 9.977809823015401e-05
Step 25900 Loss: 0.5816, lr 9.977809823015401e-05
Step 26000 Loss: 0.5820, lr 9.977809823015401e-05
Step 26100 Loss: 0.5825, lr 9.977809823015401e-05
Step 26200 Loss: 0.5837, lr 9.977809823015401e-05
Step 26300 Loss: 0.5843, lr 9.977809823015401e-05
Step 26400 Loss: 0.5860, lr 9.977809823015401e-05
Step 26500 Loss: 0.5871, lr 9.977809823015401e-05
Step 26600 Loss: 0.5878, lr 9.977809823015401e-05
Step 26700 Loss: 0.5880, lr 9.977809823015401e-05
Step 26800 Loss: 0.5883, lr 9.977809823015401e-05
Step 26900 Loss: 0.5877, lr 9.977809823015401e-05
Step 27000 Loss: 0.5871, lr 9.977809823015401e-05
Train Epoch: [4/100] Loss: 0.5873,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6755
Step 27100 Loss: 1.1043, lr 9.960573506572391e-05
Step 27200 Loss: 0.9306, lr 9.960573506572391e-05
Step 27300 Loss: 0.8558, lr 9.960573506572391e-05
Step 27400 Loss: 0.8069, lr 9.960573506572391e-05
Step 27500 Loss: 0.7816, lr 9.960573506572391e-05
Step 27600 Loss: 0.7573, lr 9.960573506572391e-05
Step 27700 Loss: 0.7391, lr 9.960573506572391e-05
Step 27800 Loss: 0.7243, lr 9.960573506572391e-05
Step 27900 Loss: 0.7110, lr 9.960573506572391e-05
Step 28000 Loss: 0.6991, lr 9.960573506572391e-05
Step 28100 Loss: 0.6912, lr 9.960573506572391e-05
Step 28200 Loss: 0.6829, lr 9.960573506572391e-05
Step 28300 Loss: 0.6763, lr 9.960573506572391e-05
Step 28400 Loss: 0.6677, lr 9.960573506572391e-05
Step 28500 Loss: 0.6614, lr 9.960573506572391e-05
Step 28600 Loss: 0.6553, lr 9.960573506572391e-05
Step 28700 Loss: 0.6489, lr 9.960573506572391e-05
Step 28800 Loss: 0.6435, lr 9.960573506572391e-05
Step 28900 Loss: 0.6376, lr 9.960573506572391e-05
Step 29000 Loss: 0.6328, lr 9.960573506572391e-05
Step 29100 Loss: 0.6273, lr 9.960573506572391e-05
Step 29200 Loss: 0.6233, lr 9.960573506572391e-05
Step 29300 Loss: 0.6188, lr 9.960573506572391e-05
Step 29400 Loss: 0.6149, lr 9.960573506572391e-05
Step 29500 Loss: 0.6108, lr 9.960573506572391e-05
Step 29600 Loss: 0.6066, lr 9.960573506572391e-05
Step 29700 Loss: 0.6030, lr 9.960573506572391e-05
Step 29800 Loss: 0.6003, lr 9.960573506572391e-05
Step 29900 Loss: 0.5966, lr 9.960573506572391e-05
Step 30000 Loss: 0.5940, lr 9.960573506572391e-05
Step 30100 Loss: 0.5913, lr 9.960573506572391e-05
Step 30200 Loss: 0.5879, lr 9.960573506572391e-05
Step 30300 Loss: 0.5853, lr 9.960573506572391e-05
Step 30400 Loss: 0.5826, lr 9.960573506572391e-05
Step 30500 Loss: 0.5799, lr 9.960573506572391e-05
Step 30600 Loss: 0.5771, lr 9.960573506572391e-05
Step 30700 Loss: 0.5748, lr 9.960573506572391e-05
Step 30800 Loss: 0.5725, lr 9.960573506572391e-05
Step 30900 Loss: 0.5703, lr 9.960573506572391e-05
Step 31000 Loss: 0.5684, lr 9.960573506572391e-05
Step 31100 Loss: 0.5669, lr 9.960573506572391e-05
Step 31200 Loss: 0.5658, lr 9.960573506572391e-05
Step 31300 Loss: 0.5642, lr 9.960573506572391e-05
Step 31400 Loss: 0.5629, lr 9.960573506572391e-05
Step 31500 Loss: 0.5613, lr 9.960573506572391e-05
Step 31600 Loss: 0.5594, lr 9.960573506572391e-05
Step 31700 Loss: 0.5579, lr 9.960573506572391e-05
Step 31800 Loss: 0.5567, lr 9.960573506572391e-05
Step 31900 Loss: 0.5561, lr 9.960573506572391e-05
Step 32000 Loss: 0.5557, lr 9.960573506572391e-05
Step 32100 Loss: 0.5549, lr 9.960573506572391e-05
Step 32200 Loss: 0.5545, lr 9.960573506572391e-05
Step 32300 Loss: 0.5537, lr 9.960573506572391e-05
Step 32400 Loss: 0.5533, lr 9.960573506572391e-05
Step 32500 Loss: 0.5531, lr 9.960573506572391e-05
Step 32600 Loss: 0.5532, lr 9.960573506572391e-05
Step 32700 Loss: 0.5531, lr 9.960573506572391e-05
Step 32800 Loss: 0.5536, lr 9.960573506572391e-05
Step 32900 Loss: 0.5544, lr 9.960573506572391e-05
Step 33000 Loss: 0.5553, lr 9.960573506572391e-05
Step 33100 Loss: 0.5570, lr 9.960573506572391e-05
Step 33200 Loss: 0.5584, lr 9.960573506572391e-05
Step 33300 Loss: 0.5591, lr 9.960573506572391e-05
Step 33400 Loss: 0.5599, lr 9.960573506572391e-05
Step 33500 Loss: 0.5601, lr 9.960573506572391e-05
Step 33600 Loss: 0.5605, lr 9.960573506572391e-05
Step 33700 Loss: 0.5598, lr 9.960573506572391e-05
Train Epoch: [5/100] Loss: 0.5601,lr 0.000100
Model Saving at epoch 5
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6755
Step 33800 Loss: 1.3179, lr 9.93844170297569e-05
Step 33900 Loss: 0.9618, lr 9.93844170297569e-05
Step 34000 Loss: 0.8724, lr 9.93844170297569e-05
Step 34100 Loss: 0.8131, lr 9.93844170297569e-05
Step 34200 Loss: 0.7743, lr 9.93844170297569e-05
Step 34300 Loss: 0.7464, lr 9.93844170297569e-05
Step 34400 Loss: 0.7221, lr 9.93844170297569e-05
Step 34500 Loss: 0.7034, lr 9.93844170297569e-05
Step 34600 Loss: 0.6879, lr 9.93844170297569e-05
Step 34700 Loss: 0.6740, lr 9.93844170297569e-05
Step 34800 Loss: 0.6620, lr 9.93844170297569e-05
Step 34900 Loss: 0.6531, lr 9.93844170297569e-05
Step 35000 Loss: 0.6429, lr 9.93844170297569e-05
Step 35100 Loss: 0.6346, lr 9.93844170297569e-05
Step 35200 Loss: 0.6275, lr 9.93844170297569e-05
Step 35300 Loss: 0.6207, lr 9.93844170297569e-05
Step 35400 Loss: 0.6136, lr 9.93844170297569e-05
Step 35500 Loss: 0.6078, lr 9.93844170297569e-05
Step 35600 Loss: 0.6017, lr 9.93844170297569e-05
Step 35700 Loss: 0.5965, lr 9.93844170297569e-05
Step 35800 Loss: 0.5913, lr 9.93844170297569e-05
Step 35900 Loss: 0.5869, lr 9.93844170297569e-05
Step 36000 Loss: 0.5823, lr 9.93844170297569e-05
Step 36100 Loss: 0.5782, lr 9.93844170297569e-05
Step 36200 Loss: 0.5750, lr 9.93844170297569e-05
Step 36300 Loss: 0.5707, lr 9.93844170297569e-05
Step 36400 Loss: 0.5668, lr 9.93844170297569e-05
Step 36500 Loss: 0.5642, lr 9.93844170297569e-05
Step 36600 Loss: 0.5615, lr 9.93844170297569e-05
Step 36700 Loss: 0.5593, lr 9.93844170297569e-05
Step 36800 Loss: 0.5573, lr 9.93844170297569e-05
Step 36900 Loss: 0.5545, lr 9.93844170297569e-05
Step 37000 Loss: 0.5522, lr 9.93844170297569e-05
Step 37100 Loss: 0.5494, lr 9.93844170297569e-05
Step 37200 Loss: 0.5470, lr 9.93844170297569e-05
Step 37300 Loss: 0.5442, lr 9.93844170297569e-05
Step 37400 Loss: 0.5419, lr 9.93844170297569e-05
Step 37500 Loss: 0.5394, lr 9.93844170297569e-05
Step 37600 Loss: 0.5374, lr 9.93844170297569e-05
Step 37700 Loss: 0.5352, lr 9.93844170297569e-05
Step 37800 Loss: 0.5337, lr 9.93844170297569e-05
Step 37900 Loss: 0.5319, lr 9.93844170297569e-05
Step 38000 Loss: 0.5308, lr 9.93844170297569e-05
Step 38100 Loss: 0.5294, lr 9.93844170297569e-05
Step 38200 Loss: 0.5281, lr 9.93844170297569e-05
Step 38300 Loss: 0.5267, lr 9.93844170297569e-05
Step 38400 Loss: 0.5254, lr 9.93844170297569e-05
Step 38500 Loss: 0.5241, lr 9.93844170297569e-05
Step 38600 Loss: 0.5238, lr 9.93844170297569e-05
Step 38700 Loss: 0.5232, lr 9.93844170297569e-05
Step 38800 Loss: 0.5227, lr 9.93844170297569e-05
Step 38900 Loss: 0.5224, lr 9.93844170297569e-05
Step 39000 Loss: 0.5219, lr 9.93844170297569e-05
Step 39100 Loss: 0.5214, lr 9.93844170297569e-05
Step 39200 Loss: 0.5210, lr 9.93844170297569e-05
Step 39300 Loss: 0.5206, lr 9.93844170297569e-05
Step 39400 Loss: 0.5207, lr 9.93844170297569e-05
Step 39500 Loss: 0.5212, lr 9.93844170297569e-05
Step 39600 Loss: 0.5216, lr 9.93844170297569e-05
Step 39700 Loss: 0.5230, lr 9.93844170297569e-05
Step 39800 Loss: 0.5236, lr 9.93844170297569e-05
Step 39900 Loss: 0.5257, lr 9.93844170297569e-05
Step 40000 Loss: 0.5277, lr 9.93844170297569e-05
Step 40100 Loss: 0.5285, lr 9.93844170297569e-05
Step 40200 Loss: 0.5291, lr 9.93844170297569e-05
Step 40300 Loss: 0.5298, lr 9.93844170297569e-05
Step 40400 Loss: 0.5295, lr 9.93844170297569e-05
Step 40500 Loss: 0.5292, lr 9.93844170297569e-05
Train Epoch: [6/100] Loss: 0.5296,lr 0.000099
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Step 40600 Loss: 1.0628, lr 9.911436253643445e-05
Step 40700 Loss: 0.8879, lr 9.911436253643445e-05
Step 40800 Loss: 0.8182, lr 9.911436253643445e-05
Step 40900 Loss: 0.7740, lr 9.911436253643445e-05
Step 41000 Loss: 0.7508, lr 9.911436253643445e-05
Step 41100 Loss: 0.7288, lr 9.911436253643445e-05
Step 41200 Loss: 0.7093, lr 9.911436253643445e-05
Step 41300 Loss: 0.6934, lr 9.911436253643445e-05
Step 41400 Loss: 0.6791, lr 9.911436253643445e-05
Step 41500 Loss: 0.6659, lr 9.911436253643445e-05
Step 41600 Loss: 0.6551, lr 9.911436253643445e-05
Step 41700 Loss: 0.6448, lr 9.911436253643445e-05
Step 41800 Loss: 0.6364, lr 9.911436253643445e-05
Step 41900 Loss: 0.6271, lr 9.911436253643445e-05
Step 42000 Loss: 0.6193, lr 9.911436253643445e-05
Step 42100 Loss: 0.6130, lr 9.911436253643445e-05
Step 42200 Loss: 0.6062, lr 9.911436253643445e-05
Step 42300 Loss: 0.6006, lr 9.911436253643445e-05
Step 42400 Loss: 0.5952, lr 9.911436253643445e-05
Step 42500 Loss: 0.5899, lr 9.911436253643445e-05
Step 42600 Loss: 0.5844, lr 9.911436253643445e-05
Step 42700 Loss: 0.5805, lr 9.911436253643445e-05
Step 42800 Loss: 0.5765, lr 9.911436253643445e-05
Step 42900 Loss: 0.5720, lr 9.911436253643445e-05
Step 43000 Loss: 0.5677, lr 9.911436253643445e-05
Step 43100 Loss: 0.5634, lr 9.911436253643445e-05
Step 43200 Loss: 0.5596, lr 9.911436253643445e-05
Step 43300 Loss: 0.5570, lr 9.911436253643445e-05
Step 43400 Loss: 0.5532, lr 9.911436253643445e-05
Step 43500 Loss: 0.5506, lr 9.911436253643445e-05
Step 43600 Loss: 0.5479, lr 9.911436253643445e-05
Step 43700 Loss: 0.5449, lr 9.911436253643445e-05
Step 43800 Loss: 0.5423, lr 9.911436253643445e-05
Step 43900 Loss: 0.5397, lr 9.911436253643445e-05
Step 44000 Loss: 0.5369, lr 9.911436253643445e-05
Step 44100 Loss: 0.5340, lr 9.911436253643445e-05
Step 44200 Loss: 0.5318, lr 9.911436253643445e-05
Step 44300 Loss: 0.5297, lr 9.911436253643445e-05
Step 44400 Loss: 0.5275, lr 9.911436253643445e-05
Step 44500 Loss: 0.5254, lr 9.911436253643445e-05
Step 44600 Loss: 0.5237, lr 9.911436253643445e-05
Step 44700 Loss: 0.5222, lr 9.911436253643445e-05
Step 44800 Loss: 0.5207, lr 9.911436253643445e-05
Step 44900 Loss: 0.5188, lr 9.911436253643445e-05
Step 45000 Loss: 0.5178, lr 9.911436253643445e-05
Step 45100 Loss: 0.5160, lr 9.911436253643445e-05
Step 45200 Loss: 0.5147, lr 9.911436253643445e-05
Step 45300 Loss: 0.5134, lr 9.911436253643445e-05
Step 45400 Loss: 0.5131, lr 9.911436253643445e-05
Step 45500 Loss: 0.5124, lr 9.911436253643445e-05
Step 45600 Loss: 0.5114, lr 9.911436253643445e-05
Step 45700 Loss: 0.5114, lr 9.911436253643445e-05
Step 45800 Loss: 0.5108, lr 9.911436253643445e-05
Step 45900 Loss: 0.5106, lr 9.911436253643445e-05
Step 46000 Loss: 0.5109, lr 9.911436253643445e-05
Step 46100 Loss: 0.5108, lr 9.911436253643445e-05
Step 46200 Loss: 0.5110, lr 9.911436253643445e-05
Step 46300 Loss: 0.5116, lr 9.911436253643445e-05
Step 46400 Loss: 0.5126, lr 9.911436253643445e-05
Step 46500 Loss: 0.5137, lr 9.911436253643445e-05
Step 46600 Loss: 0.5147, lr 9.911436253643445e-05
Step 46700 Loss: 0.5164, lr 9.911436253643445e-05
Step 46800 Loss: 0.5177, lr 9.911436253643445e-05
Step 46900 Loss: 0.5184, lr 9.911436253643445e-05
Step 47000 Loss: 0.5183, lr 9.911436253643445e-05
Step 47100 Loss: 0.5187, lr 9.911436253643445e-05
Step 47200 Loss: 0.5179, lr 9.911436253643445e-05
Train Epoch: [7/100] Loss: 0.5184,lr 0.000099
Calling G2SDataset.batch()
Done, time:  2.24 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6756
Step 47300 Loss: 1.6975, lr 9.879583809693738e-05
Step 47400 Loss: 0.9262, lr 9.879583809693738e-05
Step 47500 Loss: 0.8201, lr 9.879583809693738e-05
Step 47600 Loss: 0.7659, lr 9.879583809693738e-05
Step 47700 Loss: 0.7354, lr 9.879583809693738e-05
Step 47800 Loss: 0.7132, lr 9.879583809693738e-05
Step 47900 Loss: 0.6929, lr 9.879583809693738e-05
Step 48000 Loss: 0.6788, lr 9.879583809693738e-05
Step 48100 Loss: 0.6640, lr 9.879583809693738e-05
Step 48200 Loss: 0.6518, lr 9.879583809693738e-05
Step 48300 Loss: 0.6413, lr 9.879583809693738e-05
Step 48400 Loss: 0.6325, lr 9.879583809693738e-05
Step 48500 Loss: 0.6221, lr 9.879583809693738e-05
Step 48600 Loss: 0.6145, lr 9.879583809693738e-05
Step 48700 Loss: 0.6064, lr 9.879583809693738e-05
Step 48800 Loss: 0.5997, lr 9.879583809693738e-05
Step 48900 Loss: 0.5929, lr 9.879583809693738e-05
Step 49000 Loss: 0.5874, lr 9.879583809693738e-05
Step 49100 Loss: 0.5822, lr 9.879583809693738e-05
Step 49200 Loss: 0.5768, lr 9.879583809693738e-05
Step 49300 Loss: 0.5721, lr 9.879583809693738e-05
Step 49400 Loss: 0.5672, lr 9.879583809693738e-05
Step 49500 Loss: 0.5626, lr 9.879583809693738e-05
Step 49600 Loss: 0.5579, lr 9.879583809693738e-05
Step 49700 Loss: 0.5541, lr 9.879583809693738e-05
Step 49800 Loss: 0.5493, lr 9.879583809693738e-05
Step 49900 Loss: 0.5450, lr 9.879583809693738e-05
Step 50000 Loss: 0.5413, lr 9.879583809693738e-05
Step 50100 Loss: 0.5388, lr 9.879583809693738e-05
Step 50200 Loss: 0.5382, lr 9.879583809693738e-05
Step 50300 Loss: 0.5371, lr 9.879583809693738e-05
Step 50400 Loss: 0.5347, lr 9.879583809693738e-05
Step 50500 Loss: 0.5326, lr 9.879583809693738e-05
Step 50600 Loss: 0.5298, lr 9.879583809693738e-05
Step 50700 Loss: 0.5275, lr 9.879583809693738e-05
Step 50800 Loss: 0.5245, lr 9.879583809693738e-05
Step 50900 Loss: 0.5218, lr 9.879583809693738e-05
Step 51000 Loss: 0.5193, lr 9.879583809693738e-05
Step 51100 Loss: 0.5169, lr 9.879583809693738e-05
Step 51200 Loss: 0.5147, lr 9.879583809693738e-05
Step 51300 Loss: 0.5126, lr 9.879583809693738e-05
Step 51400 Loss: 0.5110, lr 9.879583809693738e-05
Step 51500 Loss: 0.5099, lr 9.879583809693738e-05
Step 51600 Loss: 0.5082, lr 9.879583809693738e-05
Step 51700 Loss: 0.5073, lr 9.879583809693738e-05
Step 51800 Loss: 0.5065, lr 9.879583809693738e-05
Step 51900 Loss: 0.5051, lr 9.879583809693738e-05
Step 52000 Loss: 0.5034, lr 9.879583809693738e-05
Step 52100 Loss: 0.5030, lr 9.879583809693738e-05
Step 52200 Loss: 0.5021, lr 9.879583809693738e-05
Step 52300 Loss: 0.5015, lr 9.879583809693738e-05
Step 52400 Loss: 0.5007, lr 9.879583809693738e-05
Step 52500 Loss: 0.5001, lr 9.879583809693738e-05
Step 52600 Loss: 0.4992, lr 9.879583809693738e-05
Step 52700 Loss: 0.4991, lr 9.879583809693738e-05
Step 52800 Loss: 0.4990, lr 9.879583809693738e-05
Step 52900 Loss: 0.4988, lr 9.879583809693738e-05
Step 53000 Loss: 0.4988, lr 9.879583809693738e-05
Step 53100 Loss: 0.4989, lr 9.879583809693738e-05
Step 53200 Loss: 0.4999, lr 9.879583809693738e-05
Step 53300 Loss: 0.5004, lr 9.879583809693738e-05
Step 53400 Loss: 0.5020, lr 9.879583809693738e-05
Step 53500 Loss: 0.5032, lr 9.879583809693738e-05
Step 53600 Loss: 0.5045, lr 9.879583809693738e-05
Step 53700 Loss: 0.5049, lr 9.879583809693738e-05
Step 53800 Loss: 0.5051, lr 9.879583809693738e-05
Step 53900 Loss: 0.5055, lr 9.879583809693738e-05
Step 54000 Loss: 0.5065, lr 9.879583809693738e-05
Train Epoch: [8/100] Loss: 0.5073,lr 0.000099
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Step 54100 Loss: 1.2021, lr 9.842915805643157e-05
Step 54200 Loss: 0.9250, lr 9.842915805643157e-05
Step 54300 Loss: 0.8446, lr 9.842915805643157e-05
Step 54400 Loss: 0.7924, lr 9.842915805643157e-05
Step 54500 Loss: 0.7652, lr 9.842915805643157e-05
Step 54600 Loss: 0.7403, lr 9.842915805643157e-05
Step 54700 Loss: 0.7169, lr 9.842915805643157e-05
Step 54800 Loss: 0.6986, lr 9.842915805643157e-05
Step 54900 Loss: 0.6835, lr 9.842915805643157e-05
Step 55000 Loss: 0.6696, lr 9.842915805643157e-05
Step 55100 Loss: 0.6583, lr 9.842915805643157e-05
Step 55200 Loss: 0.6468, lr 9.842915805643157e-05
Step 55300 Loss: 0.6363, lr 9.842915805643157e-05
Step 55400 Loss: 0.6269, lr 9.842915805643157e-05
Step 55500 Loss: 0.6187, lr 9.842915805643157e-05
Step 55600 Loss: 0.6117, lr 9.842915805643157e-05
Step 55700 Loss: 0.6041, lr 9.842915805643157e-05
Step 55800 Loss: 0.5984, lr 9.842915805643157e-05
Step 55900 Loss: 0.5925, lr 9.842915805643157e-05
Step 56000 Loss: 0.5868, lr 9.842915805643157e-05
Step 56100 Loss: 0.5805, lr 9.842915805643157e-05
Step 56200 Loss: 0.5759, lr 9.842915805643157e-05
Step 56300 Loss: 0.5715, lr 9.842915805643157e-05
Step 56400 Loss: 0.5669, lr 9.842915805643157e-05
Step 56500 Loss: 0.5626, lr 9.842915805643157e-05
Step 56600 Loss: 0.5584, lr 9.842915805643157e-05
Step 56700 Loss: 0.5544, lr 9.842915805643157e-05
Step 56800 Loss: 0.5508, lr 9.842915805643157e-05
Step 56900 Loss: 0.5472, lr 9.842915805643157e-05
Step 57000 Loss: 0.5443, lr 9.842915805643157e-05
Step 57100 Loss: 0.5414, lr 9.842915805643157e-05
Step 57200 Loss: 0.5379, lr 9.842915805643157e-05
Step 57300 Loss: 0.5351, lr 9.842915805643157e-05
Step 57400 Loss: 0.5315, lr 9.842915805643157e-05
Step 57500 Loss: 0.5288, lr 9.842915805643157e-05
Step 57600 Loss: 0.5255, lr 9.842915805643157e-05
Step 57700 Loss: 0.5232, lr 9.842915805643157e-05
Step 57800 Loss: 0.5202, lr 9.842915805643157e-05
Step 57900 Loss: 0.5175, lr 9.842915805643157e-05
Step 58000 Loss: 0.5151, lr 9.842915805643157e-05
Step 58100 Loss: 0.5134, lr 9.842915805643157e-05
Step 58200 Loss: 0.5115, lr 9.842915805643157e-05
Step 58300 Loss: 0.5098, lr 9.842915805643157e-05
Step 58400 Loss: 0.5076, lr 9.842915805643157e-05
Step 58500 Loss: 0.5063, lr 9.842915805643157e-05
Step 58600 Loss: 0.5044, lr 9.842915805643157e-05
Step 58700 Loss: 0.5025, lr 9.842915805643157e-05
Step 58800 Loss: 0.5008, lr 9.842915805643157e-05
Step 58900 Loss: 0.4997, lr 9.842915805643157e-05
Step 59000 Loss: 0.4989, lr 9.842915805643157e-05
Step 59100 Loss: 0.4979, lr 9.842915805643157e-05
Step 59200 Loss: 0.4975, lr 9.842915805643157e-05
Step 59300 Loss: 0.4966, lr 9.842915805643157e-05
Step 59400 Loss: 0.4964, lr 9.842915805643157e-05
Step 59500 Loss: 0.4961, lr 9.842915805643157e-05
Step 59600 Loss: 0.4957, lr 9.842915805643157e-05
Step 59700 Loss: 0.4955, lr 9.842915805643157e-05
Step 59800 Loss: 0.4956, lr 9.842915805643157e-05
Step 59900 Loss: 0.4958, lr 9.842915805643157e-05
Step 60000 Loss: 0.4966, lr 9.842915805643157e-05
Step 60100 Loss: 0.4975, lr 9.842915805643157e-05
Step 60200 Loss: 0.4987, lr 9.842915805643157e-05
Step 60300 Loss: 0.4997, lr 9.842915805643157e-05
Step 60400 Loss: 0.5009, lr 9.842915805643157e-05
Step 60500 Loss: 0.5010, lr 9.842915805643157e-05
Step 60600 Loss: 0.5015, lr 9.842915805643157e-05
Step 60700 Loss: 0.5009, lr 9.842915805643157e-05
Step 60800 Loss: 0.5009, lr 9.842915805643157e-05
Train Epoch: [9/100] Loss: 0.5010,lr 0.000098
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6756
Step 60900 Loss: 1.2338, lr 9.801468428384717e-05
Step 61000 Loss: 1.0602, lr 9.801468428384717e-05
Step 61100 Loss: 0.9491, lr 9.801468428384717e-05
Step 61200 Loss: 0.8750, lr 9.801468428384717e-05
Step 61300 Loss: 0.8259, lr 9.801468428384717e-05
Step 61400 Loss: 0.7849, lr 9.801468428384717e-05
Step 61500 Loss: 0.7549, lr 9.801468428384717e-05
Step 61600 Loss: 0.7290, lr 9.801468428384717e-05
Step 61700 Loss: 0.7071, lr 9.801468428384717e-05
Step 61800 Loss: 0.6878, lr 9.801468428384717e-05
Step 61900 Loss: 0.6719, lr 9.801468428384717e-05
Step 62000 Loss: 0.6570, lr 9.801468428384717e-05
Step 62100 Loss: 0.6457, lr 9.801468428384717e-05
Step 62200 Loss: 0.6340, lr 9.801468428384717e-05
Step 62300 Loss: 0.6244, lr 9.801468428384717e-05
Step 62400 Loss: 0.6148, lr 9.801468428384717e-05
Step 62500 Loss: 0.6063, lr 9.801468428384717e-05
Step 62600 Loss: 0.5995, lr 9.801468428384717e-05
Step 62700 Loss: 0.5924, lr 9.801468428384717e-05
Step 62800 Loss: 0.5852, lr 9.801468428384717e-05
Step 62900 Loss: 0.5783, lr 9.801468428384717e-05
Step 63000 Loss: 0.5728, lr 9.801468428384717e-05
Step 63100 Loss: 0.5670, lr 9.801468428384717e-05
Step 63200 Loss: 0.5620, lr 9.801468428384717e-05
Step 63300 Loss: 0.5563, lr 9.801468428384717e-05
Step 63400 Loss: 0.5508, lr 9.801468428384717e-05
Step 63500 Loss: 0.5463, lr 9.801468428384717e-05
Step 63600 Loss: 0.5425, lr 9.801468428384717e-05
Step 63700 Loss: 0.5385, lr 9.801468428384717e-05
Step 63800 Loss: 0.5349, lr 9.801468428384717e-05
Step 63900 Loss: 0.5311, lr 9.801468428384717e-05
Step 64000 Loss: 0.5276, lr 9.801468428384717e-05
Step 64100 Loss: 0.5243, lr 9.801468428384717e-05
Step 64200 Loss: 0.5209, lr 9.801468428384717e-05
Step 64300 Loss: 0.5176, lr 9.801468428384717e-05
Step 64400 Loss: 0.5140, lr 9.801468428384717e-05
Step 64500 Loss: 0.5112, lr 9.801468428384717e-05
Step 64600 Loss: 0.5082, lr 9.801468428384717e-05
Step 64700 Loss: 0.5054, lr 9.801468428384717e-05
Step 64800 Loss: 0.5027, lr 9.801468428384717e-05
Step 64900 Loss: 0.5003, lr 9.801468428384717e-05
Step 65000 Loss: 0.4984, lr 9.801468428384717e-05
Step 65100 Loss: 0.4964, lr 9.801468428384717e-05
Step 65200 Loss: 0.4947, lr 9.801468428384717e-05
Step 65300 Loss: 0.4932, lr 9.801468428384717e-05
Step 65400 Loss: 0.4912, lr 9.801468428384717e-05
Step 65500 Loss: 0.4899, lr 9.801468428384717e-05
Step 65600 Loss: 0.4889, lr 9.801468428384717e-05
Step 65700 Loss: 0.4879, lr 9.801468428384717e-05
Step 65800 Loss: 0.4872, lr 9.801468428384717e-05
Step 65900 Loss: 0.4860, lr 9.801468428384717e-05
Step 66000 Loss: 0.4853, lr 9.801468428384717e-05
Step 66100 Loss: 0.4845, lr 9.801468428384717e-05
Step 66200 Loss: 0.4840, lr 9.801468428384717e-05
Step 66300 Loss: 0.4837, lr 9.801468428384717e-05
Step 66400 Loss: 0.4836, lr 9.801468428384717e-05
Step 66500 Loss: 0.4833, lr 9.801468428384717e-05
Step 66600 Loss: 0.4838, lr 9.801468428384717e-05
Step 66700 Loss: 0.4849, lr 9.801468428384717e-05
Step 66800 Loss: 0.4853, lr 9.801468428384717e-05
Step 66900 Loss: 0.4865, lr 9.801468428384717e-05
Step 67000 Loss: 0.4870, lr 9.801468428384717e-05
Step 67100 Loss: 0.4874, lr 9.801468428384717e-05
Step 67200 Loss: 0.4879, lr 9.801468428384717e-05
Step 67300 Loss: 0.4874, lr 9.801468428384717e-05
Step 67400 Loss: 0.4870, lr 9.801468428384717e-05
Step 67500 Loss: 0.4861, lr 9.801468428384717e-05
Train Epoch: [10/100] Loss: 0.4862,lr 0.000098
Model Saving at epoch 10
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6755
Step 67600 Loss: 1.1764, lr 9.75528258147577e-05
Step 67700 Loss: 0.9016, lr 9.75528258147577e-05
Step 67800 Loss: 0.8144, lr 9.75528258147577e-05
Step 67900 Loss: 0.7643, lr 9.75528258147577e-05
Step 68000 Loss: 0.7345, lr 9.75528258147577e-05
Step 68100 Loss: 0.7104, lr 9.75528258147577e-05
Step 68200 Loss: 0.6853, lr 9.75528258147577e-05
Step 68300 Loss: 0.6642, lr 9.75528258147577e-05
Step 68400 Loss: 0.6481, lr 9.75528258147577e-05
Step 68500 Loss: 0.6332, lr 9.75528258147577e-05
Step 68600 Loss: 0.6200, lr 9.75528258147577e-05
Step 68700 Loss: 0.6092, lr 9.75528258147577e-05
Step 68800 Loss: 0.5986, lr 9.75528258147577e-05
Step 68900 Loss: 0.5906, lr 9.75528258147577e-05
Step 69000 Loss: 0.5826, lr 9.75528258147577e-05
Step 69100 Loss: 0.5751, lr 9.75528258147577e-05
Step 69200 Loss: 0.5673, lr 9.75528258147577e-05
Step 69300 Loss: 0.5619, lr 9.75528258147577e-05
Step 69400 Loss: 0.5563, lr 9.75528258147577e-05
Step 69500 Loss: 0.5515, lr 9.75528258147577e-05
Step 69600 Loss: 0.5459, lr 9.75528258147577e-05
Step 69700 Loss: 0.5413, lr 9.75528258147577e-05
Step 69800 Loss: 0.5361, lr 9.75528258147577e-05
Step 69900 Loss: 0.5316, lr 9.75528258147577e-05
Step 70000 Loss: 0.5276, lr 9.75528258147577e-05
Step 70100 Loss: 0.5228, lr 9.75528258147577e-05
Step 70200 Loss: 0.5187, lr 9.75528258147577e-05
Step 70300 Loss: 0.5154, lr 9.75528258147577e-05
Step 70400 Loss: 0.5118, lr 9.75528258147577e-05
Step 70500 Loss: 0.5090, lr 9.75528258147577e-05
Step 70600 Loss: 0.5062, lr 9.75528258147577e-05
Step 70700 Loss: 0.5034, lr 9.75528258147577e-05
Step 70800 Loss: 0.5004, lr 9.75528258147577e-05
Step 70900 Loss: 0.4972, lr 9.75528258147577e-05
Step 71000 Loss: 0.4945, lr 9.75528258147577e-05
Step 71100 Loss: 0.4918, lr 9.75528258147577e-05
Step 71200 Loss: 0.4892, lr 9.75528258147577e-05
Step 71300 Loss: 0.4862, lr 9.75528258147577e-05
Step 71400 Loss: 0.4838, lr 9.75528258147577e-05
Step 71500 Loss: 0.4812, lr 9.75528258147577e-05
Step 71600 Loss: 0.4795, lr 9.75528258147577e-05
Step 71700 Loss: 0.4777, lr 9.75528258147577e-05
Step 71800 Loss: 0.4763, lr 9.75528258147577e-05
Step 71900 Loss: 0.4741, lr 9.75528258147577e-05
Step 72000 Loss: 0.4727, lr 9.75528258147577e-05
Step 72100 Loss: 0.4712, lr 9.75528258147577e-05
Step 72200 Loss: 0.4696, lr 9.75528258147577e-05
Step 72300 Loss: 0.4680, lr 9.75528258147577e-05
Step 72400 Loss: 0.4670, lr 9.75528258147577e-05
Step 72500 Loss: 0.4663, lr 9.75528258147577e-05
Step 72600 Loss: 0.4655, lr 9.75528258147577e-05
Step 72700 Loss: 0.4649, lr 9.75528258147577e-05
Step 72800 Loss: 0.4647, lr 9.75528258147577e-05
Step 72900 Loss: 0.4647, lr 9.75528258147577e-05
Step 73000 Loss: 0.4645, lr 9.75528258147577e-05
Step 73100 Loss: 0.4648, lr 9.75528258147577e-05
Step 73200 Loss: 0.4650, lr 9.75528258147577e-05
Step 73300 Loss: 0.4652, lr 9.75528258147577e-05
Step 73400 Loss: 0.4657, lr 9.75528258147577e-05
Step 73500 Loss: 0.4665, lr 9.75528258147577e-05
Step 73600 Loss: 0.4671, lr 9.75528258147577e-05
Step 73700 Loss: 0.4684, lr 9.75528258147577e-05
Step 73800 Loss: 0.4690, lr 9.75528258147577e-05
Step 73900 Loss: 0.4692, lr 9.75528258147577e-05
Step 74000 Loss: 0.4689, lr 9.75528258147577e-05
Step 74100 Loss: 0.4686, lr 9.75528258147577e-05
Step 74200 Loss: 0.4678, lr 9.75528258147577e-05
Step 74300 Loss: 0.4671, lr 9.75528258147577e-05
Train Epoch: [11/100] Loss: 0.4672,lr 0.000098
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6756
Step 74400 Loss: 0.9691, lr 9.70440384477113e-05
Step 74500 Loss: 0.8264, lr 9.70440384477113e-05
Step 74600 Loss: 0.7581, lr 9.70440384477113e-05
Step 74700 Loss: 0.7188, lr 9.70440384477113e-05
Step 74800 Loss: 0.6938, lr 9.70440384477113e-05
Step 74900 Loss: 0.6708, lr 9.70440384477113e-05
Step 75000 Loss: 0.6522, lr 9.70440384477113e-05
Step 75100 Loss: 0.6349, lr 9.70440384477113e-05
Step 75200 Loss: 0.6210, lr 9.70440384477113e-05
Step 75300 Loss: 0.6081, lr 9.70440384477113e-05
Step 75400 Loss: 0.5981, lr 9.70440384477113e-05
Step 75500 Loss: 0.5878, lr 9.70440384477113e-05
Step 75600 Loss: 0.5800, lr 9.70440384477113e-05
Step 75700 Loss: 0.5719, lr 9.70440384477113e-05
Step 75800 Loss: 0.5652, lr 9.70440384477113e-05
Step 75900 Loss: 0.5588, lr 9.70440384477113e-05
Step 76000 Loss: 0.5519, lr 9.70440384477113e-05
Step 76100 Loss: 0.5479, lr 9.70440384477113e-05
Step 76200 Loss: 0.5433, lr 9.70440384477113e-05
Step 76300 Loss: 0.5386, lr 9.70440384477113e-05
Step 76400 Loss: 0.5338, lr 9.70440384477113e-05
Step 76500 Loss: 0.5297, lr 9.70440384477113e-05
Step 76600 Loss: 0.5256, lr 9.70440384477113e-05
Step 76700 Loss: 0.5215, lr 9.70440384477113e-05
Step 76800 Loss: 0.5167, lr 9.70440384477113e-05
Step 76900 Loss: 0.5127, lr 9.70440384477113e-05
Step 77000 Loss: 0.5090, lr 9.70440384477113e-05
Step 77100 Loss: 0.5065, lr 9.70440384477113e-05
Step 77200 Loss: 0.5036, lr 9.70440384477113e-05
Step 77300 Loss: 0.5013, lr 9.70440384477113e-05
Step 77400 Loss: 0.4983, lr 9.70440384477113e-05
Step 77500 Loss: 0.4951, lr 9.70440384477113e-05
Step 77600 Loss: 0.4922, lr 9.70440384477113e-05
Step 77700 Loss: 0.4891, lr 9.70440384477113e-05
Step 77800 Loss: 0.4860, lr 9.70440384477113e-05
Step 77900 Loss: 0.4828, lr 9.70440384477113e-05
Step 78000 Loss: 0.4801, lr 9.70440384477113e-05
Step 78100 Loss: 0.4771, lr 9.70440384477113e-05
Step 78200 Loss: 0.4748, lr 9.70440384477113e-05
Step 78300 Loss: 0.4728, lr 9.70440384477113e-05
Step 78400 Loss: 0.4710, lr 9.70440384477113e-05
Step 78500 Loss: 0.4694, lr 9.70440384477113e-05
Step 78600 Loss: 0.4674, lr 9.70440384477113e-05
Step 78700 Loss: 0.4660, lr 9.70440384477113e-05
Step 78800 Loss: 0.4645, lr 9.70440384477113e-05
Step 78900 Loss: 0.4624, lr 9.70440384477113e-05
Step 79000 Loss: 0.4608, lr 9.70440384477113e-05
Step 79100 Loss: 0.4596, lr 9.70440384477113e-05
Step 79200 Loss: 0.4588, lr 9.70440384477113e-05
Step 79300 Loss: 0.4580, lr 9.70440384477113e-05
Step 79400 Loss: 0.4571, lr 9.70440384477113e-05
Step 79500 Loss: 0.4569, lr 9.70440384477113e-05
Step 79600 Loss: 0.4563, lr 9.70440384477113e-05
Step 79700 Loss: 0.4562, lr 9.70440384477113e-05
Step 79800 Loss: 0.4560, lr 9.70440384477113e-05
Step 79900 Loss: 0.4557, lr 9.70440384477113e-05
Step 80000 Loss: 0.4557, lr 9.70440384477113e-05
Step 80100 Loss: 0.4559, lr 9.70440384477113e-05
Step 80200 Loss: 0.4568, lr 9.70440384477113e-05
Step 80300 Loss: 0.4572, lr 9.70440384477113e-05
Step 80400 Loss: 0.4580, lr 9.70440384477113e-05
Step 80500 Loss: 0.4591, lr 9.70440384477113e-05
Step 80600 Loss: 0.4592, lr 9.70440384477113e-05
Step 80700 Loss: 0.4596, lr 9.70440384477113e-05
Step 80800 Loss: 0.4593, lr 9.70440384477113e-05
Step 80900 Loss: 0.4590, lr 9.70440384477113e-05
Step 81000 Loss: 0.4579, lr 9.70440384477113e-05
Train Epoch: [12/100] Loss: 0.4581,lr 0.000097
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Step 81100 Loss: 1.2285, lr 9.64888242944126e-05
Step 81200 Loss: 0.8796, lr 9.64888242944126e-05
Step 81300 Loss: 0.7885, lr 9.64888242944126e-05
Step 81400 Loss: 0.7317, lr 9.64888242944126e-05
Step 81500 Loss: 0.6996, lr 9.64888242944126e-05
Step 81600 Loss: 0.6762, lr 9.64888242944126e-05
Step 81700 Loss: 0.6548, lr 9.64888242944126e-05
Step 81800 Loss: 0.6367, lr 9.64888242944126e-05
Step 81900 Loss: 0.6214, lr 9.64888242944126e-05
Step 82000 Loss: 0.6077, lr 9.64888242944126e-05
Step 82100 Loss: 0.5957, lr 9.64888242944126e-05
Step 82200 Loss: 0.5855, lr 9.64888242944126e-05
Step 82300 Loss: 0.5745, lr 9.64888242944126e-05
Step 82400 Loss: 0.5665, lr 9.64888242944126e-05
Step 82500 Loss: 0.5588, lr 9.64888242944126e-05
Step 82600 Loss: 0.5520, lr 9.64888242944126e-05
Step 82700 Loss: 0.5452, lr 9.64888242944126e-05
Step 82800 Loss: 0.5394, lr 9.64888242944126e-05
Step 82900 Loss: 0.5342, lr 9.64888242944126e-05
Step 83000 Loss: 0.5296, lr 9.64888242944126e-05
Step 83100 Loss: 0.5238, lr 9.64888242944126e-05
Step 83200 Loss: 0.5192, lr 9.64888242944126e-05
Step 83300 Loss: 0.5147, lr 9.64888242944126e-05
Step 83400 Loss: 0.5098, lr 9.64888242944126e-05
Step 83500 Loss: 0.5058, lr 9.64888242944126e-05
Step 83600 Loss: 0.5011, lr 9.64888242944126e-05
Step 83700 Loss: 0.4970, lr 9.64888242944126e-05
Step 83800 Loss: 0.4941, lr 9.64888242944126e-05
Step 83900 Loss: 0.4910, lr 9.64888242944126e-05
Step 84000 Loss: 0.4880, lr 9.64888242944126e-05
Step 84100 Loss: 0.4853, lr 9.64888242944126e-05
Step 84200 Loss: 0.4824, lr 9.64888242944126e-05
Step 84300 Loss: 0.4803, lr 9.64888242944126e-05
Step 84400 Loss: 0.4775, lr 9.64888242944126e-05
Step 84500 Loss: 0.4747, lr 9.64888242944126e-05
Step 84600 Loss: 0.4719, lr 9.64888242944126e-05
Step 84700 Loss: 0.4696, lr 9.64888242944126e-05
Step 84800 Loss: 0.4666, lr 9.64888242944126e-05
Step 84900 Loss: 0.4645, lr 9.64888242944126e-05
Step 85000 Loss: 0.4624, lr 9.64888242944126e-05
Step 85100 Loss: 0.4610, lr 9.64888242944126e-05
Step 85200 Loss: 0.4596, lr 9.64888242944126e-05
Step 85300 Loss: 0.4582, lr 9.64888242944126e-05
Step 85400 Loss: 0.4565, lr 9.64888242944126e-05
Step 85500 Loss: 0.4553, lr 9.64888242944126e-05
Step 85600 Loss: 0.4539, lr 9.64888242944126e-05
Step 85700 Loss: 0.4525, lr 9.64888242944126e-05
Step 85800 Loss: 0.4508, lr 9.64888242944126e-05
Step 85900 Loss: 0.4503, lr 9.64888242944126e-05
Step 86000 Loss: 0.4495, lr 9.64888242944126e-05
Step 86100 Loss: 0.4486, lr 9.64888242944126e-05
Step 86200 Loss: 0.4483, lr 9.64888242944126e-05
Step 86300 Loss: 0.4479, lr 9.64888242944126e-05
Step 86400 Loss: 0.4478, lr 9.64888242944126e-05
Step 86500 Loss: 0.4475, lr 9.64888242944126e-05
Step 86600 Loss: 0.4475, lr 9.64888242944126e-05
Step 86700 Loss: 0.4473, lr 9.64888242944126e-05
Step 86800 Loss: 0.4475, lr 9.64888242944126e-05
Step 86900 Loss: 0.4479, lr 9.64888242944126e-05
Step 87000 Loss: 0.4491, lr 9.64888242944126e-05
Step 87100 Loss: 0.4495, lr 9.64888242944126e-05
Step 87200 Loss: 0.4504, lr 9.64888242944126e-05
Step 87300 Loss: 0.4511, lr 9.64888242944126e-05
Step 87400 Loss: 0.4514, lr 9.64888242944126e-05
Step 87500 Loss: 0.4515, lr 9.64888242944126e-05
Step 87600 Loss: 0.4512, lr 9.64888242944126e-05
Step 87700 Loss: 0.4501, lr 9.64888242944126e-05
Step 87800 Loss: 0.4493, lr 9.64888242944126e-05
Train Epoch: [13/100] Loss: 0.4496,lr 0.000096
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Step 87900 Loss: 1.2331, lr 9.588773128419907e-05
Step 88000 Loss: 0.9531, lr 9.588773128419907e-05
Step 88100 Loss: 0.8357, lr 9.588773128419907e-05
Step 88200 Loss: 0.7702, lr 9.588773128419907e-05
Step 88300 Loss: 0.7336, lr 9.588773128419907e-05
Step 88400 Loss: 0.7031, lr 9.588773128419907e-05
Step 88500 Loss: 0.6767, lr 9.588773128419907e-05
Step 88600 Loss: 0.6561, lr 9.588773128419907e-05
Step 88700 Loss: 0.6383, lr 9.588773128419907e-05
Step 88800 Loss: 0.6215, lr 9.588773128419907e-05
Step 88900 Loss: 0.6097, lr 9.588773128419907e-05
Step 89000 Loss: 0.5967, lr 9.588773128419907e-05
Step 89100 Loss: 0.5869, lr 9.588773128419907e-05
Step 89200 Loss: 0.5767, lr 9.588773128419907e-05
Step 89300 Loss: 0.5682, lr 9.588773128419907e-05
Step 89400 Loss: 0.5603, lr 9.588773128419907e-05
Step 89500 Loss: 0.5528, lr 9.588773128419907e-05
Step 89600 Loss: 0.5469, lr 9.588773128419907e-05
Step 89700 Loss: 0.5412, lr 9.588773128419907e-05
Step 89800 Loss: 0.5349, lr 9.588773128419907e-05
Step 89900 Loss: 0.5284, lr 9.588773128419907e-05
Step 90000 Loss: 0.5239, lr 9.588773128419907e-05
Step 90100 Loss: 0.5189, lr 9.588773128419907e-05
Step 90200 Loss: 0.5143, lr 9.588773128419907e-05
Step 90300 Loss: 0.5093, lr 9.588773128419907e-05
Step 90400 Loss: 0.5047, lr 9.588773128419907e-05
Step 90500 Loss: 0.5004, lr 9.588773128419907e-05
Step 90600 Loss: 0.4972, lr 9.588773128419907e-05
Step 90700 Loss: 0.4938, lr 9.588773128419907e-05
Step 90800 Loss: 0.4912, lr 9.588773128419907e-05
Step 90900 Loss: 0.4888, lr 9.588773128419907e-05
Step 91000 Loss: 0.4856, lr 9.588773128419907e-05
Step 91100 Loss: 0.4828, lr 9.588773128419907e-05
Step 91200 Loss: 0.4796, lr 9.588773128419907e-05
Step 91300 Loss: 0.4767, lr 9.588773128419907e-05
Step 91400 Loss: 0.4736, lr 9.588773128419907e-05
Step 91500 Loss: 0.4711, lr 9.588773128419907e-05
Step 91600 Loss: 0.4681, lr 9.588773128419907e-05
Step 91700 Loss: 0.4656, lr 9.588773128419907e-05
Step 91800 Loss: 0.4637, lr 9.588773128419907e-05
Step 91900 Loss: 0.4620, lr 9.588773128419907e-05
Step 92000 Loss: 0.4604, lr 9.588773128419907e-05
Step 92100 Loss: 0.4586, lr 9.588773128419907e-05
Step 92200 Loss: 0.4571, lr 9.588773128419907e-05
Step 92300 Loss: 0.4561, lr 9.588773128419907e-05
Step 92400 Loss: 0.4546, lr 9.588773128419907e-05
Step 92500 Loss: 0.4530, lr 9.588773128419907e-05
Step 92600 Loss: 0.4516, lr 9.588773128419907e-05
Step 92700 Loss: 0.4510, lr 9.588773128419907e-05
Step 92800 Loss: 0.4505, lr 9.588773128419907e-05
Step 92900 Loss: 0.4496, lr 9.588773128419907e-05
Step 93000 Loss: 0.4495, lr 9.588773128419907e-05
Step 93100 Loss: 0.4487, lr 9.588773128419907e-05
Step 93200 Loss: 0.4484, lr 9.588773128419907e-05
Step 93300 Loss: 0.4484, lr 9.588773128419907e-05
Step 93400 Loss: 0.4480, lr 9.588773128419907e-05
Step 93500 Loss: 0.4479, lr 9.588773128419907e-05
Step 93600 Loss: 0.4478, lr 9.588773128419907e-05
Step 93700 Loss: 0.4487, lr 9.588773128419907e-05
Step 93800 Loss: 0.4491, lr 9.588773128419907e-05
Step 93900 Loss: 0.4497, lr 9.588773128419907e-05
Step 94000 Loss: 0.4511, lr 9.588773128419907e-05
Step 94100 Loss: 0.4511, lr 9.588773128419907e-05
Step 94200 Loss: 0.4510, lr 9.588773128419907e-05
Step 94300 Loss: 0.4505, lr 9.588773128419907e-05
Step 94400 Loss: 0.4501, lr 9.588773128419907e-05
Step 94500 Loss: 0.4489, lr 9.588773128419907e-05
Train Epoch: [14/100] Loss: 0.4488,lr 0.000096
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 94600 Loss: 1.3757, lr 9.5241352623301e-05
Step 94700 Loss: 0.8893, lr 9.5241352623301e-05
Step 94800 Loss: 0.7855, lr 9.5241352623301e-05
Step 94900 Loss: 0.7271, lr 9.5241352623301e-05
Step 95000 Loss: 0.6945, lr 9.5241352623301e-05
Step 95100 Loss: 0.6705, lr 9.5241352623301e-05
Step 95200 Loss: 0.6479, lr 9.5241352623301e-05
Step 95300 Loss: 0.6291, lr 9.5241352623301e-05
Step 95400 Loss: 0.6150, lr 9.5241352623301e-05
Step 95500 Loss: 0.5997, lr 9.5241352623301e-05
Step 95600 Loss: 0.5885, lr 9.5241352623301e-05
Step 95700 Loss: 0.5786, lr 9.5241352623301e-05
Step 95800 Loss: 0.5683, lr 9.5241352623301e-05
Step 95900 Loss: 0.5605, lr 9.5241352623301e-05
Step 96000 Loss: 0.5537, lr 9.5241352623301e-05
Step 96100 Loss: 0.5471, lr 9.5241352623301e-05
Step 96200 Loss: 0.5400, lr 9.5241352623301e-05
Step 96300 Loss: 0.5343, lr 9.5241352623301e-05
Step 96400 Loss: 0.5290, lr 9.5241352623301e-05
Step 96500 Loss: 0.5237, lr 9.5241352623301e-05
Step 96600 Loss: 0.5180, lr 9.5241352623301e-05
Step 96700 Loss: 0.5132, lr 9.5241352623301e-05
Step 96800 Loss: 0.5098, lr 9.5241352623301e-05
Step 96900 Loss: 0.5058, lr 9.5241352623301e-05
Step 97000 Loss: 0.5016, lr 9.5241352623301e-05
Step 97100 Loss: 0.4969, lr 9.5241352623301e-05
Step 97200 Loss: 0.4925, lr 9.5241352623301e-05
Step 97300 Loss: 0.4890, lr 9.5241352623301e-05
Step 97400 Loss: 0.4860, lr 9.5241352623301e-05
Step 97500 Loss: 0.4827, lr 9.5241352623301e-05
Step 97600 Loss: 0.4800, lr 9.5241352623301e-05
Step 97700 Loss: 0.4769, lr 9.5241352623301e-05
Step 97800 Loss: 0.4742, lr 9.5241352623301e-05
Step 97900 Loss: 0.4713, lr 9.5241352623301e-05
Step 98000 Loss: 0.4688, lr 9.5241352623301e-05
Step 98100 Loss: 0.4658, lr 9.5241352623301e-05
Step 98200 Loss: 0.4633, lr 9.5241352623301e-05
Step 98300 Loss: 0.4607, lr 9.5241352623301e-05
Step 98400 Loss: 0.4587, lr 9.5241352623301e-05
Step 98500 Loss: 0.4564, lr 9.5241352623301e-05
Step 98600 Loss: 0.4550, lr 9.5241352623301e-05
Step 98700 Loss: 0.4552, lr 9.5241352623301e-05
Step 98800 Loss: 0.4577, lr 9.5241352623301e-05
Step 98900 Loss: 0.4611, lr 9.5241352623301e-05
Step 99000 Loss: 0.4609, lr 9.5241352623301e-05
Step 99100 Loss: 0.4598, lr 9.5241352623301e-05
Step 99200 Loss: 0.4588, lr 9.5241352623301e-05
Step 99300 Loss: 0.4572, lr 9.5241352623301e-05
Step 99400 Loss: 0.4565, lr 9.5241352623301e-05
Step 99500 Loss: 0.4557, lr 9.5241352623301e-05
Step 99600 Loss: 0.4548, lr 9.5241352623301e-05
Step 99700 Loss: 0.4541, lr 9.5241352623301e-05
Step 99800 Loss: 0.4535, lr 9.5241352623301e-05
Step 99900 Loss: 0.4529, lr 9.5241352623301e-05
Step 100000 Loss: 0.4526, lr 9.5241352623301e-05
Step 100100 Loss: 0.4523, lr 9.5241352623301e-05
Step 100200 Loss: 0.4524, lr 9.5241352623301e-05
Step 100300 Loss: 0.4524, lr 9.5241352623301e-05
Step 100400 Loss: 0.4525, lr 9.5241352623301e-05
Step 100500 Loss: 0.4532, lr 9.5241352623301e-05
Step 100600 Loss: 0.4532, lr 9.5241352623301e-05
Step 100700 Loss: 0.4539, lr 9.5241352623301e-05
Step 100800 Loss: 0.4542, lr 9.5241352623301e-05
Step 100900 Loss: 0.4541, lr 9.5241352623301e-05
Step 101000 Loss: 0.4541, lr 9.5241352623301e-05
Step 101100 Loss: 0.4537, lr 9.5241352623301e-05
Step 101200 Loss: 0.4529, lr 9.5241352623301e-05
Step 101300 Loss: 0.4524, lr 9.5241352623301e-05
Train Epoch: [15/100] Loss: 0.4525,lr 0.000095
Model Saving at epoch 15
Calling G2SDataset.batch()
Done, time:  2.13 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6755
Step 101400 Loss: 1.0219, lr 9.455032620941843e-05
Step 101500 Loss: 0.8368, lr 9.455032620941843e-05
Step 101600 Loss: 0.7608, lr 9.455032620941843e-05
Step 101700 Loss: 0.7129, lr 9.455032620941843e-05
Step 101800 Loss: 0.6859, lr 9.455032620941843e-05
Step 101900 Loss: 0.6639, lr 9.455032620941843e-05
Step 102000 Loss: 0.6420, lr 9.455032620941843e-05
Step 102100 Loss: 0.6255, lr 9.455032620941843e-05
Step 102200 Loss: 0.6114, lr 9.455032620941843e-05
Step 102300 Loss: 0.5971, lr 9.455032620941843e-05
Step 102400 Loss: 0.5868, lr 9.455032620941843e-05
Step 102500 Loss: 0.5764, lr 9.455032620941843e-05
Step 102600 Loss: 0.5674, lr 9.455032620941843e-05
Step 102700 Loss: 0.5587, lr 9.455032620941843e-05
Step 102800 Loss: 0.5513, lr 9.455032620941843e-05
Step 102900 Loss: 0.5442, lr 9.455032620941843e-05
Step 103000 Loss: 0.5373, lr 9.455032620941843e-05
Step 103100 Loss: 0.5318, lr 9.455032620941843e-05
Step 103200 Loss: 0.5278, lr 9.455032620941843e-05
Step 103300 Loss: 0.5233, lr 9.455032620941843e-05
Step 103400 Loss: 0.5177, lr 9.455032620941843e-05
Step 103500 Loss: 0.5136, lr 9.455032620941843e-05
Step 103600 Loss: 0.5090, lr 9.455032620941843e-05
Step 103700 Loss: 0.5042, lr 9.455032620941843e-05
Step 103800 Loss: 0.4994, lr 9.455032620941843e-05
Step 103900 Loss: 0.4950, lr 9.455032620941843e-05
Step 104000 Loss: 0.4907, lr 9.455032620941843e-05
Step 104100 Loss: 0.4878, lr 9.455032620941843e-05
Step 104200 Loss: 0.4844, lr 9.455032620941843e-05
Step 104300 Loss: 0.4814, lr 9.455032620941843e-05
Step 104400 Loss: 0.4788, lr 9.455032620941843e-05
Step 104500 Loss: 0.4760, lr 9.455032620941843e-05
Step 104600 Loss: 0.4731, lr 9.455032620941843e-05
Step 104700 Loss: 0.4701, lr 9.455032620941843e-05
Step 104800 Loss: 0.4673, lr 9.455032620941843e-05
Step 104900 Loss: 0.4641, lr 9.455032620941843e-05
Step 105000 Loss: 0.4617, lr 9.455032620941843e-05
Step 105100 Loss: 0.4586, lr 9.455032620941843e-05
Step 105200 Loss: 0.4562, lr 9.455032620941843e-05
Step 105300 Loss: 0.4539, lr 9.455032620941843e-05
Step 105400 Loss: 0.4520, lr 9.455032620941843e-05
Step 105500 Loss: 0.4505, lr 9.455032620941843e-05
Step 105600 Loss: 0.4489, lr 9.455032620941843e-05
Step 105700 Loss: 0.4470, lr 9.455032620941843e-05
Step 105800 Loss: 0.4466, lr 9.455032620941843e-05
Step 105900 Loss: 0.4451, lr 9.455032620941843e-05
Step 106000 Loss: 0.4435, lr 9.455032620941843e-05
Step 106100 Loss: 0.4421, lr 9.455032620941843e-05
Step 106200 Loss: 0.4415, lr 9.455032620941843e-05
Step 106300 Loss: 0.4406, lr 9.455032620941843e-05
Step 106400 Loss: 0.4395, lr 9.455032620941843e-05
Step 106500 Loss: 0.4393, lr 9.455032620941843e-05
Step 106600 Loss: 0.4390, lr 9.455032620941843e-05
Step 106700 Loss: 0.4384, lr 9.455032620941843e-05
Step 106800 Loss: 0.4383, lr 9.455032620941843e-05
Step 106900 Loss: 0.4382, lr 9.455032620941843e-05
Step 107000 Loss: 0.4377, lr 9.455032620941843e-05
Step 107100 Loss: 0.4378, lr 9.455032620941843e-05
Step 107200 Loss: 0.4383, lr 9.455032620941843e-05
Step 107300 Loss: 0.4387, lr 9.455032620941843e-05
Step 107400 Loss: 0.4389, lr 9.455032620941843e-05
Step 107500 Loss: 0.4398, lr 9.455032620941843e-05
Step 107600 Loss: 0.4399, lr 9.455032620941843e-05
Step 107700 Loss: 0.4403, lr 9.455032620941843e-05
Step 107800 Loss: 0.4403, lr 9.455032620941843e-05
Step 107900 Loss: 0.4409, lr 9.455032620941843e-05
Step 108000 Loss: 0.4407, lr 9.455032620941843e-05
Train Epoch: [16/100] Loss: 0.4410,lr 0.000095
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6756
Step 108100 Loss: 2.0960, lr 9.381533400219321e-05
Step 108200 Loss: 1.1931, lr 9.381533400219321e-05
Step 108300 Loss: 0.9583, lr 9.381533400219321e-05
Step 108400 Loss: 0.8369, lr 9.381533400219321e-05
Step 108500 Loss: 0.7709, lr 9.381533400219321e-05
Step 108600 Loss: 0.7270, lr 9.381533400219321e-05
Step 108700 Loss: 0.6911, lr 9.381533400219321e-05
Step 108800 Loss: 0.6666, lr 9.381533400219321e-05
Step 108900 Loss: 0.6447, lr 9.381533400219321e-05
Step 109000 Loss: 0.6265, lr 9.381533400219321e-05
Step 109100 Loss: 0.6121, lr 9.381533400219321e-05
Step 109200 Loss: 0.6005, lr 9.381533400219321e-05
Step 109300 Loss: 0.5885, lr 9.381533400219321e-05
Step 109400 Loss: 0.5802, lr 9.381533400219321e-05
Step 109500 Loss: 0.5713, lr 9.381533400219321e-05
Step 109600 Loss: 0.5638, lr 9.381533400219321e-05
Step 109700 Loss: 0.5567, lr 9.381533400219321e-05
Step 109800 Loss: 0.5510, lr 9.381533400219321e-05
Step 109900 Loss: 0.5459, lr 9.381533400219321e-05
Step 110000 Loss: 0.5413, lr 9.381533400219321e-05
Step 110100 Loss: 0.5359, lr 9.381533400219321e-05
Step 110200 Loss: 0.5298, lr 9.381533400219321e-05
Step 110300 Loss: 0.5250, lr 9.381533400219321e-05
Step 110400 Loss: 0.5197, lr 9.381533400219321e-05
Step 110500 Loss: 0.5152, lr 9.381533400219321e-05
Step 110600 Loss: 0.5099, lr 9.381533400219321e-05
Step 110700 Loss: 0.5047, lr 9.381533400219321e-05
Step 110800 Loss: 0.5005, lr 9.381533400219321e-05
Step 110900 Loss: 0.4972, lr 9.381533400219321e-05
Step 111000 Loss: 0.4931, lr 9.381533400219321e-05
Step 111100 Loss: 0.4902, lr 9.381533400219321e-05
Step 111200 Loss: 0.4865, lr 9.381533400219321e-05
Step 111300 Loss: 0.4835, lr 9.381533400219321e-05
Step 111400 Loss: 0.4802, lr 9.381533400219321e-05
Step 111500 Loss: 0.4769, lr 9.381533400219321e-05
Step 111600 Loss: 0.4736, lr 9.381533400219321e-05
Step 111700 Loss: 0.4704, lr 9.381533400219321e-05
Step 111800 Loss: 0.4676, lr 9.381533400219321e-05
Step 111900 Loss: 0.4649, lr 9.381533400219321e-05
Step 112000 Loss: 0.4623, lr 9.381533400219321e-05
Step 112100 Loss: 0.4601, lr 9.381533400219321e-05
Step 112200 Loss: 0.4581, lr 9.381533400219321e-05
Step 112300 Loss: 0.4564, lr 9.381533400219321e-05
Step 112400 Loss: 0.4544, lr 9.381533400219321e-05
Step 112500 Loss: 0.4525, lr 9.381533400219321e-05
Step 112600 Loss: 0.4509, lr 9.381533400219321e-05
Step 112700 Loss: 0.4490, lr 9.381533400219321e-05
Step 112800 Loss: 0.4473, lr 9.381533400219321e-05
Step 112900 Loss: 0.4465, lr 9.381533400219321e-05
Step 113000 Loss: 0.4453, lr 9.381533400219321e-05
Step 113100 Loss: 0.4443, lr 9.381533400219321e-05
Step 113200 Loss: 0.4435, lr 9.381533400219321e-05
Step 113300 Loss: 0.4428, lr 9.381533400219321e-05
Step 113400 Loss: 0.4422, lr 9.381533400219321e-05
Step 113500 Loss: 0.4416, lr 9.381533400219321e-05
Step 113600 Loss: 0.4413, lr 9.381533400219321e-05
Step 113700 Loss: 0.4410, lr 9.381533400219321e-05
Step 113800 Loss: 0.4406, lr 9.381533400219321e-05
Step 113900 Loss: 0.4407, lr 9.381533400219321e-05
Step 114000 Loss: 0.4412, lr 9.381533400219321e-05
Step 114100 Loss: 0.4413, lr 9.381533400219321e-05
Step 114200 Loss: 0.4420, lr 9.381533400219321e-05
Step 114300 Loss: 0.4423, lr 9.381533400219321e-05
Step 114400 Loss: 0.4425, lr 9.381533400219321e-05
Step 114500 Loss: 0.4424, lr 9.381533400219321e-05
Step 114600 Loss: 0.4421, lr 9.381533400219321e-05
Step 114700 Loss: 0.4407, lr 9.381533400219321e-05
Step 114800 Loss: 0.4394, lr 9.381533400219321e-05
Train Epoch: [17/100] Loss: 0.4398,lr 0.000094
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6755
Step 114900 Loss: 1.1920, lr 9.303710135019722e-05
Step 115000 Loss: 0.9276, lr 9.303710135019722e-05
Step 115100 Loss: 0.8310, lr 9.303710135019722e-05
Step 115200 Loss: 0.7744, lr 9.303710135019722e-05
Step 115300 Loss: 0.7421, lr 9.303710135019722e-05
Step 115400 Loss: 0.7149, lr 9.303710135019722e-05
Step 115500 Loss: 0.6912, lr 9.303710135019722e-05
Step 115600 Loss: 0.6695, lr 9.303710135019722e-05
Step 115700 Loss: 0.6522, lr 9.303710135019722e-05
Step 115800 Loss: 0.6357, lr 9.303710135019722e-05
Step 115900 Loss: 0.6233, lr 9.303710135019722e-05
Step 116000 Loss: 0.6107, lr 9.303710135019722e-05
Step 116100 Loss: 0.5992, lr 9.303710135019722e-05
Step 116200 Loss: 0.5885, lr 9.303710135019722e-05
Step 116300 Loss: 0.5791, lr 9.303710135019722e-05
Step 116400 Loss: 0.5705, lr 9.303710135019722e-05
Step 116500 Loss: 0.5622, lr 9.303710135019722e-05
Step 116600 Loss: 0.5557, lr 9.303710135019722e-05
Step 116700 Loss: 0.5491, lr 9.303710135019722e-05
Step 116800 Loss: 0.5420, lr 9.303710135019722e-05
Step 116900 Loss: 0.5358, lr 9.303710135019722e-05
Step 117000 Loss: 0.5306, lr 9.303710135019722e-05
Step 117100 Loss: 0.5251, lr 9.303710135019722e-05
Step 117200 Loss: 0.5194, lr 9.303710135019722e-05
Step 117300 Loss: 0.5140, lr 9.303710135019722e-05
Step 117400 Loss: 0.5089, lr 9.303710135019722e-05
Step 117500 Loss: 0.5040, lr 9.303710135019722e-05
Step 117600 Loss: 0.5002, lr 9.303710135019722e-05
Step 117700 Loss: 0.4964, lr 9.303710135019722e-05
Step 117800 Loss: 0.4926, lr 9.303710135019722e-05
Step 117900 Loss: 0.4894, lr 9.303710135019722e-05
Step 118000 Loss: 0.4880, lr 9.303710135019722e-05
Step 118100 Loss: 0.4871, lr 9.303710135019722e-05
Step 118200 Loss: 0.4849, lr 9.303710135019722e-05
Step 118300 Loss: 0.4829, lr 9.303710135019722e-05
Step 118400 Loss: 0.4807, lr 9.303710135019722e-05
Step 118500 Loss: 0.4789, lr 9.303710135019722e-05
Step 118600 Loss: 0.4763, lr 9.303710135019722e-05
Step 118700 Loss: 0.4747, lr 9.303710135019722e-05
Step 118800 Loss: 0.4729, lr 9.303710135019722e-05
Step 118900 Loss: 0.4713, lr 9.303710135019722e-05
Step 119000 Loss: 0.4699, lr 9.303710135019722e-05
Step 119100 Loss: 0.4684, lr 9.303710135019722e-05
Step 119200 Loss: 0.4665, lr 9.303710135019722e-05
Step 119300 Loss: 0.4651, lr 9.303710135019722e-05
Step 119400 Loss: 0.4637, lr 9.303710135019722e-05
Step 119500 Loss: 0.4622, lr 9.303710135019722e-05
Step 119600 Loss: 0.4608, lr 9.303710135019722e-05
Step 119700 Loss: 0.4601, lr 9.303710135019722e-05
Step 119800 Loss: 0.4595, lr 9.303710135019722e-05
Step 119900 Loss: 0.4586, lr 9.303710135019722e-05
Step 120000 Loss: 0.4581, lr 9.303710135019722e-05
Step 120100 Loss: 0.4575, lr 9.303710135019722e-05
Step 120200 Loss: 0.4566, lr 9.303710135019722e-05
Step 120300 Loss: 0.4560, lr 9.303710135019722e-05
Step 120400 Loss: 0.4557, lr 9.303710135019722e-05
Step 120500 Loss: 0.4553, lr 9.303710135019722e-05
Step 120600 Loss: 0.4549, lr 9.303710135019722e-05
Step 120700 Loss: 0.4552, lr 9.303710135019722e-05
Step 120800 Loss: 0.4551, lr 9.303710135019722e-05
Step 120900 Loss: 0.4554, lr 9.303710135019722e-05
Step 121000 Loss: 0.4557, lr 9.303710135019722e-05
Step 121100 Loss: 0.4555, lr 9.303710135019722e-05
Step 121200 Loss: 0.4549, lr 9.303710135019722e-05
Step 121300 Loss: 0.4545, lr 9.303710135019722e-05
Step 121400 Loss: 0.4535, lr 9.303710135019722e-05
Step 121500 Loss: 0.4522, lr 9.303710135019722e-05
Step 121600 Loss: 0.4516, lr 9.303710135019722e-05
Train Epoch: [18/100] Loss: 0.4522,lr 0.000093
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 121700 Loss: 0.9303, lr 9.22163962751008e-05
Step 121800 Loss: 0.8009, lr 9.22163962751008e-05
Step 121900 Loss: 0.7356, lr 9.22163962751008e-05
Step 122000 Loss: 0.6974, lr 9.22163962751008e-05
Step 122100 Loss: 0.6710, lr 9.22163962751008e-05
Step 122200 Loss: 0.6468, lr 9.22163962751008e-05
Step 122300 Loss: 0.6292, lr 9.22163962751008e-05
Step 122400 Loss: 0.6129, lr 9.22163962751008e-05
Step 122500 Loss: 0.5992, lr 9.22163962751008e-05
Step 122600 Loss: 0.5857, lr 9.22163962751008e-05
Step 122700 Loss: 0.5754, lr 9.22163962751008e-05
Step 122800 Loss: 0.5646, lr 9.22163962751008e-05
Step 122900 Loss: 0.5564, lr 9.22163962751008e-05
Step 123000 Loss: 0.5475, lr 9.22163962751008e-05
Step 123100 Loss: 0.5404, lr 9.22163962751008e-05
Step 123200 Loss: 0.5334, lr 9.22163962751008e-05
Step 123300 Loss: 0.5270, lr 9.22163962751008e-05
Step 123400 Loss: 0.5221, lr 9.22163962751008e-05
Step 123500 Loss: 0.5167, lr 9.22163962751008e-05
Step 123600 Loss: 0.5114, lr 9.22163962751008e-05
Step 123700 Loss: 0.5060, lr 9.22163962751008e-05
Step 123800 Loss: 0.5019, lr 9.22163962751008e-05
Step 123900 Loss: 0.4974, lr 9.22163962751008e-05
Step 124000 Loss: 0.4947, lr 9.22163962751008e-05
Step 124100 Loss: 0.4957, lr 9.22163962751008e-05
Step 124200 Loss: 0.4951, lr 9.22163962751008e-05
Step 124300 Loss: 0.4951, lr 9.22163962751008e-05
Step 124400 Loss: 0.4956, lr 9.22163962751008e-05
Step 124500 Loss: 0.4950, lr 9.22163962751008e-05
Step 124600 Loss: 0.4947, lr 9.22163962751008e-05
Step 124700 Loss: 0.4936, lr 9.22163962751008e-05
Step 124800 Loss: 0.4924, lr 9.22163962751008e-05
Step 124900 Loss: 0.4912, lr 9.22163962751008e-05
Step 125000 Loss: 0.4896, lr 9.22163962751008e-05
Step 125100 Loss: 0.4878, lr 9.22163962751008e-05
Step 125200 Loss: 0.4861, lr 9.22163962751008e-05
Step 125300 Loss: 0.4843, lr 9.22163962751008e-05
Step 125400 Loss: 0.4828, lr 9.22163962751008e-05
Step 125500 Loss: 0.4811, lr 9.22163962751008e-05
Step 125600 Loss: 0.4800, lr 9.22163962751008e-05
Step 125700 Loss: 0.4786, lr 9.22163962751008e-05
Step 125800 Loss: 0.4778, lr 9.22163962751008e-05
Step 125900 Loss: 0.4770, lr 9.22163962751008e-05
Step 126000 Loss: 0.4762, lr 9.22163962751008e-05
Step 126100 Loss: 0.4756, lr 9.22163962751008e-05
Step 126200 Loss: 0.4745, lr 9.22163962751008e-05
Step 126300 Loss: 0.4734, lr 9.22163962751008e-05
Step 126400 Loss: 0.4721, lr 9.22163962751008e-05
Step 126500 Loss: 0.4711, lr 9.22163962751008e-05
Step 126600 Loss: 0.4702, lr 9.22163962751008e-05
Step 126700 Loss: 0.4693, lr 9.22163962751008e-05
Step 126800 Loss: 0.4684, lr 9.22163962751008e-05
Step 126900 Loss: 0.4672, lr 9.22163962751008e-05
Step 127000 Loss: 0.4667, lr 9.22163962751008e-05
Step 127100 Loss: 0.4662, lr 9.22163962751008e-05
Step 127200 Loss: 0.4661, lr 9.22163962751008e-05
Step 127300 Loss: 0.4655, lr 9.22163962751008e-05
Step 127400 Loss: 0.4653, lr 9.22163962751008e-05
Step 127500 Loss: 0.4658, lr 9.22163962751008e-05
Step 127600 Loss: 0.4656, lr 9.22163962751008e-05
Step 127700 Loss: 0.4660, lr 9.22163962751008e-05
Step 127800 Loss: 0.4662, lr 9.22163962751008e-05
Step 127900 Loss: 0.4659, lr 9.22163962751008e-05
Step 128000 Loss: 0.4660, lr 9.22163962751008e-05
Step 128100 Loss: 0.4649, lr 9.22163962751008e-05
Step 128200 Loss: 0.4639, lr 9.22163962751008e-05
Step 128300 Loss: 0.4629, lr 9.22163962751008e-05
Train Epoch: [19/100] Loss: 0.4628,lr 0.000092
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Step 128400 Loss: 1.4002, lr 9.135402871372814e-05
Step 128500 Loss: 1.0349, lr 9.135402871372814e-05
Step 128600 Loss: 0.9119, lr 9.135402871372814e-05
Step 128700 Loss: 0.8480, lr 9.135402871372814e-05
Step 128800 Loss: 0.8073, lr 9.135402871372814e-05
Step 128900 Loss: 0.7784, lr 9.135402871372814e-05
Step 129000 Loss: 0.7506, lr 9.135402871372814e-05
Step 129100 Loss: 0.7266, lr 9.135402871372814e-05
Step 129200 Loss: 0.7093, lr 9.135402871372814e-05
Step 129300 Loss: 0.6923, lr 9.135402871372814e-05
Step 129400 Loss: 0.6768, lr 9.135402871372814e-05
Step 129500 Loss: 0.6652, lr 9.135402871372814e-05
Step 129600 Loss: 0.6534, lr 9.135402871372814e-05
Step 129700 Loss: 0.6437, lr 9.135402871372814e-05
Step 129800 Loss: 0.6344, lr 9.135402871372814e-05
Step 129900 Loss: 0.6249, lr 9.135402871372814e-05
Step 130000 Loss: 0.6160, lr 9.135402871372814e-05
Step 130100 Loss: 0.6087, lr 9.135402871372814e-05
Step 130200 Loss: 0.6020, lr 9.135402871372814e-05
Step 130300 Loss: 0.5957, lr 9.135402871372814e-05
Step 130400 Loss: 0.5887, lr 9.135402871372814e-05
Step 130500 Loss: 0.5830, lr 9.135402871372814e-05
Step 130600 Loss: 0.5767, lr 9.135402871372814e-05
Step 130700 Loss: 0.5712, lr 9.135402871372814e-05
Step 130800 Loss: 0.5657, lr 9.135402871372814e-05
Step 130900 Loss: 0.5604, lr 9.135402871372814e-05
Step 131000 Loss: 0.5551, lr 9.135402871372814e-05
Step 131100 Loss: 0.5510, lr 9.135402871372814e-05
Step 131200 Loss: 0.5472, lr 9.135402871372814e-05
Step 131300 Loss: 0.5429, lr 9.135402871372814e-05
Step 131400 Loss: 0.5388, lr 9.135402871372814e-05
Step 131500 Loss: 0.5347, lr 9.135402871372814e-05
Step 131600 Loss: 0.5305, lr 9.135402871372814e-05
Step 131700 Loss: 0.5260, lr 9.135402871372814e-05
Step 131800 Loss: 0.5218, lr 9.135402871372814e-05
Step 131900 Loss: 0.5176, lr 9.135402871372814e-05
Step 132000 Loss: 0.5140, lr 9.135402871372814e-05
Step 132100 Loss: 0.5097, lr 9.135402871372814e-05
Step 132200 Loss: 0.5060, lr 9.135402871372814e-05
Step 132300 Loss: 0.5025, lr 9.135402871372814e-05
Step 132400 Loss: 0.4998, lr 9.135402871372814e-05
Step 132500 Loss: 0.4973, lr 9.135402871372814e-05
Step 132600 Loss: 0.4946, lr 9.135402871372814e-05
Step 132700 Loss: 0.4916, lr 9.135402871372814e-05
Step 132800 Loss: 0.4894, lr 9.135402871372814e-05
Step 132900 Loss: 0.4868, lr 9.135402871372814e-05
Step 133000 Loss: 0.4845, lr 9.135402871372814e-05
Step 133100 Loss: 0.4820, lr 9.135402871372814e-05
Step 133200 Loss: 0.4803, lr 9.135402871372814e-05
Step 133300 Loss: 0.4796, lr 9.135402871372814e-05
Step 133400 Loss: 0.4792, lr 9.135402871372814e-05
Step 133500 Loss: 0.4784, lr 9.135402871372814e-05
Step 133600 Loss: 0.4777, lr 9.135402871372814e-05
Step 133700 Loss: 0.4771, lr 9.135402871372814e-05
Step 133800 Loss: 0.4766, lr 9.135402871372814e-05
Step 133900 Loss: 0.4762, lr 9.135402871372814e-05
Step 134000 Loss: 0.4758, lr 9.135402871372814e-05
Step 134100 Loss: 0.4758, lr 9.135402871372814e-05
Step 134200 Loss: 0.4758, lr 9.135402871372814e-05
Step 134300 Loss: 0.4764, lr 9.135402871372814e-05
Step 134400 Loss: 0.4776, lr 9.135402871372814e-05
Step 134500 Loss: 0.4792, lr 9.135402871372814e-05
Step 134600 Loss: 0.4814, lr 9.135402871372814e-05
Step 134700 Loss: 0.4824, lr 9.135402871372814e-05
Step 134800 Loss: 0.4835, lr 9.135402871372814e-05
Step 134900 Loss: 0.4845, lr 9.135402871372814e-05
Step 135000 Loss: 0.4845, lr 9.135402871372814e-05
Step 135100 Loss: 0.4850, lr 9.135402871372814e-05
Train Epoch: [20/100] Loss: 0.4852,lr 0.000091
Model Saving at epoch 20
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 6756
Step 135200 Loss: 1.3663, lr 9.045084971874742e-05
Step 135300 Loss: 1.0754, lr 9.045084971874742e-05
Step 135400 Loss: 0.9458, lr 9.045084971874742e-05
Step 135500 Loss: 0.8735, lr 9.045084971874742e-05
Step 135600 Loss: 0.8283, lr 9.045084971874742e-05
Step 135700 Loss: 0.7910, lr 9.045084971874742e-05
Step 135800 Loss: 0.7604, lr 9.045084971874742e-05
Step 135900 Loss: 0.7354, lr 9.045084971874742e-05
Step 136000 Loss: 0.7144, lr 9.045084971874742e-05
Step 136100 Loss: 0.6963, lr 9.045084971874742e-05
Step 136200 Loss: 0.6822, lr 9.045084971874742e-05
Step 136300 Loss: 0.6687, lr 9.045084971874742e-05
Step 136400 Loss: 0.6575, lr 9.045084971874742e-05
Step 136500 Loss: 0.6465, lr 9.045084971874742e-05
Step 136600 Loss: 0.6364, lr 9.045084971874742e-05
Step 136700 Loss: 0.6274, lr 9.045084971874742e-05
Step 136800 Loss: 0.6188, lr 9.045084971874742e-05
Step 136900 Loss: 0.6124, lr 9.045084971874742e-05
Step 137000 Loss: 0.6054, lr 9.045084971874742e-05
Step 137100 Loss: 0.5987, lr 9.045084971874742e-05
Step 137200 Loss: 0.5922, lr 9.045084971874742e-05
Step 137300 Loss: 0.5867, lr 9.045084971874742e-05
Step 137400 Loss: 0.5815, lr 9.045084971874742e-05
Step 137500 Loss: 0.5764, lr 9.045084971874742e-05
Step 137600 Loss: 0.5710, lr 9.045084971874742e-05
Step 137700 Loss: 0.5660, lr 9.045084971874742e-05
Step 137800 Loss: 0.5617, lr 9.045084971874742e-05
Step 137900 Loss: 0.5582, lr 9.045084971874742e-05
Step 138000 Loss: 0.5544, lr 9.045084971874742e-05
Step 138100 Loss: 0.5511, lr 9.045084971874742e-05
Step 138200 Loss: 0.5476, lr 9.045084971874742e-05
Step 138300 Loss: 0.5444, lr 9.045084971874742e-05
Step 138400 Loss: 0.5410, lr 9.045084971874742e-05
Step 138500 Loss: 0.5368, lr 9.045084971874742e-05
Step 138600 Loss: 0.5325, lr 9.045084971874742e-05
Step 138700 Loss: 0.5281, lr 9.045084971874742e-05
Step 138800 Loss: 0.5243, lr 9.045084971874742e-05
Step 138900 Loss: 0.5205, lr 9.045084971874742e-05
Step 139000 Loss: 0.5169, lr 9.045084971874742e-05
Step 139100 Loss: 0.5137, lr 9.045084971874742e-05
Step 139200 Loss: 0.5109, lr 9.045084971874742e-05
Step 139300 Loss: 0.5082, lr 9.045084971874742e-05
Step 139400 Loss: 0.5052, lr 9.045084971874742e-05
Step 139500 Loss: 0.5024, lr 9.045084971874742e-05
Step 139600 Loss: 0.4999, lr 9.045084971874742e-05
Step 139700 Loss: 0.4969, lr 9.045084971874742e-05
Step 139800 Loss: 0.4944, lr 9.045084971874742e-05
Step 139900 Loss: 0.4922, lr 9.045084971874742e-05
Step 140000 Loss: 0.4904, lr 9.045084971874742e-05
Step 140100 Loss: 0.4888, lr 9.045084971874742e-05
Step 140200 Loss: 0.4869, lr 9.045084971874742e-05
Step 140300 Loss: 0.4856, lr 9.045084971874742e-05
Step 140400 Loss: 0.4840, lr 9.045084971874742e-05
Step 140500 Loss: 0.4829, lr 9.045084971874742e-05
Step 140600 Loss: 0.4816, lr 9.045084971874742e-05
Step 140700 Loss: 0.4806, lr 9.045084971874742e-05
Step 140800 Loss: 0.4795, lr 9.045084971874742e-05
Step 140900 Loss: 0.4789, lr 9.045084971874742e-05
Step 141000 Loss: 0.4787, lr 9.045084971874742e-05
Step 141100 Loss: 0.4782, lr 9.045084971874742e-05
Step 141200 Loss: 0.4777, lr 9.045084971874742e-05
Step 141300 Loss: 0.4776, lr 9.045084971874742e-05
Step 141400 Loss: 0.4767, lr 9.045084971874742e-05
Step 141500 Loss: 0.4762, lr 9.045084971874742e-05
Step 141600 Loss: 0.4745, lr 9.045084971874742e-05
Step 141700 Loss: 0.4731, lr 9.045084971874742e-05
Step 141800 Loss: 0.4713, lr 9.045084971874742e-05
Train Epoch: [21/100] Loss: 0.4706,lr 0.000090
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 141900 Loss: 1.2721, lr 8.950775061878456e-05
Step 142000 Loss: 0.8916, lr 8.950775061878456e-05
Step 142100 Loss: 0.7942, lr 8.950775061878456e-05
Step 142200 Loss: 0.7371, lr 8.950775061878456e-05
Step 142300 Loss: 0.7055, lr 8.950775061878456e-05
Step 142400 Loss: 0.6819, lr 8.950775061878456e-05
Step 142500 Loss: 0.6597, lr 8.950775061878456e-05
Step 142600 Loss: 0.6405, lr 8.950775061878456e-05
Step 142700 Loss: 0.6244, lr 8.950775061878456e-05
Step 142800 Loss: 0.6100, lr 8.950775061878456e-05
Step 142900 Loss: 0.5986, lr 8.950775061878456e-05
Step 143000 Loss: 0.5881, lr 8.950775061878456e-05
Step 143100 Loss: 0.5770, lr 8.950775061878456e-05
Step 143200 Loss: 0.5689, lr 8.950775061878456e-05
Step 143300 Loss: 0.5608, lr 8.950775061878456e-05
Step 143400 Loss: 0.5531, lr 8.950775061878456e-05
Step 143500 Loss: 0.5456, lr 8.950775061878456e-05
Step 143600 Loss: 0.5394, lr 8.950775061878456e-05
Step 143700 Loss: 0.5339, lr 8.950775061878456e-05
Step 143800 Loss: 0.5281, lr 8.950775061878456e-05
Step 143900 Loss: 0.5218, lr 8.950775061878456e-05
Step 144000 Loss: 0.5169, lr 8.950775061878456e-05
Step 144100 Loss: 0.5120, lr 8.950775061878456e-05
Step 144200 Loss: 0.5070, lr 8.950775061878456e-05
Step 144300 Loss: 0.5027, lr 8.950775061878456e-05
Step 144400 Loss: 0.4975, lr 8.950775061878456e-05
Step 144500 Loss: 0.4928, lr 8.950775061878456e-05
Step 144600 Loss: 0.4895, lr 8.950775061878456e-05
Step 144700 Loss: 0.4863, lr 8.950775061878456e-05
Step 144800 Loss: 0.4828, lr 8.950775061878456e-05
Step 144900 Loss: 0.4798, lr 8.950775061878456e-05
Step 145000 Loss: 0.4767, lr 8.950775061878456e-05
Step 145100 Loss: 0.4742, lr 8.950775061878456e-05
Step 145200 Loss: 0.4709, lr 8.950775061878456e-05
Step 145300 Loss: 0.4674, lr 8.950775061878456e-05
Step 145400 Loss: 0.4643, lr 8.950775061878456e-05
Step 145500 Loss: 0.4616, lr 8.950775061878456e-05
Step 145600 Loss: 0.4584, lr 8.950775061878456e-05
Step 145700 Loss: 0.4560, lr 8.950775061878456e-05
Step 145800 Loss: 0.4535, lr 8.950775061878456e-05
Step 145900 Loss: 0.4517, lr 8.950775061878456e-05
Step 146000 Loss: 0.4495, lr 8.950775061878456e-05
Step 146100 Loss: 0.4475, lr 8.950775061878456e-05
Step 146200 Loss: 0.4454, lr 8.950775061878456e-05
Step 146300 Loss: 0.4440, lr 8.950775061878456e-05
Step 146400 Loss: 0.4421, lr 8.950775061878456e-05
Step 146500 Loss: 0.4403, lr 8.950775061878456e-05
Step 146600 Loss: 0.4384, lr 8.950775061878456e-05
Step 146700 Loss: 0.4374, lr 8.950775061878456e-05
Step 146800 Loss: 0.4363, lr 8.950775061878456e-05
Step 146900 Loss: 0.4355, lr 8.950775061878456e-05
Step 147000 Loss: 0.4350, lr 8.950775061878456e-05
Step 147100 Loss: 0.4347, lr 8.950775061878456e-05
Step 147200 Loss: 0.4346, lr 8.950775061878456e-05
Step 147300 Loss: 0.4345, lr 8.950775061878456e-05
Step 147400 Loss: 0.4347, lr 8.950775061878456e-05
Step 147500 Loss: 0.4356, lr 8.950775061878456e-05
Step 147600 Loss: 0.4360, lr 8.950775061878456e-05
Step 147700 Loss: 0.4369, lr 8.950775061878456e-05
Step 147800 Loss: 0.4379, lr 8.950775061878456e-05
Step 147900 Loss: 0.4382, lr 8.950775061878456e-05
Step 148000 Loss: 0.4396, lr 8.950775061878456e-05
Step 148100 Loss: 0.4406, lr 8.950775061878456e-05
Step 148200 Loss: 0.4411, lr 8.950775061878456e-05
Step 148300 Loss: 0.4416, lr 8.950775061878456e-05
Step 148400 Loss: 0.4415, lr 8.950775061878456e-05
Step 148500 Loss: 0.4411, lr 8.950775061878456e-05
Step 148600 Loss: 0.4403, lr 8.950775061878456e-05
Train Epoch: [22/100] Loss: 0.4409,lr 0.000090
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 148700 Loss: 0.9942, lr 8.852566213878951e-05
Step 148800 Loss: 0.8325, lr 8.852566213878951e-05
Step 148900 Loss: 0.7595, lr 8.852566213878951e-05
Step 149000 Loss: 0.7123, lr 8.852566213878951e-05
Step 149100 Loss: 0.6864, lr 8.852566213878951e-05
Step 149200 Loss: 0.6634, lr 8.852566213878951e-05
Step 149300 Loss: 0.6426, lr 8.852566213878951e-05
Step 149400 Loss: 0.6258, lr 8.852566213878951e-05
Step 149500 Loss: 0.6108, lr 8.852566213878951e-05
Step 149600 Loss: 0.5972, lr 8.852566213878951e-05
Step 149700 Loss: 0.5876, lr 8.852566213878951e-05
Step 149800 Loss: 0.5775, lr 8.852566213878951e-05
Step 149900 Loss: 0.5693, lr 8.852566213878951e-05
Step 150000 Loss: 0.5610, lr 8.852566213878951e-05
Step 150100 Loss: 0.5540, lr 8.852566213878951e-05
Step 150200 Loss: 0.5482, lr 8.852566213878951e-05
Step 150300 Loss: 0.5420, lr 8.852566213878951e-05
Step 150400 Loss: 0.5371, lr 8.852566213878951e-05
Step 150500 Loss: 0.5323, lr 8.852566213878951e-05
Step 150600 Loss: 0.5271, lr 8.852566213878951e-05
Step 150700 Loss: 0.5216, lr 8.852566213878951e-05
Step 150800 Loss: 0.5180, lr 8.852566213878951e-05
Step 150900 Loss: 0.5140, lr 8.852566213878951e-05
Step 151000 Loss: 0.5102, lr 8.852566213878951e-05
Step 151100 Loss: 0.5056, lr 8.852566213878951e-05
Step 151200 Loss: 0.5013, lr 8.852566213878951e-05
Step 151300 Loss: 0.4971, lr 8.852566213878951e-05
Step 151400 Loss: 0.4942, lr 8.852566213878951e-05
Step 151500 Loss: 0.4908, lr 8.852566213878951e-05
Step 151600 Loss: 0.4882, lr 8.852566213878951e-05
Step 151700 Loss: 0.4857, lr 8.852566213878951e-05
Step 151800 Loss: 0.4827, lr 8.852566213878951e-05
Step 151900 Loss: 0.4804, lr 8.852566213878951e-05
Step 152000 Loss: 0.4770, lr 8.852566213878951e-05
Step 152100 Loss: 0.4741, lr 8.852566213878951e-05
Step 152200 Loss: 0.4713, lr 8.852566213878951e-05
Step 152300 Loss: 0.4692, lr 8.852566213878951e-05
Step 152400 Loss: 0.4664, lr 8.852566213878951e-05
Step 152500 Loss: 0.4642, lr 8.852566213878951e-05
Step 152600 Loss: 0.4623, lr 8.852566213878951e-05
Step 152700 Loss: 0.4605, lr 8.852566213878951e-05
Step 152800 Loss: 0.4590, lr 8.852566213878951e-05
Step 152900 Loss: 0.4573, lr 8.852566213878951e-05
Step 153000 Loss: 0.4553, lr 8.852566213878951e-05
Step 153100 Loss: 0.4538, lr 8.852566213878951e-05
Step 153200 Loss: 0.4520, lr 8.852566213878951e-05
Step 153300 Loss: 0.4502, lr 8.852566213878951e-05
Step 153400 Loss: 0.4486, lr 8.852566213878951e-05
Step 153500 Loss: 0.4478, lr 8.852566213878951e-05
Step 153600 Loss: 0.4469, lr 8.852566213878951e-05
Step 153700 Loss: 0.4459, lr 8.852566213878951e-05
Step 153800 Loss: 0.4455, lr 8.852566213878951e-05
Step 153900 Loss: 0.4449, lr 8.852566213878951e-05
Step 154000 Loss: 0.4443, lr 8.852566213878951e-05
Step 154100 Loss: 0.4438, lr 8.852566213878951e-05
Step 154200 Loss: 0.4434, lr 8.852566213878951e-05
Step 154300 Loss: 0.4429, lr 8.852566213878951e-05
Step 154400 Loss: 0.4431, lr 8.852566213878951e-05
Step 154500 Loss: 0.4436, lr 8.852566213878951e-05
Step 154600 Loss: 0.4438, lr 8.852566213878951e-05
Step 154700 Loss: 0.4437, lr 8.852566213878951e-05
Step 154800 Loss: 0.4448, lr 8.852566213878951e-05
Step 154900 Loss: 0.4449, lr 8.852566213878951e-05
Step 155000 Loss: 0.4450, lr 8.852566213878951e-05
Step 155100 Loss: 0.4456, lr 8.852566213878951e-05
Step 155200 Loss: 0.4461, lr 8.852566213878951e-05
Step 155300 Loss: 0.4450, lr 8.852566213878951e-05
Train Epoch: [23/100] Loss: 0.4448,lr 0.000089
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 155400 Loss: 1.4087, lr 8.750555348152303e-05
Step 155500 Loss: 0.9036, lr 8.750555348152303e-05
Step 155600 Loss: 0.7948, lr 8.750555348152303e-05
Step 155700 Loss: 0.7359, lr 8.750555348152303e-05
Step 155800 Loss: 0.7031, lr 8.750555348152303e-05
Step 155900 Loss: 0.6791, lr 8.750555348152303e-05
Step 156000 Loss: 0.6552, lr 8.750555348152303e-05
Step 156100 Loss: 0.6374, lr 8.750555348152303e-05
Step 156200 Loss: 0.6228, lr 8.750555348152303e-05
Step 156300 Loss: 0.6080, lr 8.750555348152303e-05
Step 156400 Loss: 0.5950, lr 8.750555348152303e-05
Step 156500 Loss: 0.5857, lr 8.750555348152303e-05
Step 156600 Loss: 0.5738, lr 8.750555348152303e-05
Step 156700 Loss: 0.5658, lr 8.750555348152303e-05
Step 156800 Loss: 0.5574, lr 8.750555348152303e-05
Step 156900 Loss: 0.5502, lr 8.750555348152303e-05
Step 157000 Loss: 0.5425, lr 8.750555348152303e-05
Step 157100 Loss: 0.5368, lr 8.750555348152303e-05
Step 157200 Loss: 0.5310, lr 8.750555348152303e-05
Step 157300 Loss: 0.5257, lr 8.750555348152303e-05
Step 157400 Loss: 0.5200, lr 8.750555348152303e-05
Step 157500 Loss: 0.5147, lr 8.750555348152303e-05
Step 157600 Loss: 0.5098, lr 8.750555348152303e-05
Step 157700 Loss: 0.5054, lr 8.750555348152303e-05
Step 157800 Loss: 0.5014, lr 8.750555348152303e-05
Step 157900 Loss: 0.4964, lr 8.750555348152303e-05
Step 158000 Loss: 0.4919, lr 8.750555348152303e-05
Step 158100 Loss: 0.4885, lr 8.750555348152303e-05
Step 158200 Loss: 0.4859, lr 8.750555348152303e-05
Step 158300 Loss: 0.4826, lr 8.750555348152303e-05
Step 158400 Loss: 0.4797, lr 8.750555348152303e-05
Step 158500 Loss: 0.4764, lr 8.750555348152303e-05
Step 158600 Loss: 0.4737, lr 8.750555348152303e-05
Step 158700 Loss: 0.4706, lr 8.750555348152303e-05
Step 158800 Loss: 0.4676, lr 8.750555348152303e-05
Step 158900 Loss: 0.4645, lr 8.750555348152303e-05
Step 159000 Loss: 0.4616, lr 8.750555348152303e-05
Step 159100 Loss: 0.4586, lr 8.750555348152303e-05
Step 159200 Loss: 0.4561, lr 8.750555348152303e-05
Step 159300 Loss: 0.4537, lr 8.750555348152303e-05
Step 159400 Loss: 0.4518, lr 8.750555348152303e-05
Step 159500 Loss: 0.4499, lr 8.750555348152303e-05
Step 159600 Loss: 0.4486, lr 8.750555348152303e-05
Step 159700 Loss: 0.4468, lr 8.750555348152303e-05
Step 159800 Loss: 0.4451, lr 8.750555348152303e-05
Step 159900 Loss: 0.4438, lr 8.750555348152303e-05
Step 160000 Loss: 0.4422, lr 8.750555348152303e-05
Step 160100 Loss: 0.4405, lr 8.750555348152303e-05
Step 160200 Loss: 0.4397, lr 8.750555348152303e-05
Step 160300 Loss: 0.4386, lr 8.750555348152303e-05
Step 160400 Loss: 0.4376, lr 8.750555348152303e-05
Step 160500 Loss: 0.4372, lr 8.750555348152303e-05
Step 160600 Loss: 0.4367, lr 8.750555348152303e-05
Step 160700 Loss: 0.4363, lr 8.750555348152303e-05
Step 160800 Loss: 0.4358, lr 8.750555348152303e-05
Step 160900 Loss: 0.4352, lr 8.750555348152303e-05
Step 161000 Loss: 0.4349, lr 8.750555348152303e-05
Step 161100 Loss: 0.4349, lr 8.750555348152303e-05
Step 161200 Loss: 0.4348, lr 8.750555348152303e-05
Step 161300 Loss: 0.4354, lr 8.750555348152303e-05
Step 161400 Loss: 0.4351, lr 8.750555348152303e-05
Step 161500 Loss: 0.4357, lr 8.750555348152303e-05
Step 161600 Loss: 0.4359, lr 8.750555348152303e-05
Step 161700 Loss: 0.4358, lr 8.750555348152303e-05
Step 161800 Loss: 0.4356, lr 8.750555348152303e-05
Step 161900 Loss: 0.4349, lr 8.750555348152303e-05
Step 162000 Loss: 0.4335, lr 8.750555348152303e-05
Step 162100 Loss: 0.4326, lr 8.750555348152303e-05
Train Epoch: [24/100] Loss: 0.4324,lr 0.000088
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6755
Step 162200 Loss: 1.0206, lr 8.644843137107063e-05
Step 162300 Loss: 0.8322, lr 8.644843137107063e-05
Step 162400 Loss: 0.7562, lr 8.644843137107063e-05
Step 162500 Loss: 0.7074, lr 8.644843137107063e-05
Step 162600 Loss: 0.6817, lr 8.644843137107063e-05
Step 162700 Loss: 0.6596, lr 8.644843137107063e-05
Step 162800 Loss: 0.6370, lr 8.644843137107063e-05
Step 162900 Loss: 0.6206, lr 8.644843137107063e-05
Step 163000 Loss: 0.6072, lr 8.644843137107063e-05
Step 163100 Loss: 0.5937, lr 8.644843137107063e-05
Step 163200 Loss: 0.5837, lr 8.644843137107063e-05
Step 163300 Loss: 0.5722, lr 8.644843137107063e-05
Step 163400 Loss: 0.5630, lr 8.644843137107063e-05
Step 163500 Loss: 0.5542, lr 8.644843137107063e-05
Step 163600 Loss: 0.5468, lr 8.644843137107063e-05
Step 163700 Loss: 0.5402, lr 8.644843137107063e-05
Step 163800 Loss: 0.5332, lr 8.644843137107063e-05
Step 163900 Loss: 0.5278, lr 8.644843137107063e-05
Step 164000 Loss: 0.5229, lr 8.644843137107063e-05
Step 164100 Loss: 0.5168, lr 8.644843137107063e-05
Step 164200 Loss: 0.5112, lr 8.644843137107063e-05
Step 164300 Loss: 0.5067, lr 8.644843137107063e-05
Step 164400 Loss: 0.5023, lr 8.644843137107063e-05
Step 164500 Loss: 0.4980, lr 8.644843137107063e-05
Step 164600 Loss: 0.4938, lr 8.644843137107063e-05
Step 164700 Loss: 0.4893, lr 8.644843137107063e-05
Step 164800 Loss: 0.4852, lr 8.644843137107063e-05
Step 164900 Loss: 0.4825, lr 8.644843137107063e-05
Step 165000 Loss: 0.4793, lr 8.644843137107063e-05
Step 165100 Loss: 0.4766, lr 8.644843137107063e-05
Step 165200 Loss: 0.4740, lr 8.644843137107063e-05
Step 165300 Loss: 0.4709, lr 8.644843137107063e-05
Step 165400 Loss: 0.4682, lr 8.644843137107063e-05
Step 165500 Loss: 0.4648, lr 8.644843137107063e-05
Step 165600 Loss: 0.4618, lr 8.644843137107063e-05
Step 165700 Loss: 0.4587, lr 8.644843137107063e-05
Step 165800 Loss: 0.4561, lr 8.644843137107063e-05
Step 165900 Loss: 0.4530, lr 8.644843137107063e-05
Step 166000 Loss: 0.4505, lr 8.644843137107063e-05
Step 166100 Loss: 0.4480, lr 8.644843137107063e-05
Step 166200 Loss: 0.4461, lr 8.644843137107063e-05
Step 166300 Loss: 0.4443, lr 8.644843137107063e-05
Step 166400 Loss: 0.4426, lr 8.644843137107063e-05
Step 166500 Loss: 0.4405, lr 8.644843137107063e-05
Step 166600 Loss: 0.4391, lr 8.644843137107063e-05
Step 166700 Loss: 0.4371, lr 8.644843137107063e-05
Step 166800 Loss: 0.4354, lr 8.644843137107063e-05
Step 166900 Loss: 0.4339, lr 8.644843137107063e-05
Step 167000 Loss: 0.4329, lr 8.644843137107063e-05
Step 167100 Loss: 0.4320, lr 8.644843137107063e-05
Step 167200 Loss: 0.4308, lr 8.644843137107063e-05
Step 167300 Loss: 0.4303, lr 8.644843137107063e-05
Step 167400 Loss: 0.4300, lr 8.644843137107063e-05
Step 167500 Loss: 0.4294, lr 8.644843137107063e-05
Step 167600 Loss: 0.4288, lr 8.644843137107063e-05
Step 167700 Loss: 0.4283, lr 8.644843137107063e-05
Step 167800 Loss: 0.4279, lr 8.644843137107063e-05
Step 167900 Loss: 0.4279, lr 8.644843137107063e-05
Step 168000 Loss: 0.4283, lr 8.644843137107063e-05
Step 168100 Loss: 0.4284, lr 8.644843137107063e-05
Step 168200 Loss: 0.4286, lr 8.644843137107063e-05
Step 168300 Loss: 0.4296, lr 8.644843137107063e-05
Step 168400 Loss: 0.4299, lr 8.644843137107063e-05
Step 168500 Loss: 0.4299, lr 8.644843137107063e-05
Step 168600 Loss: 0.4293, lr 8.644843137107063e-05
Step 168700 Loss: 0.4287, lr 8.644843137107063e-05
Step 168800 Loss: 0.4270, lr 8.644843137107063e-05
Train Epoch: [25/100] Loss: 0.4262,lr 0.000086
Model Saving at epoch 25
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6756
Step 168900 Loss: 1.8186, lr 8.535533905932742e-05
Step 169000 Loss: 0.9078, lr 8.535533905932742e-05
Step 169100 Loss: 0.7844, lr 8.535533905932742e-05
Step 169200 Loss: 0.7212, lr 8.535533905932742e-05
Step 169300 Loss: 0.6867, lr 8.535533905932742e-05
Step 169400 Loss: 0.6606, lr 8.535533905932742e-05
Step 169500 Loss: 0.6374, lr 8.535533905932742e-05
Step 169600 Loss: 0.6196, lr 8.535533905932742e-05
Step 169700 Loss: 0.6035, lr 8.535533905932742e-05
Step 169800 Loss: 0.5895, lr 8.535533905932742e-05
Step 169900 Loss: 0.5764, lr 8.535533905932742e-05
Step 170000 Loss: 0.5667, lr 8.535533905932742e-05
Step 170100 Loss: 0.5562, lr 8.535533905932742e-05
Step 170200 Loss: 0.5475, lr 8.535533905932742e-05
Step 170300 Loss: 0.5393, lr 8.535533905932742e-05
Step 170400 Loss: 0.5326, lr 8.535533905932742e-05
Step 170500 Loss: 0.5254, lr 8.535533905932742e-05
Step 170600 Loss: 0.5196, lr 8.535533905932742e-05
Step 170700 Loss: 0.5142, lr 8.535533905932742e-05
Step 170800 Loss: 0.5088, lr 8.535533905932742e-05
Step 170900 Loss: 0.5035, lr 8.535533905932742e-05
Step 171000 Loss: 0.4981, lr 8.535533905932742e-05
Step 171100 Loss: 0.4939, lr 8.535533905932742e-05
Step 171200 Loss: 0.4903, lr 8.535533905932742e-05
Step 171300 Loss: 0.4868, lr 8.535533905932742e-05
Step 171400 Loss: 0.4821, lr 8.535533905932742e-05
Step 171500 Loss: 0.4773, lr 8.535533905932742e-05
Step 171600 Loss: 0.4737, lr 8.535533905932742e-05
Step 171700 Loss: 0.4711, lr 8.535533905932742e-05
Step 171800 Loss: 0.4679, lr 8.535533905932742e-05
Step 171900 Loss: 0.4652, lr 8.535533905932742e-05
Step 172000 Loss: 0.4619, lr 8.535533905932742e-05
Step 172100 Loss: 0.4591, lr 8.535533905932742e-05
Step 172200 Loss: 0.4560, lr 8.535533905932742e-05
Step 172300 Loss: 0.4533, lr 8.535533905932742e-05
Step 172400 Loss: 0.4501, lr 8.535533905932742e-05
Step 172500 Loss: 0.4472, lr 8.535533905932742e-05
Step 172600 Loss: 0.4444, lr 8.535533905932742e-05
Step 172700 Loss: 0.4418, lr 8.535533905932742e-05
Step 172800 Loss: 0.4393, lr 8.535533905932742e-05
Step 172900 Loss: 0.4372, lr 8.535533905932742e-05
Step 173000 Loss: 0.4353, lr 8.535533905932742e-05
Step 173100 Loss: 0.4337, lr 8.535533905932742e-05
Step 173200 Loss: 0.4320, lr 8.535533905932742e-05
Step 173300 Loss: 0.4305, lr 8.535533905932742e-05
Step 173400 Loss: 0.4290, lr 8.535533905932742e-05
Step 173500 Loss: 0.4271, lr 8.535533905932742e-05
Step 173600 Loss: 0.4256, lr 8.535533905932742e-05
Step 173700 Loss: 0.4247, lr 8.535533905932742e-05
Step 173800 Loss: 0.4238, lr 8.535533905932742e-05
Step 173900 Loss: 0.4231, lr 8.535533905932742e-05
Step 174000 Loss: 0.4225, lr 8.535533905932742e-05
Step 174100 Loss: 0.4218, lr 8.535533905932742e-05
Step 174200 Loss: 0.4213, lr 8.535533905932742e-05
Step 174300 Loss: 0.4211, lr 8.535533905932742e-05
Step 174400 Loss: 0.4209, lr 8.535533905932742e-05
Step 174500 Loss: 0.4204, lr 8.535533905932742e-05
Step 174600 Loss: 0.4199, lr 8.535533905932742e-05
Step 174700 Loss: 0.4201, lr 8.535533905932742e-05
Step 174800 Loss: 0.4207, lr 8.535533905932742e-05
Step 174900 Loss: 0.4205, lr 8.535533905932742e-05
Step 175000 Loss: 0.4211, lr 8.535533905932742e-05
Step 175100 Loss: 0.4216, lr 8.535533905932742e-05
Step 175200 Loss: 0.4213, lr 8.535533905932742e-05
Step 175300 Loss: 0.4212, lr 8.535533905932742e-05
Step 175400 Loss: 0.4210, lr 8.535533905932742e-05
Step 175500 Loss: 0.4197, lr 8.535533905932742e-05
Step 175600 Loss: 0.4191, lr 8.535533905932742e-05
Train Epoch: [26/100] Loss: 0.4191,lr 0.000085
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 175700 Loss: 1.0572, lr 8.422735529643448e-05
Step 175800 Loss: 0.8234, lr 8.422735529643448e-05
Step 175900 Loss: 0.7456, lr 8.422735529643448e-05
Step 176000 Loss: 0.6942, lr 8.422735529643448e-05
Step 176100 Loss: 0.6675, lr 8.422735529643448e-05
Step 176200 Loss: 0.6448, lr 8.422735529643448e-05
Step 176300 Loss: 0.6239, lr 8.422735529643448e-05
Step 176400 Loss: 0.6061, lr 8.422735529643448e-05
Step 176500 Loss: 0.5920, lr 8.422735529643448e-05
Step 176600 Loss: 0.5775, lr 8.422735529643448e-05
Step 176700 Loss: 0.5668, lr 8.422735529643448e-05
Step 176800 Loss: 0.5562, lr 8.422735529643448e-05
Step 176900 Loss: 0.5463, lr 8.422735529643448e-05
Step 177000 Loss: 0.5385, lr 8.422735529643448e-05
Step 177100 Loss: 0.5310, lr 8.422735529643448e-05
Step 177200 Loss: 0.5242, lr 8.422735529643448e-05
Step 177300 Loss: 0.5171, lr 8.422735529643448e-05
Step 177400 Loss: 0.5117, lr 8.422735529643448e-05
Step 177500 Loss: 0.5065, lr 8.422735529643448e-05
Step 177600 Loss: 0.5010, lr 8.422735529643448e-05
Step 177700 Loss: 0.4957, lr 8.422735529643448e-05
Step 177800 Loss: 0.4912, lr 8.422735529643448e-05
Step 177900 Loss: 0.4865, lr 8.422735529643448e-05
Step 178000 Loss: 0.4824, lr 8.422735529643448e-05
Step 178100 Loss: 0.4783, lr 8.422735529643448e-05
Step 178200 Loss: 0.4735, lr 8.422735529643448e-05
Step 178300 Loss: 0.4696, lr 8.422735529643448e-05
Step 178400 Loss: 0.4666, lr 8.422735529643448e-05
Step 178500 Loss: 0.4630, lr 8.422735529643448e-05
Step 178600 Loss: 0.4601, lr 8.422735529643448e-05
Step 178700 Loss: 0.4573, lr 8.422735529643448e-05
Step 178800 Loss: 0.4543, lr 8.422735529643448e-05
Step 178900 Loss: 0.4518, lr 8.422735529643448e-05
Step 179000 Loss: 0.4486, lr 8.422735529643448e-05
Step 179100 Loss: 0.4460, lr 8.422735529643448e-05
Step 179200 Loss: 0.4431, lr 8.422735529643448e-05
Step 179300 Loss: 0.4404, lr 8.422735529643448e-05
Step 179400 Loss: 0.4375, lr 8.422735529643448e-05
Step 179500 Loss: 0.4352, lr 8.422735529643448e-05
Step 179600 Loss: 0.4327, lr 8.422735529643448e-05
Step 179700 Loss: 0.4311, lr 8.422735529643448e-05
Step 179800 Loss: 0.4295, lr 8.422735529643448e-05
Step 179900 Loss: 0.4282, lr 8.422735529643448e-05
Step 180000 Loss: 0.4261, lr 8.422735529643448e-05
Step 180100 Loss: 0.4248, lr 8.422735529643448e-05
Step 180200 Loss: 0.4234, lr 8.422735529643448e-05
Step 180300 Loss: 0.4220, lr 8.422735529643448e-05
Step 180400 Loss: 0.4203, lr 8.422735529643448e-05
Step 180500 Loss: 0.4196, lr 8.422735529643448e-05
Step 180600 Loss: 0.4190, lr 8.422735529643448e-05
Step 180700 Loss: 0.4179, lr 8.422735529643448e-05
Step 180800 Loss: 0.4173, lr 8.422735529643448e-05
Step 180900 Loss: 0.4170, lr 8.422735529643448e-05
Step 181000 Loss: 0.4166, lr 8.422735529643448e-05
Step 181100 Loss: 0.4161, lr 8.422735529643448e-05
Step 181200 Loss: 0.4158, lr 8.422735529643448e-05
Step 181300 Loss: 0.4157, lr 8.422735529643448e-05
Step 181400 Loss: 0.4156, lr 8.422735529643448e-05
Step 181500 Loss: 0.4159, lr 8.422735529643448e-05
Step 181600 Loss: 0.4163, lr 8.422735529643448e-05
Step 181700 Loss: 0.4165, lr 8.422735529643448e-05
Step 181800 Loss: 0.4173, lr 8.422735529643448e-05
Step 181900 Loss: 0.4174, lr 8.422735529643448e-05
Step 182000 Loss: 0.4172, lr 8.422735529643448e-05
Step 182100 Loss: 0.4170, lr 8.422735529643448e-05
Step 182200 Loss: 0.4166, lr 8.422735529643448e-05
Step 182300 Loss: 0.4154, lr 8.422735529643448e-05
Step 182400 Loss: 0.4150, lr 8.422735529643448e-05
Train Epoch: [27/100] Loss: 0.4154,lr 0.000084
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Step 182500 Loss: 0.9149, lr 8.306559326618263e-05
Step 182600 Loss: 0.7836, lr 8.306559326618263e-05
Step 182700 Loss: 0.7159, lr 8.306559326618263e-05
Step 182800 Loss: 0.6783, lr 8.306559326618263e-05
Step 182900 Loss: 0.6531, lr 8.306559326618263e-05
Step 183000 Loss: 0.6301, lr 8.306559326618263e-05
Step 183100 Loss: 0.6133, lr 8.306559326618263e-05
Step 183200 Loss: 0.5965, lr 8.306559326618263e-05
Step 183300 Loss: 0.5832, lr 8.306559326618263e-05
Step 183400 Loss: 0.5699, lr 8.306559326618263e-05
Step 183500 Loss: 0.5603, lr 8.306559326618263e-05
Step 183600 Loss: 0.5494, lr 8.306559326618263e-05
Step 183700 Loss: 0.5419, lr 8.306559326618263e-05
Step 183800 Loss: 0.5336, lr 8.306559326618263e-05
Step 183900 Loss: 0.5271, lr 8.306559326618263e-05
Step 184000 Loss: 0.5204, lr 8.306559326618263e-05
Step 184100 Loss: 0.5146, lr 8.306559326618263e-05
Step 184200 Loss: 0.5093, lr 8.306559326618263e-05
Step 184300 Loss: 0.5040, lr 8.306559326618263e-05
Step 184400 Loss: 0.4982, lr 8.306559326618263e-05
Step 184500 Loss: 0.4926, lr 8.306559326618263e-05
Step 184600 Loss: 0.4885, lr 8.306559326618263e-05
Step 184700 Loss: 0.4840, lr 8.306559326618263e-05
Step 184800 Loss: 0.4804, lr 8.306559326618263e-05
Step 184900 Loss: 0.4757, lr 8.306559326618263e-05
Step 185000 Loss: 0.4710, lr 8.306559326618263e-05
Step 185100 Loss: 0.4672, lr 8.306559326618263e-05
Step 185200 Loss: 0.4641, lr 8.306559326618263e-05
Step 185300 Loss: 0.4606, lr 8.306559326618263e-05
Step 185400 Loss: 0.4579, lr 8.306559326618263e-05
Step 185500 Loss: 0.4547, lr 8.306559326618263e-05
Step 185600 Loss: 0.4516, lr 8.306559326618263e-05
Step 185700 Loss: 0.4491, lr 8.306559326618263e-05
Step 185800 Loss: 0.4468, lr 8.306559326618263e-05
Step 185900 Loss: 0.4438, lr 8.306559326618263e-05
Step 186000 Loss: 0.4414, lr 8.306559326618263e-05
Step 186100 Loss: 0.4388, lr 8.306559326618263e-05
Step 186200 Loss: 0.4363, lr 8.306559326618263e-05
Step 186300 Loss: 0.4339, lr 8.306559326618263e-05
Step 186400 Loss: 0.4320, lr 8.306559326618263e-05
Step 186500 Loss: 0.4304, lr 8.306559326618263e-05
Step 186600 Loss: 0.4287, lr 8.306559326618263e-05
Step 186700 Loss: 0.4269, lr 8.306559326618263e-05
Step 186800 Loss: 0.4251, lr 8.306559326618263e-05
Step 186900 Loss: 0.4236, lr 8.306559326618263e-05
Step 187000 Loss: 0.4218, lr 8.306559326618263e-05
Step 187100 Loss: 0.4204, lr 8.306559326618263e-05
Step 187200 Loss: 0.4191, lr 8.306559326618263e-05
Step 187300 Loss: 0.4180, lr 8.306559326618263e-05
Step 187400 Loss: 0.4170, lr 8.306559326618263e-05
Step 187500 Loss: 0.4164, lr 8.306559326618263e-05
Step 187600 Loss: 0.4157, lr 8.306559326618263e-05
Step 187700 Loss: 0.4151, lr 8.306559326618263e-05
Step 187800 Loss: 0.4147, lr 8.306559326618263e-05
Step 187900 Loss: 0.4144, lr 8.306559326618263e-05
Step 188000 Loss: 0.4139, lr 8.306559326618263e-05
Step 188100 Loss: 0.4134, lr 8.306559326618263e-05
Step 188200 Loss: 0.4134, lr 8.306559326618263e-05
Step 188300 Loss: 0.4139, lr 8.306559326618263e-05
Step 188400 Loss: 0.4138, lr 8.306559326618263e-05
Step 188500 Loss: 0.4141, lr 8.306559326618263e-05
Step 188600 Loss: 0.4148, lr 8.306559326618263e-05
Step 188700 Loss: 0.4150, lr 8.306559326618263e-05
Step 188800 Loss: 0.4152, lr 8.306559326618263e-05
Step 188900 Loss: 0.4145, lr 8.306559326618263e-05
Step 189000 Loss: 0.4138, lr 8.306559326618263e-05
Step 189100 Loss: 0.4126, lr 8.306559326618263e-05
Train Epoch: [28/100] Loss: 0.4123,lr 0.000083
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6755
Step 189200 Loss: 1.1198, lr 8.187119948743452e-05
Step 189300 Loss: 0.8364, lr 8.187119948743452e-05
Step 189400 Loss: 0.7502, lr 8.187119948743452e-05
Step 189500 Loss: 0.6970, lr 8.187119948743452e-05
Step 189600 Loss: 0.6670, lr 8.187119948743452e-05
Step 189700 Loss: 0.6448, lr 8.187119948743452e-05
Step 189800 Loss: 0.6227, lr 8.187119948743452e-05
Step 189900 Loss: 0.6036, lr 8.187119948743452e-05
Step 190000 Loss: 0.5887, lr 8.187119948743452e-05
Step 190100 Loss: 0.5751, lr 8.187119948743452e-05
Step 190200 Loss: 0.5630, lr 8.187119948743452e-05
Step 190300 Loss: 0.5535, lr 8.187119948743452e-05
Step 190400 Loss: 0.5429, lr 8.187119948743452e-05
Step 190500 Loss: 0.5352, lr 8.187119948743452e-05
Step 190600 Loss: 0.5275, lr 8.187119948743452e-05
Step 190700 Loss: 0.5211, lr 8.187119948743452e-05
Step 190800 Loss: 0.5142, lr 8.187119948743452e-05
Step 190900 Loss: 0.5089, lr 8.187119948743452e-05
Step 191000 Loss: 0.5034, lr 8.187119948743452e-05
Step 191100 Loss: 0.4983, lr 8.187119948743452e-05
Step 191200 Loss: 0.4924, lr 8.187119948743452e-05
Step 191300 Loss: 0.4878, lr 8.187119948743452e-05
Step 191400 Loss: 0.4832, lr 8.187119948743452e-05
Step 191500 Loss: 0.4791, lr 8.187119948743452e-05
Step 191600 Loss: 0.4748, lr 8.187119948743452e-05
Step 191700 Loss: 0.4700, lr 8.187119948743452e-05
Step 191800 Loss: 0.4659, lr 8.187119948743452e-05
Step 191900 Loss: 0.4630, lr 8.187119948743452e-05
Step 192000 Loss: 0.4601, lr 8.187119948743452e-05
Step 192100 Loss: 0.4576, lr 8.187119948743452e-05
Step 192200 Loss: 0.4552, lr 8.187119948743452e-05
Step 192300 Loss: 0.4527, lr 8.187119948743452e-05
Step 192400 Loss: 0.4503, lr 8.187119948743452e-05
Step 192500 Loss: 0.4475, lr 8.187119948743452e-05
Step 192600 Loss: 0.4448, lr 8.187119948743452e-05
Step 192700 Loss: 0.4419, lr 8.187119948743452e-05
Step 192800 Loss: 0.4396, lr 8.187119948743452e-05
Step 192900 Loss: 0.4368, lr 8.187119948743452e-05
Step 193000 Loss: 0.4347, lr 8.187119948743452e-05
Step 193100 Loss: 0.4323, lr 8.187119948743452e-05
Step 193200 Loss: 0.4304, lr 8.187119948743452e-05
Step 193300 Loss: 0.4288, lr 8.187119948743452e-05
Step 193400 Loss: 0.4276, lr 8.187119948743452e-05
Step 193500 Loss: 0.4256, lr 8.187119948743452e-05
Step 193600 Loss: 0.4246, lr 8.187119948743452e-05
Step 193700 Loss: 0.4229, lr 8.187119948743452e-05
Step 193800 Loss: 0.4216, lr 8.187119948743452e-05
Step 193900 Loss: 0.4197, lr 8.187119948743452e-05
Step 194000 Loss: 0.4186, lr 8.187119948743452e-05
Step 194100 Loss: 0.4178, lr 8.187119948743452e-05
Step 194200 Loss: 0.4170, lr 8.187119948743452e-05
Step 194300 Loss: 0.4164, lr 8.187119948743452e-05
Step 194400 Loss: 0.4160, lr 8.187119948743452e-05
Step 194500 Loss: 0.4154, lr 8.187119948743452e-05
Step 194600 Loss: 0.4151, lr 8.187119948743452e-05
Step 194700 Loss: 0.4145, lr 8.187119948743452e-05
Step 194800 Loss: 0.4140, lr 8.187119948743452e-05
Step 194900 Loss: 0.4139, lr 8.187119948743452e-05
Step 195000 Loss: 0.4139, lr 8.187119948743452e-05
Step 195100 Loss: 0.4143, lr 8.187119948743452e-05
Step 195200 Loss: 0.4144, lr 8.187119948743452e-05
Step 195300 Loss: 0.4151, lr 8.187119948743452e-05
Step 195400 Loss: 0.4152, lr 8.187119948743452e-05
Step 195500 Loss: 0.4150, lr 8.187119948743452e-05
Step 195600 Loss: 0.4143, lr 8.187119948743452e-05
Step 195700 Loss: 0.4137, lr 8.187119948743452e-05
Step 195800 Loss: 0.4122, lr 8.187119948743452e-05
Step 195900 Loss: 0.4117, lr 8.187119948743452e-05
Train Epoch: [29/100] Loss: 0.4119,lr 0.000082
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Step 196000 Loss: 0.8967, lr 8.064535268264887e-05
Step 196100 Loss: 0.7549, lr 8.064535268264887e-05
Step 196200 Loss: 0.6932, lr 8.064535268264887e-05
Step 196300 Loss: 0.6598, lr 8.064535268264887e-05
Step 196400 Loss: 0.6393, lr 8.064535268264887e-05
Step 196500 Loss: 0.6178, lr 8.064535268264887e-05
Step 196600 Loss: 0.6010, lr 8.064535268264887e-05
Step 196700 Loss: 0.5876, lr 8.064535268264887e-05
Step 196800 Loss: 0.5749, lr 8.064535268264887e-05
Step 196900 Loss: 0.5639, lr 8.064535268264887e-05
Step 197000 Loss: 0.5566, lr 8.064535268264887e-05
Step 197100 Loss: 0.5472, lr 8.064535268264887e-05
Step 197200 Loss: 0.5399, lr 8.064535268264887e-05
Step 197300 Loss: 0.5328, lr 8.064535268264887e-05
Step 197400 Loss: 0.5263, lr 8.064535268264887e-05
Step 197500 Loss: 0.5207, lr 8.064535268264887e-05
Step 197600 Loss: 0.5149, lr 8.064535268264887e-05
Step 197700 Loss: 0.5109, lr 8.064535268264887e-05
Step 197800 Loss: 0.5060, lr 8.064535268264887e-05
Step 197900 Loss: 0.5014, lr 8.064535268264887e-05
Step 198000 Loss: 0.4961, lr 8.064535268264887e-05
Step 198100 Loss: 0.4926, lr 8.064535268264887e-05
Step 198200 Loss: 0.4889, lr 8.064535268264887e-05
Step 198300 Loss: 0.4854, lr 8.064535268264887e-05
Step 198400 Loss: 0.4808, lr 8.064535268264887e-05
Step 198500 Loss: 0.4765, lr 8.064535268264887e-05
Step 198600 Loss: 0.4731, lr 8.064535268264887e-05
Step 198700 Loss: 0.4709, lr 8.064535268264887e-05
Step 198800 Loss: 0.4680, lr 8.064535268264887e-05
Step 198900 Loss: 0.4653, lr 8.064535268264887e-05
Step 199000 Loss: 0.4629, lr 8.064535268264887e-05
Step 199100 Loss: 0.4602, lr 8.064535268264887e-05
Step 199200 Loss: 0.4580, lr 8.064535268264887e-05
Step 199300 Loss: 0.4553, lr 8.064535268264887e-05
Step 199400 Loss: 0.4525, lr 8.064535268264887e-05
Step 199500 Loss: 0.4500, lr 8.064535268264887e-05
Step 199600 Loss: 0.4480, lr 8.064535268264887e-05
Step 199700 Loss: 0.4459, lr 8.064535268264887e-05
Step 199800 Loss: 0.4441, lr 8.064535268264887e-05
Step 199900 Loss: 0.4422, lr 8.064535268264887e-05
Step 200000 Loss: 0.4407, lr 8.064535268264887e-05
Step 200100 Loss: 0.4393, lr 8.064535268264887e-05
Step 200200 Loss: 0.4376, lr 8.064535268264887e-05
Step 200300 Loss: 0.4365, lr 8.064535268264887e-05
Step 200400 Loss: 0.4350, lr 8.064535268264887e-05
Step 200500 Loss: 0.4332, lr 8.064535268264887e-05
Step 200600 Loss: 0.4317, lr 8.064535268264887e-05
Step 200700 Loss: 0.4304, lr 8.064535268264887e-05
Step 200800 Loss: 0.4296, lr 8.064535268264887e-05
Step 200900 Loss: 0.4287, lr 8.064535268264887e-05
Step 201000 Loss: 0.4279, lr 8.064535268264887e-05
Step 201100 Loss: 0.4274, lr 8.064535268264887e-05
Step 201200 Loss: 0.4264, lr 8.064535268264887e-05
Step 201300 Loss: 0.4259, lr 8.064535268264887e-05
Step 201400 Loss: 0.4255, lr 8.064535268264887e-05
Step 201500 Loss: 0.4248, lr 8.064535268264887e-05
Step 201600 Loss: 0.4248, lr 8.064535268264887e-05
Step 201700 Loss: 0.4246, lr 8.064535268264887e-05
Step 201800 Loss: 0.4252, lr 8.064535268264887e-05
Step 201900 Loss: 0.4252, lr 8.064535268264887e-05
Step 202000 Loss: 0.4251, lr 8.064535268264887e-05
Step 202100 Loss: 0.4256, lr 8.064535268264887e-05
Step 202200 Loss: 0.4251, lr 8.064535268264887e-05
Step 202300 Loss: 0.4248, lr 8.064535268264887e-05
Step 202400 Loss: 0.4241, lr 8.064535268264887e-05
Step 202500 Loss: 0.4231, lr 8.064535268264887e-05
Step 202600 Loss: 0.4217, lr 8.064535268264887e-05
Train Epoch: [30/100] Loss: 0.4217,lr 0.000081
Model Saving at epoch 30
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 202700 Loss: 1.1104, lr 7.938926261462371e-05
Step 202800 Loss: 0.7826, lr 7.938926261462371e-05
Step 202900 Loss: 0.7074, lr 7.938926261462371e-05
Step 203000 Loss: 0.6642, lr 7.938926261462371e-05
Step 203100 Loss: 0.6379, lr 7.938926261462371e-05
Step 203200 Loss: 0.6194, lr 7.938926261462371e-05
Step 203300 Loss: 0.6000, lr 7.938926261462371e-05
Step 203400 Loss: 0.5853, lr 7.938926261462371e-05
Step 203500 Loss: 0.5718, lr 7.938926261462371e-05
Step 203600 Loss: 0.5606, lr 7.938926261462371e-05
Step 203700 Loss: 0.5514, lr 7.938926261462371e-05
Step 203800 Loss: 0.5441, lr 7.938926261462371e-05
Step 203900 Loss: 0.5348, lr 7.938926261462371e-05
Step 204000 Loss: 0.5271, lr 7.938926261462371e-05
Step 204100 Loss: 0.5202, lr 7.938926261462371e-05
Step 204200 Loss: 0.5143, lr 7.938926261462371e-05
Step 204300 Loss: 0.5080, lr 7.938926261462371e-05
Step 204400 Loss: 0.5030, lr 7.938926261462371e-05
Step 204500 Loss: 0.4983, lr 7.938926261462371e-05
Step 204600 Loss: 0.4941, lr 7.938926261462371e-05
Step 204700 Loss: 0.4890, lr 7.938926261462371e-05
Step 204800 Loss: 0.4845, lr 7.938926261462371e-05
Step 204900 Loss: 0.4804, lr 7.938926261462371e-05
Step 205000 Loss: 0.4768, lr 7.938926261462371e-05
Step 205100 Loss: 0.4739, lr 7.938926261462371e-05
Step 205200 Loss: 0.4691, lr 7.938926261462371e-05
Step 205300 Loss: 0.4651, lr 7.938926261462371e-05
Step 205400 Loss: 0.4623, lr 7.938926261462371e-05
Step 205500 Loss: 0.4598, lr 7.938926261462371e-05
Step 205600 Loss: 0.4568, lr 7.938926261462371e-05
Step 205700 Loss: 0.4540, lr 7.938926261462371e-05
Step 205800 Loss: 0.4509, lr 7.938926261462371e-05
Step 205900 Loss: 0.4484, lr 7.938926261462371e-05
Step 206000 Loss: 0.4453, lr 7.938926261462371e-05
Step 206100 Loss: 0.4427, lr 7.938926261462371e-05
Step 206200 Loss: 0.4397, lr 7.938926261462371e-05
Step 206300 Loss: 0.4374, lr 7.938926261462371e-05
Step 206400 Loss: 0.4347, lr 7.938926261462371e-05
Step 206500 Loss: 0.4326, lr 7.938926261462371e-05
Step 206600 Loss: 0.4307, lr 7.938926261462371e-05
Step 206700 Loss: 0.4289, lr 7.938926261462371e-05
Step 206800 Loss: 0.4270, lr 7.938926261462371e-05
Step 206900 Loss: 0.4259, lr 7.938926261462371e-05
Step 207000 Loss: 0.4242, lr 7.938926261462371e-05
Step 207100 Loss: 0.4232, lr 7.938926261462371e-05
Step 207200 Loss: 0.4217, lr 7.938926261462371e-05
Step 207300 Loss: 0.4204, lr 7.938926261462371e-05
Step 207400 Loss: 0.4187, lr 7.938926261462371e-05
Step 207500 Loss: 0.4177, lr 7.938926261462371e-05
Step 207600 Loss: 0.4168, lr 7.938926261462371e-05
Step 207700 Loss: 0.4159, lr 7.938926261462371e-05
Step 207800 Loss: 0.4157, lr 7.938926261462371e-05
Step 207900 Loss: 0.4151, lr 7.938926261462371e-05
Step 208000 Loss: 0.4146, lr 7.938926261462371e-05
Step 208100 Loss: 0.4140, lr 7.938926261462371e-05
Step 208200 Loss: 0.4134, lr 7.938926261462371e-05
Step 208300 Loss: 0.4130, lr 7.938926261462371e-05
Step 208400 Loss: 0.4131, lr 7.938926261462371e-05
Step 208500 Loss: 0.4132, lr 7.938926261462371e-05
Step 208600 Loss: 0.4138, lr 7.938926261462371e-05
Step 208700 Loss: 0.4138, lr 7.938926261462371e-05
Step 208800 Loss: 0.4146, lr 7.938926261462371e-05
Step 208900 Loss: 0.4149, lr 7.938926261462371e-05
Step 209000 Loss: 0.4151, lr 7.938926261462371e-05
Step 209100 Loss: 0.4151, lr 7.938926261462371e-05
Step 209200 Loss: 0.4150, lr 7.938926261462371e-05
Step 209300 Loss: 0.4139, lr 7.938926261462371e-05
Step 209400 Loss: 0.4130, lr 7.938926261462371e-05
Train Epoch: [31/100] Loss: 0.4134,lr 0.000079
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 209500 Loss: 0.8630, lr 7.810416889260659e-05
Step 209600 Loss: 0.7286, lr 7.810416889260659e-05
Step 209700 Loss: 0.6726, lr 7.810416889260659e-05
Step 209800 Loss: 0.6352, lr 7.810416889260659e-05
Step 209900 Loss: 0.6180, lr 7.810416889260659e-05
Step 210000 Loss: 0.5996, lr 7.810416889260659e-05
Step 210100 Loss: 0.5830, lr 7.810416889260659e-05
Step 210200 Loss: 0.5710, lr 7.810416889260659e-05
Step 210300 Loss: 0.5596, lr 7.810416889260659e-05
Step 210400 Loss: 0.5493, lr 7.810416889260659e-05
Step 210500 Loss: 0.5420, lr 7.810416889260659e-05
Step 210600 Loss: 0.5341, lr 7.810416889260659e-05
Step 210700 Loss: 0.5273, lr 7.810416889260659e-05
Step 210800 Loss: 0.5204, lr 7.810416889260659e-05
Step 210900 Loss: 0.5149, lr 7.810416889260659e-05
Step 211000 Loss: 0.5097, lr 7.810416889260659e-05
Step 211100 Loss: 0.5043, lr 7.810416889260659e-05
Step 211200 Loss: 0.5008, lr 7.810416889260659e-05
Step 211300 Loss: 0.4968, lr 7.810416889260659e-05
Step 211400 Loss: 0.4927, lr 7.810416889260659e-05
Step 211500 Loss: 0.4879, lr 7.810416889260659e-05
Step 211600 Loss: 0.4849, lr 7.810416889260659e-05
Step 211700 Loss: 0.4814, lr 7.810416889260659e-05
Step 211800 Loss: 0.4781, lr 7.810416889260659e-05
Step 211900 Loss: 0.4743, lr 7.810416889260659e-05
Step 212000 Loss: 0.4704, lr 7.810416889260659e-05
Step 212100 Loss: 0.4670, lr 7.810416889260659e-05
Step 212200 Loss: 0.4651, lr 7.810416889260659e-05
Step 212300 Loss: 0.4622, lr 7.810416889260659e-05
Step 212400 Loss: 0.4599, lr 7.810416889260659e-05
Step 212500 Loss: 0.4576, lr 7.810416889260659e-05
Step 212600 Loss: 0.4549, lr 7.810416889260659e-05
Step 212700 Loss: 0.4532, lr 7.810416889260659e-05
Step 212800 Loss: 0.4505, lr 7.810416889260659e-05
Step 212900 Loss: 0.4482, lr 7.810416889260659e-05
Step 213000 Loss: 0.4456, lr 7.810416889260659e-05
Step 213100 Loss: 0.4438, lr 7.810416889260659e-05
Step 213200 Loss: 0.4413, lr 7.810416889260659e-05
Step 213300 Loss: 0.4396, lr 7.810416889260659e-05
Step 213400 Loss: 0.4379, lr 7.810416889260659e-05
Step 213500 Loss: 0.4365, lr 7.810416889260659e-05
Step 213600 Loss: 0.4350, lr 7.810416889260659e-05
Step 213700 Loss: 0.4336, lr 7.810416889260659e-05
Step 213800 Loss: 0.4322, lr 7.810416889260659e-05
Step 213900 Loss: 0.4311, lr 7.810416889260659e-05
Step 214000 Loss: 0.4295, lr 7.810416889260659e-05
Step 214100 Loss: 0.4281, lr 7.810416889260659e-05
Step 214200 Loss: 0.4268, lr 7.810416889260659e-05
Step 214300 Loss: 0.4263, lr 7.810416889260659e-05
Step 214400 Loss: 0.4255, lr 7.810416889260659e-05
Step 214500 Loss: 0.4249, lr 7.810416889260659e-05
Step 214600 Loss: 0.4245, lr 7.810416889260659e-05
Step 214700 Loss: 0.4240, lr 7.810416889260659e-05
Step 214800 Loss: 0.4234, lr 7.810416889260659e-05
Step 214900 Loss: 0.4230, lr 7.810416889260659e-05
Step 215000 Loss: 0.4226, lr 7.810416889260659e-05
Step 215100 Loss: 0.4223, lr 7.810416889260659e-05
Step 215200 Loss: 0.4225, lr 7.810416889260659e-05
Step 215300 Loss: 0.4231, lr 7.810416889260659e-05
Step 215400 Loss: 0.4234, lr 7.810416889260659e-05
Step 215500 Loss: 0.4231, lr 7.810416889260659e-05
Step 215600 Loss: 0.4240, lr 7.810416889260659e-05
Step 215700 Loss: 0.4239, lr 7.810416889260659e-05
Step 215800 Loss: 0.4240, lr 7.810416889260659e-05
Step 215900 Loss: 0.4234, lr 7.810416889260659e-05
Step 216000 Loss: 0.4227, lr 7.810416889260659e-05
Step 216100 Loss: 0.4209, lr 7.810416889260659e-05
Train Epoch: [32/100] Loss: 0.4202,lr 0.000078
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6756
Step 216200 Loss: 1.2893, lr 7.679133974894988e-05
Step 216300 Loss: 0.7821, lr 7.679133974894988e-05
Step 216400 Loss: 0.6917, lr 7.679133974894988e-05
Step 216500 Loss: 0.6444, lr 7.679133974894988e-05
Step 216600 Loss: 0.6186, lr 7.679133974894988e-05
Step 216700 Loss: 0.5986, lr 7.679133974894988e-05
Step 216800 Loss: 0.5793, lr 7.679133974894988e-05
Step 216900 Loss: 0.5659, lr 7.679133974894988e-05
Step 217000 Loss: 0.5535, lr 7.679133974894988e-05
Step 217100 Loss: 0.5414, lr 7.679133974894988e-05
Step 217200 Loss: 0.5313, lr 7.679133974894988e-05
Step 217300 Loss: 0.5246, lr 7.679133974894988e-05
Step 217400 Loss: 0.5159, lr 7.679133974894988e-05
Step 217500 Loss: 0.5093, lr 7.679133974894988e-05
Step 217600 Loss: 0.5026, lr 7.679133974894988e-05
Step 217700 Loss: 0.4969, lr 7.679133974894988e-05
Step 217800 Loss: 0.4913, lr 7.679133974894988e-05
Step 217900 Loss: 0.4871, lr 7.679133974894988e-05
Step 218000 Loss: 0.4827, lr 7.679133974894988e-05
Step 218100 Loss: 0.4783, lr 7.679133974894988e-05
Step 218200 Loss: 0.4737, lr 7.679133974894988e-05
Step 218300 Loss: 0.4696, lr 7.679133974894988e-05
Step 218400 Loss: 0.4658, lr 7.679133974894988e-05
Step 218500 Loss: 0.4625, lr 7.679133974894988e-05
Step 218600 Loss: 0.4595, lr 7.679133974894988e-05
Step 218700 Loss: 0.4553, lr 7.679133974894988e-05
Step 218800 Loss: 0.4518, lr 7.679133974894988e-05
Step 218900 Loss: 0.4490, lr 7.679133974894988e-05
Step 219000 Loss: 0.4468, lr 7.679133974894988e-05
Step 219100 Loss: 0.4439, lr 7.679133974894988e-05
Step 219200 Loss: 0.4418, lr 7.679133974894988e-05
Step 219300 Loss: 0.4389, lr 7.679133974894988e-05
Step 219400 Loss: 0.4369, lr 7.679133974894988e-05
Step 219500 Loss: 0.4342, lr 7.679133974894988e-05
Step 219600 Loss: 0.4318, lr 7.679133974894988e-05
Step 219700 Loss: 0.4291, lr 7.679133974894988e-05
Step 219800 Loss: 0.4268, lr 7.679133974894988e-05
Step 219900 Loss: 0.4244, lr 7.679133974894988e-05
Step 220000 Loss: 0.4224, lr 7.679133974894988e-05
Step 220100 Loss: 0.4204, lr 7.679133974894988e-05
Step 220200 Loss: 0.4187, lr 7.679133974894988e-05
Step 220300 Loss: 0.4172, lr 7.679133974894988e-05
Step 220400 Loss: 0.4163, lr 7.679133974894988e-05
Step 220500 Loss: 0.4150, lr 7.679133974894988e-05
Step 220600 Loss: 0.4139, lr 7.679133974894988e-05
Step 220700 Loss: 0.4130, lr 7.679133974894988e-05
Step 220800 Loss: 0.4116, lr 7.679133974894988e-05
Step 220900 Loss: 0.4099, lr 7.679133974894988e-05
Step 221000 Loss: 0.4089, lr 7.679133974894988e-05
Step 221100 Loss: 0.4081, lr 7.679133974894988e-05
Step 221200 Loss: 0.4074, lr 7.679133974894988e-05
Step 221300 Loss: 0.4069, lr 7.679133974894988e-05
Step 221400 Loss: 0.4066, lr 7.679133974894988e-05
Step 221500 Loss: 0.4060, lr 7.679133974894988e-05
Step 221600 Loss: 0.4055, lr 7.679133974894988e-05
Step 221700 Loss: 0.4050, lr 7.679133974894988e-05
Step 221800 Loss: 0.4049, lr 7.679133974894988e-05
Step 221900 Loss: 0.4048, lr 7.679133974894988e-05
Step 222000 Loss: 0.4048, lr 7.679133974894988e-05
Step 222100 Loss: 0.4058, lr 7.679133974894988e-05
Step 222200 Loss: 0.4058, lr 7.679133974894988e-05
Step 222300 Loss: 0.4067, lr 7.679133974894988e-05
Step 222400 Loss: 0.4075, lr 7.679133974894988e-05
Step 222500 Loss: 0.4078, lr 7.679133974894988e-05
Step 222600 Loss: 0.4074, lr 7.679133974894988e-05
Step 222700 Loss: 0.4071, lr 7.679133974894988e-05
Step 222800 Loss: 0.4058, lr 7.679133974894988e-05
Step 222900 Loss: 0.4050, lr 7.679133974894988e-05
Train Epoch: [33/100] Loss: 0.4051,lr 0.000077
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6755
Step 223000 Loss: 0.8982, lr 7.545207078751862e-05
Step 223100 Loss: 0.7301, lr 7.545207078751862e-05
Step 223200 Loss: 0.6712, lr 7.545207078751862e-05
Step 223300 Loss: 0.6298, lr 7.545207078751862e-05
Step 223400 Loss: 0.6104, lr 7.545207078751862e-05
Step 223500 Loss: 0.5917, lr 7.545207078751862e-05
Step 223600 Loss: 0.5732, lr 7.545207078751862e-05
Step 223700 Loss: 0.5595, lr 7.545207078751862e-05
Step 223800 Loss: 0.5476, lr 7.545207078751862e-05
Step 223900 Loss: 0.5367, lr 7.545207078751862e-05
Step 224000 Loss: 0.5288, lr 7.545207078751862e-05
Step 224100 Loss: 0.5201, lr 7.545207078751862e-05
Step 224200 Loss: 0.5128, lr 7.545207078751862e-05
Step 224300 Loss: 0.5056, lr 7.545207078751862e-05
Step 224400 Loss: 0.4996, lr 7.545207078751862e-05
Step 224500 Loss: 0.4945, lr 7.545207078751862e-05
Step 224600 Loss: 0.4887, lr 7.545207078751862e-05
Step 224700 Loss: 0.4847, lr 7.545207078751862e-05
Step 224800 Loss: 0.4806, lr 7.545207078751862e-05
Step 224900 Loss: 0.4756, lr 7.545207078751862e-05
Step 225000 Loss: 0.4708, lr 7.545207078751862e-05
Step 225100 Loss: 0.4674, lr 7.545207078751862e-05
Step 225200 Loss: 0.4642, lr 7.545207078751862e-05
Step 225300 Loss: 0.4606, lr 7.545207078751862e-05
Step 225400 Loss: 0.4567, lr 7.545207078751862e-05
Step 225500 Loss: 0.4526, lr 7.545207078751862e-05
Step 225600 Loss: 0.4492, lr 7.545207078751862e-05
Step 225700 Loss: 0.4473, lr 7.545207078751862e-05
Step 225800 Loss: 0.4442, lr 7.545207078751862e-05
Step 225900 Loss: 0.4419, lr 7.545207078751862e-05
Step 226000 Loss: 0.4393, lr 7.545207078751862e-05
Step 226100 Loss: 0.4365, lr 7.545207078751862e-05
Step 226200 Loss: 0.4345, lr 7.545207078751862e-05
Step 226300 Loss: 0.4315, lr 7.545207078751862e-05
Step 226400 Loss: 0.4293, lr 7.545207078751862e-05
Step 226500 Loss: 0.4267, lr 7.545207078751862e-05
Step 226600 Loss: 0.4245, lr 7.545207078751862e-05
Step 226700 Loss: 0.4218, lr 7.545207078751862e-05
Step 226800 Loss: 0.4199, lr 7.545207078751862e-05
Step 226900 Loss: 0.4178, lr 7.545207078751862e-05
Step 227000 Loss: 0.4161, lr 7.545207078751862e-05
Step 227100 Loss: 0.4146, lr 7.545207078751862e-05
Step 227200 Loss: 0.4135, lr 7.545207078751862e-05
Step 227300 Loss: 0.4121, lr 7.545207078751862e-05
Step 227400 Loss: 0.4110, lr 7.545207078751862e-05
Step 227500 Loss: 0.4096, lr 7.545207078751862e-05
Step 227600 Loss: 0.4081, lr 7.545207078751862e-05
Step 227700 Loss: 0.4069, lr 7.545207078751862e-05
Step 227800 Loss: 0.4061, lr 7.545207078751862e-05
Step 227900 Loss: 0.4053, lr 7.545207078751862e-05
Step 228000 Loss: 0.4047, lr 7.545207078751862e-05
Step 228100 Loss: 0.4043, lr 7.545207078751862e-05
Step 228200 Loss: 0.4038, lr 7.545207078751862e-05
Step 228300 Loss: 0.4037, lr 7.545207078751862e-05
Step 228400 Loss: 0.4033, lr 7.545207078751862e-05
Step 228500 Loss: 0.4030, lr 7.545207078751862e-05
Step 228600 Loss: 0.4027, lr 7.545207078751862e-05
Step 228700 Loss: 0.4027, lr 7.545207078751862e-05
Step 228800 Loss: 0.4033, lr 7.545207078751862e-05
Step 228900 Loss: 0.4036, lr 7.545207078751862e-05
Step 229000 Loss: 0.4036, lr 7.545207078751862e-05
Step 229100 Loss: 0.4044, lr 7.545207078751862e-05
Step 229200 Loss: 0.4046, lr 7.545207078751862e-05
Step 229300 Loss: 0.4043, lr 7.545207078751862e-05
Step 229400 Loss: 0.4037, lr 7.545207078751862e-05
Step 229500 Loss: 0.4032, lr 7.545207078751862e-05
Step 229600 Loss: 0.4020, lr 7.545207078751862e-05
Train Epoch: [34/100] Loss: 0.4015,lr 0.000075
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6755
Step 229700 Loss: 1.7860, lr 7.408768370508582e-05
Step 229800 Loss: 0.7885, lr 7.408768370508582e-05
Step 229900 Loss: 0.6935, lr 7.408768370508582e-05
Step 230000 Loss: 0.6426, lr 7.408768370508582e-05
Step 230100 Loss: 0.6147, lr 7.408768370508582e-05
Step 230200 Loss: 0.5953, lr 7.408768370508582e-05
Step 230300 Loss: 0.5767, lr 7.408768370508582e-05
Step 230400 Loss: 0.5635, lr 7.408768370508582e-05
Step 230500 Loss: 0.5502, lr 7.408768370508582e-05
Step 230600 Loss: 0.5391, lr 7.408768370508582e-05
Step 230700 Loss: 0.5288, lr 7.408768370508582e-05
Step 230800 Loss: 0.5219, lr 7.408768370508582e-05
Step 230900 Loss: 0.5136, lr 7.408768370508582e-05
Step 231000 Loss: 0.5074, lr 7.408768370508582e-05
Step 231100 Loss: 0.5008, lr 7.408768370508582e-05
Step 231200 Loss: 0.4950, lr 7.408768370508582e-05
Step 231300 Loss: 0.4899, lr 7.408768370508582e-05
Step 231400 Loss: 0.4854, lr 7.408768370508582e-05
Step 231500 Loss: 0.4813, lr 7.408768370508582e-05
Step 231600 Loss: 0.4769, lr 7.408768370508582e-05
Step 231700 Loss: 0.4722, lr 7.408768370508582e-05
Step 231800 Loss: 0.4674, lr 7.408768370508582e-05
Step 231900 Loss: 0.4639, lr 7.408768370508582e-05
Step 232000 Loss: 0.4603, lr 7.408768370508582e-05
Step 232100 Loss: 0.4574, lr 7.408768370508582e-05
Step 232200 Loss: 0.4533, lr 7.408768370508582e-05
Step 232300 Loss: 0.4493, lr 7.408768370508582e-05
Step 232400 Loss: 0.4463, lr 7.408768370508582e-05
Step 232500 Loss: 0.4443, lr 7.408768370508582e-05
Step 232600 Loss: 0.4413, lr 7.408768370508582e-05
Step 232700 Loss: 0.4390, lr 7.408768370508582e-05
Step 232800 Loss: 0.4360, lr 7.408768370508582e-05
Step 232900 Loss: 0.4337, lr 7.408768370508582e-05
Step 233000 Loss: 0.4312, lr 7.408768370508582e-05
Step 233100 Loss: 0.4287, lr 7.408768370508582e-05
Step 233200 Loss: 0.4261, lr 7.408768370508582e-05
Step 233300 Loss: 0.4234, lr 7.408768370508582e-05
Step 233400 Loss: 0.4212, lr 7.408768370508582e-05
Step 233500 Loss: 0.4192, lr 7.408768370508582e-05
Step 233600 Loss: 0.4170, lr 7.408768370508582e-05
Step 233700 Loss: 0.4152, lr 7.408768370508582e-05
Step 233800 Loss: 0.4139, lr 7.408768370508582e-05
Step 233900 Loss: 0.4126, lr 7.408768370508582e-05
Step 234000 Loss: 0.4112, lr 7.408768370508582e-05
Step 234100 Loss: 0.4100, lr 7.408768370508582e-05
Step 234200 Loss: 0.4089, lr 7.408768370508582e-05
Step 234300 Loss: 0.4080, lr 7.408768370508582e-05
Step 234400 Loss: 0.4071, lr 7.408768370508582e-05
Step 234500 Loss: 0.4066, lr 7.408768370508582e-05
Step 234600 Loss: 0.4064, lr 7.408768370508582e-05
Step 234700 Loss: 0.4065, lr 7.408768370508582e-05
Step 234800 Loss: 0.4065, lr 7.408768370508582e-05
Step 234900 Loss: 0.4063, lr 7.408768370508582e-05
Step 235000 Loss: 0.4062, lr 7.408768370508582e-05
Step 235100 Loss: 0.4060, lr 7.408768370508582e-05
Step 235200 Loss: 0.4061, lr 7.408768370508582e-05
Step 235300 Loss: 0.4061, lr 7.408768370508582e-05
Step 235400 Loss: 0.4063, lr 7.408768370508582e-05
Step 235500 Loss: 0.4066, lr 7.408768370508582e-05
Step 235600 Loss: 0.4074, lr 7.408768370508582e-05
Step 235700 Loss: 0.4072, lr 7.408768370508582e-05
Step 235800 Loss: 0.4079, lr 7.408768370508582e-05
Step 235900 Loss: 0.4084, lr 7.408768370508582e-05
Step 236000 Loss: 0.4081, lr 7.408768370508582e-05
Step 236100 Loss: 0.4082, lr 7.408768370508582e-05
Step 236200 Loss: 0.4077, lr 7.408768370508582e-05
Step 236300 Loss: 0.4069, lr 7.408768370508582e-05
Step 236400 Loss: 0.4056, lr 7.408768370508582e-05
Train Epoch: [35/100] Loss: 0.4056,lr 0.000074
Model Saving at epoch 35
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6755
Step 236500 Loss: 1.1080, lr 7.26995249869774e-05
Step 236600 Loss: 0.8923, lr 7.26995249869774e-05
Step 236700 Loss: 0.8234, lr 7.26995249869774e-05
Step 236800 Loss: 0.7774, lr 7.26995249869774e-05
Step 236900 Loss: 0.7494, lr 7.26995249869774e-05
Step 237000 Loss: 0.7274, lr 7.26995249869774e-05
Step 237100 Loss: 0.7043, lr 7.26995249869774e-05
Step 237200 Loss: 0.6858, lr 7.26995249869774e-05
Step 237300 Loss: 0.6706, lr 7.26995249869774e-05
Step 237400 Loss: 0.6560, lr 7.26995249869774e-05
Step 237500 Loss: 0.6452, lr 7.26995249869774e-05
Step 237600 Loss: 0.6341, lr 7.26995249869774e-05
Step 237700 Loss: 0.6244, lr 7.26995249869774e-05
Step 237800 Loss: 0.6153, lr 7.26995249869774e-05
Step 237900 Loss: 0.6063, lr 7.26995249869774e-05
Step 238000 Loss: 0.5992, lr 7.26995249869774e-05
Step 238100 Loss: 0.5919, lr 7.26995249869774e-05
Step 238200 Loss: 0.5855, lr 7.26995249869774e-05
Step 238300 Loss: 0.5795, lr 7.26995249869774e-05
Step 238400 Loss: 0.5732, lr 7.26995249869774e-05
Step 238500 Loss: 0.5674, lr 7.26995249869774e-05
Step 238600 Loss: 0.5622, lr 7.26995249869774e-05
Step 238700 Loss: 0.5567, lr 7.26995249869774e-05
Step 238800 Loss: 0.5517, lr 7.26995249869774e-05
Step 238900 Loss: 0.5467, lr 7.26995249869774e-05
Step 239000 Loss: 0.5411, lr 7.26995249869774e-05
Step 239100 Loss: 0.5366, lr 7.26995249869774e-05
Step 239200 Loss: 0.5327, lr 7.26995249869774e-05
Step 239300 Loss: 0.5290, lr 7.26995249869774e-05
Step 239400 Loss: 0.5254, lr 7.26995249869774e-05
Step 239500 Loss: 0.5217, lr 7.26995249869774e-05
Step 239600 Loss: 0.5177, lr 7.26995249869774e-05
Step 239700 Loss: 0.5142, lr 7.26995249869774e-05
Step 239800 Loss: 0.5104, lr 7.26995249869774e-05
Step 239900 Loss: 0.5069, lr 7.26995249869774e-05
Step 240000 Loss: 0.5032, lr 7.26995249869774e-05
Step 240100 Loss: 0.4997, lr 7.26995249869774e-05
Step 240200 Loss: 0.4961, lr 7.26995249869774e-05
Step 240300 Loss: 0.4933, lr 7.26995249869774e-05
Step 240400 Loss: 0.4903, lr 7.26995249869774e-05
Step 240500 Loss: 0.4876, lr 7.26995249869774e-05
Step 240600 Loss: 0.4850, lr 7.26995249869774e-05
Step 240700 Loss: 0.4826, lr 7.26995249869774e-05
Step 240800 Loss: 0.4799, lr 7.26995249869774e-05
Step 240900 Loss: 0.4776, lr 7.26995249869774e-05
Step 241000 Loss: 0.4752, lr 7.26995249869774e-05
Step 241100 Loss: 0.4731, lr 7.26995249869774e-05
Step 241200 Loss: 0.4707, lr 7.26995249869774e-05
Step 241300 Loss: 0.4693, lr 7.26995249869774e-05
Step 241400 Loss: 0.4678, lr 7.26995249869774e-05
Step 241500 Loss: 0.4662, lr 7.26995249869774e-05
Step 241600 Loss: 0.4648, lr 7.26995249869774e-05
Step 241700 Loss: 0.4632, lr 7.26995249869774e-05
Step 241800 Loss: 0.4620, lr 7.26995249869774e-05
Step 241900 Loss: 0.4603, lr 7.26995249869774e-05
Step 242000 Loss: 0.4592, lr 7.26995249869774e-05
Step 242100 Loss: 0.4583, lr 7.26995249869774e-05
Step 242200 Loss: 0.4575, lr 7.26995249869774e-05
Step 242300 Loss: 0.4571, lr 7.26995249869774e-05
Step 242400 Loss: 0.4568, lr 7.26995249869774e-05
Step 242500 Loss: 0.4562, lr 7.26995249869774e-05
Step 242600 Loss: 0.4560, lr 7.26995249869774e-05
Step 242700 Loss: 0.4553, lr 7.26995249869774e-05
Step 242800 Loss: 0.4547, lr 7.26995249869774e-05
Step 242900 Loss: 0.4533, lr 7.26995249869774e-05
Step 243000 Loss: 0.4520, lr 7.26995249869774e-05
Step 243100 Loss: 0.4503, lr 7.26995249869774e-05
Step 243200 Loss: 0.4488, lr 7.26995249869774e-05
Train Epoch: [36/100] Loss: 0.4488,lr 0.000073
Calling G2SDataset.batch()
Done, time:  2.31 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6755
Step 243300 Loss: 1.0531, lr 7.12889645782537e-05
Step 243400 Loss: 0.9261, lr 7.12889645782537e-05
Step 243500 Loss: 0.8518, lr 7.12889645782537e-05
Step 243600 Loss: 0.8159, lr 7.12889645782537e-05
Step 243700 Loss: 0.7885, lr 7.12889645782537e-05
Step 243800 Loss: 0.7632, lr 7.12889645782537e-05
Step 243900 Loss: 0.7412, lr 7.12889645782537e-05
Step 244000 Loss: 0.7227, lr 7.12889645782537e-05
Step 244100 Loss: 0.7052, lr 7.12889645782537e-05
Step 244200 Loss: 0.6908, lr 7.12889645782537e-05
Step 244300 Loss: 0.6790, lr 7.12889645782537e-05
Step 244400 Loss: 0.6660, lr 7.12889645782537e-05
Step 244500 Loss: 0.6557, lr 7.12889645782537e-05
Step 244600 Loss: 0.6446, lr 7.12889645782537e-05
Step 244700 Loss: 0.6350, lr 7.12889645782537e-05
Step 244800 Loss: 0.6261, lr 7.12889645782537e-05
Step 244900 Loss: 0.6179, lr 7.12889645782537e-05
Step 245000 Loss: 0.6104, lr 7.12889645782537e-05
Step 245100 Loss: 0.6031, lr 7.12889645782537e-05
Step 245200 Loss: 0.5963, lr 7.12889645782537e-05
Step 245300 Loss: 0.5898, lr 7.12889645782537e-05
Step 245400 Loss: 0.5844, lr 7.12889645782537e-05
Step 245500 Loss: 0.5783, lr 7.12889645782537e-05
Step 245600 Loss: 0.5730, lr 7.12889645782537e-05
Step 245700 Loss: 0.5663, lr 7.12889645782537e-05
Step 245800 Loss: 0.5604, lr 7.12889645782537e-05
Step 245900 Loss: 0.5555, lr 7.12889645782537e-05
Step 246000 Loss: 0.5516, lr 7.12889645782537e-05
Step 246100 Loss: 0.5466, lr 7.12889645782537e-05
Step 246200 Loss: 0.5420, lr 7.12889645782537e-05
Step 246300 Loss: 0.5375, lr 7.12889645782537e-05
Step 246400 Loss: 0.5329, lr 7.12889645782537e-05
Step 246500 Loss: 0.5289, lr 7.12889645782537e-05
Step 246600 Loss: 0.5250, lr 7.12889645782537e-05
Step 246700 Loss: 0.5208, lr 7.12889645782537e-05
Step 246800 Loss: 0.5167, lr 7.12889645782537e-05
Step 246900 Loss: 0.5130, lr 7.12889645782537e-05
Step 247000 Loss: 0.5096, lr 7.12889645782537e-05
Step 247100 Loss: 0.5062, lr 7.12889645782537e-05
Step 247200 Loss: 0.5031, lr 7.12889645782537e-05
Step 247300 Loss: 0.5001, lr 7.12889645782537e-05
Step 247400 Loss: 0.4975, lr 7.12889645782537e-05
Step 247500 Loss: 0.4943, lr 7.12889645782537e-05
Step 247600 Loss: 0.4917, lr 7.12889645782537e-05
Step 247700 Loss: 0.4890, lr 7.12889645782537e-05
Step 247800 Loss: 0.4861, lr 7.12889645782537e-05
Step 247900 Loss: 0.4836, lr 7.12889645782537e-05
Step 248000 Loss: 0.4814, lr 7.12889645782537e-05
Step 248100 Loss: 0.4793, lr 7.12889645782537e-05
Step 248200 Loss: 0.4775, lr 7.12889645782537e-05
Step 248300 Loss: 0.4757, lr 7.12889645782537e-05
Step 248400 Loss: 0.4741, lr 7.12889645782537e-05
Step 248500 Loss: 0.4722, lr 7.12889645782537e-05
Step 248600 Loss: 0.4707, lr 7.12889645782537e-05
Step 248700 Loss: 0.4693, lr 7.12889645782537e-05
Step 248800 Loss: 0.4679, lr 7.12889645782537e-05
Step 248900 Loss: 0.4668, lr 7.12889645782537e-05
Step 249000 Loss: 0.4658, lr 7.12889645782537e-05
Step 249100 Loss: 0.4655, lr 7.12889645782537e-05
Step 249200 Loss: 0.4648, lr 7.12889645782537e-05
Step 249300 Loss: 0.4645, lr 7.12889645782537e-05
Step 249400 Loss: 0.4642, lr 7.12889645782537e-05
Step 249500 Loss: 0.4629, lr 7.12889645782537e-05
Step 249600 Loss: 0.4619, lr 7.12889645782537e-05
Step 249700 Loss: 0.4600, lr 7.12889645782537e-05
Step 249800 Loss: 0.4579, lr 7.12889645782537e-05
Step 249900 Loss: 0.4556, lr 7.12889645782537e-05
Train Epoch: [37/100] Loss: 0.4547,lr 0.000071
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6756
Step 250000 Loss: 1.2422, lr 6.985739453173908e-05
Step 250100 Loss: 0.9695, lr 6.985739453173908e-05
Step 250200 Loss: 0.8774, lr 6.985739453173908e-05
Step 250300 Loss: 0.8261, lr 6.985739453173908e-05
Step 250400 Loss: 0.7928, lr 6.985739453173908e-05
Step 250500 Loss: 0.7699, lr 6.985739453173908e-05
Step 250600 Loss: 0.7439, lr 6.985739453173908e-05
Step 250700 Loss: 0.7211, lr 6.985739453173908e-05
Step 250800 Loss: 0.7043, lr 6.985739453173908e-05
Step 250900 Loss: 0.6878, lr 6.985739453173908e-05
Step 251000 Loss: 0.6740, lr 6.985739453173908e-05
Step 251100 Loss: 0.6626, lr 6.985739453173908e-05
Step 251200 Loss: 0.6503, lr 6.985739453173908e-05
Step 251300 Loss: 0.6398, lr 6.985739453173908e-05
Step 251400 Loss: 0.6296, lr 6.985739453173908e-05
Step 251500 Loss: 0.6204, lr 6.985739453173908e-05
Step 251600 Loss: 0.6112, lr 6.985739453173908e-05
Step 251700 Loss: 0.6043, lr 6.985739453173908e-05
Step 251800 Loss: 0.5967, lr 6.985739453173908e-05
Step 251900 Loss: 0.5898, lr 6.985739453173908e-05
Step 252000 Loss: 0.5832, lr 6.985739453173908e-05
Step 252100 Loss: 0.5774, lr 6.985739453173908e-05
Step 252200 Loss: 0.5719, lr 6.985739453173908e-05
Step 252300 Loss: 0.5663, lr 6.985739453173908e-05
Step 252400 Loss: 0.5608, lr 6.985739453173908e-05
Step 252500 Loss: 0.5551, lr 6.985739453173908e-05
Step 252600 Loss: 0.5502, lr 6.985739453173908e-05
Step 252700 Loss: 0.5461, lr 6.985739453173908e-05
Step 252800 Loss: 0.5418, lr 6.985739453173908e-05
Step 252900 Loss: 0.5377, lr 6.985739453173908e-05
Step 253000 Loss: 0.5338, lr 6.985739453173908e-05
Step 253100 Loss: 0.5300, lr 6.985739453173908e-05
Step 253200 Loss: 0.5264, lr 6.985739453173908e-05
Step 253300 Loss: 0.5227, lr 6.985739453173908e-05
Step 253400 Loss: 0.5189, lr 6.985739453173908e-05
Step 253500 Loss: 0.5155, lr 6.985739453173908e-05
Step 253600 Loss: 0.5125, lr 6.985739453173908e-05
Step 253700 Loss: 0.5090, lr 6.985739453173908e-05
Step 253800 Loss: 0.5065, lr 6.985739453173908e-05
Step 253900 Loss: 0.5040, lr 6.985739453173908e-05
Step 254000 Loss: 0.5020, lr 6.985739453173908e-05
Step 254100 Loss: 0.5002, lr 6.985739453173908e-05
Step 254200 Loss: 0.4985, lr 6.985739453173908e-05
Step 254300 Loss: 0.4966, lr 6.985739453173908e-05
Step 254400 Loss: 0.4955, lr 6.985739453173908e-05
Step 254500 Loss: 0.4948, lr 6.985739453173908e-05
Step 254600 Loss: 0.4936, lr 6.985739453173908e-05
Step 254700 Loss: 0.4907, lr 6.985739453173908e-05
Step 254800 Loss: 0.4885, lr 6.985739453173908e-05
Step 254900 Loss: 0.4865, lr 6.985739453173908e-05
Step 255000 Loss: 0.4846, lr 6.985739453173908e-05
Step 255100 Loss: 0.4826, lr 6.985739453173908e-05
Step 255200 Loss: 0.4808, lr 6.985739453173908e-05
Step 255300 Loss: 0.4791, lr 6.985739453173908e-05
Step 255400 Loss: 0.4775, lr 6.985739453173908e-05
Step 255500 Loss: 0.4762, lr 6.985739453173908e-05
Step 255600 Loss: 0.4750, lr 6.985739453173908e-05
Step 255700 Loss: 0.4738, lr 6.985739453173908e-05
Step 255800 Loss: 0.4729, lr 6.985739453173908e-05
Step 255900 Loss: 0.4723, lr 6.985739453173908e-05
Step 256000 Loss: 0.4711, lr 6.985739453173908e-05
Step 256100 Loss: 0.4706, lr 6.985739453173908e-05
Step 256200 Loss: 0.4699, lr 6.985739453173908e-05
Step 256300 Loss: 0.4687, lr 6.985739453173908e-05
Step 256400 Loss: 0.4669, lr 6.985739453173908e-05
Step 256500 Loss: 0.4653, lr 6.985739453173908e-05
Step 256600 Loss: 0.4629, lr 6.985739453173908e-05
Step 256700 Loss: 0.4612, lr 6.985739453173908e-05
Train Epoch: [38/100] Loss: 0.4611,lr 0.000070
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 256800 Loss: 0.9712, lr 6.840622763423397e-05
Step 256900 Loss: 0.8476, lr 6.840622763423397e-05
Step 257000 Loss: 0.7817, lr 6.840622763423397e-05
Step 257100 Loss: 0.7469, lr 6.840622763423397e-05
Step 257200 Loss: 0.7207, lr 6.840622763423397e-05
Step 257300 Loss: 0.6987, lr 6.840622763423397e-05
Step 257400 Loss: 0.6781, lr 6.840622763423397e-05
Step 257500 Loss: 0.6616, lr 6.840622763423397e-05
Step 257600 Loss: 0.6464, lr 6.840622763423397e-05
Step 257700 Loss: 0.6329, lr 6.840622763423397e-05
Step 257800 Loss: 0.6223, lr 6.840622763423397e-05
Step 257900 Loss: 0.6108, lr 6.840622763423397e-05
Step 258000 Loss: 0.6025, lr 6.840622763423397e-05
Step 258100 Loss: 0.5933, lr 6.840622763423397e-05
Step 258200 Loss: 0.5843, lr 6.840622763423397e-05
Step 258300 Loss: 0.5761, lr 6.840622763423397e-05
Step 258400 Loss: 0.5670, lr 6.840622763423397e-05
Step 258500 Loss: 0.5594, lr 6.840622763423397e-05
Step 258600 Loss: 0.5512, lr 6.840622763423397e-05
Step 258700 Loss: 0.5436, lr 6.840622763423397e-05
Step 258800 Loss: 0.5353, lr 6.840622763423397e-05
Step 258900 Loss: 0.5291, lr 6.840622763423397e-05
Step 259000 Loss: 0.5231, lr 6.840622763423397e-05
Step 259100 Loss: 0.5181, lr 6.840622763423397e-05
Step 259200 Loss: 0.5118, lr 6.840622763423397e-05
Step 259300 Loss: 0.5061, lr 6.840622763423397e-05
Step 259400 Loss: 0.5011, lr 6.840622763423397e-05
Step 259500 Loss: 0.4971, lr 6.840622763423397e-05
Step 259600 Loss: 0.4929, lr 6.840622763423397e-05
Step 259700 Loss: 0.4889, lr 6.840622763423397e-05
Step 259800 Loss: 0.4848, lr 6.840622763423397e-05
Step 259900 Loss: 0.4807, lr 6.840622763423397e-05
Step 260000 Loss: 0.4771, lr 6.840622763423397e-05
Step 260100 Loss: 0.4731, lr 6.840622763423397e-05
Step 260200 Loss: 0.4692, lr 6.840622763423397e-05
Step 260300 Loss: 0.4653, lr 6.840622763423397e-05
Step 260400 Loss: 0.4622, lr 6.840622763423397e-05
Step 260500 Loss: 0.4589, lr 6.840622763423397e-05
Step 260600 Loss: 0.4560, lr 6.840622763423397e-05
Step 260700 Loss: 0.4533, lr 6.840622763423397e-05
Step 260800 Loss: 0.4509, lr 6.840622763423397e-05
Step 260900 Loss: 0.4487, lr 6.840622763423397e-05
Step 261000 Loss: 0.4462, lr 6.840622763423397e-05
Step 261100 Loss: 0.4444, lr 6.840622763423397e-05
Step 261200 Loss: 0.4423, lr 6.840622763423397e-05
Step 261300 Loss: 0.4399, lr 6.840622763423397e-05
Step 261400 Loss: 0.4379, lr 6.840622763423397e-05
Step 261500 Loss: 0.4361, lr 6.840622763423397e-05
Step 261600 Loss: 0.4350, lr 6.840622763423397e-05
Step 261700 Loss: 0.4340, lr 6.840622763423397e-05
Step 261800 Loss: 0.4326, lr 6.840622763423397e-05
Step 261900 Loss: 0.4318, lr 6.840622763423397e-05
Step 262000 Loss: 0.4307, lr 6.840622763423397e-05
Step 262100 Loss: 0.4297, lr 6.840622763423397e-05
Step 262200 Loss: 0.4289, lr 6.840622763423397e-05
Step 262300 Loss: 0.4279, lr 6.840622763423397e-05
Step 262400 Loss: 0.4274, lr 6.840622763423397e-05
Step 262500 Loss: 0.4270, lr 6.840622763423397e-05
Step 262600 Loss: 0.4273, lr 6.840622763423397e-05
Step 262700 Loss: 0.4268, lr 6.840622763423397e-05
Step 262800 Loss: 0.4266, lr 6.840622763423397e-05
Step 262900 Loss: 0.4267, lr 6.840622763423397e-05
Step 263000 Loss: 0.4259, lr 6.840622763423397e-05
Step 263100 Loss: 0.4254, lr 6.840622763423397e-05
Step 263200 Loss: 0.4243, lr 6.840622763423397e-05
Step 263300 Loss: 0.4234, lr 6.840622763423397e-05
Step 263400 Loss: 0.4219, lr 6.840622763423397e-05
Train Epoch: [39/100] Loss: 0.4214,lr 0.000068
Calling G2SDataset.batch()
Done, time:  1.92 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 263500 Loss: 1.1178, lr 6.693689601226464e-05
Step 263600 Loss: 0.7651, lr 6.693689601226464e-05
Step 263700 Loss: 0.6882, lr 6.693689601226464e-05
Step 263800 Loss: 0.6420, lr 6.693689601226464e-05
Step 263900 Loss: 0.6156, lr 6.693689601226464e-05
Step 264000 Loss: 0.5972, lr 6.693689601226464e-05
Step 264100 Loss: 0.5781, lr 6.693689601226464e-05
Step 264200 Loss: 0.5641, lr 6.693689601226464e-05
Step 264300 Loss: 0.5507, lr 6.693689601226464e-05
Step 264400 Loss: 0.5399, lr 6.693689601226464e-05
Step 264500 Loss: 0.5302, lr 6.693689601226464e-05
Step 264600 Loss: 0.5230, lr 6.693689601226464e-05
Step 264700 Loss: 0.5143, lr 6.693689601226464e-05
Step 264800 Loss: 0.5075, lr 6.693689601226464e-05
Step 264900 Loss: 0.5009, lr 6.693689601226464e-05
Step 265000 Loss: 0.4953, lr 6.693689601226464e-05
Step 265100 Loss: 0.4897, lr 6.693689601226464e-05
Step 265200 Loss: 0.4850, lr 6.693689601226464e-05
Step 265300 Loss: 0.4806, lr 6.693689601226464e-05
Step 265400 Loss: 0.4761, lr 6.693689601226464e-05
Step 265500 Loss: 0.4711, lr 6.693689601226464e-05
Step 265600 Loss: 0.4669, lr 6.693689601226464e-05
Step 265700 Loss: 0.4635, lr 6.693689601226464e-05
Step 265800 Loss: 0.4599, lr 6.693689601226464e-05
Step 265900 Loss: 0.4568, lr 6.693689601226464e-05
Step 266000 Loss: 0.4522, lr 6.693689601226464e-05
Step 266100 Loss: 0.4485, lr 6.693689601226464e-05
Step 266200 Loss: 0.4459, lr 6.693689601226464e-05
Step 266300 Loss: 0.4434, lr 6.693689601226464e-05
Step 266400 Loss: 0.4407, lr 6.693689601226464e-05
Step 266500 Loss: 0.4383, lr 6.693689601226464e-05
Step 266600 Loss: 0.4357, lr 6.693689601226464e-05
Step 266700 Loss: 0.4333, lr 6.693689601226464e-05
Step 266800 Loss: 0.4305, lr 6.693689601226464e-05
Step 266900 Loss: 0.4278, lr 6.693689601226464e-05
Step 267000 Loss: 0.4252, lr 6.693689601226464e-05
Step 267100 Loss: 0.4229, lr 6.693689601226464e-05
Step 267200 Loss: 0.4202, lr 6.693689601226464e-05
Step 267300 Loss: 0.4182, lr 6.693689601226464e-05
Step 267400 Loss: 0.4162, lr 6.693689601226464e-05
Step 267500 Loss: 0.4145, lr 6.693689601226464e-05
Step 267600 Loss: 0.4128, lr 6.693689601226464e-05
Step 267700 Loss: 0.4113, lr 6.693689601226464e-05
Step 267800 Loss: 0.4097, lr 6.693689601226464e-05
Step 267900 Loss: 0.4088, lr 6.693689601226464e-05
Step 268000 Loss: 0.4073, lr 6.693689601226464e-05
Step 268100 Loss: 0.4057, lr 6.693689601226464e-05
Step 268200 Loss: 0.4040, lr 6.693689601226464e-05
Step 268300 Loss: 0.4032, lr 6.693689601226464e-05
Step 268400 Loss: 0.4022, lr 6.693689601226464e-05
Step 268500 Loss: 0.4015, lr 6.693689601226464e-05
Step 268600 Loss: 0.4010, lr 6.693689601226464e-05
Step 268700 Loss: 0.4006, lr 6.693689601226464e-05
Step 268800 Loss: 0.4002, lr 6.693689601226464e-05
Step 268900 Loss: 0.3997, lr 6.693689601226464e-05
Step 269000 Loss: 0.3993, lr 6.693689601226464e-05
Step 269100 Loss: 0.3989, lr 6.693689601226464e-05
Step 269200 Loss: 0.3987, lr 6.693689601226464e-05
Step 269300 Loss: 0.3987, lr 6.693689601226464e-05
Step 269400 Loss: 0.3992, lr 6.693689601226464e-05
Step 269500 Loss: 0.3990, lr 6.693689601226464e-05
Step 269600 Loss: 0.3993, lr 6.693689601226464e-05
Step 269700 Loss: 0.3995, lr 6.693689601226464e-05
Step 269800 Loss: 0.3993, lr 6.693689601226464e-05
Step 269900 Loss: 0.3988, lr 6.693689601226464e-05
Step 270000 Loss: 0.3986, lr 6.693689601226464e-05
Step 270100 Loss: 0.3970, lr 6.693689601226464e-05
Step 270200 Loss: 0.3963, lr 6.693689601226464e-05
Train Epoch: [40/100] Loss: 0.3963,lr 0.000067
Model Saving at epoch 40
Calling G2SDataset.batch()
Done, time:  1.92 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Step 270300 Loss: 0.8484, lr 6.545084971874744e-05
Step 270400 Loss: 0.7122, lr 6.545084971874744e-05
Step 270500 Loss: 0.6553, lr 6.545084971874744e-05
Step 270600 Loss: 0.6184, lr 6.545084971874744e-05
Step 270700 Loss: 0.6001, lr 6.545084971874744e-05
Step 270800 Loss: 0.5818, lr 6.545084971874744e-05
Step 270900 Loss: 0.5646, lr 6.545084971874744e-05
Step 271000 Loss: 0.5526, lr 6.545084971874744e-05
Step 271100 Loss: 0.5415, lr 6.545084971874744e-05
Step 271200 Loss: 0.5308, lr 6.545084971874744e-05
Step 271300 Loss: 0.5236, lr 6.545084971874744e-05
Step 271400 Loss: 0.5155, lr 6.545084971874744e-05
Step 271500 Loss: 0.5090, lr 6.545084971874744e-05
Step 271600 Loss: 0.5014, lr 6.545084971874744e-05
Step 271700 Loss: 0.4960, lr 6.545084971874744e-05
Step 271800 Loss: 0.4909, lr 6.545084971874744e-05
Step 271900 Loss: 0.4853, lr 6.545084971874744e-05
Step 272000 Loss: 0.4815, lr 6.545084971874744e-05
Step 272100 Loss: 0.4776, lr 6.545084971874744e-05
Step 272200 Loss: 0.4733, lr 6.545084971874744e-05
Step 272300 Loss: 0.4682, lr 6.545084971874744e-05
Step 272400 Loss: 0.4646, lr 6.545084971874744e-05
Step 272500 Loss: 0.4612, lr 6.545084971874744e-05
Step 272600 Loss: 0.4577, lr 6.545084971874744e-05
Step 272700 Loss: 0.4537, lr 6.545084971874744e-05
Step 272800 Loss: 0.4499, lr 6.545084971874744e-05
Step 272900 Loss: 0.4464, lr 6.545084971874744e-05
Step 273000 Loss: 0.4445, lr 6.545084971874744e-05
Step 273100 Loss: 0.4415, lr 6.545084971874744e-05
Step 273200 Loss: 0.4393, lr 6.545084971874744e-05
Step 273300 Loss: 0.4368, lr 6.545084971874744e-05
Step 273400 Loss: 0.4340, lr 6.545084971874744e-05
Step 273500 Loss: 0.4318, lr 6.545084971874744e-05
Step 273600 Loss: 0.4289, lr 6.545084971874744e-05
Step 273700 Loss: 0.4263, lr 6.545084971874744e-05
Step 273800 Loss: 0.4235, lr 6.545084971874744e-05
Step 273900 Loss: 0.4214, lr 6.545084971874744e-05
Step 274000 Loss: 0.4189, lr 6.545084971874744e-05
Step 274100 Loss: 0.4169, lr 6.545084971874744e-05
Step 274200 Loss: 0.4149, lr 6.545084971874744e-05
Step 274300 Loss: 0.4132, lr 6.545084971874744e-05
Step 274400 Loss: 0.4116, lr 6.545084971874744e-05
Step 274500 Loss: 0.4102, lr 6.545084971874744e-05
Step 274600 Loss: 0.4088, lr 6.545084971874744e-05
Step 274700 Loss: 0.4077, lr 6.545084971874744e-05
Step 274800 Loss: 0.4061, lr 6.545084971874744e-05
Step 274900 Loss: 0.4045, lr 6.545084971874744e-05
Step 275000 Loss: 0.4031, lr 6.545084971874744e-05
Step 275100 Loss: 0.4025, lr 6.545084971874744e-05
Step 275200 Loss: 0.4016, lr 6.545084971874744e-05
Step 275300 Loss: 0.4008, lr 6.545084971874744e-05
Step 275400 Loss: 0.4004, lr 6.545084971874744e-05
Step 275500 Loss: 0.3997, lr 6.545084971874744e-05
Step 275600 Loss: 0.3993, lr 6.545084971874744e-05
Step 275700 Loss: 0.3990, lr 6.545084971874744e-05
Step 275800 Loss: 0.3983, lr 6.545084971874744e-05
Step 275900 Loss: 0.3979, lr 6.545084971874744e-05
Step 276000 Loss: 0.3981, lr 6.545084971874744e-05
Step 276100 Loss: 0.3987, lr 6.545084971874744e-05
Step 276200 Loss: 0.3989, lr 6.545084971874744e-05
Step 276300 Loss: 0.3987, lr 6.545084971874744e-05
Step 276400 Loss: 0.3991, lr 6.545084971874744e-05
Step 276500 Loss: 0.3986, lr 6.545084971874744e-05
Step 276600 Loss: 0.3987, lr 6.545084971874744e-05
Step 276700 Loss: 0.3980, lr 6.545084971874744e-05
Step 276800 Loss: 0.3976, lr 6.545084971874744e-05
Step 276900 Loss: 0.3966, lr 6.545084971874744e-05
Train Epoch: [41/100] Loss: 0.3964,lr 0.000065
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 277000 Loss: 1.4972, lr 6.394955530196154e-05
Step 277100 Loss: 0.9051, lr 6.394955530196154e-05
Step 277200 Loss: 0.7883, lr 6.394955530196154e-05
Step 277300 Loss: 0.7270, lr 6.394955530196154e-05
Step 277400 Loss: 0.6924, lr 6.394955530196154e-05
Step 277500 Loss: 0.6663, lr 6.394955530196154e-05
Step 277600 Loss: 0.6417, lr 6.394955530196154e-05
Step 277700 Loss: 0.6248, lr 6.394955530196154e-05
Step 277800 Loss: 0.6082, lr 6.394955530196154e-05
Step 277900 Loss: 0.5931, lr 6.394955530196154e-05
Step 278000 Loss: 0.5801, lr 6.394955530196154e-05
Step 278100 Loss: 0.5717, lr 6.394955530196154e-05
Step 278200 Loss: 0.5610, lr 6.394955530196154e-05
Step 278300 Loss: 0.5516, lr 6.394955530196154e-05
Step 278400 Loss: 0.5429, lr 6.394955530196154e-05
Step 278500 Loss: 0.5348, lr 6.394955530196154e-05
Step 278600 Loss: 0.5271, lr 6.394955530196154e-05
Step 278700 Loss: 0.5212, lr 6.394955530196154e-05
Step 278800 Loss: 0.5149, lr 6.394955530196154e-05
Step 278900 Loss: 0.5089, lr 6.394955530196154e-05
Step 279000 Loss: 0.5029, lr 6.394955530196154e-05
Step 279100 Loss: 0.4972, lr 6.394955530196154e-05
Step 279200 Loss: 0.4926, lr 6.394955530196154e-05
Step 279300 Loss: 0.4881, lr 6.394955530196154e-05
Step 279400 Loss: 0.4839, lr 6.394955530196154e-05
Step 279500 Loss: 0.4788, lr 6.394955530196154e-05
Step 279600 Loss: 0.4743, lr 6.394955530196154e-05
Step 279700 Loss: 0.4709, lr 6.394955530196154e-05
Step 279800 Loss: 0.4677, lr 6.394955530196154e-05
Step 279900 Loss: 0.4640, lr 6.394955530196154e-05
Step 280000 Loss: 0.4611, lr 6.394955530196154e-05
Step 280100 Loss: 0.4574, lr 6.394955530196154e-05
Step 280200 Loss: 0.4547, lr 6.394955530196154e-05
Step 280300 Loss: 0.4513, lr 6.394955530196154e-05
Step 280400 Loss: 0.4480, lr 6.394955530196154e-05
Step 280500 Loss: 0.4449, lr 6.394955530196154e-05
Step 280600 Loss: 0.4419, lr 6.394955530196154e-05
Step 280700 Loss: 0.4391, lr 6.394955530196154e-05
Step 280800 Loss: 0.4365, lr 6.394955530196154e-05
Step 280900 Loss: 0.4339, lr 6.394955530196154e-05
Step 281000 Loss: 0.4316, lr 6.394955530196154e-05
Step 281100 Loss: 0.4296, lr 6.394955530196154e-05
Step 281200 Loss: 0.4278, lr 6.394955530196154e-05
Step 281300 Loss: 0.4260, lr 6.394955530196154e-05
Step 281400 Loss: 0.4241, lr 6.394955530196154e-05
Step 281500 Loss: 0.4223, lr 6.394955530196154e-05
Step 281600 Loss: 0.4203, lr 6.394955530196154e-05
Step 281700 Loss: 0.4183, lr 6.394955530196154e-05
Step 281800 Loss: 0.4169, lr 6.394955530196154e-05
Step 281900 Loss: 0.4157, lr 6.394955530196154e-05
Step 282000 Loss: 0.4147, lr 6.394955530196154e-05
Step 282100 Loss: 0.4139, lr 6.394955530196154e-05
Step 282200 Loss: 0.4131, lr 6.394955530196154e-05
Step 282300 Loss: 0.4122, lr 6.394955530196154e-05
Step 282400 Loss: 0.4113, lr 6.394955530196154e-05
Step 282500 Loss: 0.4105, lr 6.394955530196154e-05
Step 282600 Loss: 0.4097, lr 6.394955530196154e-05
Step 282700 Loss: 0.4092, lr 6.394955530196154e-05
Step 282800 Loss: 0.4090, lr 6.394955530196154e-05
Step 282900 Loss: 0.4093, lr 6.394955530196154e-05
Step 283000 Loss: 0.4090, lr 6.394955530196154e-05
Step 283100 Loss: 0.4093, lr 6.394955530196154e-05
Step 283200 Loss: 0.4093, lr 6.394955530196154e-05
Step 283300 Loss: 0.4085, lr 6.394955530196154e-05
Step 283400 Loss: 0.4086, lr 6.394955530196154e-05
Step 283500 Loss: 0.4077, lr 6.394955530196154e-05
Step 283600 Loss: 0.4061, lr 6.394955530196154e-05
Step 283700 Loss: 0.4047, lr 6.394955530196154e-05
Train Epoch: [42/100] Loss: 0.4046,lr 0.000064
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 6755
Step 283800 Loss: 1.0334, lr 6.24344943582428e-05
Step 283900 Loss: 0.8236, lr 6.24344943582428e-05
Step 284000 Loss: 0.7452, lr 6.24344943582428e-05
Step 284100 Loss: 0.6962, lr 6.24344943582428e-05
Step 284200 Loss: 0.6688, lr 6.24344943582428e-05
Step 284300 Loss: 0.6472, lr 6.24344943582428e-05
Step 284400 Loss: 0.6246, lr 6.24344943582428e-05
Step 284500 Loss: 0.6076, lr 6.24344943582428e-05
Step 284600 Loss: 0.5926, lr 6.24344943582428e-05
Step 284700 Loss: 0.5793, lr 6.24344943582428e-05
Step 284800 Loss: 0.5688, lr 6.24344943582428e-05
Step 284900 Loss: 0.5576, lr 6.24344943582428e-05
Step 285000 Loss: 0.5477, lr 6.24344943582428e-05
Step 285100 Loss: 0.5382, lr 6.24344943582428e-05
Step 285200 Loss: 0.5302, lr 6.24344943582428e-05
Step 285300 Loss: 0.5227, lr 6.24344943582428e-05
Step 285400 Loss: 0.5152, lr 6.24344943582428e-05
Step 285500 Loss: 0.5094, lr 6.24344943582428e-05
Step 285600 Loss: 0.5039, lr 6.24344943582428e-05
Step 285700 Loss: 0.4981, lr 6.24344943582428e-05
Step 285800 Loss: 0.4922, lr 6.24344943582428e-05
Step 285900 Loss: 0.4878, lr 6.24344943582428e-05
Step 286000 Loss: 0.4831, lr 6.24344943582428e-05
Step 286100 Loss: 0.4785, lr 6.24344943582428e-05
Step 286200 Loss: 0.4738, lr 6.24344943582428e-05
Step 286300 Loss: 0.4692, lr 6.24344943582428e-05
Step 286400 Loss: 0.4649, lr 6.24344943582428e-05
Step 286500 Loss: 0.4621, lr 6.24344943582428e-05
Step 286600 Loss: 0.4584, lr 6.24344943582428e-05
Step 286700 Loss: 0.4555, lr 6.24344943582428e-05
Step 286800 Loss: 0.4523, lr 6.24344943582428e-05
Step 286900 Loss: 0.4487, lr 6.24344943582428e-05
Step 287000 Loss: 0.4458, lr 6.24344943582428e-05
Step 287100 Loss: 0.4424, lr 6.24344943582428e-05
Step 287200 Loss: 0.4400, lr 6.24344943582428e-05
Step 287300 Loss: 0.4374, lr 6.24344943582428e-05
Step 287400 Loss: 0.4350, lr 6.24344943582428e-05
Step 287500 Loss: 0.4324, lr 6.24344943582428e-05
Step 287600 Loss: 0.4303, lr 6.24344943582428e-05
Step 287700 Loss: 0.4279, lr 6.24344943582428e-05
Step 287800 Loss: 0.4261, lr 6.24344943582428e-05
Step 287900 Loss: 0.4242, lr 6.24344943582428e-05
Step 288000 Loss: 0.4228, lr 6.24344943582428e-05
Step 288100 Loss: 0.4210, lr 6.24344943582428e-05
Step 288200 Loss: 0.4197, lr 6.24344943582428e-05
Step 288300 Loss: 0.4179, lr 6.24344943582428e-05
Step 288400 Loss: 0.4163, lr 6.24344943582428e-05
Step 288500 Loss: 0.4150, lr 6.24344943582428e-05
Step 288600 Loss: 0.4139, lr 6.24344943582428e-05
Step 288700 Loss: 0.4128, lr 6.24344943582428e-05
Step 288800 Loss: 0.4120, lr 6.24344943582428e-05
Step 288900 Loss: 0.4114, lr 6.24344943582428e-05
Step 289000 Loss: 0.4105, lr 6.24344943582428e-05
Step 289100 Loss: 0.4100, lr 6.24344943582428e-05
Step 289200 Loss: 0.4094, lr 6.24344943582428e-05
Step 289300 Loss: 0.4086, lr 6.24344943582428e-05
Step 289400 Loss: 0.4081, lr 6.24344943582428e-05
Step 289500 Loss: 0.4080, lr 6.24344943582428e-05
Step 289600 Loss: 0.4083, lr 6.24344943582428e-05
Step 289700 Loss: 0.4082, lr 6.24344943582428e-05
Step 289800 Loss: 0.4083, lr 6.24344943582428e-05
Step 289900 Loss: 0.4090, lr 6.24344943582428e-05
Step 290000 Loss: 0.4090, lr 6.24344943582428e-05
Step 290100 Loss: 0.4089, lr 6.24344943582428e-05
Step 290200 Loss: 0.4084, lr 6.24344943582428e-05
Step 290300 Loss: 0.4080, lr 6.24344943582428e-05
Step 290400 Loss: 0.4063, lr 6.24344943582428e-05
Train Epoch: [43/100] Loss: 0.4057,lr 0.000062
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6755
Step 290500 Loss: 2.0544, lr 6.0907162069827195e-05
Step 290600 Loss: 0.9134, lr 6.0907162069827195e-05
Step 290700 Loss: 0.7907, lr 6.0907162069827195e-05
Step 290800 Loss: 0.7293, lr 6.0907162069827195e-05
Step 290900 Loss: 0.6938, lr 6.0907162069827195e-05
Step 291000 Loss: 0.6676, lr 6.0907162069827195e-05
Step 291100 Loss: 0.6428, lr 6.0907162069827195e-05
Step 291200 Loss: 0.6260, lr 6.0907162069827195e-05
Step 291300 Loss: 0.6090, lr 6.0907162069827195e-05
Step 291400 Loss: 0.5938, lr 6.0907162069827195e-05
Step 291500 Loss: 0.5809, lr 6.0907162069827195e-05
Step 291600 Loss: 0.5710, lr 6.0907162069827195e-05
Step 291700 Loss: 0.5602, lr 6.0907162069827195e-05
Step 291800 Loss: 0.5510, lr 6.0907162069827195e-05
Step 291900 Loss: 0.5415, lr 6.0907162069827195e-05
Step 292000 Loss: 0.5332, lr 6.0907162069827195e-05
Step 292100 Loss: 0.5255, lr 6.0907162069827195e-05
Step 292200 Loss: 0.5191, lr 6.0907162069827195e-05
Step 292300 Loss: 0.5133, lr 6.0907162069827195e-05
Step 292400 Loss: 0.5075, lr 6.0907162069827195e-05
Step 292500 Loss: 0.5011, lr 6.0907162069827195e-05
Step 292600 Loss: 0.4951, lr 6.0907162069827195e-05
Step 292700 Loss: 0.4906, lr 6.0907162069827195e-05
Step 292800 Loss: 0.4857, lr 6.0907162069827195e-05
Step 292900 Loss: 0.4816, lr 6.0907162069827195e-05
Step 293000 Loss: 0.4764, lr 6.0907162069827195e-05
Step 293100 Loss: 0.4716, lr 6.0907162069827195e-05
Step 293200 Loss: 0.4680, lr 6.0907162069827195e-05
Step 293300 Loss: 0.4650, lr 6.0907162069827195e-05
Step 293400 Loss: 0.4612, lr 6.0907162069827195e-05
Step 293500 Loss: 0.4581, lr 6.0907162069827195e-05
Step 293600 Loss: 0.4546, lr 6.0907162069827195e-05
Step 293700 Loss: 0.4517, lr 6.0907162069827195e-05
Step 293800 Loss: 0.4484, lr 6.0907162069827195e-05
Step 293900 Loss: 0.4454, lr 6.0907162069827195e-05
Step 294000 Loss: 0.4425, lr 6.0907162069827195e-05
Step 294100 Loss: 0.4394, lr 6.0907162069827195e-05
Step 294200 Loss: 0.4368, lr 6.0907162069827195e-05
Step 294300 Loss: 0.4345, lr 6.0907162069827195e-05
Step 294400 Loss: 0.4320, lr 6.0907162069827195e-05
Step 294500 Loss: 0.4297, lr 6.0907162069827195e-05
Step 294600 Loss: 0.4276, lr 6.0907162069827195e-05
Step 294700 Loss: 0.4259, lr 6.0907162069827195e-05
Step 294800 Loss: 0.4239, lr 6.0907162069827195e-05
Step 294900 Loss: 0.4223, lr 6.0907162069827195e-05
Step 295000 Loss: 0.4206, lr 6.0907162069827195e-05
Step 295100 Loss: 0.4185, lr 6.0907162069827195e-05
Step 295200 Loss: 0.4168, lr 6.0907162069827195e-05
Step 295300 Loss: 0.4155, lr 6.0907162069827195e-05
Step 295400 Loss: 0.4142, lr 6.0907162069827195e-05
Step 295500 Loss: 0.4133, lr 6.0907162069827195e-05
Step 295600 Loss: 0.4126, lr 6.0907162069827195e-05
Step 295700 Loss: 0.4118, lr 6.0907162069827195e-05
Step 295800 Loss: 0.4110, lr 6.0907162069827195e-05
Step 295900 Loss: 0.4103, lr 6.0907162069827195e-05
Step 296000 Loss: 0.4095, lr 6.0907162069827195e-05
Step 296100 Loss: 0.4087, lr 6.0907162069827195e-05
Step 296200 Loss: 0.4085, lr 6.0907162069827195e-05
Step 296300 Loss: 0.4083, lr 6.0907162069827195e-05
Step 296400 Loss: 0.4085, lr 6.0907162069827195e-05
Step 296500 Loss: 0.4085, lr 6.0907162069827195e-05
Step 296600 Loss: 0.4091, lr 6.0907162069827195e-05
Step 296700 Loss: 0.4094, lr 6.0907162069827195e-05
Step 296800 Loss: 0.4087, lr 6.0907162069827195e-05
Step 296900 Loss: 0.4085, lr 6.0907162069827195e-05
Step 297000 Loss: 0.4078, lr 6.0907162069827195e-05
Step 297100 Loss: 0.4068, lr 6.0907162069827195e-05
Step 297200 Loss: 0.4055, lr 6.0907162069827195e-05
Train Epoch: [44/100] Loss: 0.4056,lr 0.000061
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6756
Step 297300 Loss: 0.9314, lr 5.93690657292863e-05
Step 297400 Loss: 0.7295, lr 5.93690657292863e-05
Step 297500 Loss: 0.6718, lr 5.93690657292863e-05
Step 297600 Loss: 0.6300, lr 5.93690657292863e-05
Step 297700 Loss: 0.6100, lr 5.93690657292863e-05
Step 297800 Loss: 0.5920, lr 5.93690657292863e-05
Step 297900 Loss: 0.5730, lr 5.93690657292863e-05
Step 298000 Loss: 0.5586, lr 5.93690657292863e-05
Step 298100 Loss: 0.5461, lr 5.93690657292863e-05
Step 298200 Loss: 0.5356, lr 5.93690657292863e-05
Step 298300 Loss: 0.5270, lr 5.93690657292863e-05
Step 298400 Loss: 0.5185, lr 5.93690657292863e-05
Step 298500 Loss: 0.5108, lr 5.93690657292863e-05
Step 298600 Loss: 0.5037, lr 5.93690657292863e-05
Step 298700 Loss: 0.4981, lr 5.93690657292863e-05
Step 298800 Loss: 0.4934, lr 5.93690657292863e-05
Step 298900 Loss: 0.4879, lr 5.93690657292863e-05
Step 299000 Loss: 0.4837, lr 5.93690657292863e-05
Step 299100 Loss: 0.4796, lr 5.93690657292863e-05
Step 299200 Loss: 0.4750, lr 5.93690657292863e-05
Step 299300 Loss: 0.4703, lr 5.93690657292863e-05
Step 299400 Loss: 0.4669, lr 5.93690657292863e-05
Step 299500 Loss: 0.4633, lr 5.93690657292863e-05
Step 299600 Loss: 0.4597, lr 5.93690657292863e-05
Step 299700 Loss: 0.4563, lr 5.93690657292863e-05
Step 299800 Loss: 0.4522, lr 5.93690657292863e-05
Step 299900 Loss: 0.4491, lr 5.93690657292863e-05
Step 300000 Loss: 0.4469, lr 5.93690657292863e-05
Step 300100 Loss: 0.4441, lr 5.93690657292863e-05
Step 300200 Loss: 0.4416, lr 5.93690657292863e-05
Step 300300 Loss: 0.4393, lr 5.93690657292863e-05
Step 300400 Loss: 0.4364, lr 5.93690657292863e-05
Step 300500 Loss: 0.4341, lr 5.93690657292863e-05
Step 300600 Loss: 0.4314, lr 5.93690657292863e-05
Step 300700 Loss: 0.4289, lr 5.93690657292863e-05
Step 300800 Loss: 0.4265, lr 5.93690657292863e-05
Step 300900 Loss: 0.4244, lr 5.93690657292863e-05
Step 301000 Loss: 0.4220, lr 5.93690657292863e-05
Step 301100 Loss: 0.4204, lr 5.93690657292863e-05
Step 301200 Loss: 0.4186, lr 5.93690657292863e-05
Step 301300 Loss: 0.4176, lr 5.93690657292863e-05
Step 301400 Loss: 0.4163, lr 5.93690657292863e-05
Step 301500 Loss: 0.4155, lr 5.93690657292863e-05
Step 301600 Loss: 0.4142, lr 5.93690657292863e-05
Step 301700 Loss: 0.4135, lr 5.93690657292863e-05
Step 301800 Loss: 0.4125, lr 5.93690657292863e-05
Step 301900 Loss: 0.4112, lr 5.93690657292863e-05
Step 302000 Loss: 0.4102, lr 5.93690657292863e-05
Step 302100 Loss: 0.4095, lr 5.93690657292863e-05
Step 302200 Loss: 0.4092, lr 5.93690657292863e-05
Step 302300 Loss: 0.4084, lr 5.93690657292863e-05
Step 302400 Loss: 0.4082, lr 5.93690657292863e-05
Step 302500 Loss: 0.4079, lr 5.93690657292863e-05
Step 302600 Loss: 0.4077, lr 5.93690657292863e-05
Step 302700 Loss: 0.4073, lr 5.93690657292863e-05
Step 302800 Loss: 0.4071, lr 5.93690657292863e-05
Step 302900 Loss: 0.4067, lr 5.93690657292863e-05
Step 303000 Loss: 0.4066, lr 5.93690657292863e-05
Step 303100 Loss: 0.4067, lr 5.93690657292863e-05
Step 303200 Loss: 0.4066, lr 5.93690657292863e-05
Step 303300 Loss: 0.4065, lr 5.93690657292863e-05
Step 303400 Loss: 0.4068, lr 5.93690657292863e-05
Step 303500 Loss: 0.4068, lr 5.93690657292863e-05
Step 303600 Loss: 0.4064, lr 5.93690657292863e-05
Step 303700 Loss: 0.4059, lr 5.93690657292863e-05
Step 303800 Loss: 0.4054, lr 5.93690657292863e-05
Step 303900 Loss: 0.4040, lr 5.93690657292863e-05
Step 304000 Loss: 0.4035, lr 5.93690657292863e-05
Train Epoch: [45/100] Loss: 0.4036,lr 0.000059
Model Saving at epoch 45
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.22 s, total batches: 6755
Step 304100 Loss: 0.8026, lr 5.7821723252011606e-05
Step 304200 Loss: 0.6967, lr 5.7821723252011606e-05
Step 304300 Loss: 0.6438, lr 5.7821723252011606e-05
Step 304400 Loss: 0.6138, lr 5.7821723252011606e-05
Step 304500 Loss: 0.5940, lr 5.7821723252011606e-05
Step 304600 Loss: 0.5752, lr 5.7821723252011606e-05
Step 304700 Loss: 0.5612, lr 5.7821723252011606e-05
Step 304800 Loss: 0.5481, lr 5.7821723252011606e-05
Step 304900 Loss: 0.5372, lr 5.7821723252011606e-05
Step 305000 Loss: 0.5272, lr 5.7821723252011606e-05
Step 305100 Loss: 0.5205, lr 5.7821723252011606e-05
Step 305200 Loss: 0.5119, lr 5.7821723252011606e-05
Step 305300 Loss: 0.5054, lr 5.7821723252011606e-05
Step 305400 Loss: 0.4984, lr 5.7821723252011606e-05
Step 305500 Loss: 0.4930, lr 5.7821723252011606e-05
Step 305600 Loss: 0.4879, lr 5.7821723252011606e-05
Step 305700 Loss: 0.4828, lr 5.7821723252011606e-05
Step 305800 Loss: 0.4790, lr 5.7821723252011606e-05
Step 305900 Loss: 0.4749, lr 5.7821723252011606e-05
Step 306000 Loss: 0.4699, lr 5.7821723252011606e-05
Step 306100 Loss: 0.4653, lr 5.7821723252011606e-05
Step 306200 Loss: 0.4621, lr 5.7821723252011606e-05
Step 306300 Loss: 0.4586, lr 5.7821723252011606e-05
Step 306400 Loss: 0.4556, lr 5.7821723252011606e-05
Step 306500 Loss: 0.4517, lr 5.7821723252011606e-05
Step 306600 Loss: 0.4479, lr 5.7821723252011606e-05
Step 306700 Loss: 0.4448, lr 5.7821723252011606e-05
Step 306800 Loss: 0.4427, lr 5.7821723252011606e-05
Step 306900 Loss: 0.4400, lr 5.7821723252011606e-05
Step 307000 Loss: 0.4376, lr 5.7821723252011606e-05
Step 307100 Loss: 0.4351, lr 5.7821723252011606e-05
Step 307200 Loss: 0.4326, lr 5.7821723252011606e-05
Step 307300 Loss: 0.4300, lr 5.7821723252011606e-05
Step 307400 Loss: 0.4274, lr 5.7821723252011606e-05
Step 307500 Loss: 0.4246, lr 5.7821723252011606e-05
Step 307600 Loss: 0.4222, lr 5.7821723252011606e-05
Step 307700 Loss: 0.4200, lr 5.7821723252011606e-05
Step 307800 Loss: 0.4180, lr 5.7821723252011606e-05
Step 307900 Loss: 0.4159, lr 5.7821723252011606e-05
Step 308000 Loss: 0.4142, lr 5.7821723252011606e-05
Step 308100 Loss: 0.4125, lr 5.7821723252011606e-05
Step 308200 Loss: 0.4113, lr 5.7821723252011606e-05
Step 308300 Loss: 0.4099, lr 5.7821723252011606e-05
Step 308400 Loss: 0.4087, lr 5.7821723252011606e-05
Step 308500 Loss: 0.4074, lr 5.7821723252011606e-05
Step 308600 Loss: 0.4056, lr 5.7821723252011606e-05
Step 308700 Loss: 0.4044, lr 5.7821723252011606e-05
Step 308800 Loss: 0.4033, lr 5.7821723252011606e-05
Step 308900 Loss: 0.4024, lr 5.7821723252011606e-05
Step 309000 Loss: 0.4016, lr 5.7821723252011606e-05
Step 309100 Loss: 0.4009, lr 5.7821723252011606e-05
Step 309200 Loss: 0.4004, lr 5.7821723252011606e-05
Step 309300 Loss: 0.3997, lr 5.7821723252011606e-05
Step 309400 Loss: 0.3993, lr 5.7821723252011606e-05
Step 309500 Loss: 0.3990, lr 5.7821723252011606e-05
Step 309600 Loss: 0.3984, lr 5.7821723252011606e-05
Step 309700 Loss: 0.3983, lr 5.7821723252011606e-05
Step 309800 Loss: 0.3986, lr 5.7821723252011606e-05
Step 309900 Loss: 0.3993, lr 5.7821723252011606e-05
Step 310000 Loss: 0.3989, lr 5.7821723252011606e-05
Step 310100 Loss: 0.3994, lr 5.7821723252011606e-05
Step 310200 Loss: 0.3998, lr 5.7821723252011606e-05
Step 310300 Loss: 0.3994, lr 5.7821723252011606e-05
Step 310400 Loss: 0.3995, lr 5.7821723252011606e-05
Step 310500 Loss: 0.3988, lr 5.7821723252011606e-05
Step 310600 Loss: 0.3979, lr 5.7821723252011606e-05
Step 310700 Loss: 0.3967, lr 5.7821723252011606e-05
Train Epoch: [46/100] Loss: 0.3968,lr 0.000058
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6756
Step 310800 Loss: 0.9850, lr 5.6266661678215284e-05
Step 310900 Loss: 0.7389, lr 5.6266661678215284e-05
Step 311000 Loss: 0.6695, lr 5.6266661678215284e-05
Step 311100 Loss: 0.6282, lr 5.6266661678215284e-05
Step 311200 Loss: 0.6038, lr 5.6266661678215284e-05
Step 311300 Loss: 0.5863, lr 5.6266661678215284e-05
Step 311400 Loss: 0.5691, lr 5.6266661678215284e-05
Step 311500 Loss: 0.5544, lr 5.6266661678215284e-05
Step 311600 Loss: 0.5426, lr 5.6266661678215284e-05
Step 311700 Loss: 0.5321, lr 5.6266661678215284e-05
Step 311800 Loss: 0.5239, lr 5.6266661678215284e-05
Step 311900 Loss: 0.5165, lr 5.6266661678215284e-05
Step 312000 Loss: 0.5086, lr 5.6266661678215284e-05
Step 312100 Loss: 0.5019, lr 5.6266661678215284e-05
Step 312200 Loss: 0.4958, lr 5.6266661678215284e-05
Step 312300 Loss: 0.4908, lr 5.6266661678215284e-05
Step 312400 Loss: 0.4851, lr 5.6266661678215284e-05
Step 312500 Loss: 0.4812, lr 5.6266661678215284e-05
Step 312600 Loss: 0.4769, lr 5.6266661678215284e-05
Step 312700 Loss: 0.4724, lr 5.6266661678215284e-05
Step 312800 Loss: 0.4678, lr 5.6266661678215284e-05
Step 312900 Loss: 0.4643, lr 5.6266661678215284e-05
Step 313000 Loss: 0.4607, lr 5.6266661678215284e-05
Step 313100 Loss: 0.4571, lr 5.6266661678215284e-05
Step 313200 Loss: 0.4540, lr 5.6266661678215284e-05
Step 313300 Loss: 0.4499, lr 5.6266661678215284e-05
Step 313400 Loss: 0.4466, lr 5.6266661678215284e-05
Step 313500 Loss: 0.4444, lr 5.6266661678215284e-05
Step 313600 Loss: 0.4420, lr 5.6266661678215284e-05
Step 313700 Loss: 0.4392, lr 5.6266661678215284e-05
Step 313800 Loss: 0.4368, lr 5.6266661678215284e-05
Step 313900 Loss: 0.4341, lr 5.6266661678215284e-05
Step 314000 Loss: 0.4317, lr 5.6266661678215284e-05
Step 314100 Loss: 0.4288, lr 5.6266661678215284e-05
Step 314200 Loss: 0.4262, lr 5.6266661678215284e-05
Step 314300 Loss: 0.4237, lr 5.6266661678215284e-05
Step 314400 Loss: 0.4216, lr 5.6266661678215284e-05
Step 314500 Loss: 0.4193, lr 5.6266661678215284e-05
Step 314600 Loss: 0.4173, lr 5.6266661678215284e-05
Step 314700 Loss: 0.4154, lr 5.6266661678215284e-05
Step 314800 Loss: 0.4138, lr 5.6266661678215284e-05
Step 314900 Loss: 0.4121, lr 5.6266661678215284e-05
Step 315000 Loss: 0.4108, lr 5.6266661678215284e-05
Step 315100 Loss: 0.4094, lr 5.6266661678215284e-05
Step 315200 Loss: 0.4083, lr 5.6266661678215284e-05
Step 315300 Loss: 0.4067, lr 5.6266661678215284e-05
Step 315400 Loss: 0.4053, lr 5.6266661678215284e-05
Step 315500 Loss: 0.4038, lr 5.6266661678215284e-05
Step 315600 Loss: 0.4029, lr 5.6266661678215284e-05
Step 315700 Loss: 0.4020, lr 5.6266661678215284e-05
Step 315800 Loss: 0.4012, lr 5.6266661678215284e-05
Step 315900 Loss: 0.4008, lr 5.6266661678215284e-05
Step 316000 Loss: 0.4003, lr 5.6266661678215284e-05
Step 316100 Loss: 0.3998, lr 5.6266661678215284e-05
Step 316200 Loss: 0.3993, lr 5.6266661678215284e-05
Step 316300 Loss: 0.3988, lr 5.6266661678215284e-05
Step 316400 Loss: 0.3985, lr 5.6266661678215284e-05
Step 316500 Loss: 0.3985, lr 5.6266661678215284e-05
Step 316600 Loss: 0.3987, lr 5.6266661678215284e-05
Step 316700 Loss: 0.3990, lr 5.6266661678215284e-05
Step 316800 Loss: 0.3989, lr 5.6266661678215284e-05
Step 316900 Loss: 0.3993, lr 5.6266661678215284e-05
Step 317000 Loss: 0.3995, lr 5.6266661678215284e-05
Step 317100 Loss: 0.3992, lr 5.6266661678215284e-05
Step 317200 Loss: 0.3987, lr 5.6266661678215284e-05
Step 317300 Loss: 0.3983, lr 5.6266661678215284e-05
Step 317400 Loss: 0.3969, lr 5.6266661678215284e-05
Step 317500 Loss: 0.3963, lr 5.6266661678215284e-05
Train Epoch: [47/100] Loss: 0.3964,lr 0.000056
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6756
Step 317600 Loss: 0.8213, lr 5.470541566592577e-05
Step 317700 Loss: 0.7019, lr 5.470541566592577e-05
Step 317800 Loss: 0.6480, lr 5.470541566592577e-05
Step 317900 Loss: 0.6143, lr 5.470541566592577e-05
Step 318000 Loss: 0.5952, lr 5.470541566592577e-05
Step 318100 Loss: 0.5762, lr 5.470541566592577e-05
Step 318200 Loss: 0.5616, lr 5.470541566592577e-05
Step 318300 Loss: 0.5486, lr 5.470541566592577e-05
Step 318400 Loss: 0.5373, lr 5.470541566592577e-05
Step 318500 Loss: 0.5278, lr 5.470541566592577e-05
Step 318600 Loss: 0.5205, lr 5.470541566592577e-05
Step 318700 Loss: 0.5120, lr 5.470541566592577e-05
Step 318800 Loss: 0.5056, lr 5.470541566592577e-05
Step 318900 Loss: 0.4986, lr 5.470541566592577e-05
Step 319000 Loss: 0.4925, lr 5.470541566592577e-05
Step 319100 Loss: 0.4876, lr 5.470541566592577e-05
Step 319200 Loss: 0.4823, lr 5.470541566592577e-05
Step 319300 Loss: 0.4783, lr 5.470541566592577e-05
Step 319400 Loss: 0.4739, lr 5.470541566592577e-05
Step 319500 Loss: 0.4695, lr 5.470541566592577e-05
Step 319600 Loss: 0.4647, lr 5.470541566592577e-05
Step 319700 Loss: 0.4613, lr 5.470541566592577e-05
Step 319800 Loss: 0.4580, lr 5.470541566592577e-05
Step 319900 Loss: 0.4551, lr 5.470541566592577e-05
Step 320000 Loss: 0.4511, lr 5.470541566592577e-05
Step 320100 Loss: 0.4475, lr 5.470541566592577e-05
Step 320200 Loss: 0.4445, lr 5.470541566592577e-05
Step 320300 Loss: 0.4427, lr 5.470541566592577e-05
Step 320400 Loss: 0.4399, lr 5.470541566592577e-05
Step 320500 Loss: 0.4375, lr 5.470541566592577e-05
Step 320600 Loss: 0.4350, lr 5.470541566592577e-05
Step 320700 Loss: 0.4322, lr 5.470541566592577e-05
Step 320800 Loss: 0.4297, lr 5.470541566592577e-05
Step 320900 Loss: 0.4270, lr 5.470541566592577e-05
Step 321000 Loss: 0.4245, lr 5.470541566592577e-05
Step 321100 Loss: 0.4219, lr 5.470541566592577e-05
Step 321200 Loss: 0.4198, lr 5.470541566592577e-05
Step 321300 Loss: 0.4174, lr 5.470541566592577e-05
Step 321400 Loss: 0.4155, lr 5.470541566592577e-05
Step 321500 Loss: 0.4136, lr 5.470541566592577e-05
Step 321600 Loss: 0.4121, lr 5.470541566592577e-05
Step 321700 Loss: 0.4108, lr 5.470541566592577e-05
Step 321800 Loss: 0.4094, lr 5.470541566592577e-05
Step 321900 Loss: 0.4083, lr 5.470541566592577e-05
Step 322000 Loss: 0.4068, lr 5.470541566592577e-05
Step 322100 Loss: 0.4052, lr 5.470541566592577e-05
Step 322200 Loss: 0.4037, lr 5.470541566592577e-05
Step 322300 Loss: 0.4024, lr 5.470541566592577e-05
Step 322400 Loss: 0.4018, lr 5.470541566592577e-05
Step 322500 Loss: 0.4012, lr 5.470541566592577e-05
Step 322600 Loss: 0.4004, lr 5.470541566592577e-05
Step 322700 Loss: 0.4001, lr 5.470541566592577e-05
Step 322800 Loss: 0.3994, lr 5.470541566592577e-05
Step 322900 Loss: 0.3987, lr 5.470541566592577e-05
Step 323000 Loss: 0.3984, lr 5.470541566592577e-05
Step 323100 Loss: 0.3978, lr 5.470541566592577e-05
Step 323200 Loss: 0.3976, lr 5.470541566592577e-05
Step 323300 Loss: 0.3977, lr 5.470541566592577e-05
Step 323400 Loss: 0.3981, lr 5.470541566592577e-05
Step 323500 Loss: 0.3981, lr 5.470541566592577e-05
Step 323600 Loss: 0.3983, lr 5.470541566592577e-05
Step 323700 Loss: 0.3985, lr 5.470541566592577e-05
Step 323800 Loss: 0.3982, lr 5.470541566592577e-05
Step 323900 Loss: 0.3983, lr 5.470541566592577e-05
Step 324000 Loss: 0.3974, lr 5.470541566592577e-05
Step 324100 Loss: 0.3969, lr 5.470541566592577e-05
Step 324200 Loss: 0.3959, lr 5.470541566592577e-05
Train Epoch: [48/100] Loss: 0.3955,lr 0.000055
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6755
Step 324300 Loss: 1.0841, lr 5.313952597646573e-05
Step 324400 Loss: 0.7513, lr 5.313952597646573e-05
Step 324500 Loss: 0.6736, lr 5.313952597646573e-05
Step 324600 Loss: 0.6294, lr 5.313952597646573e-05
Step 324700 Loss: 0.6043, lr 5.313952597646573e-05
Step 324800 Loss: 0.5879, lr 5.313952597646573e-05
Step 324900 Loss: 0.5696, lr 5.313952597646573e-05
Step 325000 Loss: 0.5559, lr 5.313952597646573e-05
Step 325100 Loss: 0.5436, lr 5.313952597646573e-05
Step 325200 Loss: 0.5330, lr 5.313952597646573e-05
Step 325300 Loss: 0.5241, lr 5.313952597646573e-05
Step 325400 Loss: 0.5173, lr 5.313952597646573e-05
Step 325500 Loss: 0.5090, lr 5.313952597646573e-05
Step 325600 Loss: 0.5024, lr 5.313952597646573e-05
Step 325700 Loss: 0.4960, lr 5.313952597646573e-05
Step 325800 Loss: 0.4909, lr 5.313952597646573e-05
Step 325900 Loss: 0.4854, lr 5.313952597646573e-05
Step 326000 Loss: 0.4809, lr 5.313952597646573e-05
Step 326100 Loss: 0.4765, lr 5.313952597646573e-05
Step 326200 Loss: 0.4719, lr 5.313952597646573e-05
Step 326300 Loss: 0.4673, lr 5.313952597646573e-05
Step 326400 Loss: 0.4633, lr 5.313952597646573e-05
Step 326500 Loss: 0.4596, lr 5.313952597646573e-05
Step 326600 Loss: 0.4562, lr 5.313952597646573e-05
Step 326700 Loss: 0.4532, lr 5.313952597646573e-05
Step 326800 Loss: 0.4488, lr 5.313952597646573e-05
Step 326900 Loss: 0.4453, lr 5.313952597646573e-05
Step 327000 Loss: 0.4428, lr 5.313952597646573e-05
Step 327100 Loss: 0.4407, lr 5.313952597646573e-05
Step 327200 Loss: 0.4378, lr 5.313952597646573e-05
Step 327300 Loss: 0.4355, lr 5.313952597646573e-05
Step 327400 Loss: 0.4327, lr 5.313952597646573e-05
Step 327500 Loss: 0.4303, lr 5.313952597646573e-05
Step 327600 Loss: 0.4276, lr 5.313952597646573e-05
Step 327700 Loss: 0.4249, lr 5.313952597646573e-05
Step 327800 Loss: 0.4224, lr 5.313952597646573e-05
Step 327900 Loss: 0.4203, lr 5.313952597646573e-05
Step 328000 Loss: 0.4177, lr 5.313952597646573e-05
Step 328100 Loss: 0.4157, lr 5.313952597646573e-05
Step 328200 Loss: 0.4136, lr 5.313952597646573e-05
Step 328300 Loss: 0.4120, lr 5.313952597646573e-05
Step 328400 Loss: 0.4103, lr 5.313952597646573e-05
Step 328500 Loss: 0.4091, lr 5.313952597646573e-05
Step 328600 Loss: 0.4076, lr 5.313952597646573e-05
Step 328700 Loss: 0.4066, lr 5.313952597646573e-05
Step 328800 Loss: 0.4052, lr 5.313952597646573e-05
Step 328900 Loss: 0.4037, lr 5.313952597646573e-05
Step 329000 Loss: 0.4019, lr 5.313952597646573e-05
Step 329100 Loss: 0.4010, lr 5.313952597646573e-05
Step 329200 Loss: 0.4001, lr 5.313952597646573e-05
Step 329300 Loss: 0.3995, lr 5.313952597646573e-05
Step 329400 Loss: 0.3989, lr 5.313952597646573e-05
Step 329500 Loss: 0.3983, lr 5.313952597646573e-05
Step 329600 Loss: 0.3979, lr 5.313952597646573e-05
Step 329700 Loss: 0.3972, lr 5.313952597646573e-05
Step 329800 Loss: 0.3967, lr 5.313952597646573e-05
Step 329900 Loss: 0.3965, lr 5.313952597646573e-05
Step 330000 Loss: 0.3963, lr 5.313952597646573e-05
Step 330100 Loss: 0.3962, lr 5.313952597646573e-05
Step 330200 Loss: 0.3969, lr 5.313952597646573e-05
Step 330300 Loss: 0.3969, lr 5.313952597646573e-05
Step 330400 Loss: 0.3973, lr 5.313952597646573e-05
Step 330500 Loss: 0.3975, lr 5.313952597646573e-05
Step 330600 Loss: 0.3970, lr 5.313952597646573e-05
Step 330700 Loss: 0.3962, lr 5.313952597646573e-05
Step 330800 Loss: 0.3958, lr 5.313952597646573e-05
Step 330900 Loss: 0.3947, lr 5.313952597646573e-05
Step 331000 Loss: 0.3937, lr 5.313952597646573e-05
Train Epoch: [49/100] Loss: 0.3939,lr 0.000053
Calling G2SDataset.batch()
Done, time:  2.02 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 331100 Loss: 0.8475, lr 5.1570537953906474e-05
Step 331200 Loss: 0.7133, lr 5.1570537953906474e-05
Step 331300 Loss: 0.6538, lr 5.1570537953906474e-05
Step 331400 Loss: 0.6158, lr 5.1570537953906474e-05
Step 331500 Loss: 0.5983, lr 5.1570537953906474e-05
Step 331600 Loss: 0.5794, lr 5.1570537953906474e-05
Step 331700 Loss: 0.5624, lr 5.1570537953906474e-05
Step 331800 Loss: 0.5495, lr 5.1570537953906474e-05
Step 331900 Loss: 0.5378, lr 5.1570537953906474e-05
Step 332000 Loss: 0.5270, lr 5.1570537953906474e-05
Step 332100 Loss: 0.5204, lr 5.1570537953906474e-05
Step 332200 Loss: 0.5117, lr 5.1570537953906474e-05
Step 332300 Loss: 0.5052, lr 5.1570537953906474e-05
Step 332400 Loss: 0.4977, lr 5.1570537953906474e-05
Step 332500 Loss: 0.4922, lr 5.1570537953906474e-05
Step 332600 Loss: 0.4870, lr 5.1570537953906474e-05
Step 332700 Loss: 0.4818, lr 5.1570537953906474e-05
Step 332800 Loss: 0.4778, lr 5.1570537953906474e-05
Step 332900 Loss: 0.4734, lr 5.1570537953906474e-05
Step 333000 Loss: 0.4688, lr 5.1570537953906474e-05
Step 333100 Loss: 0.4639, lr 5.1570537953906474e-05
Step 333200 Loss: 0.4606, lr 5.1570537953906474e-05
Step 333300 Loss: 0.4573, lr 5.1570537953906474e-05
Step 333400 Loss: 0.4541, lr 5.1570537953906474e-05
Step 333500 Loss: 0.4502, lr 5.1570537953906474e-05
Step 333600 Loss: 0.4464, lr 5.1570537953906474e-05
Step 333700 Loss: 0.4431, lr 5.1570537953906474e-05
Step 333800 Loss: 0.4408, lr 5.1570537953906474e-05
Step 333900 Loss: 0.4379, lr 5.1570537953906474e-05
Step 334000 Loss: 0.4355, lr 5.1570537953906474e-05
Step 334100 Loss: 0.4328, lr 5.1570537953906474e-05
Step 334200 Loss: 0.4299, lr 5.1570537953906474e-05
Step 334300 Loss: 0.4276, lr 5.1570537953906474e-05
Step 334400 Loss: 0.4248, lr 5.1570537953906474e-05
Step 334500 Loss: 0.4223, lr 5.1570537953906474e-05
Step 334600 Loss: 0.4194, lr 5.1570537953906474e-05
Step 334700 Loss: 0.4174, lr 5.1570537953906474e-05
Step 334800 Loss: 0.4149, lr 5.1570537953906474e-05
Step 334900 Loss: 0.4128, lr 5.1570537953906474e-05
Step 335000 Loss: 0.4109, lr 5.1570537953906474e-05
Step 335100 Loss: 0.4092, lr 5.1570537953906474e-05
Step 335200 Loss: 0.4077, lr 5.1570537953906474e-05
Step 335300 Loss: 0.4063, lr 5.1570537953906474e-05
Step 335400 Loss: 0.4050, lr 5.1570537953906474e-05
Step 335500 Loss: 0.4036, lr 5.1570537953906474e-05
Step 335600 Loss: 0.4018, lr 5.1570537953906474e-05
Step 335700 Loss: 0.4003, lr 5.1570537953906474e-05
Step 335800 Loss: 0.3989, lr 5.1570537953906474e-05
Step 335900 Loss: 0.3981, lr 5.1570537953906474e-05
Step 336000 Loss: 0.3973, lr 5.1570537953906474e-05
Step 336100 Loss: 0.3966, lr 5.1570537953906474e-05
Step 336200 Loss: 0.3962, lr 5.1570537953906474e-05
Step 336300 Loss: 0.3956, lr 5.1570537953906474e-05
Step 336400 Loss: 0.3950, lr 5.1570537953906474e-05
Step 336500 Loss: 0.3946, lr 5.1570537953906474e-05
Step 336600 Loss: 0.3938, lr 5.1570537953906474e-05
Step 336700 Loss: 0.3933, lr 5.1570537953906474e-05
Step 336800 Loss: 0.3935, lr 5.1570537953906474e-05
Step 336900 Loss: 0.3939, lr 5.1570537953906474e-05
Step 337000 Loss: 0.3940, lr 5.1570537953906474e-05
Step 337100 Loss: 0.3940, lr 5.1570537953906474e-05
Step 337200 Loss: 0.3945, lr 5.1570537953906474e-05
Step 337300 Loss: 0.3941, lr 5.1570537953906474e-05
Step 337400 Loss: 0.3938, lr 5.1570537953906474e-05
Step 337500 Loss: 0.3930, lr 5.1570537953906474e-05
Step 337600 Loss: 0.3925, lr 5.1570537953906474e-05
Step 337700 Loss: 0.3915, lr 5.1570537953906474e-05
Train Epoch: [50/100] Loss: 0.3919,lr 0.000052
Model Saving at epoch 50
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 337800 Loss: 1.2638, lr 5.0000000000000057e-05
Step 337900 Loss: 0.7719, lr 5.0000000000000057e-05
Step 338000 Loss: 0.6822, lr 5.0000000000000057e-05
Step 338100 Loss: 0.6356, lr 5.0000000000000057e-05
Step 338200 Loss: 0.6092, lr 5.0000000000000057e-05
Step 338300 Loss: 0.5905, lr 5.0000000000000057e-05
Step 338400 Loss: 0.5707, lr 5.0000000000000057e-05
Step 338500 Loss: 0.5573, lr 5.0000000000000057e-05
Step 338600 Loss: 0.5450, lr 5.0000000000000057e-05
Step 338700 Loss: 0.5331, lr 5.0000000000000057e-05
Step 338800 Loss: 0.5237, lr 5.0000000000000057e-05
Step 338900 Loss: 0.5175, lr 5.0000000000000057e-05
Step 339000 Loss: 0.5091, lr 5.0000000000000057e-05
Step 339100 Loss: 0.5026, lr 5.0000000000000057e-05
Step 339200 Loss: 0.4964, lr 5.0000000000000057e-05
Step 339300 Loss: 0.4907, lr 5.0000000000000057e-05
Step 339400 Loss: 0.4852, lr 5.0000000000000057e-05
Step 339500 Loss: 0.4810, lr 5.0000000000000057e-05
Step 339600 Loss: 0.4767, lr 5.0000000000000057e-05
Step 339700 Loss: 0.4727, lr 5.0000000000000057e-05
Step 339800 Loss: 0.4680, lr 5.0000000000000057e-05
Step 339900 Loss: 0.4636, lr 5.0000000000000057e-05
Step 340000 Loss: 0.4602, lr 5.0000000000000057e-05
Step 340100 Loss: 0.4563, lr 5.0000000000000057e-05
Step 340200 Loss: 0.4535, lr 5.0000000000000057e-05
Step 340300 Loss: 0.4493, lr 5.0000000000000057e-05
Step 340400 Loss: 0.4458, lr 5.0000000000000057e-05
Step 340500 Loss: 0.4430, lr 5.0000000000000057e-05
Step 340600 Loss: 0.4410, lr 5.0000000000000057e-05
Step 340700 Loss: 0.4383, lr 5.0000000000000057e-05
Step 340800 Loss: 0.4359, lr 5.0000000000000057e-05
Step 340900 Loss: 0.4331, lr 5.0000000000000057e-05
Step 341000 Loss: 0.4309, lr 5.0000000000000057e-05
Step 341100 Loss: 0.4281, lr 5.0000000000000057e-05
Step 341200 Loss: 0.4255, lr 5.0000000000000057e-05
Step 341300 Loss: 0.4228, lr 5.0000000000000057e-05
Step 341400 Loss: 0.4204, lr 5.0000000000000057e-05
Step 341500 Loss: 0.4180, lr 5.0000000000000057e-05
Step 341600 Loss: 0.4161, lr 5.0000000000000057e-05
Step 341700 Loss: 0.4141, lr 5.0000000000000057e-05
Step 341800 Loss: 0.4122, lr 5.0000000000000057e-05
Step 341900 Loss: 0.4109, lr 5.0000000000000057e-05
Step 342000 Loss: 0.4097, lr 5.0000000000000057e-05
Step 342100 Loss: 0.4083, lr 5.0000000000000057e-05
Step 342200 Loss: 0.4073, lr 5.0000000000000057e-05
Step 342300 Loss: 0.4059, lr 5.0000000000000057e-05
Step 342400 Loss: 0.4044, lr 5.0000000000000057e-05
Step 342500 Loss: 0.4027, lr 5.0000000000000057e-05
Step 342600 Loss: 0.4015, lr 5.0000000000000057e-05
Step 342700 Loss: 0.4005, lr 5.0000000000000057e-05
Step 342800 Loss: 0.4000, lr 5.0000000000000057e-05
Step 342900 Loss: 0.3996, lr 5.0000000000000057e-05
Step 343000 Loss: 0.3992, lr 5.0000000000000057e-05
Step 343100 Loss: 0.3987, lr 5.0000000000000057e-05
Step 343200 Loss: 0.3982, lr 5.0000000000000057e-05
Step 343300 Loss: 0.3977, lr 5.0000000000000057e-05
Step 343400 Loss: 0.3973, lr 5.0000000000000057e-05
Step 343500 Loss: 0.3974, lr 5.0000000000000057e-05
Step 343600 Loss: 0.3974, lr 5.0000000000000057e-05
Step 343700 Loss: 0.3981, lr 5.0000000000000057e-05
Step 343800 Loss: 0.3978, lr 5.0000000000000057e-05
Step 343900 Loss: 0.3982, lr 5.0000000000000057e-05
Step 344000 Loss: 0.3989, lr 5.0000000000000057e-05
Step 344100 Loss: 0.3984, lr 5.0000000000000057e-05
Step 344200 Loss: 0.3983, lr 5.0000000000000057e-05
Step 344300 Loss: 0.3977, lr 5.0000000000000057e-05
Step 344400 Loss: 0.3964, lr 5.0000000000000057e-05
Step 344500 Loss: 0.3960, lr 5.0000000000000057e-05
Train Epoch: [51/100] Loss: 0.3959,lr 0.000050
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 344600 Loss: 0.8826, lr 4.842946204609364e-05
Step 344700 Loss: 0.7165, lr 4.842946204609364e-05
Step 344800 Loss: 0.6566, lr 4.842946204609364e-05
Step 344900 Loss: 0.6170, lr 4.842946204609364e-05
Step 345000 Loss: 0.5976, lr 4.842946204609364e-05
Step 345100 Loss: 0.5808, lr 4.842946204609364e-05
Step 345200 Loss: 0.5632, lr 4.842946204609364e-05
Step 345300 Loss: 0.5503, lr 4.842946204609364e-05
Step 345400 Loss: 0.5384, lr 4.842946204609364e-05
Step 345500 Loss: 0.5278, lr 4.842946204609364e-05
Step 345600 Loss: 0.5203, lr 4.842946204609364e-05
Step 345700 Loss: 0.5118, lr 4.842946204609364e-05
Step 345800 Loss: 0.5052, lr 4.842946204609364e-05
Step 345900 Loss: 0.4975, lr 4.842946204609364e-05
Step 346000 Loss: 0.4919, lr 4.842946204609364e-05
Step 346100 Loss: 0.4868, lr 4.842946204609364e-05
Step 346200 Loss: 0.4814, lr 4.842946204609364e-05
Step 346300 Loss: 0.4772, lr 4.842946204609364e-05
Step 346400 Loss: 0.4731, lr 4.842946204609364e-05
Step 346500 Loss: 0.4687, lr 4.842946204609364e-05
Step 346600 Loss: 0.4639, lr 4.842946204609364e-05
Step 346700 Loss: 0.4606, lr 4.842946204609364e-05
Step 346800 Loss: 0.4571, lr 4.842946204609364e-05
Step 346900 Loss: 0.4534, lr 4.842946204609364e-05
Step 347000 Loss: 0.4498, lr 4.842946204609364e-05
Step 347100 Loss: 0.4458, lr 4.842946204609364e-05
Step 347200 Loss: 0.4426, lr 4.842946204609364e-05
Step 347300 Loss: 0.4405, lr 4.842946204609364e-05
Step 347400 Loss: 0.4377, lr 4.842946204609364e-05
Step 347500 Loss: 0.4356, lr 4.842946204609364e-05
Step 347600 Loss: 0.4329, lr 4.842946204609364e-05
Step 347700 Loss: 0.4301, lr 4.842946204609364e-05
Step 347800 Loss: 0.4282, lr 4.842946204609364e-05
Step 347900 Loss: 0.4251, lr 4.842946204609364e-05
Step 348000 Loss: 0.4228, lr 4.842946204609364e-05
Step 348100 Loss: 0.4204, lr 4.842946204609364e-05
Step 348200 Loss: 0.4183, lr 4.842946204609364e-05
Step 348300 Loss: 0.4159, lr 4.842946204609364e-05
Step 348400 Loss: 0.4141, lr 4.842946204609364e-05
Step 348500 Loss: 0.4120, lr 4.842946204609364e-05
Step 348600 Loss: 0.4105, lr 4.842946204609364e-05
Step 348700 Loss: 0.4090, lr 4.842946204609364e-05
Step 348800 Loss: 0.4078, lr 4.842946204609364e-05
Step 348900 Loss: 0.4063, lr 4.842946204609364e-05
Step 349000 Loss: 0.4052, lr 4.842946204609364e-05
Step 349100 Loss: 0.4035, lr 4.842946204609364e-05
Step 349200 Loss: 0.4018, lr 4.842946204609364e-05
Step 349300 Loss: 0.4006, lr 4.842946204609364e-05
Step 349400 Loss: 0.3998, lr 4.842946204609364e-05
Step 349500 Loss: 0.3991, lr 4.842946204609364e-05
Step 349600 Loss: 0.3984, lr 4.842946204609364e-05
Step 349700 Loss: 0.3980, lr 4.842946204609364e-05
Step 349800 Loss: 0.3973, lr 4.842946204609364e-05
Step 349900 Loss: 0.3969, lr 4.842946204609364e-05
Step 350000 Loss: 0.3965, lr 4.842946204609364e-05
Step 350100 Loss: 0.3962, lr 4.842946204609364e-05
Step 350200 Loss: 0.3960, lr 4.842946204609364e-05
Step 350300 Loss: 0.3960, lr 4.842946204609364e-05
Step 350400 Loss: 0.3966, lr 4.842946204609364e-05
Step 350500 Loss: 0.3970, lr 4.842946204609364e-05
Step 350600 Loss: 0.3971, lr 4.842946204609364e-05
Step 350700 Loss: 0.3980, lr 4.842946204609364e-05
Step 350800 Loss: 0.3980, lr 4.842946204609364e-05
Step 350900 Loss: 0.3979, lr 4.842946204609364e-05
Step 351000 Loss: 0.3974, lr 4.842946204609364e-05
Step 351100 Loss: 0.3969, lr 4.842946204609364e-05
Step 351200 Loss: 0.3956, lr 4.842946204609364e-05
Train Epoch: [52/100] Loss: 0.3955,lr 0.000048
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6755
Step 351300 Loss: 1.7253, lr 4.686047402353438e-05
Step 351400 Loss: 0.7813, lr 4.686047402353438e-05
Step 351500 Loss: 0.6861, lr 4.686047402353438e-05
Step 351600 Loss: 0.6362, lr 4.686047402353438e-05
Step 351700 Loss: 0.6083, lr 4.686047402353438e-05
Step 351800 Loss: 0.5885, lr 4.686047402353438e-05
Step 351900 Loss: 0.5696, lr 4.686047402353438e-05
Step 352000 Loss: 0.5571, lr 4.686047402353438e-05
Step 352100 Loss: 0.5441, lr 4.686047402353438e-05
Step 352200 Loss: 0.5332, lr 4.686047402353438e-05
Step 352300 Loss: 0.5233, lr 4.686047402353438e-05
Step 352400 Loss: 0.5162, lr 4.686047402353438e-05
Step 352500 Loss: 0.5080, lr 4.686047402353438e-05
Step 352600 Loss: 0.5018, lr 4.686047402353438e-05
Step 352700 Loss: 0.4955, lr 4.686047402353438e-05
Step 352800 Loss: 0.4896, lr 4.686047402353438e-05
Step 352900 Loss: 0.4842, lr 4.686047402353438e-05
Step 353000 Loss: 0.4797, lr 4.686047402353438e-05
Step 353100 Loss: 0.4754, lr 4.686047402353438e-05
Step 353200 Loss: 0.4713, lr 4.686047402353438e-05
Step 353300 Loss: 0.4667, lr 4.686047402353438e-05
Step 353400 Loss: 0.4626, lr 4.686047402353438e-05
Step 353500 Loss: 0.4592, lr 4.686047402353438e-05
Step 353600 Loss: 0.4557, lr 4.686047402353438e-05
Step 353700 Loss: 0.4530, lr 4.686047402353438e-05
Step 353800 Loss: 0.4489, lr 4.686047402353438e-05
Step 353900 Loss: 0.4451, lr 4.686047402353438e-05
Step 354000 Loss: 0.4424, lr 4.686047402353438e-05
Step 354100 Loss: 0.4402, lr 4.686047402353438e-05
Step 354200 Loss: 0.4374, lr 4.686047402353438e-05
Step 354300 Loss: 0.4351, lr 4.686047402353438e-05
Step 354400 Loss: 0.4325, lr 4.686047402353438e-05
Step 354500 Loss: 0.4301, lr 4.686047402353438e-05
Step 354600 Loss: 0.4275, lr 4.686047402353438e-05
Step 354700 Loss: 0.4251, lr 4.686047402353438e-05
Step 354800 Loss: 0.4224, lr 4.686047402353438e-05
Step 354900 Loss: 0.4198, lr 4.686047402353438e-05
Step 355000 Loss: 0.4176, lr 4.686047402353438e-05
Step 355100 Loss: 0.4156, lr 4.686047402353438e-05
Step 355200 Loss: 0.4138, lr 4.686047402353438e-05
Step 355300 Loss: 0.4119, lr 4.686047402353438e-05
Step 355400 Loss: 0.4103, lr 4.686047402353438e-05
Step 355500 Loss: 0.4091, lr 4.686047402353438e-05
Step 355600 Loss: 0.4075, lr 4.686047402353438e-05
Step 355700 Loss: 0.4063, lr 4.686047402353438e-05
Step 355800 Loss: 0.4048, lr 4.686047402353438e-05
Step 355900 Loss: 0.4031, lr 4.686047402353438e-05
Step 356000 Loss: 0.4017, lr 4.686047402353438e-05
Step 356100 Loss: 0.4008, lr 4.686047402353438e-05
Step 356200 Loss: 0.3998, lr 4.686047402353438e-05
Step 356300 Loss: 0.3991, lr 4.686047402353438e-05
Step 356400 Loss: 0.3986, lr 4.686047402353438e-05
Step 356500 Loss: 0.3982, lr 4.686047402353438e-05
Step 356600 Loss: 0.3974, lr 4.686047402353438e-05
Step 356700 Loss: 0.3972, lr 4.686047402353438e-05
Step 356800 Loss: 0.3968, lr 4.686047402353438e-05
Step 356900 Loss: 0.3965, lr 4.686047402353438e-05
Step 357000 Loss: 0.3962, lr 4.686047402353438e-05
Step 357100 Loss: 0.3962, lr 4.686047402353438e-05
Step 357200 Loss: 0.3967, lr 4.686047402353438e-05
Step 357300 Loss: 0.3965, lr 4.686047402353438e-05
Step 357400 Loss: 0.3967, lr 4.686047402353438e-05
Step 357500 Loss: 0.3971, lr 4.686047402353438e-05
Step 357600 Loss: 0.3969, lr 4.686047402353438e-05
Step 357700 Loss: 0.3968, lr 4.686047402353438e-05
Step 357800 Loss: 0.3961, lr 4.686047402353438e-05
Step 357900 Loss: 0.3950, lr 4.686047402353438e-05
Step 358000 Loss: 0.3942, lr 4.686047402353438e-05
Train Epoch: [53/100] Loss: 0.3941,lr 0.000047
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Step 358100 Loss: 0.9356, lr 4.529458433407433e-05
Step 358200 Loss: 0.7243, lr 4.529458433407433e-05
Step 358300 Loss: 0.6612, lr 4.529458433407433e-05
Step 358400 Loss: 0.6203, lr 4.529458433407433e-05
Step 358500 Loss: 0.5998, lr 4.529458433407433e-05
Step 358600 Loss: 0.5822, lr 4.529458433407433e-05
Step 358700 Loss: 0.5640, lr 4.529458433407433e-05
Step 358800 Loss: 0.5500, lr 4.529458433407433e-05
Step 358900 Loss: 0.5385, lr 4.529458433407433e-05
Step 359000 Loss: 0.5278, lr 4.529458433407433e-05
Step 359100 Loss: 0.5201, lr 4.529458433407433e-05
Step 359200 Loss: 0.5123, lr 4.529458433407433e-05
Step 359300 Loss: 0.5052, lr 4.529458433407433e-05
Step 359400 Loss: 0.4978, lr 4.529458433407433e-05
Step 359500 Loss: 0.4921, lr 4.529458433407433e-05
Step 359600 Loss: 0.4873, lr 4.529458433407433e-05
Step 359700 Loss: 0.4816, lr 4.529458433407433e-05
Step 359800 Loss: 0.4780, lr 4.529458433407433e-05
Step 359900 Loss: 0.4735, lr 4.529458433407433e-05
Step 360000 Loss: 0.4691, lr 4.529458433407433e-05
Step 360100 Loss: 0.4646, lr 4.529458433407433e-05
Step 360200 Loss: 0.4611, lr 4.529458433407433e-05
Step 360300 Loss: 0.4572, lr 4.529458433407433e-05
Step 360400 Loss: 0.4539, lr 4.529458433407433e-05
Step 360500 Loss: 0.4503, lr 4.529458433407433e-05
Step 360600 Loss: 0.4464, lr 4.529458433407433e-05
Step 360700 Loss: 0.4431, lr 4.529458433407433e-05
Step 360800 Loss: 0.4407, lr 4.529458433407433e-05
Step 360900 Loss: 0.4381, lr 4.529458433407433e-05
Step 361000 Loss: 0.4358, lr 4.529458433407433e-05
Step 361100 Loss: 0.4333, lr 4.529458433407433e-05
Step 361200 Loss: 0.4305, lr 4.529458433407433e-05
Step 361300 Loss: 0.4280, lr 4.529458433407433e-05
Step 361400 Loss: 0.4251, lr 4.529458433407433e-05
Step 361500 Loss: 0.4228, lr 4.529458433407433e-05
Step 361600 Loss: 0.4203, lr 4.529458433407433e-05
Step 361700 Loss: 0.4180, lr 4.529458433407433e-05
Step 361800 Loss: 0.4157, lr 4.529458433407433e-05
Step 361900 Loss: 0.4140, lr 4.529458433407433e-05
Step 362000 Loss: 0.4120, lr 4.529458433407433e-05
Step 362100 Loss: 0.4104, lr 4.529458433407433e-05
Step 362200 Loss: 0.4089, lr 4.529458433407433e-05
Step 362300 Loss: 0.4078, lr 4.529458433407433e-05
Step 362400 Loss: 0.4062, lr 4.529458433407433e-05
Step 362500 Loss: 0.4052, lr 4.529458433407433e-05
Step 362600 Loss: 0.4034, lr 4.529458433407433e-05
Step 362700 Loss: 0.4018, lr 4.529458433407433e-05
Step 362800 Loss: 0.4005, lr 4.529458433407433e-05
Step 362900 Loss: 0.3997, lr 4.529458433407433e-05
Step 363000 Loss: 0.3992, lr 4.529458433407433e-05
Step 363100 Loss: 0.3985, lr 4.529458433407433e-05
Step 363200 Loss: 0.3980, lr 4.529458433407433e-05
Step 363300 Loss: 0.3974, lr 4.529458433407433e-05
Step 363400 Loss: 0.3970, lr 4.529458433407433e-05
Step 363500 Loss: 0.3964, lr 4.529458433407433e-05
Step 363600 Loss: 0.3959, lr 4.529458433407433e-05
Step 363700 Loss: 0.3958, lr 4.529458433407433e-05
Step 363800 Loss: 0.3958, lr 4.529458433407433e-05
Step 363900 Loss: 0.3960, lr 4.529458433407433e-05
Step 364000 Loss: 0.3963, lr 4.529458433407433e-05
Step 364100 Loss: 0.3964, lr 4.529458433407433e-05
Step 364200 Loss: 0.3971, lr 4.529458433407433e-05
Step 364300 Loss: 0.3972, lr 4.529458433407433e-05
Step 364400 Loss: 0.3973, lr 4.529458433407433e-05
Step 364500 Loss: 0.3971, lr 4.529458433407433e-05
Step 364600 Loss: 0.3979, lr 4.529458433407433e-05
Step 364700 Loss: 0.3970, lr 4.529458433407433e-05
Step 364800 Loss: 0.3969, lr 4.529458433407433e-05
Train Epoch: [54/100] Loss: 0.3970,lr 0.000045
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 364900 Loss: 0.8024, lr 4.3733338321784836e-05
Step 365000 Loss: 0.6945, lr 4.3733338321784836e-05
Step 365100 Loss: 0.6423, lr 4.3733338321784836e-05
Step 365200 Loss: 0.6117, lr 4.3733338321784836e-05
Step 365300 Loss: 0.5914, lr 4.3733338321784836e-05
Step 365400 Loss: 0.5726, lr 4.3733338321784836e-05
Step 365500 Loss: 0.5582, lr 4.3733338321784836e-05
Step 365600 Loss: 0.5455, lr 4.3733338321784836e-05
Step 365700 Loss: 0.5347, lr 4.3733338321784836e-05
Step 365800 Loss: 0.5243, lr 4.3733338321784836e-05
Step 365900 Loss: 0.5170, lr 4.3733338321784836e-05
Step 366000 Loss: 0.5086, lr 4.3733338321784836e-05
Step 366100 Loss: 0.5022, lr 4.3733338321784836e-05
Step 366200 Loss: 0.4954, lr 4.3733338321784836e-05
Step 366300 Loss: 0.4894, lr 4.3733338321784836e-05
Step 366400 Loss: 0.4844, lr 4.3733338321784836e-05
Step 366500 Loss: 0.4795, lr 4.3733338321784836e-05
Step 366600 Loss: 0.4756, lr 4.3733338321784836e-05
Step 366700 Loss: 0.4715, lr 4.3733338321784836e-05
Step 366800 Loss: 0.4665, lr 4.3733338321784836e-05
Step 366900 Loss: 0.4618, lr 4.3733338321784836e-05
Step 367000 Loss: 0.4586, lr 4.3733338321784836e-05
Step 367100 Loss: 0.4551, lr 4.3733338321784836e-05
Step 367200 Loss: 0.4520, lr 4.3733338321784836e-05
Step 367300 Loss: 0.4481, lr 4.3733338321784836e-05
Step 367400 Loss: 0.4444, lr 4.3733338321784836e-05
Step 367500 Loss: 0.4413, lr 4.3733338321784836e-05
Step 367600 Loss: 0.4390, lr 4.3733338321784836e-05
Step 367700 Loss: 0.4363, lr 4.3733338321784836e-05
Step 367800 Loss: 0.4338, lr 4.3733338321784836e-05
Step 367900 Loss: 0.4313, lr 4.3733338321784836e-05
Step 368000 Loss: 0.4288, lr 4.3733338321784836e-05
Step 368100 Loss: 0.4262, lr 4.3733338321784836e-05
Step 368200 Loss: 0.4237, lr 4.3733338321784836e-05
Step 368300 Loss: 0.4212, lr 4.3733338321784836e-05
Step 368400 Loss: 0.4185, lr 4.3733338321784836e-05
Step 368500 Loss: 0.4165, lr 4.3733338321784836e-05
Step 368600 Loss: 0.4145, lr 4.3733338321784836e-05
Step 368700 Loss: 0.4124, lr 4.3733338321784836e-05
Step 368800 Loss: 0.4107, lr 4.3733338321784836e-05
Step 368900 Loss: 0.4090, lr 4.3733338321784836e-05
Step 369000 Loss: 0.4078, lr 4.3733338321784836e-05
Step 369100 Loss: 0.4063, lr 4.3733338321784836e-05
Step 369200 Loss: 0.4051, lr 4.3733338321784836e-05
Step 369300 Loss: 0.4037, lr 4.3733338321784836e-05
Step 369400 Loss: 0.4018, lr 4.3733338321784836e-05
Step 369500 Loss: 0.4006, lr 4.3733338321784836e-05
Step 369600 Loss: 0.3994, lr 4.3733338321784836e-05
Step 369700 Loss: 0.3985, lr 4.3733338321784836e-05
Step 369800 Loss: 0.3979, lr 4.3733338321784836e-05
Step 369900 Loss: 0.3972, lr 4.3733338321784836e-05
Step 370000 Loss: 0.3969, lr 4.3733338321784836e-05
Step 370100 Loss: 0.3960, lr 4.3733338321784836e-05
Step 370200 Loss: 0.3958, lr 4.3733338321784836e-05
Step 370300 Loss: 0.3955, lr 4.3733338321784836e-05
Step 370400 Loss: 0.3951, lr 4.3733338321784836e-05
Step 370500 Loss: 0.3949, lr 4.3733338321784836e-05
Step 370600 Loss: 0.3951, lr 4.3733338321784836e-05
Step 370700 Loss: 0.3956, lr 4.3733338321784836e-05
Step 370800 Loss: 0.3957, lr 4.3733338321784836e-05
Step 370900 Loss: 0.3964, lr 4.3733338321784836e-05
Step 371000 Loss: 0.3971, lr 4.3733338321784836e-05
Step 371100 Loss: 0.3971, lr 4.3733338321784836e-05
Step 371200 Loss: 0.3972, lr 4.3733338321784836e-05
Step 371300 Loss: 0.3966, lr 4.3733338321784836e-05
Step 371400 Loss: 0.3955, lr 4.3733338321784836e-05
Step 371500 Loss: 0.3943, lr 4.3733338321784836e-05
Train Epoch: [55/100] Loss: 0.3943,lr 0.000044
Model Saving at epoch 55
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Step 371600 Loss: 0.9829, lr 4.21782767479885e-05
Step 371700 Loss: 0.7332, lr 4.21782767479885e-05
Step 371800 Loss: 0.6647, lr 4.21782767479885e-05
Step 371900 Loss: 0.6243, lr 4.21782767479885e-05
Step 372000 Loss: 0.6009, lr 4.21782767479885e-05
Step 372100 Loss: 0.5836, lr 4.21782767479885e-05
Step 372200 Loss: 0.5652, lr 4.21782767479885e-05
Step 372300 Loss: 0.5514, lr 4.21782767479885e-05
Step 372400 Loss: 0.5394, lr 4.21782767479885e-05
Step 372500 Loss: 0.5290, lr 4.21782767479885e-05
Step 372600 Loss: 0.5206, lr 4.21782767479885e-05
Step 372700 Loss: 0.5135, lr 4.21782767479885e-05
Step 372800 Loss: 0.5053, lr 4.21782767479885e-05
Step 372900 Loss: 0.4985, lr 4.21782767479885e-05
Step 373000 Loss: 0.4923, lr 4.21782767479885e-05
Step 373100 Loss: 0.4873, lr 4.21782767479885e-05
Step 373200 Loss: 0.4817, lr 4.21782767479885e-05
Step 373300 Loss: 0.4776, lr 4.21782767479885e-05
Step 373400 Loss: 0.4732, lr 4.21782767479885e-05
Step 373500 Loss: 0.4684, lr 4.21782767479885e-05
Step 373600 Loss: 0.4639, lr 4.21782767479885e-05
Step 373700 Loss: 0.4600, lr 4.21782767479885e-05
Step 373800 Loss: 0.4564, lr 4.21782767479885e-05
Step 373900 Loss: 0.4527, lr 4.21782767479885e-05
Step 374000 Loss: 0.4497, lr 4.21782767479885e-05
Step 374100 Loss: 0.4455, lr 4.21782767479885e-05
Step 374200 Loss: 0.4421, lr 4.21782767479885e-05
Step 374300 Loss: 0.4400, lr 4.21782767479885e-05
Step 374400 Loss: 0.4373, lr 4.21782767479885e-05
Step 374500 Loss: 0.4347, lr 4.21782767479885e-05
Step 374600 Loss: 0.4324, lr 4.21782767479885e-05
Step 374700 Loss: 0.4297, lr 4.21782767479885e-05
Step 374800 Loss: 0.4273, lr 4.21782767479885e-05
Step 374900 Loss: 0.4247, lr 4.21782767479885e-05
Step 375000 Loss: 0.4221, lr 4.21782767479885e-05
Step 375100 Loss: 0.4196, lr 4.21782767479885e-05
Step 375200 Loss: 0.4176, lr 4.21782767479885e-05
Step 375300 Loss: 0.4149, lr 4.21782767479885e-05
Step 375400 Loss: 0.4132, lr 4.21782767479885e-05
Step 375500 Loss: 0.4112, lr 4.21782767479885e-05
Step 375600 Loss: 0.4094, lr 4.21782767479885e-05
Step 375700 Loss: 0.4080, lr 4.21782767479885e-05
Step 375800 Loss: 0.4067, lr 4.21782767479885e-05
Step 375900 Loss: 0.4053, lr 4.21782767479885e-05
Step 376000 Loss: 0.4043, lr 4.21782767479885e-05
Step 376100 Loss: 0.4028, lr 4.21782767479885e-05
Step 376200 Loss: 0.4014, lr 4.21782767479885e-05
Step 376300 Loss: 0.3998, lr 4.21782767479885e-05
Step 376400 Loss: 0.3989, lr 4.21782767479885e-05
Step 376500 Loss: 0.3982, lr 4.21782767479885e-05
Step 376600 Loss: 0.3974, lr 4.21782767479885e-05
Step 376700 Loss: 0.3970, lr 4.21782767479885e-05
Step 376800 Loss: 0.3964, lr 4.21782767479885e-05
Step 376900 Loss: 0.3959, lr 4.21782767479885e-05
Step 377000 Loss: 0.3953, lr 4.21782767479885e-05
Step 377100 Loss: 0.3949, lr 4.21782767479885e-05
Step 377200 Loss: 0.3946, lr 4.21782767479885e-05
Step 377300 Loss: 0.3947, lr 4.21782767479885e-05
Step 377400 Loss: 0.3945, lr 4.21782767479885e-05
Step 377500 Loss: 0.3950, lr 4.21782767479885e-05
Step 377600 Loss: 0.3950, lr 4.21782767479885e-05
Step 377700 Loss: 0.3952, lr 4.21782767479885e-05
Step 377800 Loss: 0.3956, lr 4.21782767479885e-05
Step 377900 Loss: 0.3953, lr 4.21782767479885e-05
Step 378000 Loss: 0.3949, lr 4.21782767479885e-05
Step 378100 Loss: 0.3946, lr 4.21782767479885e-05
Step 378200 Loss: 0.3934, lr 4.21782767479885e-05
Step 378300 Loss: 0.3927, lr 4.21782767479885e-05
Train Epoch: [56/100] Loss: 0.3927,lr 0.000042
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Step 378400 Loss: 0.8161, lr 4.0630934270713814e-05
Step 378500 Loss: 0.6944, lr 4.0630934270713814e-05
Step 378600 Loss: 0.6417, lr 4.0630934270713814e-05
Step 378700 Loss: 0.6098, lr 4.0630934270713814e-05
Step 378800 Loss: 0.5898, lr 4.0630934270713814e-05
Step 378900 Loss: 0.5720, lr 4.0630934270713814e-05
Step 379000 Loss: 0.5559, lr 4.0630934270713814e-05
Step 379100 Loss: 0.5436, lr 4.0630934270713814e-05
Step 379200 Loss: 0.5318, lr 4.0630934270713814e-05
Step 379300 Loss: 0.5222, lr 4.0630934270713814e-05
Step 379400 Loss: 0.5147, lr 4.0630934270713814e-05
Step 379500 Loss: 0.5063, lr 4.0630934270713814e-05
Step 379600 Loss: 0.5001, lr 4.0630934270713814e-05
Step 379700 Loss: 0.4932, lr 4.0630934270713814e-05
Step 379800 Loss: 0.4875, lr 4.0630934270713814e-05
Step 379900 Loss: 0.4825, lr 4.0630934270713814e-05
Step 380000 Loss: 0.4770, lr 4.0630934270713814e-05
Step 380100 Loss: 0.4735, lr 4.0630934270713814e-05
Step 380200 Loss: 0.4690, lr 4.0630934270713814e-05
Step 380300 Loss: 0.4647, lr 4.0630934270713814e-05
Step 380400 Loss: 0.4600, lr 4.0630934270713814e-05
Step 380500 Loss: 0.4568, lr 4.0630934270713814e-05
Step 380600 Loss: 0.4535, lr 4.0630934270713814e-05
Step 380700 Loss: 0.4507, lr 4.0630934270713814e-05
Step 380800 Loss: 0.4469, lr 4.0630934270713814e-05
Step 380900 Loss: 0.4432, lr 4.0630934270713814e-05
Step 381000 Loss: 0.4401, lr 4.0630934270713814e-05
Step 381100 Loss: 0.4379, lr 4.0630934270713814e-05
Step 381200 Loss: 0.4352, lr 4.0630934270713814e-05
Step 381300 Loss: 0.4328, lr 4.0630934270713814e-05
Step 381400 Loss: 0.4303, lr 4.0630934270713814e-05
Step 381500 Loss: 0.4275, lr 4.0630934270713814e-05
Step 381600 Loss: 0.4253, lr 4.0630934270713814e-05
Step 381700 Loss: 0.4227, lr 4.0630934270713814e-05
Step 381800 Loss: 0.4201, lr 4.0630934270713814e-05
Step 381900 Loss: 0.4175, lr 4.0630934270713814e-05
Step 382000 Loss: 0.4153, lr 4.0630934270713814e-05
Step 382100 Loss: 0.4129, lr 4.0630934270713814e-05
Step 382200 Loss: 0.4110, lr 4.0630934270713814e-05
Step 382300 Loss: 0.4092, lr 4.0630934270713814e-05
Step 382400 Loss: 0.4076, lr 4.0630934270713814e-05
Step 382500 Loss: 0.4064, lr 4.0630934270713814e-05
Step 382600 Loss: 0.4049, lr 4.0630934270713814e-05
Step 382700 Loss: 0.4038, lr 4.0630934270713814e-05
Step 382800 Loss: 0.4024, lr 4.0630934270713814e-05
Step 382900 Loss: 0.4005, lr 4.0630934270713814e-05
Step 383000 Loss: 0.3991, lr 4.0630934270713814e-05
Step 383100 Loss: 0.3977, lr 4.0630934270713814e-05
Step 383200 Loss: 0.3971, lr 4.0630934270713814e-05
Step 383300 Loss: 0.3964, lr 4.0630934270713814e-05
Step 383400 Loss: 0.3956, lr 4.0630934270713814e-05
Step 383500 Loss: 0.3952, lr 4.0630934270713814e-05
Step 383600 Loss: 0.3944, lr 4.0630934270713814e-05
Step 383700 Loss: 0.3938, lr 4.0630934270713814e-05
Step 383800 Loss: 0.3936, lr 4.0630934270713814e-05
Step 383900 Loss: 0.3932, lr 4.0630934270713814e-05
Step 384000 Loss: 0.3930, lr 4.0630934270713814e-05
Step 384100 Loss: 0.3934, lr 4.0630934270713814e-05
Step 384200 Loss: 0.3936, lr 4.0630934270713814e-05
Step 384300 Loss: 0.3938, lr 4.0630934270713814e-05
Step 384400 Loss: 0.3939, lr 4.0630934270713814e-05
Step 384500 Loss: 0.3940, lr 4.0630934270713814e-05
Step 384600 Loss: 0.3937, lr 4.0630934270713814e-05
Step 384700 Loss: 0.3938, lr 4.0630934270713814e-05
Step 384800 Loss: 0.3930, lr 4.0630934270713814e-05
Step 384900 Loss: 0.3922, lr 4.0630934270713814e-05
Step 385000 Loss: 0.3908, lr 4.0630934270713814e-05
Train Epoch: [57/100] Loss: 0.3909,lr 0.000041
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 385100 Loss: 1.0871, lr 3.909283793017293e-05
Step 385200 Loss: 0.7423, lr 3.909283793017293e-05
Step 385300 Loss: 0.6691, lr 3.909283793017293e-05
Step 385400 Loss: 0.6249, lr 3.909283793017293e-05
Step 385500 Loss: 0.6004, lr 3.909283793017293e-05
Step 385600 Loss: 0.5821, lr 3.909283793017293e-05
Step 385700 Loss: 0.5638, lr 3.909283793017293e-05
Step 385800 Loss: 0.5510, lr 3.909283793017293e-05
Step 385900 Loss: 0.5384, lr 3.909283793017293e-05
Step 386000 Loss: 0.5277, lr 3.909283793017293e-05
Step 386100 Loss: 0.5191, lr 3.909283793017293e-05
Step 386200 Loss: 0.5123, lr 3.909283793017293e-05
Step 386300 Loss: 0.5040, lr 3.909283793017293e-05
Step 386400 Loss: 0.4976, lr 3.909283793017293e-05
Step 386500 Loss: 0.4914, lr 3.909283793017293e-05
Step 386600 Loss: 0.4863, lr 3.909283793017293e-05
Step 386700 Loss: 0.4807, lr 3.909283793017293e-05
Step 386800 Loss: 0.4765, lr 3.909283793017293e-05
Step 386900 Loss: 0.4722, lr 3.909283793017293e-05
Step 387000 Loss: 0.4680, lr 3.909283793017293e-05
Step 387100 Loss: 0.4633, lr 3.909283793017293e-05
Step 387200 Loss: 0.4593, lr 3.909283793017293e-05
Step 387300 Loss: 0.4556, lr 3.909283793017293e-05
Step 387400 Loss: 0.4519, lr 3.909283793017293e-05
Step 387500 Loss: 0.4488, lr 3.909283793017293e-05
Step 387600 Loss: 0.4443, lr 3.909283793017293e-05
Step 387700 Loss: 0.4411, lr 3.909283793017293e-05
Step 387800 Loss: 0.4385, lr 3.909283793017293e-05
Step 387900 Loss: 0.4361, lr 3.909283793017293e-05
Step 388000 Loss: 0.4332, lr 3.909283793017293e-05
Step 388100 Loss: 0.4310, lr 3.909283793017293e-05
Step 388200 Loss: 0.4283, lr 3.909283793017293e-05
Step 388300 Loss: 0.4259, lr 3.909283793017293e-05
Step 388400 Loss: 0.4231, lr 3.909283793017293e-05
Step 388500 Loss: 0.4205, lr 3.909283793017293e-05
Step 388600 Loss: 0.4179, lr 3.909283793017293e-05
Step 388700 Loss: 0.4155, lr 3.909283793017293e-05
Step 388800 Loss: 0.4130, lr 3.909283793017293e-05
Step 388900 Loss: 0.4110, lr 3.909283793017293e-05
Step 389000 Loss: 0.4091, lr 3.909283793017293e-05
Step 389100 Loss: 0.4073, lr 3.909283793017293e-05
Step 389200 Loss: 0.4058, lr 3.909283793017293e-05
Step 389300 Loss: 0.4046, lr 3.909283793017293e-05
Step 389400 Loss: 0.4031, lr 3.909283793017293e-05
Step 389500 Loss: 0.4022, lr 3.909283793017293e-05
Step 389600 Loss: 0.4007, lr 3.909283793017293e-05
Step 389700 Loss: 0.3990, lr 3.909283793017293e-05
Step 389800 Loss: 0.3975, lr 3.909283793017293e-05
Step 389900 Loss: 0.3967, lr 3.909283793017293e-05
Step 390000 Loss: 0.3957, lr 3.909283793017293e-05
Step 390100 Loss: 0.3949, lr 3.909283793017293e-05
Step 390200 Loss: 0.3945, lr 3.909283793017293e-05
Step 390300 Loss: 0.3940, lr 3.909283793017293e-05
Step 390400 Loss: 0.3933, lr 3.909283793017293e-05
Step 390500 Loss: 0.3927, lr 3.909283793017293e-05
Step 390600 Loss: 0.3923, lr 3.909283793017293e-05
Step 390700 Loss: 0.3921, lr 3.909283793017293e-05
Step 390800 Loss: 0.3920, lr 3.909283793017293e-05
Step 390900 Loss: 0.3920, lr 3.909283793017293e-05
Step 391000 Loss: 0.3924, lr 3.909283793017293e-05
Step 391100 Loss: 0.3924, lr 3.909283793017293e-05
Step 391200 Loss: 0.3927, lr 3.909283793017293e-05
Step 391300 Loss: 0.3931, lr 3.909283793017293e-05
Step 391400 Loss: 0.3927, lr 3.909283793017293e-05
Step 391500 Loss: 0.3922, lr 3.909283793017293e-05
Step 391600 Loss: 0.3914, lr 3.909283793017293e-05
Step 391700 Loss: 0.3901, lr 3.909283793017293e-05
Step 391800 Loss: 0.3895, lr 3.909283793017293e-05
Train Epoch: [58/100] Loss: 0.3896,lr 0.000039
Calling G2SDataset.batch()
Done, time:  2.13 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6756
Step 391900 Loss: 0.8437, lr 3.75655056417573e-05
Step 392000 Loss: 0.7053, lr 3.75655056417573e-05
Step 392100 Loss: 0.6501, lr 3.75655056417573e-05
Step 392200 Loss: 0.6131, lr 3.75655056417573e-05
Step 392300 Loss: 0.5935, lr 3.75655056417573e-05
Step 392400 Loss: 0.5758, lr 3.75655056417573e-05
Step 392500 Loss: 0.5584, lr 3.75655056417573e-05
Step 392600 Loss: 0.5462, lr 3.75655056417573e-05
Step 392700 Loss: 0.5345, lr 3.75655056417573e-05
Step 392800 Loss: 0.5244, lr 3.75655056417573e-05
Step 392900 Loss: 0.5170, lr 3.75655056417573e-05
Step 393000 Loss: 0.5090, lr 3.75655056417573e-05
Step 393100 Loss: 0.5024, lr 3.75655056417573e-05
Step 393200 Loss: 0.4948, lr 3.75655056417573e-05
Step 393300 Loss: 0.4891, lr 3.75655056417573e-05
Step 393400 Loss: 0.4840, lr 3.75655056417573e-05
Step 393500 Loss: 0.4784, lr 3.75655056417573e-05
Step 393600 Loss: 0.4741, lr 3.75655056417573e-05
Step 393700 Loss: 0.4699, lr 3.75655056417573e-05
Step 393800 Loss: 0.4655, lr 3.75655056417573e-05
Step 393900 Loss: 0.4604, lr 3.75655056417573e-05
Step 394000 Loss: 0.4570, lr 3.75655056417573e-05
Step 394100 Loss: 0.4537, lr 3.75655056417573e-05
Step 394200 Loss: 0.4503, lr 3.75655056417573e-05
Step 394300 Loss: 0.4467, lr 3.75655056417573e-05
Step 394400 Loss: 0.4430, lr 3.75655056417573e-05
Step 394500 Loss: 0.4394, lr 3.75655056417573e-05
Step 394600 Loss: 0.4373, lr 3.75655056417573e-05
Step 394700 Loss: 0.4345, lr 3.75655056417573e-05
Step 394800 Loss: 0.4324, lr 3.75655056417573e-05
Step 394900 Loss: 0.4299, lr 3.75655056417573e-05
Step 395000 Loss: 0.4270, lr 3.75655056417573e-05
Step 395100 Loss: 0.4248, lr 3.75655056417573e-05
Step 395200 Loss: 0.4219, lr 3.75655056417573e-05
Step 395300 Loss: 0.4193, lr 3.75655056417573e-05
Step 395400 Loss: 0.4167, lr 3.75655056417573e-05
Step 395500 Loss: 0.4146, lr 3.75655056417573e-05
Step 395600 Loss: 0.4122, lr 3.75655056417573e-05
Step 395700 Loss: 0.4103, lr 3.75655056417573e-05
Step 395800 Loss: 0.4085, lr 3.75655056417573e-05
Step 395900 Loss: 0.4069, lr 3.75655056417573e-05
Step 396000 Loss: 0.4054, lr 3.75655056417573e-05
Step 396100 Loss: 0.4043, lr 3.75655056417573e-05
Step 396200 Loss: 0.4028, lr 3.75655056417573e-05
Step 396300 Loss: 0.4017, lr 3.75655056417573e-05
Step 396400 Loss: 0.4000, lr 3.75655056417573e-05
Step 396500 Loss: 0.3985, lr 3.75655056417573e-05
Step 396600 Loss: 0.3973, lr 3.75655056417573e-05
Step 396700 Loss: 0.3965, lr 3.75655056417573e-05
Step 396800 Loss: 0.3956, lr 3.75655056417573e-05
Step 396900 Loss: 0.3949, lr 3.75655056417573e-05
Step 397000 Loss: 0.3943, lr 3.75655056417573e-05
Step 397100 Loss: 0.3936, lr 3.75655056417573e-05
Step 397200 Loss: 0.3930, lr 3.75655056417573e-05
Step 397300 Loss: 0.3927, lr 3.75655056417573e-05
Step 397400 Loss: 0.3920, lr 3.75655056417573e-05
Step 397500 Loss: 0.3917, lr 3.75655056417573e-05
Step 397600 Loss: 0.3917, lr 3.75655056417573e-05
Step 397700 Loss: 0.3921, lr 3.75655056417573e-05
Step 397800 Loss: 0.3921, lr 3.75655056417573e-05
Step 397900 Loss: 0.3921, lr 3.75655056417573e-05
Step 398000 Loss: 0.3924, lr 3.75655056417573e-05
Step 398100 Loss: 0.3921, lr 3.75655056417573e-05
Step 398200 Loss: 0.3918, lr 3.75655056417573e-05
Step 398300 Loss: 0.3911, lr 3.75655056417573e-05
Step 398400 Loss: 0.3902, lr 3.75655056417573e-05
Step 398500 Loss: 0.3889, lr 3.75655056417573e-05
Train Epoch: [59/100] Loss: 0.3884,lr 0.000038
Calling G2SDataset.batch()
Done, time:  2.39 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Step 398600 Loss: 1.2831, lr 3.605044469803859e-05
Step 398700 Loss: 0.7643, lr 3.605044469803859e-05
Step 398800 Loss: 0.6765, lr 3.605044469803859e-05
Step 398900 Loss: 0.6299, lr 3.605044469803859e-05
Step 399000 Loss: 0.6040, lr 3.605044469803859e-05
Step 399100 Loss: 0.5848, lr 3.605044469803859e-05
Step 399200 Loss: 0.5659, lr 3.605044469803859e-05
Step 399300 Loss: 0.5524, lr 3.605044469803859e-05
Step 399400 Loss: 0.5401, lr 3.605044469803859e-05
Step 399500 Loss: 0.5291, lr 3.605044469803859e-05
Step 399600 Loss: 0.5189, lr 3.605044469803859e-05
Step 399700 Loss: 0.5121, lr 3.605044469803859e-05
Step 399800 Loss: 0.5036, lr 3.605044469803859e-05
Step 399900 Loss: 0.4972, lr 3.605044469803859e-05
Step 400000 Loss: 0.4906, lr 3.605044469803859e-05
Step 400100 Loss: 0.4850, lr 3.605044469803859e-05
Step 400200 Loss: 0.4794, lr 3.605044469803859e-05
Step 400300 Loss: 0.4752, lr 3.605044469803859e-05
Step 400400 Loss: 0.4706, lr 3.605044469803859e-05
Step 400500 Loss: 0.4664, lr 3.605044469803859e-05
Step 400600 Loss: 0.4617, lr 3.605044469803859e-05
Step 400700 Loss: 0.4574, lr 3.605044469803859e-05
Step 400800 Loss: 0.4536, lr 3.605044469803859e-05
Step 400900 Loss: 0.4500, lr 3.605044469803859e-05
Step 401000 Loss: 0.4472, lr 3.605044469803859e-05
Step 401100 Loss: 0.4428, lr 3.605044469803859e-05
Step 401200 Loss: 0.4392, lr 3.605044469803859e-05
Step 401300 Loss: 0.4362, lr 3.605044469803859e-05
Step 401400 Loss: 0.4338, lr 3.605044469803859e-05
Step 401500 Loss: 0.4310, lr 3.605044469803859e-05
Step 401600 Loss: 0.4290, lr 3.605044469803859e-05
Step 401700 Loss: 0.4260, lr 3.605044469803859e-05
Step 401800 Loss: 0.4237, lr 3.605044469803859e-05
Step 401900 Loss: 0.4208, lr 3.605044469803859e-05
Step 402000 Loss: 0.4182, lr 3.605044469803859e-05
Step 402100 Loss: 0.4154, lr 3.605044469803859e-05
Step 402200 Loss: 0.4128, lr 3.605044469803859e-05
Step 402300 Loss: 0.4105, lr 3.605044469803859e-05
Step 402400 Loss: 0.4084, lr 3.605044469803859e-05
Step 402500 Loss: 0.4061, lr 3.605044469803859e-05
Step 402600 Loss: 0.4042, lr 3.605044469803859e-05
Step 402700 Loss: 0.4025, lr 3.605044469803859e-05
Step 402800 Loss: 0.4011, lr 3.605044469803859e-05
Step 402900 Loss: 0.3994, lr 3.605044469803859e-05
Step 403000 Loss: 0.3980, lr 3.605044469803859e-05
Step 403100 Loss: 0.3968, lr 3.605044469803859e-05
Step 403200 Loss: 0.3951, lr 3.605044469803859e-05
Step 403300 Loss: 0.3936, lr 3.605044469803859e-05
Step 403400 Loss: 0.3925, lr 3.605044469803859e-05
Step 403500 Loss: 0.3913, lr 3.605044469803859e-05
Step 403600 Loss: 0.3908, lr 3.605044469803859e-05
Step 403700 Loss: 0.3903, lr 3.605044469803859e-05
Step 403800 Loss: 0.3898, lr 3.605044469803859e-05
Step 403900 Loss: 0.3890, lr 3.605044469803859e-05
Step 404000 Loss: 0.3885, lr 3.605044469803859e-05
Step 404100 Loss: 0.3879, lr 3.605044469803859e-05
Step 404200 Loss: 0.3875, lr 3.605044469803859e-05
Step 404300 Loss: 0.3873, lr 3.605044469803859e-05
Step 404400 Loss: 0.3872, lr 3.605044469803859e-05
Step 404500 Loss: 0.3876, lr 3.605044469803859e-05
Step 404600 Loss: 0.3871, lr 3.605044469803859e-05
Step 404700 Loss: 0.3873, lr 3.605044469803859e-05
Step 404800 Loss: 0.3876, lr 3.605044469803859e-05
Step 404900 Loss: 0.3870, lr 3.605044469803859e-05
Step 405000 Loss: 0.3864, lr 3.605044469803859e-05
Step 405100 Loss: 0.3854, lr 3.605044469803859e-05
Step 405200 Loss: 0.3840, lr 3.605044469803859e-05
Step 405300 Loss: 0.3833, lr 3.605044469803859e-05
Train Epoch: [60/100] Loss: 0.3831,lr 0.000036
Model Saving at epoch 60
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Step 405400 Loss: 0.8923, lr 3.454915028125269e-05
Step 405500 Loss: 0.7145, lr 3.454915028125269e-05
Step 405600 Loss: 0.6529, lr 3.454915028125269e-05
Step 405700 Loss: 0.6112, lr 3.454915028125269e-05
Step 405800 Loss: 0.5916, lr 3.454915028125269e-05
Step 405900 Loss: 0.5726, lr 3.454915028125269e-05
Step 406000 Loss: 0.5537, lr 3.454915028125269e-05
Step 406100 Loss: 0.5397, lr 3.454915028125269e-05
Step 406200 Loss: 0.5273, lr 3.454915028125269e-05
Step 406300 Loss: 0.5152, lr 3.454915028125269e-05
Step 406400 Loss: 0.5073, lr 3.454915028125269e-05
Step 406500 Loss: 0.4990, lr 3.454915028125269e-05
Step 406600 Loss: 0.4912, lr 3.454915028125269e-05
Step 406700 Loss: 0.4837, lr 3.454915028125269e-05
Step 406800 Loss: 0.4771, lr 3.454915028125269e-05
Step 406900 Loss: 0.4710, lr 3.454915028125269e-05
Step 407000 Loss: 0.4648, lr 3.454915028125269e-05
Step 407100 Loss: 0.4603, lr 3.454915028125269e-05
Step 407200 Loss: 0.4551, lr 3.454915028125269e-05
Step 407300 Loss: 0.4500, lr 3.454915028125269e-05
Step 407400 Loss: 0.4446, lr 3.454915028125269e-05
Step 407500 Loss: 0.4407, lr 3.454915028125269e-05
Step 407600 Loss: 0.4369, lr 3.454915028125269e-05
Step 407700 Loss: 0.4328, lr 3.454915028125269e-05
Step 407800 Loss: 0.4288, lr 3.454915028125269e-05
Step 407900 Loss: 0.4247, lr 3.454915028125269e-05
Step 408000 Loss: 0.4209, lr 3.454915028125269e-05
Step 408100 Loss: 0.4186, lr 3.454915028125269e-05
Step 408200 Loss: 0.4153, lr 3.454915028125269e-05
Step 408300 Loss: 0.4130, lr 3.454915028125269e-05
Step 408400 Loss: 0.4102, lr 3.454915028125269e-05
Step 408500 Loss: 0.4071, lr 3.454915028125269e-05
Step 408600 Loss: 0.4047, lr 3.454915028125269e-05
Step 408700 Loss: 0.4016, lr 3.454915028125269e-05
Step 408800 Loss: 0.3989, lr 3.454915028125269e-05
Step 408900 Loss: 0.3957, lr 3.454915028125269e-05
Step 409000 Loss: 0.3932, lr 3.454915028125269e-05
Step 409100 Loss: 0.3907, lr 3.454915028125269e-05
Step 409200 Loss: 0.3883, lr 3.454915028125269e-05
Step 409300 Loss: 0.3861, lr 3.454915028125269e-05
Step 409400 Loss: 0.3846, lr 3.454915028125269e-05
Step 409500 Loss: 0.3828, lr 3.454915028125269e-05
Step 409600 Loss: 0.3816, lr 3.454915028125269e-05
Step 409700 Loss: 0.3798, lr 3.454915028125269e-05
Step 409800 Loss: 0.3786, lr 3.454915028125269e-05
Step 409900 Loss: 0.3769, lr 3.454915028125269e-05
Step 410000 Loss: 0.3753, lr 3.454915028125269e-05
Step 410100 Loss: 0.3737, lr 3.454915028125269e-05
Step 410200 Loss: 0.3730, lr 3.454915028125269e-05
Step 410300 Loss: 0.3721, lr 3.454915028125269e-05
Step 410400 Loss: 0.3714, lr 3.454915028125269e-05
Step 410500 Loss: 0.3710, lr 3.454915028125269e-05
Step 410600 Loss: 0.3704, lr 3.454915028125269e-05
Step 410700 Loss: 0.3699, lr 3.454915028125269e-05
Step 410800 Loss: 0.3695, lr 3.454915028125269e-05
Step 410900 Loss: 0.3691, lr 3.454915028125269e-05
Step 411000 Loss: 0.3687, lr 3.454915028125269e-05
Step 411100 Loss: 0.3688, lr 3.454915028125269e-05
Step 411200 Loss: 0.3690, lr 3.454915028125269e-05
Step 411300 Loss: 0.3691, lr 3.454915028125269e-05
Step 411400 Loss: 0.3692, lr 3.454915028125269e-05
Step 411500 Loss: 0.3697, lr 3.454915028125269e-05
Step 411600 Loss: 0.3698, lr 3.454915028125269e-05
Step 411700 Loss: 0.3694, lr 3.454915028125269e-05
Step 411800 Loss: 0.3686, lr 3.454915028125269e-05
Step 411900 Loss: 0.3678, lr 3.454915028125269e-05
Step 412000 Loss: 0.3664, lr 3.454915028125269e-05
Train Epoch: [61/100] Loss: 0.3660,lr 0.000035
Calling G2SDataset.batch()
Done, time:  2.11 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6755
Step 412100 Loss: 1.8133, lr 3.3063103987735474e-05
Step 412200 Loss: 0.7710, lr 3.3063103987735474e-05
Step 412300 Loss: 0.6654, lr 3.3063103987735474e-05
Step 412400 Loss: 0.6110, lr 3.3063103987735474e-05
Step 412500 Loss: 0.5802, lr 3.3063103987735474e-05
Step 412600 Loss: 0.5598, lr 3.3063103987735474e-05
Step 412700 Loss: 0.5405, lr 3.3063103987735474e-05
Step 412800 Loss: 0.5279, lr 3.3063103987735474e-05
Step 412900 Loss: 0.5143, lr 3.3063103987735474e-05
Step 413000 Loss: 0.5024, lr 3.3063103987735474e-05
Step 413100 Loss: 0.4919, lr 3.3063103987735474e-05
Step 413200 Loss: 0.4844, lr 3.3063103987735474e-05
Step 413300 Loss: 0.4762, lr 3.3063103987735474e-05
Step 413400 Loss: 0.4696, lr 3.3063103987735474e-05
Step 413500 Loss: 0.4626, lr 3.3063103987735474e-05
Step 413600 Loss: 0.4565, lr 3.3063103987735474e-05
Step 413700 Loss: 0.4507, lr 3.3063103987735474e-05
Step 413800 Loss: 0.4463, lr 3.3063103987735474e-05
Step 413900 Loss: 0.4418, lr 3.3063103987735474e-05
Step 414000 Loss: 0.4373, lr 3.3063103987735474e-05
Step 414100 Loss: 0.4322, lr 3.3063103987735474e-05
Step 414200 Loss: 0.4276, lr 3.3063103987735474e-05
Step 414300 Loss: 0.4237, lr 3.3063103987735474e-05
Step 414400 Loss: 0.4205, lr 3.3063103987735474e-05
Step 414500 Loss: 0.4177, lr 3.3063103987735474e-05
Step 414600 Loss: 0.4135, lr 3.3063103987735474e-05
Step 414700 Loss: 0.4097, lr 3.3063103987735474e-05
Step 414800 Loss: 0.4066, lr 3.3063103987735474e-05
Step 414900 Loss: 0.4044, lr 3.3063103987735474e-05
Step 415000 Loss: 0.4014, lr 3.3063103987735474e-05
Step 415100 Loss: 0.3992, lr 3.3063103987735474e-05
Step 415200 Loss: 0.3962, lr 3.3063103987735474e-05
Step 415300 Loss: 0.3938, lr 3.3063103987735474e-05
Step 415400 Loss: 0.3915, lr 3.3063103987735474e-05
Step 415500 Loss: 0.3889, lr 3.3063103987735474e-05
Step 415600 Loss: 0.3864, lr 3.3063103987735474e-05
Step 415700 Loss: 0.3837, lr 3.3063103987735474e-05
Step 415800 Loss: 0.3815, lr 3.3063103987735474e-05
Step 415900 Loss: 0.3794, lr 3.3063103987735474e-05
Step 416000 Loss: 0.3773, lr 3.3063103987735474e-05
Step 416100 Loss: 0.3755, lr 3.3063103987735474e-05
Step 416200 Loss: 0.3740, lr 3.3063103987735474e-05
Step 416300 Loss: 0.3728, lr 3.3063103987735474e-05
Step 416400 Loss: 0.3714, lr 3.3063103987735474e-05
Step 416500 Loss: 0.3702, lr 3.3063103987735474e-05
Step 416600 Loss: 0.3691, lr 3.3063103987735474e-05
Step 416700 Loss: 0.3673, lr 3.3063103987735474e-05
Step 416800 Loss: 0.3662, lr 3.3063103987735474e-05
Step 416900 Loss: 0.3652, lr 3.3063103987735474e-05
Step 417000 Loss: 0.3643, lr 3.3063103987735474e-05
Step 417100 Loss: 0.3638, lr 3.3063103987735474e-05
Step 417200 Loss: 0.3634, lr 3.3063103987735474e-05
Step 417300 Loss: 0.3629, lr 3.3063103987735474e-05
Step 417400 Loss: 0.3622, lr 3.3063103987735474e-05
Step 417500 Loss: 0.3619, lr 3.3063103987735474e-05
Step 417600 Loss: 0.3616, lr 3.3063103987735474e-05
Step 417700 Loss: 0.3613, lr 3.3063103987735474e-05
Step 417800 Loss: 0.3614, lr 3.3063103987735474e-05
Step 417900 Loss: 0.3615, lr 3.3063103987735474e-05
Step 418000 Loss: 0.3620, lr 3.3063103987735474e-05
Step 418100 Loss: 0.3621, lr 3.3063103987735474e-05
Step 418200 Loss: 0.3628, lr 3.3063103987735474e-05
Step 418300 Loss: 0.3632, lr 3.3063103987735474e-05
Step 418400 Loss: 0.3629, lr 3.3063103987735474e-05
Step 418500 Loss: 0.3628, lr 3.3063103987735474e-05
Step 418600 Loss: 0.3619, lr 3.3063103987735474e-05
Step 418700 Loss: 0.3610, lr 3.3063103987735474e-05
Step 418800 Loss: 0.3601, lr 3.3063103987735474e-05
Train Epoch: [62/100] Loss: 0.3601,lr 0.000033
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.20 s, total batches: 6756
Step 418900 Loss: 0.9352, lr 3.159377236576616e-05
Step 419000 Loss: 0.7030, lr 3.159377236576616e-05
Step 419100 Loss: 0.6364, lr 3.159377236576616e-05
Step 419200 Loss: 0.5921, lr 3.159377236576616e-05
Step 419300 Loss: 0.5695, lr 3.159377236576616e-05
Step 419400 Loss: 0.5525, lr 3.159377236576616e-05
Step 419500 Loss: 0.5341, lr 3.159377236576616e-05
Step 419600 Loss: 0.5185, lr 3.159377236576616e-05
Step 419700 Loss: 0.5068, lr 3.159377236576616e-05
Step 419800 Loss: 0.4952, lr 3.159377236576616e-05
Step 419900 Loss: 0.4865, lr 3.159377236576616e-05
Step 420000 Loss: 0.4787, lr 3.159377236576616e-05
Step 420100 Loss: 0.4708, lr 3.159377236576616e-05
Step 420200 Loss: 0.4637, lr 3.159377236576616e-05
Step 420300 Loss: 0.4580, lr 3.159377236576616e-05
Step 420400 Loss: 0.4526, lr 3.159377236576616e-05
Step 420500 Loss: 0.4469, lr 3.159377236576616e-05
Step 420600 Loss: 0.4423, lr 3.159377236576616e-05
Step 420700 Loss: 0.4377, lr 3.159377236576616e-05
Step 420800 Loss: 0.4328, lr 3.159377236576616e-05
Step 420900 Loss: 0.4279, lr 3.159377236576616e-05
Step 421000 Loss: 0.4244, lr 3.159377236576616e-05
Step 421100 Loss: 0.4206, lr 3.159377236576616e-05
Step 421200 Loss: 0.4172, lr 3.159377236576616e-05
Step 421300 Loss: 0.4137, lr 3.159377236576616e-05
Step 421400 Loss: 0.4097, lr 3.159377236576616e-05
Step 421500 Loss: 0.4064, lr 3.159377236576616e-05
Step 421600 Loss: 0.4040, lr 3.159377236576616e-05
Step 421700 Loss: 0.4008, lr 3.159377236576616e-05
Step 421800 Loss: 0.3986, lr 3.159377236576616e-05
Step 421900 Loss: 0.3959, lr 3.159377236576616e-05
Step 422000 Loss: 0.3930, lr 3.159377236576616e-05
Step 422100 Loss: 0.3906, lr 3.159377236576616e-05
Step 422200 Loss: 0.3879, lr 3.159377236576616e-05
Step 422300 Loss: 0.3856, lr 3.159377236576616e-05
Step 422400 Loss: 0.3831, lr 3.159377236576616e-05
Step 422500 Loss: 0.3807, lr 3.159377236576616e-05
Step 422600 Loss: 0.3784, lr 3.159377236576616e-05
Step 422700 Loss: 0.3763, lr 3.159377236576616e-05
Step 422800 Loss: 0.3743, lr 3.159377236576616e-05
Step 422900 Loss: 0.3729, lr 3.159377236576616e-05
Step 423000 Loss: 0.3714, lr 3.159377236576616e-05
Step 423100 Loss: 0.3703, lr 3.159377236576616e-05
Step 423200 Loss: 0.3686, lr 3.159377236576616e-05
Step 423300 Loss: 0.3675, lr 3.159377236576616e-05
Step 423400 Loss: 0.3658, lr 3.159377236576616e-05
Step 423500 Loss: 0.3645, lr 3.159377236576616e-05
Step 423600 Loss: 0.3632, lr 3.159377236576616e-05
Step 423700 Loss: 0.3624, lr 3.159377236576616e-05
Step 423800 Loss: 0.3619, lr 3.159377236576616e-05
Step 423900 Loss: 0.3614, lr 3.159377236576616e-05
Step 424000 Loss: 0.3610, lr 3.159377236576616e-05
Step 424100 Loss: 0.3606, lr 3.159377236576616e-05
Step 424200 Loss: 0.3602, lr 3.159377236576616e-05
Step 424300 Loss: 0.3598, lr 3.159377236576616e-05
Step 424400 Loss: 0.3594, lr 3.159377236576616e-05
Step 424500 Loss: 0.3593, lr 3.159377236576616e-05
Step 424600 Loss: 0.3594, lr 3.159377236576616e-05
Step 424700 Loss: 0.3595, lr 3.159377236576616e-05
Step 424800 Loss: 0.3599, lr 3.159377236576616e-05
Step 424900 Loss: 0.3602, lr 3.159377236576616e-05
Step 425000 Loss: 0.3605, lr 3.159377236576616e-05
Step 425100 Loss: 0.3608, lr 3.159377236576616e-05
Step 425200 Loss: 0.3604, lr 3.159377236576616e-05
Step 425300 Loss: 0.3600, lr 3.159377236576616e-05
Step 425400 Loss: 0.3595, lr 3.159377236576616e-05
Step 425500 Loss: 0.3583, lr 3.159377236576616e-05
Step 425600 Loss: 0.3577, lr 3.159377236576616e-05
Train Epoch: [63/100] Loss: 0.3578,lr 0.000032
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6756
Step 425700 Loss: 0.7928, lr 3.0142605468261005e-05
Step 425800 Loss: 0.6687, lr 3.0142605468261005e-05
Step 425900 Loss: 0.6129, lr 3.0142605468261005e-05
Step 426000 Loss: 0.5799, lr 3.0142605468261005e-05
Step 426100 Loss: 0.5600, lr 3.0142605468261005e-05
Step 426200 Loss: 0.5413, lr 3.0142605468261005e-05
Step 426300 Loss: 0.5264, lr 3.0142605468261005e-05
Step 426400 Loss: 0.5125, lr 3.0142605468261005e-05
Step 426500 Loss: 0.5008, lr 3.0142605468261005e-05
Step 426600 Loss: 0.4905, lr 3.0142605468261005e-05
Step 426700 Loss: 0.4829, lr 3.0142605468261005e-05
Step 426800 Loss: 0.4743, lr 3.0142605468261005e-05
Step 426900 Loss: 0.4677, lr 3.0142605468261005e-05
Step 427000 Loss: 0.4609, lr 3.0142605468261005e-05
Step 427100 Loss: 0.4546, lr 3.0142605468261005e-05
Step 427200 Loss: 0.4488, lr 3.0142605468261005e-05
Step 427300 Loss: 0.4438, lr 3.0142605468261005e-05
Step 427400 Loss: 0.4396, lr 3.0142605468261005e-05
Step 427500 Loss: 0.4351, lr 3.0142605468261005e-05
Step 427600 Loss: 0.4302, lr 3.0142605468261005e-05
Step 427700 Loss: 0.4255, lr 3.0142605468261005e-05
Step 427800 Loss: 0.4221, lr 3.0142605468261005e-05
Step 427900 Loss: 0.4187, lr 3.0142605468261005e-05
Step 428000 Loss: 0.4156, lr 3.0142605468261005e-05
Step 428100 Loss: 0.4119, lr 3.0142605468261005e-05
Step 428200 Loss: 0.4081, lr 3.0142605468261005e-05
Step 428300 Loss: 0.4052, lr 3.0142605468261005e-05
Step 428400 Loss: 0.4028, lr 3.0142605468261005e-05
Step 428500 Loss: 0.3997, lr 3.0142605468261005e-05
Step 428600 Loss: 0.3974, lr 3.0142605468261005e-05
Step 428700 Loss: 0.3947, lr 3.0142605468261005e-05
Step 428800 Loss: 0.3922, lr 3.0142605468261005e-05
Step 428900 Loss: 0.3898, lr 3.0142605468261005e-05
Step 429000 Loss: 0.3871, lr 3.0142605468261005e-05
Step 429100 Loss: 0.3845, lr 3.0142605468261005e-05
Step 429200 Loss: 0.3820, lr 3.0142605468261005e-05
Step 429300 Loss: 0.3797, lr 3.0142605468261005e-05
Step 429400 Loss: 0.3776, lr 3.0142605468261005e-05
Step 429500 Loss: 0.3755, lr 3.0142605468261005e-05
Step 429600 Loss: 0.3737, lr 3.0142605468261005e-05
Step 429700 Loss: 0.3722, lr 3.0142605468261005e-05
Step 429800 Loss: 0.3708, lr 3.0142605468261005e-05
Step 429900 Loss: 0.3694, lr 3.0142605468261005e-05
Step 430000 Loss: 0.3684, lr 3.0142605468261005e-05
Step 430100 Loss: 0.3672, lr 3.0142605468261005e-05
Step 430200 Loss: 0.3655, lr 3.0142605468261005e-05
Step 430300 Loss: 0.3641, lr 3.0142605468261005e-05
Step 430400 Loss: 0.3630, lr 3.0142605468261005e-05
Step 430500 Loss: 0.3622, lr 3.0142605468261005e-05
Step 430600 Loss: 0.3618, lr 3.0142605468261005e-05
Step 430700 Loss: 0.3613, lr 3.0142605468261005e-05
Step 430800 Loss: 0.3610, lr 3.0142605468261005e-05
Step 430900 Loss: 0.3605, lr 3.0142605468261005e-05
Step 431000 Loss: 0.3599, lr 3.0142605468261005e-05
Step 431100 Loss: 0.3597, lr 3.0142605468261005e-05
Step 431200 Loss: 0.3593, lr 3.0142605468261005e-05
Step 431300 Loss: 0.3596, lr 3.0142605468261005e-05
Step 431400 Loss: 0.3596, lr 3.0142605468261005e-05
Step 431500 Loss: 0.3602, lr 3.0142605468261005e-05
Step 431600 Loss: 0.3602, lr 3.0142605468261005e-05
Step 431700 Loss: 0.3606, lr 3.0142605468261005e-05
Step 431800 Loss: 0.3610, lr 3.0142605468261005e-05
Step 431900 Loss: 0.3606, lr 3.0142605468261005e-05
Step 432000 Loss: 0.3605, lr 3.0142605468261005e-05
Step 432100 Loss: 0.3596, lr 3.0142605468261005e-05
Step 432200 Loss: 0.3587, lr 3.0142605468261005e-05
Step 432300 Loss: 0.3575, lr 3.0142605468261005e-05
Train Epoch: [64/100] Loss: 0.3576,lr 0.000030
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6755
Step 432400 Loss: 0.9936, lr 2.8711035421746404e-05
Step 432500 Loss: 0.7112, lr 2.8711035421746404e-05
Step 432600 Loss: 0.6370, lr 2.8711035421746404e-05
Step 432700 Loss: 0.5936, lr 2.8711035421746404e-05
Step 432800 Loss: 0.5687, lr 2.8711035421746404e-05
Step 432900 Loss: 0.5506, lr 2.8711035421746404e-05
Step 433000 Loss: 0.5322, lr 2.8711035421746404e-05
Step 433100 Loss: 0.5181, lr 2.8711035421746404e-05
Step 433200 Loss: 0.5056, lr 2.8711035421746404e-05
Step 433300 Loss: 0.4941, lr 2.8711035421746404e-05
Step 433400 Loss: 0.4846, lr 2.8711035421746404e-05
Step 433500 Loss: 0.4771, lr 2.8711035421746404e-05
Step 433600 Loss: 0.4693, lr 2.8711035421746404e-05
Step 433700 Loss: 0.4621, lr 2.8711035421746404e-05
Step 433800 Loss: 0.4562, lr 2.8711035421746404e-05
Step 433900 Loss: 0.4508, lr 2.8711035421746404e-05
Step 434000 Loss: 0.4452, lr 2.8711035421746404e-05
Step 434100 Loss: 0.4410, lr 2.8711035421746404e-05
Step 434200 Loss: 0.4363, lr 2.8711035421746404e-05
Step 434300 Loss: 0.4319, lr 2.8711035421746404e-05
Step 434400 Loss: 0.4270, lr 2.8711035421746404e-05
Step 434500 Loss: 0.4232, lr 2.8711035421746404e-05
Step 434600 Loss: 0.4196, lr 2.8711035421746404e-05
Step 434700 Loss: 0.4160, lr 2.8711035421746404e-05
Step 434800 Loss: 0.4131, lr 2.8711035421746404e-05
Step 434900 Loss: 0.4092, lr 2.8711035421746404e-05
Step 435000 Loss: 0.4055, lr 2.8711035421746404e-05
Step 435100 Loss: 0.4032, lr 2.8711035421746404e-05
Step 435200 Loss: 0.4006, lr 2.8711035421746404e-05
Step 435300 Loss: 0.3977, lr 2.8711035421746404e-05
Step 435400 Loss: 0.3955, lr 2.8711035421746404e-05
Step 435500 Loss: 0.3926, lr 2.8711035421746404e-05
Step 435600 Loss: 0.3904, lr 2.8711035421746404e-05
Step 435700 Loss: 0.3877, lr 2.8711035421746404e-05
Step 435800 Loss: 0.3850, lr 2.8711035421746404e-05
Step 435900 Loss: 0.3826, lr 2.8711035421746404e-05
Step 436000 Loss: 0.3803, lr 2.8711035421746404e-05
Step 436100 Loss: 0.3778, lr 2.8711035421746404e-05
Step 436200 Loss: 0.3759, lr 2.8711035421746404e-05
Step 436300 Loss: 0.3738, lr 2.8711035421746404e-05
Step 436400 Loss: 0.3724, lr 2.8711035421746404e-05
Step 436500 Loss: 0.3707, lr 2.8711035421746404e-05
Step 436600 Loss: 0.3695, lr 2.8711035421746404e-05
Step 436700 Loss: 0.3682, lr 2.8711035421746404e-05
Step 436800 Loss: 0.3670, lr 2.8711035421746404e-05
Step 436900 Loss: 0.3656, lr 2.8711035421746404e-05
Step 437000 Loss: 0.3641, lr 2.8711035421746404e-05
Step 437100 Loss: 0.3627, lr 2.8711035421746404e-05
Step 437200 Loss: 0.3619, lr 2.8711035421746404e-05
Step 437300 Loss: 0.3611, lr 2.8711035421746404e-05
Step 437400 Loss: 0.3607, lr 2.8711035421746404e-05
Step 437500 Loss: 0.3603, lr 2.8711035421746404e-05
Step 437600 Loss: 0.3599, lr 2.8711035421746404e-05
Step 437700 Loss: 0.3593, lr 2.8711035421746404e-05
Step 437800 Loss: 0.3591, lr 2.8711035421746404e-05
Step 437900 Loss: 0.3587, lr 2.8711035421746404e-05
Step 438000 Loss: 0.3585, lr 2.8711035421746404e-05
Step 438100 Loss: 0.3587, lr 2.8711035421746404e-05
Step 438200 Loss: 0.3589, lr 2.8711035421746404e-05
Step 438300 Loss: 0.3593, lr 2.8711035421746404e-05
Step 438400 Loss: 0.3592, lr 2.8711035421746404e-05
Step 438500 Loss: 0.3596, lr 2.8711035421746404e-05
Step 438600 Loss: 0.3600, lr 2.8711035421746404e-05
Step 438700 Loss: 0.3596, lr 2.8711035421746404e-05
Step 438800 Loss: 0.3589, lr 2.8711035421746404e-05
Step 438900 Loss: 0.3585, lr 2.8711035421746404e-05
Step 439000 Loss: 0.3572, lr 2.8711035421746404e-05
Step 439100 Loss: 0.3566, lr 2.8711035421746404e-05
Train Epoch: [65/100] Loss: 0.3567,lr 0.000029
Model Saving at epoch 65
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6756
Step 439200 Loss: 0.8041, lr 2.73004750130227e-05
Step 439300 Loss: 0.6725, lr 2.73004750130227e-05
Step 439400 Loss: 0.6167, lr 2.73004750130227e-05
Step 439500 Loss: 0.5801, lr 2.73004750130227e-05
Step 439600 Loss: 0.5608, lr 2.73004750130227e-05
Step 439700 Loss: 0.5424, lr 2.73004750130227e-05
Step 439800 Loss: 0.5259, lr 2.73004750130227e-05
Step 439900 Loss: 0.5130, lr 2.73004750130227e-05
Step 440000 Loss: 0.5012, lr 2.73004750130227e-05
Step 440100 Loss: 0.4903, lr 2.73004750130227e-05
Step 440200 Loss: 0.4825, lr 2.73004750130227e-05
Step 440300 Loss: 0.4743, lr 2.73004750130227e-05
Step 440400 Loss: 0.4674, lr 2.73004750130227e-05
Step 440500 Loss: 0.4603, lr 2.73004750130227e-05
Step 440600 Loss: 0.4543, lr 2.73004750130227e-05
Step 440700 Loss: 0.4485, lr 2.73004750130227e-05
Step 440800 Loss: 0.4434, lr 2.73004750130227e-05
Step 440900 Loss: 0.4392, lr 2.73004750130227e-05
Step 441000 Loss: 0.4345, lr 2.73004750130227e-05
Step 441100 Loss: 0.4299, lr 2.73004750130227e-05
Step 441200 Loss: 0.4248, lr 2.73004750130227e-05
Step 441300 Loss: 0.4214, lr 2.73004750130227e-05
Step 441400 Loss: 0.4181, lr 2.73004750130227e-05
Step 441500 Loss: 0.4150, lr 2.73004750130227e-05
Step 441600 Loss: 0.4110, lr 2.73004750130227e-05
Step 441700 Loss: 0.4070, lr 2.73004750130227e-05
Step 441800 Loss: 0.4040, lr 2.73004750130227e-05
Step 441900 Loss: 0.4017, lr 2.73004750130227e-05
Step 442000 Loss: 0.3987, lr 2.73004750130227e-05
Step 442100 Loss: 0.3963, lr 2.73004750130227e-05
Step 442200 Loss: 0.3939, lr 2.73004750130227e-05
Step 442300 Loss: 0.3911, lr 2.73004750130227e-05
Step 442400 Loss: 0.3889, lr 2.73004750130227e-05
Step 442500 Loss: 0.3861, lr 2.73004750130227e-05
Step 442600 Loss: 0.3835, lr 2.73004750130227e-05
Step 442700 Loss: 0.3807, lr 2.73004750130227e-05
Step 442800 Loss: 0.3785, lr 2.73004750130227e-05
Step 442900 Loss: 0.3762, lr 2.73004750130227e-05
Step 443000 Loss: 0.3741, lr 2.73004750130227e-05
Step 443100 Loss: 0.3723, lr 2.73004750130227e-05
Step 443200 Loss: 0.3709, lr 2.73004750130227e-05
Step 443300 Loss: 0.3694, lr 2.73004750130227e-05
Step 443400 Loss: 0.3682, lr 2.73004750130227e-05
Step 443500 Loss: 0.3671, lr 2.73004750130227e-05
Step 443600 Loss: 0.3658, lr 2.73004750130227e-05
Step 443700 Loss: 0.3641, lr 2.73004750130227e-05
Step 443800 Loss: 0.3627, lr 2.73004750130227e-05
Step 443900 Loss: 0.3617, lr 2.73004750130227e-05
Step 444000 Loss: 0.3610, lr 2.73004750130227e-05
Step 444100 Loss: 0.3604, lr 2.73004750130227e-05
Step 444200 Loss: 0.3598, lr 2.73004750130227e-05
Step 444300 Loss: 0.3595, lr 2.73004750130227e-05
Step 444400 Loss: 0.3589, lr 2.73004750130227e-05
Step 444500 Loss: 0.3584, lr 2.73004750130227e-05
Step 444600 Loss: 0.3581, lr 2.73004750130227e-05
Step 444700 Loss: 0.3576, lr 2.73004750130227e-05
Step 444800 Loss: 0.3574, lr 2.73004750130227e-05
Step 444900 Loss: 0.3576, lr 2.73004750130227e-05
Step 445000 Loss: 0.3581, lr 2.73004750130227e-05
Step 445100 Loss: 0.3583, lr 2.73004750130227e-05
Step 445200 Loss: 0.3584, lr 2.73004750130227e-05
Step 445300 Loss: 0.3591, lr 2.73004750130227e-05
Step 445400 Loss: 0.3588, lr 2.73004750130227e-05
Step 445500 Loss: 0.3586, lr 2.73004750130227e-05
Step 445600 Loss: 0.3578, lr 2.73004750130227e-05
Step 445700 Loss: 0.3572, lr 2.73004750130227e-05
Step 445800 Loss: 0.3559, lr 2.73004750130227e-05
Train Epoch: [66/100] Loss: 0.3555,lr 0.000027
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6755
Step 445900 Loss: 1.0892, lr 2.5912316294914263e-05
Step 446000 Loss: 0.7261, lr 2.5912316294914263e-05
Step 446100 Loss: 0.6416, lr 2.5912316294914263e-05
Step 446200 Loss: 0.5962, lr 2.5912316294914263e-05
Step 446300 Loss: 0.5708, lr 2.5912316294914263e-05
Step 446400 Loss: 0.5520, lr 2.5912316294914263e-05
Step 446500 Loss: 0.5327, lr 2.5912316294914263e-05
Step 446600 Loss: 0.5189, lr 2.5912316294914263e-05
Step 446700 Loss: 0.5065, lr 2.5912316294914263e-05
Step 446800 Loss: 0.4949, lr 2.5912316294914263e-05
Step 446900 Loss: 0.4852, lr 2.5912316294914263e-05
Step 447000 Loss: 0.4783, lr 2.5912316294914263e-05
Step 447100 Loss: 0.4696, lr 2.5912316294914263e-05
Step 447200 Loss: 0.4626, lr 2.5912316294914263e-05
Step 447300 Loss: 0.4564, lr 2.5912316294914263e-05
Step 447400 Loss: 0.4505, lr 2.5912316294914263e-05
Step 447500 Loss: 0.4450, lr 2.5912316294914263e-05
Step 447600 Loss: 0.4407, lr 2.5912316294914263e-05
Step 447700 Loss: 0.4359, lr 2.5912316294914263e-05
Step 447800 Loss: 0.4314, lr 2.5912316294914263e-05
Step 447900 Loss: 0.4265, lr 2.5912316294914263e-05
Step 448000 Loss: 0.4227, lr 2.5912316294914263e-05
Step 448100 Loss: 0.4189, lr 2.5912316294914263e-05
Step 448200 Loss: 0.4155, lr 2.5912316294914263e-05
Step 448300 Loss: 0.4126, lr 2.5912316294914263e-05
Step 448400 Loss: 0.4083, lr 2.5912316294914263e-05
Step 448500 Loss: 0.4048, lr 2.5912316294914263e-05
Step 448600 Loss: 0.4021, lr 2.5912316294914263e-05
Step 448700 Loss: 0.3998, lr 2.5912316294914263e-05
Step 448800 Loss: 0.3968, lr 2.5912316294914263e-05
Step 448900 Loss: 0.3946, lr 2.5912316294914263e-05
Step 449000 Loss: 0.3916, lr 2.5912316294914263e-05
Step 449100 Loss: 0.3894, lr 2.5912316294914263e-05
Step 449200 Loss: 0.3869, lr 2.5912316294914263e-05
Step 449300 Loss: 0.3841, lr 2.5912316294914263e-05
Step 449400 Loss: 0.3816, lr 2.5912316294914263e-05
Step 449500 Loss: 0.3794, lr 2.5912316294914263e-05
Step 449600 Loss: 0.3770, lr 2.5912316294914263e-05
Step 449700 Loss: 0.3749, lr 2.5912316294914263e-05
Step 449800 Loss: 0.3728, lr 2.5912316294914263e-05
Step 449900 Loss: 0.3712, lr 2.5912316294914263e-05
Step 450000 Loss: 0.3695, lr 2.5912316294914263e-05
Step 450100 Loss: 0.3683, lr 2.5912316294914263e-05
Step 450200 Loss: 0.3669, lr 2.5912316294914263e-05
Step 450300 Loss: 0.3660, lr 2.5912316294914263e-05
Step 450400 Loss: 0.3645, lr 2.5912316294914263e-05
Step 450500 Loss: 0.3631, lr 2.5912316294914263e-05
Step 450600 Loss: 0.3615, lr 2.5912316294914263e-05
Step 450700 Loss: 0.3607, lr 2.5912316294914263e-05
Step 450800 Loss: 0.3599, lr 2.5912316294914263e-05
Step 450900 Loss: 0.3593, lr 2.5912316294914263e-05
Step 451000 Loss: 0.3590, lr 2.5912316294914263e-05
Step 451100 Loss: 0.3585, lr 2.5912316294914263e-05
Step 451200 Loss: 0.3582, lr 2.5912316294914263e-05
Step 451300 Loss: 0.3577, lr 2.5912316294914263e-05
Step 451400 Loss: 0.3573, lr 2.5912316294914263e-05
Step 451500 Loss: 0.3570, lr 2.5912316294914263e-05
Step 451600 Loss: 0.3571, lr 2.5912316294914263e-05
Step 451700 Loss: 0.3569, lr 2.5912316294914263e-05
Step 451800 Loss: 0.3575, lr 2.5912316294914263e-05
Step 451900 Loss: 0.3575, lr 2.5912316294914263e-05
Step 452000 Loss: 0.3579, lr 2.5912316294914263e-05
Step 452100 Loss: 0.3582, lr 2.5912316294914263e-05
Step 452200 Loss: 0.3582, lr 2.5912316294914263e-05
Step 452300 Loss: 0.3581, lr 2.5912316294914263e-05
Step 452400 Loss: 0.3578, lr 2.5912316294914263e-05
Step 452500 Loss: 0.3569, lr 2.5912316294914263e-05
Step 452600 Loss: 0.3566, lr 2.5912316294914263e-05
Train Epoch: [67/100] Loss: 0.3566,lr 0.000026
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Step 452700 Loss: 0.8538, lr 2.4547929212481466e-05
Step 452800 Loss: 0.7168, lr 2.4547929212481466e-05
Step 452900 Loss: 0.6601, lr 2.4547929212481466e-05
Step 453000 Loss: 0.6207, lr 2.4547929212481466e-05
Step 453100 Loss: 0.6013, lr 2.4547929212481466e-05
Step 453200 Loss: 0.5835, lr 2.4547929212481466e-05
Step 453300 Loss: 0.5663, lr 2.4547929212481466e-05
Step 453400 Loss: 0.5537, lr 2.4547929212481466e-05
Step 453500 Loss: 0.5414, lr 2.4547929212481466e-05
Step 453600 Loss: 0.5306, lr 2.4547929212481466e-05
Step 453700 Loss: 0.5228, lr 2.4547929212481466e-05
Step 453800 Loss: 0.5143, lr 2.4547929212481466e-05
Step 453900 Loss: 0.5074, lr 2.4547929212481466e-05
Step 454000 Loss: 0.4999, lr 2.4547929212481466e-05
Step 454100 Loss: 0.4937, lr 2.4547929212481466e-05
Step 454200 Loss: 0.4888, lr 2.4547929212481466e-05
Step 454300 Loss: 0.4833, lr 2.4547929212481466e-05
Step 454400 Loss: 0.4790, lr 2.4547929212481466e-05
Step 454500 Loss: 0.4746, lr 2.4547929212481466e-05
Step 454600 Loss: 0.4696, lr 2.4547929212481466e-05
Step 454700 Loss: 0.4644, lr 2.4547929212481466e-05
Step 454800 Loss: 0.4607, lr 2.4547929212481466e-05
Step 454900 Loss: 0.4569, lr 2.4547929212481466e-05
Step 455000 Loss: 0.4532, lr 2.4547929212481466e-05
Step 455100 Loss: 0.4491, lr 2.4547929212481466e-05
Step 455200 Loss: 0.4452, lr 2.4547929212481466e-05
Step 455300 Loss: 0.4412, lr 2.4547929212481466e-05
Step 455400 Loss: 0.4387, lr 2.4547929212481466e-05
Step 455500 Loss: 0.4354, lr 2.4547929212481466e-05
Step 455600 Loss: 0.4327, lr 2.4547929212481466e-05
Step 455700 Loss: 0.4299, lr 2.4547929212481466e-05
Step 455800 Loss: 0.4265, lr 2.4547929212481466e-05
Step 455900 Loss: 0.4240, lr 2.4547929212481466e-05
Step 456000 Loss: 0.4203, lr 2.4547929212481466e-05
Step 456100 Loss: 0.4172, lr 2.4547929212481466e-05
Step 456200 Loss: 0.4136, lr 2.4547929212481466e-05
Step 456300 Loss: 0.4107, lr 2.4547929212481466e-05
Step 456400 Loss: 0.4076, lr 2.4547929212481466e-05
Step 456500 Loss: 0.4046, lr 2.4547929212481466e-05
Step 456600 Loss: 0.4021, lr 2.4547929212481466e-05
Step 456700 Loss: 0.3999, lr 2.4547929212481466e-05
Step 456800 Loss: 0.3980, lr 2.4547929212481466e-05
Step 456900 Loss: 0.3961, lr 2.4547929212481466e-05
Step 457000 Loss: 0.3940, lr 2.4547929212481466e-05
Step 457100 Loss: 0.3923, lr 2.4547929212481466e-05
Step 457200 Loss: 0.3902, lr 2.4547929212481466e-05
Step 457300 Loss: 0.3882, lr 2.4547929212481466e-05
Step 457400 Loss: 0.3863, lr 2.4547929212481466e-05
Step 457500 Loss: 0.3853, lr 2.4547929212481466e-05
Step 457600 Loss: 0.3841, lr 2.4547929212481466e-05
Step 457700 Loss: 0.3831, lr 2.4547929212481466e-05
Step 457800 Loss: 0.3823, lr 2.4547929212481466e-05
Step 457900 Loss: 0.3814, lr 2.4547929212481466e-05
Step 458000 Loss: 0.3804, lr 2.4547929212481466e-05
Step 458100 Loss: 0.3796, lr 2.4547929212481466e-05
Step 458200 Loss: 0.3788, lr 2.4547929212481466e-05
Step 458300 Loss: 0.3781, lr 2.4547929212481466e-05
Step 458400 Loss: 0.3778, lr 2.4547929212481466e-05
Step 458500 Loss: 0.3778, lr 2.4547929212481466e-05
Step 458600 Loss: 0.3776, lr 2.4547929212481466e-05
Step 458700 Loss: 0.3774, lr 2.4547929212481466e-05
Step 458800 Loss: 0.3777, lr 2.4547929212481466e-05
Step 458900 Loss: 0.3774, lr 2.4547929212481466e-05
Step 459000 Loss: 0.3769, lr 2.4547929212481466e-05
Step 459100 Loss: 0.3759, lr 2.4547929212481466e-05
Step 459200 Loss: 0.3749, lr 2.4547929212481466e-05
Step 459300 Loss: 0.3732, lr 2.4547929212481466e-05
Train Epoch: [68/100] Loss: 0.3728,lr 0.000025
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 459400 Loss: 1.3323, lr 2.320866025105019e-05
Step 459500 Loss: 0.7445, lr 2.320866025105019e-05
Step 459600 Loss: 0.6490, lr 2.320866025105019e-05
Step 459700 Loss: 0.6013, lr 2.320866025105019e-05
Step 459800 Loss: 0.5751, lr 2.320866025105019e-05
Step 459900 Loss: 0.5550, lr 2.320866025105019e-05
Step 460000 Loss: 0.5354, lr 2.320866025105019e-05
Step 460100 Loss: 0.5224, lr 2.320866025105019e-05
Step 460200 Loss: 0.5101, lr 2.320866025105019e-05
Step 460300 Loss: 0.4979, lr 2.320866025105019e-05
Step 460400 Loss: 0.4874, lr 2.320866025105019e-05
Step 460500 Loss: 0.4810, lr 2.320866025105019e-05
Step 460600 Loss: 0.4725, lr 2.320866025105019e-05
Step 460700 Loss: 0.4654, lr 2.320866025105019e-05
Step 460800 Loss: 0.4593, lr 2.320866025105019e-05
Step 460900 Loss: 0.4531, lr 2.320866025105019e-05
Step 461000 Loss: 0.4475, lr 2.320866025105019e-05
Step 461100 Loss: 0.4433, lr 2.320866025105019e-05
Step 461200 Loss: 0.4384, lr 2.320866025105019e-05
Step 461300 Loss: 0.4341, lr 2.320866025105019e-05
Step 461400 Loss: 0.4295, lr 2.320866025105019e-05
Step 461500 Loss: 0.4252, lr 2.320866025105019e-05
Step 461600 Loss: 0.4218, lr 2.320866025105019e-05
Step 461700 Loss: 0.4183, lr 2.320866025105019e-05
Step 461800 Loss: 0.4153, lr 2.320866025105019e-05
Step 461900 Loss: 0.4112, lr 2.320866025105019e-05
Step 462000 Loss: 0.4075, lr 2.320866025105019e-05
Step 462100 Loss: 0.4045, lr 2.320866025105019e-05
Step 462200 Loss: 0.4025, lr 2.320866025105019e-05
Step 462300 Loss: 0.3995, lr 2.320866025105019e-05
Step 462400 Loss: 0.3974, lr 2.320866025105019e-05
Step 462500 Loss: 0.3946, lr 2.320866025105019e-05
Step 462600 Loss: 0.3922, lr 2.320866025105019e-05
Step 462700 Loss: 0.3896, lr 2.320866025105019e-05
Step 462800 Loss: 0.3868, lr 2.320866025105019e-05
Step 462900 Loss: 0.3842, lr 2.320866025105019e-05
Step 463000 Loss: 0.3817, lr 2.320866025105019e-05
Step 463100 Loss: 0.3794, lr 2.320866025105019e-05
Step 463200 Loss: 0.3772, lr 2.320866025105019e-05
Step 463300 Loss: 0.3751, lr 2.320866025105019e-05
Step 463400 Loss: 0.3733, lr 2.320866025105019e-05
Step 463500 Loss: 0.3718, lr 2.320866025105019e-05
Step 463600 Loss: 0.3704, lr 2.320866025105019e-05
Step 463700 Loss: 0.3690, lr 2.320866025105019e-05
Step 463800 Loss: 0.3680, lr 2.320866025105019e-05
Step 463900 Loss: 0.3665, lr 2.320866025105019e-05
Step 464000 Loss: 0.3649, lr 2.320866025105019e-05
Step 464100 Loss: 0.3635, lr 2.320866025105019e-05
Step 464200 Loss: 0.3626, lr 2.320866025105019e-05
Step 464300 Loss: 0.3616, lr 2.320866025105019e-05
Step 464400 Loss: 0.3610, lr 2.320866025105019e-05
Step 464500 Loss: 0.3606, lr 2.320866025105019e-05
Step 464600 Loss: 0.3599, lr 2.320866025105019e-05
Step 464700 Loss: 0.3595, lr 2.320866025105019e-05
Step 464800 Loss: 0.3590, lr 2.320866025105019e-05
Step 464900 Loss: 0.3586, lr 2.320866025105019e-05
Step 465000 Loss: 0.3582, lr 2.320866025105019e-05
Step 465100 Loss: 0.3584, lr 2.320866025105019e-05
Step 465200 Loss: 0.3584, lr 2.320866025105019e-05
Step 465300 Loss: 0.3590, lr 2.320866025105019e-05
Step 465400 Loss: 0.3588, lr 2.320866025105019e-05
Step 465500 Loss: 0.3594, lr 2.320866025105019e-05
Step 465600 Loss: 0.3598, lr 2.320866025105019e-05
Step 465700 Loss: 0.3596, lr 2.320866025105019e-05
Step 465800 Loss: 0.3594, lr 2.320866025105019e-05
Step 465900 Loss: 0.3586, lr 2.320866025105019e-05
Step 466000 Loss: 0.3575, lr 2.320866025105019e-05
Step 466100 Loss: 0.3567, lr 2.320866025105019e-05
Train Epoch: [69/100] Loss: 0.3567,lr 0.000023
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 6755
Step 466200 Loss: 0.8724, lr 2.1895831107393498e-05
Step 466300 Loss: 0.6848, lr 2.1895831107393498e-05
Step 466400 Loss: 0.6255, lr 2.1895831107393498e-05
Step 466500 Loss: 0.5835, lr 2.1895831107393498e-05
Step 466600 Loss: 0.5633, lr 2.1895831107393498e-05
Step 466700 Loss: 0.5458, lr 2.1895831107393498e-05
Step 466800 Loss: 0.5280, lr 2.1895831107393498e-05
Step 466900 Loss: 0.5140, lr 2.1895831107393498e-05
Step 467000 Loss: 0.5025, lr 2.1895831107393498e-05
Step 467100 Loss: 0.4908, lr 2.1895831107393498e-05
Step 467200 Loss: 0.4832, lr 2.1895831107393498e-05
Step 467300 Loss: 0.4752, lr 2.1895831107393498e-05
Step 467400 Loss: 0.4673, lr 2.1895831107393498e-05
Step 467500 Loss: 0.4603, lr 2.1895831107393498e-05
Step 467600 Loss: 0.4544, lr 2.1895831107393498e-05
Step 467700 Loss: 0.4486, lr 2.1895831107393498e-05
Step 467800 Loss: 0.4432, lr 2.1895831107393498e-05
Step 467900 Loss: 0.4388, lr 2.1895831107393498e-05
Step 468000 Loss: 0.4342, lr 2.1895831107393498e-05
Step 468100 Loss: 0.4295, lr 2.1895831107393498e-05
Step 468200 Loss: 0.4247, lr 2.1895831107393498e-05
Step 468300 Loss: 0.4212, lr 2.1895831107393498e-05
Step 468400 Loss: 0.4174, lr 2.1895831107393498e-05
Step 468500 Loss: 0.4141, lr 2.1895831107393498e-05
Step 468600 Loss: 0.4104, lr 2.1895831107393498e-05
Step 468700 Loss: 0.4066, lr 2.1895831107393498e-05
Step 468800 Loss: 0.4032, lr 2.1895831107393498e-05
Step 468900 Loss: 0.4011, lr 2.1895831107393498e-05
Step 469000 Loss: 0.3980, lr 2.1895831107393498e-05
Step 469100 Loss: 0.3955, lr 2.1895831107393498e-05
Step 469200 Loss: 0.3931, lr 2.1895831107393498e-05
Step 469300 Loss: 0.3901, lr 2.1895831107393498e-05
Step 469400 Loss: 0.3880, lr 2.1895831107393498e-05
Step 469500 Loss: 0.3851, lr 2.1895831107393498e-05
Step 469600 Loss: 0.3829, lr 2.1895831107393498e-05
Step 469700 Loss: 0.3801, lr 2.1895831107393498e-05
Step 469800 Loss: 0.3779, lr 2.1895831107393498e-05
Step 469900 Loss: 0.3755, lr 2.1895831107393498e-05
Step 470000 Loss: 0.3733, lr 2.1895831107393498e-05
Step 470100 Loss: 0.3713, lr 2.1895831107393498e-05
Step 470200 Loss: 0.3699, lr 2.1895831107393498e-05
Step 470300 Loss: 0.3683, lr 2.1895831107393498e-05
Step 470400 Loss: 0.3672, lr 2.1895831107393498e-05
Step 470500 Loss: 0.3657, lr 2.1895831107393498e-05
Step 470600 Loss: 0.3646, lr 2.1895831107393498e-05
Step 470700 Loss: 0.3631, lr 2.1895831107393498e-05
Step 470800 Loss: 0.3618, lr 2.1895831107393498e-05
Step 470900 Loss: 0.3606, lr 2.1895831107393498e-05
Step 471000 Loss: 0.3598, lr 2.1895831107393498e-05
Step 471100 Loss: 0.3593, lr 2.1895831107393498e-05
Step 471200 Loss: 0.3586, lr 2.1895831107393498e-05
Step 471300 Loss: 0.3583, lr 2.1895831107393498e-05
Step 471400 Loss: 0.3577, lr 2.1895831107393498e-05
Step 471500 Loss: 0.3571, lr 2.1895831107393498e-05
Step 471600 Loss: 0.3568, lr 2.1895831107393498e-05
Step 471700 Loss: 0.3562, lr 2.1895831107393498e-05
Step 471800 Loss: 0.3559, lr 2.1895831107393498e-05
Step 471900 Loss: 0.3561, lr 2.1895831107393498e-05
Step 472000 Loss: 0.3565, lr 2.1895831107393498e-05
Step 472100 Loss: 0.3567, lr 2.1895831107393498e-05
Step 472200 Loss: 0.3569, lr 2.1895831107393498e-05
Step 472300 Loss: 0.3573, lr 2.1895831107393498e-05
Step 472400 Loss: 0.3573, lr 2.1895831107393498e-05
Step 472500 Loss: 0.3570, lr 2.1895831107393498e-05
Step 472600 Loss: 0.3562, lr 2.1895831107393498e-05
Step 472700 Loss: 0.3556, lr 2.1895831107393498e-05
Step 472800 Loss: 0.3545, lr 2.1895831107393498e-05
Train Epoch: [70/100] Loss: 0.3541,lr 0.000022
Model Saving at epoch 70
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6756
Step 472900 Loss: 1.7477, lr 2.061073738537638e-05
Step 473000 Loss: 0.7600, lr 2.061073738537638e-05
Step 473100 Loss: 0.6544, lr 2.061073738537638e-05
Step 473200 Loss: 0.6032, lr 2.061073738537638e-05
Step 473300 Loss: 0.5731, lr 2.061073738537638e-05
Step 473400 Loss: 0.5525, lr 2.061073738537638e-05
Step 473500 Loss: 0.5336, lr 2.061073738537638e-05
Step 473600 Loss: 0.5204, lr 2.061073738537638e-05
Step 473700 Loss: 0.5074, lr 2.061073738537638e-05
Step 473800 Loss: 0.4955, lr 2.061073738537638e-05
Step 473900 Loss: 0.4849, lr 2.061073738537638e-05
Step 474000 Loss: 0.4776, lr 2.061073738537638e-05
Step 474100 Loss: 0.4696, lr 2.061073738537638e-05
Step 474200 Loss: 0.4632, lr 2.061073738537638e-05
Step 474300 Loss: 0.4570, lr 2.061073738537638e-05
Step 474400 Loss: 0.4508, lr 2.061073738537638e-05
Step 474500 Loss: 0.4455, lr 2.061073738537638e-05
Step 474600 Loss: 0.4411, lr 2.061073738537638e-05
Step 474700 Loss: 0.4367, lr 2.061073738537638e-05
Step 474800 Loss: 0.4322, lr 2.061073738537638e-05
Step 474900 Loss: 0.4275, lr 2.061073738537638e-05
Step 475000 Loss: 0.4228, lr 2.061073738537638e-05
Step 475100 Loss: 0.4193, lr 2.061073738537638e-05
Step 475200 Loss: 0.4156, lr 2.061073738537638e-05
Step 475300 Loss: 0.4128, lr 2.061073738537638e-05
Step 475400 Loss: 0.4091, lr 2.061073738537638e-05
Step 475500 Loss: 0.4051, lr 2.061073738537638e-05
Step 475600 Loss: 0.4022, lr 2.061073738537638e-05
Step 475700 Loss: 0.3997, lr 2.061073738537638e-05
Step 475800 Loss: 0.3968, lr 2.061073738537638e-05
Step 475900 Loss: 0.3945, lr 2.061073738537638e-05
Step 476000 Loss: 0.3916, lr 2.061073738537638e-05
Step 476100 Loss: 0.3893, lr 2.061073738537638e-05
Step 476200 Loss: 0.3871, lr 2.061073738537638e-05
Step 476300 Loss: 0.3844, lr 2.061073738537638e-05
Step 476400 Loss: 0.3818, lr 2.061073738537638e-05
Step 476500 Loss: 0.3790, lr 2.061073738537638e-05
Step 476600 Loss: 0.3768, lr 2.061073738537638e-05
Step 476700 Loss: 0.3748, lr 2.061073738537638e-05
Step 476800 Loss: 0.3727, lr 2.061073738537638e-05
Step 476900 Loss: 0.3709, lr 2.061073738537638e-05
Step 477000 Loss: 0.3694, lr 2.061073738537638e-05
Step 477100 Loss: 0.3681, lr 2.061073738537638e-05
Step 477200 Loss: 0.3668, lr 2.061073738537638e-05
Step 477300 Loss: 0.3656, lr 2.061073738537638e-05
Step 477400 Loss: 0.3644, lr 2.061073738537638e-05
Step 477500 Loss: 0.3627, lr 2.061073738537638e-05
Step 477600 Loss: 0.3615, lr 2.061073738537638e-05
Step 477700 Loss: 0.3606, lr 2.061073738537638e-05
Step 477800 Loss: 0.3597, lr 2.061073738537638e-05
Step 477900 Loss: 0.3591, lr 2.061073738537638e-05
Step 478000 Loss: 0.3587, lr 2.061073738537638e-05
Step 478100 Loss: 0.3583, lr 2.061073738537638e-05
Step 478200 Loss: 0.3576, lr 2.061073738537638e-05
Step 478300 Loss: 0.3569, lr 2.061073738537638e-05
Step 478400 Loss: 0.3569, lr 2.061073738537638e-05
Step 478500 Loss: 0.3563, lr 2.061073738537638e-05
Step 478600 Loss: 0.3562, lr 2.061073738537638e-05
Step 478700 Loss: 0.3565, lr 2.061073738537638e-05
Step 478800 Loss: 0.3568, lr 2.061073738537638e-05
Step 478900 Loss: 0.3566, lr 2.061073738537638e-05
Step 479000 Loss: 0.3572, lr 2.061073738537638e-05
Step 479100 Loss: 0.3575, lr 2.061073738537638e-05
Step 479200 Loss: 0.3570, lr 2.061073738537638e-05
Step 479300 Loss: 0.3569, lr 2.061073738537638e-05
Step 479400 Loss: 0.3563, lr 2.061073738537638e-05
Step 479500 Loss: 0.3550, lr 2.061073738537638e-05
Step 479600 Loss: 0.3542, lr 2.061073738537638e-05
Train Epoch: [71/100] Loss: 0.3542,lr 0.000021
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Step 479700 Loss: 0.9246, lr 1.9354647317351198e-05
Step 479800 Loss: 0.6956, lr 1.9354647317351198e-05
Step 479900 Loss: 0.6277, lr 1.9354647317351198e-05
Step 480000 Loss: 0.5860, lr 1.9354647317351198e-05
Step 480100 Loss: 0.5642, lr 1.9354647317351198e-05
Step 480200 Loss: 0.5469, lr 1.9354647317351198e-05
Step 480300 Loss: 0.5294, lr 1.9354647317351198e-05
Step 480400 Loss: 0.5146, lr 1.9354647317351198e-05
Step 480500 Loss: 0.5031, lr 1.9354647317351198e-05
Step 480600 Loss: 0.4918, lr 1.9354647317351198e-05
Step 480700 Loss: 0.4831, lr 1.9354647317351198e-05
Step 480800 Loss: 0.4759, lr 1.9354647317351198e-05
Step 480900 Loss: 0.4680, lr 1.9354647317351198e-05
Step 481000 Loss: 0.4607, lr 1.9354647317351198e-05
Step 481100 Loss: 0.4551, lr 1.9354647317351198e-05
Step 481200 Loss: 0.4496, lr 1.9354647317351198e-05
Step 481300 Loss: 0.4436, lr 1.9354647317351198e-05
Step 481400 Loss: 0.4396, lr 1.9354647317351198e-05
Step 481500 Loss: 0.4351, lr 1.9354647317351198e-05
Step 481600 Loss: 0.4303, lr 1.9354647317351198e-05
Step 481700 Loss: 0.4256, lr 1.9354647317351198e-05
Step 481800 Loss: 0.4219, lr 1.9354647317351198e-05
Step 481900 Loss: 0.4181, lr 1.9354647317351198e-05
Step 482000 Loss: 0.4146, lr 1.9354647317351198e-05
Step 482100 Loss: 0.4115, lr 1.9354647317351198e-05
Step 482200 Loss: 0.4077, lr 1.9354647317351198e-05
Step 482300 Loss: 0.4044, lr 1.9354647317351198e-05
Step 482400 Loss: 0.4021, lr 1.9354647317351198e-05
Step 482500 Loss: 0.3991, lr 1.9354647317351198e-05
Step 482600 Loss: 0.3965, lr 1.9354647317351198e-05
Step 482700 Loss: 0.3938, lr 1.9354647317351198e-05
Step 482800 Loss: 0.3912, lr 1.9354647317351198e-05
Step 482900 Loss: 0.3889, lr 1.9354647317351198e-05
Step 483000 Loss: 0.3860, lr 1.9354647317351198e-05
Step 483100 Loss: 0.3834, lr 1.9354647317351198e-05
Step 483200 Loss: 0.3811, lr 1.9354647317351198e-05
Step 483300 Loss: 0.3787, lr 1.9354647317351198e-05
Step 483400 Loss: 0.3762, lr 1.9354647317351198e-05
Step 483500 Loss: 0.3743, lr 1.9354647317351198e-05
Step 483600 Loss: 0.3721, lr 1.9354647317351198e-05
Step 483700 Loss: 0.3707, lr 1.9354647317351198e-05
Step 483800 Loss: 0.3691, lr 1.9354647317351198e-05
Step 483900 Loss: 0.3680, lr 1.9354647317351198e-05
Step 484000 Loss: 0.3664, lr 1.9354647317351198e-05
Step 484100 Loss: 0.3654, lr 1.9354647317351198e-05
Step 484200 Loss: 0.3637, lr 1.9354647317351198e-05
Step 484300 Loss: 0.3622, lr 1.9354647317351198e-05
Step 484400 Loss: 0.3608, lr 1.9354647317351198e-05
Step 484500 Loss: 0.3600, lr 1.9354647317351198e-05
Step 484600 Loss: 0.3593, lr 1.9354647317351198e-05
Step 484700 Loss: 0.3586, lr 1.9354647317351198e-05
Step 484800 Loss: 0.3581, lr 1.9354647317351198e-05
Step 484900 Loss: 0.3575, lr 1.9354647317351198e-05
Step 485000 Loss: 0.3570, lr 1.9354647317351198e-05
Step 485100 Loss: 0.3564, lr 1.9354647317351198e-05
Step 485200 Loss: 0.3559, lr 1.9354647317351198e-05
Step 485300 Loss: 0.3557, lr 1.9354647317351198e-05
Step 485400 Loss: 0.3559, lr 1.9354647317351198e-05
Step 485500 Loss: 0.3559, lr 1.9354647317351198e-05
Step 485600 Loss: 0.3561, lr 1.9354647317351198e-05
Step 485700 Loss: 0.3564, lr 1.9354647317351198e-05
Step 485800 Loss: 0.3564, lr 1.9354647317351198e-05
Step 485900 Loss: 0.3568, lr 1.9354647317351198e-05
Step 486000 Loss: 0.3566, lr 1.9354647317351198e-05
Step 486100 Loss: 0.3560, lr 1.9354647317351198e-05
Step 486200 Loss: 0.3555, lr 1.9354647317351198e-05
Step 486300 Loss: 0.3542, lr 1.9354647317351198e-05
Step 486400 Loss: 0.3538, lr 1.9354647317351198e-05
Train Epoch: [72/100] Loss: 0.3539,lr 0.000019
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 6755
Step 486500 Loss: 0.7836, lr 1.812880051256554e-05
Step 486600 Loss: 0.6587, lr 1.812880051256554e-05
Step 486700 Loss: 0.6063, lr 1.812880051256554e-05
Step 486800 Loss: 0.5741, lr 1.812880051256554e-05
Step 486900 Loss: 0.5545, lr 1.812880051256554e-05
Step 487000 Loss: 0.5368, lr 1.812880051256554e-05
Step 487100 Loss: 0.5227, lr 1.812880051256554e-05
Step 487200 Loss: 0.5089, lr 1.812880051256554e-05
Step 487300 Loss: 0.4972, lr 1.812880051256554e-05
Step 487400 Loss: 0.4865, lr 1.812880051256554e-05
Step 487500 Loss: 0.4787, lr 1.812880051256554e-05
Step 487600 Loss: 0.4704, lr 1.812880051256554e-05
Step 487700 Loss: 0.4637, lr 1.812880051256554e-05
Step 487800 Loss: 0.4572, lr 1.812880051256554e-05
Step 487900 Loss: 0.4512, lr 1.812880051256554e-05
Step 488000 Loss: 0.4453, lr 1.812880051256554e-05
Step 488100 Loss: 0.4407, lr 1.812880051256554e-05
Step 488200 Loss: 0.4363, lr 1.812880051256554e-05
Step 488300 Loss: 0.4316, lr 1.812880051256554e-05
Step 488400 Loss: 0.4267, lr 1.812880051256554e-05
Step 488500 Loss: 0.4221, lr 1.812880051256554e-05
Step 488600 Loss: 0.4185, lr 1.812880051256554e-05
Step 488700 Loss: 0.4152, lr 1.812880051256554e-05
Step 488800 Loss: 0.4121, lr 1.812880051256554e-05
Step 488900 Loss: 0.4082, lr 1.812880051256554e-05
Step 489000 Loss: 0.4044, lr 1.812880051256554e-05
Step 489100 Loss: 0.4014, lr 1.812880051256554e-05
Step 489200 Loss: 0.3993, lr 1.812880051256554e-05
Step 489300 Loss: 0.3961, lr 1.812880051256554e-05
Step 489400 Loss: 0.3937, lr 1.812880051256554e-05
Step 489500 Loss: 0.3913, lr 1.812880051256554e-05
Step 489600 Loss: 0.3886, lr 1.812880051256554e-05
Step 489700 Loss: 0.3862, lr 1.812880051256554e-05
Step 489800 Loss: 0.3836, lr 1.812880051256554e-05
Step 489900 Loss: 0.3811, lr 1.812880051256554e-05
Step 490000 Loss: 0.3783, lr 1.812880051256554e-05
Step 490100 Loss: 0.3761, lr 1.812880051256554e-05
Step 490200 Loss: 0.3739, lr 1.812880051256554e-05
Step 490300 Loss: 0.3720, lr 1.812880051256554e-05
Step 490400 Loss: 0.3702, lr 1.812880051256554e-05
Step 490500 Loss: 0.3686, lr 1.812880051256554e-05
Step 490600 Loss: 0.3672, lr 1.812880051256554e-05
Step 490700 Loss: 0.3659, lr 1.812880051256554e-05
Step 490800 Loss: 0.3647, lr 1.812880051256554e-05
Step 490900 Loss: 0.3635, lr 1.812880051256554e-05
Step 491000 Loss: 0.3618, lr 1.812880051256554e-05
Step 491100 Loss: 0.3605, lr 1.812880051256554e-05
Step 491200 Loss: 0.3595, lr 1.812880051256554e-05
Step 491300 Loss: 0.3584, lr 1.812880051256554e-05
Step 491400 Loss: 0.3581, lr 1.812880051256554e-05
Step 491500 Loss: 0.3575, lr 1.812880051256554e-05
Step 491600 Loss: 0.3569, lr 1.812880051256554e-05
Step 491700 Loss: 0.3562, lr 1.812880051256554e-05
Step 491800 Loss: 0.3557, lr 1.812880051256554e-05
Step 491900 Loss: 0.3554, lr 1.812880051256554e-05
Step 492000 Loss: 0.3548, lr 1.812880051256554e-05
Step 492100 Loss: 0.3549, lr 1.812880051256554e-05
Step 492200 Loss: 0.3548, lr 1.812880051256554e-05
Step 492300 Loss: 0.3554, lr 1.812880051256554e-05
Step 492400 Loss: 0.3555, lr 1.812880051256554e-05
Step 492500 Loss: 0.3559, lr 1.812880051256554e-05
Step 492600 Loss: 0.3563, lr 1.812880051256554e-05
Step 492700 Loss: 0.3560, lr 1.812880051256554e-05
Step 492800 Loss: 0.3559, lr 1.812880051256554e-05
Step 492900 Loss: 0.3551, lr 1.812880051256554e-05
Step 493000 Loss: 0.3542, lr 1.812880051256554e-05
Step 493100 Loss: 0.3535, lr 1.812880051256554e-05
Train Epoch: [73/100] Loss: 0.3534,lr 0.000018
Calling G2SDataset.batch()
Done, time:  1.51 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 6755
Step 493200 Loss: 0.9868, lr 1.693440673381744e-05
Step 493300 Loss: 0.7068, lr 1.693440673381744e-05
Step 493400 Loss: 0.6308, lr 1.693440673381744e-05
Step 493500 Loss: 0.5904, lr 1.693440673381744e-05
Step 493600 Loss: 0.5650, lr 1.693440673381744e-05
Step 493700 Loss: 0.5480, lr 1.693440673381744e-05
Step 493800 Loss: 0.5297, lr 1.693440673381744e-05
Step 493900 Loss: 0.5152, lr 1.693440673381744e-05
Step 494000 Loss: 0.5029, lr 1.693440673381744e-05
Step 494100 Loss: 0.4910, lr 1.693440673381744e-05
Step 494200 Loss: 0.4814, lr 1.693440673381744e-05
Step 494300 Loss: 0.4742, lr 1.693440673381744e-05
Step 494400 Loss: 0.4659, lr 1.693440673381744e-05
Step 494500 Loss: 0.4593, lr 1.693440673381744e-05
Step 494600 Loss: 0.4535, lr 1.693440673381744e-05
Step 494700 Loss: 0.4478, lr 1.693440673381744e-05
Step 494800 Loss: 0.4424, lr 1.693440673381744e-05
Step 494900 Loss: 0.4379, lr 1.693440673381744e-05
Step 495000 Loss: 0.4333, lr 1.693440673381744e-05
Step 495100 Loss: 0.4288, lr 1.693440673381744e-05
Step 495200 Loss: 0.4239, lr 1.693440673381744e-05
Step 495300 Loss: 0.4200, lr 1.693440673381744e-05
Step 495400 Loss: 0.4165, lr 1.693440673381744e-05
Step 495500 Loss: 0.4127, lr 1.693440673381744e-05
Step 495600 Loss: 0.4099, lr 1.693440673381744e-05
Step 495700 Loss: 0.4057, lr 1.693440673381744e-05
Step 495800 Loss: 0.4022, lr 1.693440673381744e-05
Step 495900 Loss: 0.3999, lr 1.693440673381744e-05
Step 496000 Loss: 0.3971, lr 1.693440673381744e-05
Step 496100 Loss: 0.3943, lr 1.693440673381744e-05
Step 496200 Loss: 0.3920, lr 1.693440673381744e-05
Step 496300 Loss: 0.3894, lr 1.693440673381744e-05
Step 496400 Loss: 0.3872, lr 1.693440673381744e-05
Step 496500 Loss: 0.3843, lr 1.693440673381744e-05
Step 496600 Loss: 0.3817, lr 1.693440673381744e-05
Step 496700 Loss: 0.3794, lr 1.693440673381744e-05
Step 496800 Loss: 0.3769, lr 1.693440673381744e-05
Step 496900 Loss: 0.3746, lr 1.693440673381744e-05
Step 497000 Loss: 0.3724, lr 1.693440673381744e-05
Step 497100 Loss: 0.3705, lr 1.693440673381744e-05
Step 497200 Loss: 0.3690, lr 1.693440673381744e-05
Step 497300 Loss: 0.3675, lr 1.693440673381744e-05
Step 497400 Loss: 0.3664, lr 1.693440673381744e-05
Step 497500 Loss: 0.3649, lr 1.693440673381744e-05
Step 497600 Loss: 0.3639, lr 1.693440673381744e-05
Step 497700 Loss: 0.3622, lr 1.693440673381744e-05
Step 497800 Loss: 0.3610, lr 1.693440673381744e-05
Step 497900 Loss: 0.3596, lr 1.693440673381744e-05
Step 498000 Loss: 0.3587, lr 1.693440673381744e-05
Step 498100 Loss: 0.3580, lr 1.693440673381744e-05
Step 498200 Loss: 0.3574, lr 1.693440673381744e-05
Step 498300 Loss: 0.3572, lr 1.693440673381744e-05
Step 498400 Loss: 0.3566, lr 1.693440673381744e-05
Step 498500 Loss: 0.3561, lr 1.693440673381744e-05
Step 498600 Loss: 0.3554, lr 1.693440673381744e-05
Step 498700 Loss: 0.3551, lr 1.693440673381744e-05
Step 498800 Loss: 0.3549, lr 1.693440673381744e-05
Step 498900 Loss: 0.3551, lr 1.693440673381744e-05
Step 499000 Loss: 0.3551, lr 1.693440673381744e-05
Step 499100 Loss: 0.3553, lr 1.693440673381744e-05
Step 499200 Loss: 0.3557, lr 1.693440673381744e-05
Step 499300 Loss: 0.3558, lr 1.693440673381744e-05
Step 499400 Loss: 0.3561, lr 1.693440673381744e-05
Step 499500 Loss: 0.3558, lr 1.693440673381744e-05
Step 499600 Loss: 0.3552, lr 1.693440673381744e-05
Step 499700 Loss: 0.3546, lr 1.693440673381744e-05
Step 499800 Loss: 0.3533, lr 1.693440673381744e-05
Step 499900 Loss: 0.3525, lr 1.693440673381744e-05
Train Epoch: [74/100] Loss: 0.3525,lr 0.000017
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6756
Step 500000 Loss: 0.7937, lr 1.577264470356559e-05
Step 500100 Loss: 0.6652, lr 1.577264470356559e-05
Step 500200 Loss: 0.6108, lr 1.577264470356559e-05
Step 500300 Loss: 0.5752, lr 1.577264470356559e-05
Step 500400 Loss: 0.5567, lr 1.577264470356559e-05
Step 500500 Loss: 0.5385, lr 1.577264470356559e-05
Step 500600 Loss: 0.5220, lr 1.577264470356559e-05
Step 500700 Loss: 0.5094, lr 1.577264470356559e-05
Step 500800 Loss: 0.4973, lr 1.577264470356559e-05
Step 500900 Loss: 0.4864, lr 1.577264470356559e-05
Step 501000 Loss: 0.4786, lr 1.577264470356559e-05
Step 501100 Loss: 0.4705, lr 1.577264470356559e-05
Step 501200 Loss: 0.4638, lr 1.577264470356559e-05
Step 501300 Loss: 0.4568, lr 1.577264470356559e-05
Step 501400 Loss: 0.4511, lr 1.577264470356559e-05
Step 501500 Loss: 0.4456, lr 1.577264470356559e-05
Step 501600 Loss: 0.4404, lr 1.577264470356559e-05
Step 501700 Loss: 0.4365, lr 1.577264470356559e-05
Step 501800 Loss: 0.4317, lr 1.577264470356559e-05
Step 501900 Loss: 0.4269, lr 1.577264470356559e-05
Step 502000 Loss: 0.4220, lr 1.577264470356559e-05
Step 502100 Loss: 0.4187, lr 1.577264470356559e-05
Step 502200 Loss: 0.4154, lr 1.577264470356559e-05
Step 502300 Loss: 0.4123, lr 1.577264470356559e-05
Step 502400 Loss: 0.4084, lr 1.577264470356559e-05
Step 502500 Loss: 0.4047, lr 1.577264470356559e-05
Step 502600 Loss: 0.4014, lr 1.577264470356559e-05
Step 502700 Loss: 0.3991, lr 1.577264470356559e-05
Step 502800 Loss: 0.3963, lr 1.577264470356559e-05
Step 502900 Loss: 0.3938, lr 1.577264470356559e-05
Step 503000 Loss: 0.3914, lr 1.577264470356559e-05
Step 503100 Loss: 0.3886, lr 1.577264470356559e-05
Step 503200 Loss: 0.3865, lr 1.577264470356559e-05
Step 503300 Loss: 0.3837, lr 1.577264470356559e-05
Step 503400 Loss: 0.3813, lr 1.577264470356559e-05
Step 503500 Loss: 0.3783, lr 1.577264470356559e-05
Step 503600 Loss: 0.3760, lr 1.577264470356559e-05
Step 503700 Loss: 0.3738, lr 1.577264470356559e-05
Step 503800 Loss: 0.3719, lr 1.577264470356559e-05
Step 503900 Loss: 0.3700, lr 1.577264470356559e-05
Step 504000 Loss: 0.3686, lr 1.577264470356559e-05
Step 504100 Loss: 0.3672, lr 1.577264470356559e-05
Step 504200 Loss: 0.3660, lr 1.577264470356559e-05
Step 504300 Loss: 0.3647, lr 1.577264470356559e-05
Step 504400 Loss: 0.3635, lr 1.577264470356559e-05
Step 504500 Loss: 0.3618, lr 1.577264470356559e-05
Step 504600 Loss: 0.3605, lr 1.577264470356559e-05
Step 504700 Loss: 0.3592, lr 1.577264470356559e-05
Step 504800 Loss: 0.3586, lr 1.577264470356559e-05
Step 504900 Loss: 0.3579, lr 1.577264470356559e-05
Step 505000 Loss: 0.3573, lr 1.577264470356559e-05
Step 505100 Loss: 0.3569, lr 1.577264470356559e-05
Step 505200 Loss: 0.3562, lr 1.577264470356559e-05
Step 505300 Loss: 0.3557, lr 1.577264470356559e-05
Step 505400 Loss: 0.3555, lr 1.577264470356559e-05
Step 505500 Loss: 0.3551, lr 1.577264470356559e-05
Step 505600 Loss: 0.3547, lr 1.577264470356559e-05
Step 505700 Loss: 0.3549, lr 1.577264470356559e-05
Step 505800 Loss: 0.3552, lr 1.577264470356559e-05
Step 505900 Loss: 0.3553, lr 1.577264470356559e-05
Step 506000 Loss: 0.3554, lr 1.577264470356559e-05
Step 506100 Loss: 0.3561, lr 1.577264470356559e-05
Step 506200 Loss: 0.3558, lr 1.577264470356559e-05
Step 506300 Loss: 0.3557, lr 1.577264470356559e-05
Step 506400 Loss: 0.3548, lr 1.577264470356559e-05
Step 506500 Loss: 0.3540, lr 1.577264470356559e-05
Step 506600 Loss: 0.3526, lr 1.577264470356559e-05
Train Epoch: [75/100] Loss: 0.3524,lr 0.000016
Model Saving at epoch 75
Calling G2SDataset.batch()
Done, time:  1.97 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6755
Step 506700 Loss: 1.0812, lr 1.464466094067265e-05
Step 506800 Loss: 0.7173, lr 1.464466094067265e-05
Step 506900 Loss: 0.6355, lr 1.464466094067265e-05
Step 507000 Loss: 0.5913, lr 1.464466094067265e-05
Step 507100 Loss: 0.5675, lr 1.464466094067265e-05
Step 507200 Loss: 0.5491, lr 1.464466094067265e-05
Step 507300 Loss: 0.5310, lr 1.464466094067265e-05
Step 507400 Loss: 0.5172, lr 1.464466094067265e-05
Step 507500 Loss: 0.5046, lr 1.464466094067265e-05
Step 507600 Loss: 0.4922, lr 1.464466094067265e-05
Step 507700 Loss: 0.4829, lr 1.464466094067265e-05
Step 507800 Loss: 0.4758, lr 1.464466094067265e-05
Step 507900 Loss: 0.4675, lr 1.464466094067265e-05
Step 508000 Loss: 0.4605, lr 1.464466094067265e-05
Step 508100 Loss: 0.4543, lr 1.464466094067265e-05
Step 508200 Loss: 0.4487, lr 1.464466094067265e-05
Step 508300 Loss: 0.4430, lr 1.464466094067265e-05
Step 508400 Loss: 0.4389, lr 1.464466094067265e-05
Step 508500 Loss: 0.4341, lr 1.464466094067265e-05
Step 508600 Loss: 0.4297, lr 1.464466094067265e-05
Step 508700 Loss: 0.4247, lr 1.464466094067265e-05
Step 508800 Loss: 0.4205, lr 1.464466094067265e-05
Step 508900 Loss: 0.4170, lr 1.464466094067265e-05
Step 509000 Loss: 0.4133, lr 1.464466094067265e-05
Step 509100 Loss: 0.4105, lr 1.464466094067265e-05
Step 509200 Loss: 0.4061, lr 1.464466094067265e-05
Step 509300 Loss: 0.4027, lr 1.464466094067265e-05
Step 509400 Loss: 0.3999, lr 1.464466094067265e-05
Step 509500 Loss: 0.3976, lr 1.464466094067265e-05
Step 509600 Loss: 0.3947, lr 1.464466094067265e-05
Step 509700 Loss: 0.3926, lr 1.464466094067265e-05
Step 509800 Loss: 0.3898, lr 1.464466094067265e-05
Step 509900 Loss: 0.3875, lr 1.464466094067265e-05
Step 510000 Loss: 0.3849, lr 1.464466094067265e-05
Step 510100 Loss: 0.3823, lr 1.464466094067265e-05
Step 510200 Loss: 0.3797, lr 1.464466094067265e-05
Step 510300 Loss: 0.3773, lr 1.464466094067265e-05
Step 510400 Loss: 0.3750, lr 1.464466094067265e-05
Step 510500 Loss: 0.3730, lr 1.464466094067265e-05
Step 510600 Loss: 0.3709, lr 1.464466094067265e-05
Step 510700 Loss: 0.3694, lr 1.464466094067265e-05
Step 510800 Loss: 0.3679, lr 1.464466094067265e-05
Step 510900 Loss: 0.3665, lr 1.464466094067265e-05
Step 511000 Loss: 0.3651, lr 1.464466094067265e-05
Step 511100 Loss: 0.3641, lr 1.464466094067265e-05
Step 511200 Loss: 0.3626, lr 1.464466094067265e-05
Step 511300 Loss: 0.3611, lr 1.464466094067265e-05
Step 511400 Loss: 0.3596, lr 1.464466094067265e-05
Step 511500 Loss: 0.3588, lr 1.464466094067265e-05
Step 511600 Loss: 0.3579, lr 1.464466094067265e-05
Step 511700 Loss: 0.3574, lr 1.464466094067265e-05
Step 511800 Loss: 0.3570, lr 1.464466094067265e-05
Step 511900 Loss: 0.3564, lr 1.464466094067265e-05
Step 512000 Loss: 0.3558, lr 1.464466094067265e-05
Step 512100 Loss: 0.3552, lr 1.464466094067265e-05
Step 512200 Loss: 0.3548, lr 1.464466094067265e-05
Step 512300 Loss: 0.3544, lr 1.464466094067265e-05
Step 512400 Loss: 0.3544, lr 1.464466094067265e-05
Step 512500 Loss: 0.3544, lr 1.464466094067265e-05
Step 512600 Loss: 0.3550, lr 1.464466094067265e-05
Step 512700 Loss: 0.3551, lr 1.464466094067265e-05
Step 512800 Loss: 0.3556, lr 1.464466094067265e-05
Step 512900 Loss: 0.3556, lr 1.464466094067265e-05
Step 513000 Loss: 0.3554, lr 1.464466094067265e-05
Step 513100 Loss: 0.3548, lr 1.464466094067265e-05
Step 513200 Loss: 0.3543, lr 1.464466094067265e-05
Step 513300 Loss: 0.3531, lr 1.464466094067265e-05
Step 513400 Loss: 0.3522, lr 1.464466094067265e-05
Train Epoch: [76/100] Loss: 0.3523,lr 0.000015
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6754
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6754
Step 513500 Loss: 0.8195, lr 1.3551568628929456e-05
Step 513600 Loss: 0.6738, lr 1.3551568628929456e-05
Step 513700 Loss: 0.6166, lr 1.3551568628929456e-05
Step 513800 Loss: 0.5787, lr 1.3551568628929456e-05
Step 513900 Loss: 0.5597, lr 1.3551568628929456e-05
Step 514000 Loss: 0.5420, lr 1.3551568628929456e-05
Step 514100 Loss: 0.5249, lr 1.3551568628929456e-05
Step 514200 Loss: 0.5117, lr 1.3551568628929456e-05
Step 514300 Loss: 0.4994, lr 1.3551568628929456e-05
Step 514400 Loss: 0.4887, lr 1.3551568628929456e-05
Step 514500 Loss: 0.4803, lr 1.3551568628929456e-05
Step 514600 Loss: 0.4722, lr 1.3551568628929456e-05
Step 514700 Loss: 0.4654, lr 1.3551568628929456e-05
Step 514800 Loss: 0.4581, lr 1.3551568628929456e-05
Step 514900 Loss: 0.4524, lr 1.3551568628929456e-05
Step 515000 Loss: 0.4465, lr 1.3551568628929456e-05
Step 515100 Loss: 0.4411, lr 1.3551568628929456e-05
Step 515200 Loss: 0.4369, lr 1.3551568628929456e-05
Step 515300 Loss: 0.4324, lr 1.3551568628929456e-05
Step 515400 Loss: 0.4274, lr 1.3551568628929456e-05
Step 515500 Loss: 0.4224, lr 1.3551568628929456e-05
Step 515600 Loss: 0.4189, lr 1.3551568628929456e-05
Step 515700 Loss: 0.4155, lr 1.3551568628929456e-05
Step 515800 Loss: 0.4123, lr 1.3551568628929456e-05
Step 515900 Loss: 0.4086, lr 1.3551568628929456e-05
Step 516000 Loss: 0.4050, lr 1.3551568628929456e-05
Step 516100 Loss: 0.4016, lr 1.3551568628929456e-05
Step 516200 Loss: 0.3995, lr 1.3551568628929456e-05
Step 516300 Loss: 0.3964, lr 1.3551568628929456e-05
Step 516400 Loss: 0.3941, lr 1.3551568628929456e-05
Step 516500 Loss: 0.3917, lr 1.3551568628929456e-05
Step 516600 Loss: 0.3888, lr 1.3551568628929456e-05
Step 516700 Loss: 0.3867, lr 1.3551568628929456e-05
Step 516800 Loss: 0.3837, lr 1.3551568628929456e-05
Step 516900 Loss: 0.3813, lr 1.3551568628929456e-05
Step 517000 Loss: 0.3787, lr 1.3551568628929456e-05
Step 517100 Loss: 0.3764, lr 1.3551568628929456e-05
Step 517200 Loss: 0.3742, lr 1.3551568628929456e-05
Step 517300 Loss: 0.3721, lr 1.3551568628929456e-05
Step 517400 Loss: 0.3702, lr 1.3551568628929456e-05
Step 517500 Loss: 0.3690, lr 1.3551568628929456e-05
Step 517600 Loss: 0.3674, lr 1.3551568628929456e-05
Step 517700 Loss: 0.3661, lr 1.3551568628929456e-05
Step 517800 Loss: 0.3647, lr 1.3551568628929456e-05
Step 517900 Loss: 0.3635, lr 1.3551568628929456e-05
Step 518000 Loss: 0.3619, lr 1.3551568628929456e-05
Step 518100 Loss: 0.3604, lr 1.3551568628929456e-05
Step 518200 Loss: 0.3594, lr 1.3551568628929456e-05
Step 518300 Loss: 0.3586, lr 1.3551568628929456e-05
Step 518400 Loss: 0.3579, lr 1.3551568628929456e-05
Step 518500 Loss: 0.3575, lr 1.3551568628929456e-05
Step 518600 Loss: 0.3570, lr 1.3551568628929456e-05
Step 518700 Loss: 0.3563, lr 1.3551568628929456e-05
Step 518800 Loss: 0.3557, lr 1.3551568628929456e-05
Step 518900 Loss: 0.3553, lr 1.3551568628929456e-05
Step 519000 Loss: 0.3548, lr 1.3551568628929456e-05
Step 519100 Loss: 0.3545, lr 1.3551568628929456e-05
Step 519200 Loss: 0.3545, lr 1.3551568628929456e-05
Step 519300 Loss: 0.3549, lr 1.3551568628929456e-05
Step 519400 Loss: 0.3551, lr 1.3551568628929456e-05
Step 519500 Loss: 0.3553, lr 1.3551568628929456e-05
Step 519600 Loss: 0.3558, lr 1.3551568628929456e-05
Step 519700 Loss: 0.3556, lr 1.3551568628929456e-05
Step 519800 Loss: 0.3556, lr 1.3551568628929456e-05
Step 519900 Loss: 0.3547, lr 1.3551568628929456e-05
Step 520000 Loss: 0.3541, lr 1.3551568628929456e-05
Step 520100 Loss: 0.3528, lr 1.3551568628929456e-05
Train Epoch: [77/100] Loss: 0.3528,lr 0.000014
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 6756
Step 520200 Loss: 1.2281, lr 1.249444651847704e-05
Step 520300 Loss: 0.7342, lr 1.249444651847704e-05
Step 520400 Loss: 0.6415, lr 1.249444651847704e-05
Step 520500 Loss: 0.5941, lr 1.249444651847704e-05
Step 520600 Loss: 0.5681, lr 1.249444651847704e-05
Step 520700 Loss: 0.5493, lr 1.249444651847704e-05
Step 520800 Loss: 0.5305, lr 1.249444651847704e-05
Step 520900 Loss: 0.5176, lr 1.249444651847704e-05
Step 521000 Loss: 0.5049, lr 1.249444651847704e-05
Step 521100 Loss: 0.4925, lr 1.249444651847704e-05
Step 521200 Loss: 0.4828, lr 1.249444651847704e-05
Step 521300 Loss: 0.4760, lr 1.249444651847704e-05
Step 521400 Loss: 0.4674, lr 1.249444651847704e-05
Step 521500 Loss: 0.4608, lr 1.249444651847704e-05
Step 521600 Loss: 0.4546, lr 1.249444651847704e-05
Step 521700 Loss: 0.4485, lr 1.249444651847704e-05
Step 521800 Loss: 0.4427, lr 1.249444651847704e-05
Step 521900 Loss: 0.4386, lr 1.249444651847704e-05
Step 522000 Loss: 0.4338, lr 1.249444651847704e-05
Step 522100 Loss: 0.4293, lr 1.249444651847704e-05
Step 522200 Loss: 0.4243, lr 1.249444651847704e-05
Step 522300 Loss: 0.4198, lr 1.249444651847704e-05
Step 522400 Loss: 0.4165, lr 1.249444651847704e-05
Step 522500 Loss: 0.4129, lr 1.249444651847704e-05
Step 522600 Loss: 0.4102, lr 1.249444651847704e-05
Step 522700 Loss: 0.4059, lr 1.249444651847704e-05
Step 522800 Loss: 0.4022, lr 1.249444651847704e-05
Step 522900 Loss: 0.3997, lr 1.249444651847704e-05
Step 523000 Loss: 0.3971, lr 1.249444651847704e-05
Step 523100 Loss: 0.3940, lr 1.249444651847704e-05
Step 523200 Loss: 0.3918, lr 1.249444651847704e-05
Step 523300 Loss: 0.3889, lr 1.249444651847704e-05
Step 523400 Loss: 0.3867, lr 1.249444651847704e-05
Step 523500 Loss: 0.3841, lr 1.249444651847704e-05
Step 523600 Loss: 0.3818, lr 1.249444651847704e-05
Step 523700 Loss: 0.3793, lr 1.249444651847704e-05
Step 523800 Loss: 0.3770, lr 1.249444651847704e-05
Step 523900 Loss: 0.3747, lr 1.249444651847704e-05
Step 524000 Loss: 0.3726, lr 1.249444651847704e-05
Step 524100 Loss: 0.3707, lr 1.249444651847704e-05
Step 524200 Loss: 0.3688, lr 1.249444651847704e-05
Step 524300 Loss: 0.3675, lr 1.249444651847704e-05
Step 524400 Loss: 0.3661, lr 1.249444651847704e-05
Step 524500 Loss: 0.3647, lr 1.249444651847704e-05
Step 524600 Loss: 0.3636, lr 1.249444651847704e-05
Step 524700 Loss: 0.3622, lr 1.249444651847704e-05
Step 524800 Loss: 0.3607, lr 1.249444651847704e-05
Step 524900 Loss: 0.3593, lr 1.249444651847704e-05
Step 525000 Loss: 0.3583, lr 1.249444651847704e-05
Step 525100 Loss: 0.3574, lr 1.249444651847704e-05
Step 525200 Loss: 0.3568, lr 1.249444651847704e-05
Step 525300 Loss: 0.3563, lr 1.249444651847704e-05
Step 525400 Loss: 0.3558, lr 1.249444651847704e-05
Step 525500 Loss: 0.3551, lr 1.249444651847704e-05
Step 525600 Loss: 0.3545, lr 1.249444651847704e-05
Step 525700 Loss: 0.3541, lr 1.249444651847704e-05
Step 525800 Loss: 0.3538, lr 1.249444651847704e-05
Step 525900 Loss: 0.3537, lr 1.249444651847704e-05
Step 526000 Loss: 0.3537, lr 1.249444651847704e-05
Step 526100 Loss: 0.3542, lr 1.249444651847704e-05
Step 526200 Loss: 0.3542, lr 1.249444651847704e-05
Step 526300 Loss: 0.3547, lr 1.249444651847704e-05
Step 526400 Loss: 0.3547, lr 1.249444651847704e-05
Step 526500 Loss: 0.3546, lr 1.249444651847704e-05
Step 526600 Loss: 0.3542, lr 1.249444651847704e-05
Step 526700 Loss: 0.3537, lr 1.249444651847704e-05
Step 526800 Loss: 0.3524, lr 1.249444651847704e-05
Step 526900 Loss: 0.3517, lr 1.249444651847704e-05
Train Epoch: [78/100] Loss: 0.3516,lr 0.000012
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 527000 Loss: 0.8537, lr 1.1474337861210562e-05
Step 527100 Loss: 0.6792, lr 1.1474337861210562e-05
Step 527200 Loss: 0.6176, lr 1.1474337861210562e-05
Step 527300 Loss: 0.5784, lr 1.1474337861210562e-05
Step 527400 Loss: 0.5593, lr 1.1474337861210562e-05
Step 527500 Loss: 0.5416, lr 1.1474337861210562e-05
Step 527600 Loss: 0.5243, lr 1.1474337861210562e-05
Step 527700 Loss: 0.5098, lr 1.1474337861210562e-05
Step 527800 Loss: 0.4985, lr 1.1474337861210562e-05
Step 527900 Loss: 0.4875, lr 1.1474337861210562e-05
Step 528000 Loss: 0.4794, lr 1.1474337861210562e-05
Step 528100 Loss: 0.4712, lr 1.1474337861210562e-05
Step 528200 Loss: 0.4643, lr 1.1474337861210562e-05
Step 528300 Loss: 0.4567, lr 1.1474337861210562e-05
Step 528400 Loss: 0.4509, lr 1.1474337861210562e-05
Step 528500 Loss: 0.4455, lr 1.1474337861210562e-05
Step 528600 Loss: 0.4402, lr 1.1474337861210562e-05
Step 528700 Loss: 0.4358, lr 1.1474337861210562e-05
Step 528800 Loss: 0.4313, lr 1.1474337861210562e-05
Step 528900 Loss: 0.4264, lr 1.1474337861210562e-05
Step 529000 Loss: 0.4214, lr 1.1474337861210562e-05
Step 529100 Loss: 0.4180, lr 1.1474337861210562e-05
Step 529200 Loss: 0.4147, lr 1.1474337861210562e-05
Step 529300 Loss: 0.4110, lr 1.1474337861210562e-05
Step 529400 Loss: 0.4075, lr 1.1474337861210562e-05
Step 529500 Loss: 0.4038, lr 1.1474337861210562e-05
Step 529600 Loss: 0.4006, lr 1.1474337861210562e-05
Step 529700 Loss: 0.3987, lr 1.1474337861210562e-05
Step 529800 Loss: 0.3955, lr 1.1474337861210562e-05
Step 529900 Loss: 0.3934, lr 1.1474337861210562e-05
Step 530000 Loss: 0.3910, lr 1.1474337861210562e-05
Step 530100 Loss: 0.3880, lr 1.1474337861210562e-05
Step 530200 Loss: 0.3861, lr 1.1474337861210562e-05
Step 530300 Loss: 0.3832, lr 1.1474337861210562e-05
Step 530400 Loss: 0.3807, lr 1.1474337861210562e-05
Step 530500 Loss: 0.3779, lr 1.1474337861210562e-05
Step 530600 Loss: 0.3757, lr 1.1474337861210562e-05
Step 530700 Loss: 0.3735, lr 1.1474337861210562e-05
Step 530800 Loss: 0.3714, lr 1.1474337861210562e-05
Step 530900 Loss: 0.3695, lr 1.1474337861210562e-05
Step 531000 Loss: 0.3679, lr 1.1474337861210562e-05
Step 531100 Loss: 0.3666, lr 1.1474337861210562e-05
Step 531200 Loss: 0.3654, lr 1.1474337861210562e-05
Step 531300 Loss: 0.3639, lr 1.1474337861210562e-05
Step 531400 Loss: 0.3627, lr 1.1474337861210562e-05
Step 531500 Loss: 0.3609, lr 1.1474337861210562e-05
Step 531600 Loss: 0.3595, lr 1.1474337861210562e-05
Step 531700 Loss: 0.3584, lr 1.1474337861210562e-05
Step 531800 Loss: 0.3577, lr 1.1474337861210562e-05
Step 531900 Loss: 0.3571, lr 1.1474337861210562e-05
Step 532000 Loss: 0.3564, lr 1.1474337861210562e-05
Step 532100 Loss: 0.3559, lr 1.1474337861210562e-05
Step 532200 Loss: 0.3554, lr 1.1474337861210562e-05
Step 532300 Loss: 0.3549, lr 1.1474337861210562e-05
Step 532400 Loss: 0.3544, lr 1.1474337861210562e-05
Step 532500 Loss: 0.3539, lr 1.1474337861210562e-05
Step 532600 Loss: 0.3536, lr 1.1474337861210562e-05
Step 532700 Loss: 0.3536, lr 1.1474337861210562e-05
Step 532800 Loss: 0.3539, lr 1.1474337861210562e-05
Step 532900 Loss: 0.3542, lr 1.1474337861210562e-05
Step 533000 Loss: 0.3542, lr 1.1474337861210562e-05
Step 533100 Loss: 0.3546, lr 1.1474337861210562e-05
Step 533200 Loss: 0.3547, lr 1.1474337861210562e-05
Step 533300 Loss: 0.3545, lr 1.1474337861210562e-05
Step 533400 Loss: 0.3539, lr 1.1474337861210562e-05
Step 533500 Loss: 0.3532, lr 1.1474337861210562e-05
Step 533600 Loss: 0.3521, lr 1.1474337861210562e-05
Train Epoch: [79/100] Loss: 0.3518,lr 0.000011
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6755
Step 533700 Loss: 1.7417, lr 1.0492249381215497e-05
Step 533800 Loss: 0.7520, lr 1.0492249381215497e-05
Step 533900 Loss: 0.6508, lr 1.0492249381215497e-05
Step 534000 Loss: 0.5987, lr 1.0492249381215497e-05
Step 534100 Loss: 0.5699, lr 1.0492249381215497e-05
Step 534200 Loss: 0.5505, lr 1.0492249381215497e-05
Step 534300 Loss: 0.5315, lr 1.0492249381215497e-05
Step 534400 Loss: 0.5189, lr 1.0492249381215497e-05
Step 534500 Loss: 0.5053, lr 1.0492249381215497e-05
Step 534600 Loss: 0.4941, lr 1.0492249381215497e-05
Step 534700 Loss: 0.4835, lr 1.0492249381215497e-05
Step 534800 Loss: 0.4763, lr 1.0492249381215497e-05
Step 534900 Loss: 0.4678, lr 1.0492249381215497e-05
Step 535000 Loss: 0.4613, lr 1.0492249381215497e-05
Step 535100 Loss: 0.4549, lr 1.0492249381215497e-05
Step 535200 Loss: 0.4489, lr 1.0492249381215497e-05
Step 535300 Loss: 0.4433, lr 1.0492249381215497e-05
Step 535400 Loss: 0.4389, lr 1.0492249381215497e-05
Step 535500 Loss: 0.4345, lr 1.0492249381215497e-05
Step 535600 Loss: 0.4299, lr 1.0492249381215497e-05
Step 535700 Loss: 0.4251, lr 1.0492249381215497e-05
Step 535800 Loss: 0.4205, lr 1.0492249381215497e-05
Step 535900 Loss: 0.4171, lr 1.0492249381215497e-05
Step 536000 Loss: 0.4135, lr 1.0492249381215497e-05
Step 536100 Loss: 0.4106, lr 1.0492249381215497e-05
Step 536200 Loss: 0.4067, lr 1.0492249381215497e-05
Step 536300 Loss: 0.4030, lr 1.0492249381215497e-05
Step 536400 Loss: 0.4002, lr 1.0492249381215497e-05
Step 536500 Loss: 0.3980, lr 1.0492249381215497e-05
Step 536600 Loss: 0.3949, lr 1.0492249381215497e-05
Step 536700 Loss: 0.3928, lr 1.0492249381215497e-05
Step 536800 Loss: 0.3898, lr 1.0492249381215497e-05
Step 536900 Loss: 0.3875, lr 1.0492249381215497e-05
Step 537000 Loss: 0.3849, lr 1.0492249381215497e-05
Step 537100 Loss: 0.3823, lr 1.0492249381215497e-05
Step 537200 Loss: 0.3797, lr 1.0492249381215497e-05
Step 537300 Loss: 0.3770, lr 1.0492249381215497e-05
Step 537400 Loss: 0.3749, lr 1.0492249381215497e-05
Step 537500 Loss: 0.3730, lr 1.0492249381215497e-05
Step 537600 Loss: 0.3709, lr 1.0492249381215497e-05
Step 537700 Loss: 0.3691, lr 1.0492249381215497e-05
Step 537800 Loss: 0.3676, lr 1.0492249381215497e-05
Step 537900 Loss: 0.3664, lr 1.0492249381215497e-05
Step 538000 Loss: 0.3650, lr 1.0492249381215497e-05
Step 538100 Loss: 0.3637, lr 1.0492249381215497e-05
Step 538200 Loss: 0.3626, lr 1.0492249381215497e-05
Step 538300 Loss: 0.3608, lr 1.0492249381215497e-05
Step 538400 Loss: 0.3596, lr 1.0492249381215497e-05
Step 538500 Loss: 0.3587, lr 1.0492249381215497e-05
Step 538600 Loss: 0.3578, lr 1.0492249381215497e-05
Step 538700 Loss: 0.3573, lr 1.0492249381215497e-05
Step 538800 Loss: 0.3569, lr 1.0492249381215497e-05
Step 538900 Loss: 0.3563, lr 1.0492249381215497e-05
Step 539000 Loss: 0.3557, lr 1.0492249381215497e-05
Step 539100 Loss: 0.3550, lr 1.0492249381215497e-05
Step 539200 Loss: 0.3546, lr 1.0492249381215497e-05
Step 539300 Loss: 0.3543, lr 1.0492249381215497e-05
Step 539400 Loss: 0.3541, lr 1.0492249381215497e-05
Step 539500 Loss: 0.3543, lr 1.0492249381215497e-05
Step 539600 Loss: 0.3548, lr 1.0492249381215497e-05
Step 539700 Loss: 0.3548, lr 1.0492249381215497e-05
Step 539800 Loss: 0.3552, lr 1.0492249381215497e-05
Step 539900 Loss: 0.3554, lr 1.0492249381215497e-05
Step 540000 Loss: 0.3551, lr 1.0492249381215497e-05
Step 540100 Loss: 0.3548, lr 1.0492249381215497e-05
Step 540200 Loss: 0.3542, lr 1.0492249381215497e-05
Step 540300 Loss: 0.3532, lr 1.0492249381215497e-05
Step 540400 Loss: 0.3523, lr 1.0492249381215497e-05
Train Epoch: [80/100] Loss: 0.3523,lr 0.000010
Model Saving at epoch 80
Calling G2SDataset.batch()
Done, time:  1.51 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 6756
Step 540500 Loss: 0.9064, lr 9.549150281252648e-06
Step 540600 Loss: 0.6895, lr 9.549150281252648e-06
Step 540700 Loss: 0.6231, lr 9.549150281252648e-06
Step 540800 Loss: 0.5827, lr 9.549150281252648e-06
Step 540900 Loss: 0.5616, lr 9.549150281252648e-06
Step 541000 Loss: 0.5440, lr 9.549150281252648e-06
Step 541100 Loss: 0.5261, lr 9.549150281252648e-06
Step 541200 Loss: 0.5115, lr 9.549150281252648e-06
Step 541300 Loss: 0.4999, lr 9.549150281252648e-06
Step 541400 Loss: 0.4883, lr 9.549150281252648e-06
Step 541500 Loss: 0.4798, lr 9.549150281252648e-06
Step 541600 Loss: 0.4721, lr 9.549150281252648e-06
Step 541700 Loss: 0.4639, lr 9.549150281252648e-06
Step 541800 Loss: 0.4568, lr 9.549150281252648e-06
Step 541900 Loss: 0.4511, lr 9.549150281252648e-06
Step 542000 Loss: 0.4460, lr 9.549150281252648e-06
Step 542100 Loss: 0.4402, lr 9.549150281252648e-06
Step 542200 Loss: 0.4362, lr 9.549150281252648e-06
Step 542300 Loss: 0.4318, lr 9.549150281252648e-06
Step 542400 Loss: 0.4271, lr 9.549150281252648e-06
Step 542500 Loss: 0.4224, lr 9.549150281252648e-06
Step 542600 Loss: 0.4188, lr 9.549150281252648e-06
Step 542700 Loss: 0.4150, lr 9.549150281252648e-06
Step 542800 Loss: 0.4116, lr 9.549150281252648e-06
Step 542900 Loss: 0.4084, lr 9.549150281252648e-06
Step 543000 Loss: 0.4046, lr 9.549150281252648e-06
Step 543100 Loss: 0.4012, lr 9.549150281252648e-06
Step 543200 Loss: 0.3989, lr 9.549150281252648e-06
Step 543300 Loss: 0.3959, lr 9.549150281252648e-06
Step 543400 Loss: 0.3935, lr 9.549150281252648e-06
Step 543500 Loss: 0.3910, lr 9.549150281252648e-06
Step 543600 Loss: 0.3884, lr 9.549150281252648e-06
Step 543700 Loss: 0.3862, lr 9.549150281252648e-06
Step 543800 Loss: 0.3832, lr 9.549150281252648e-06
Step 543900 Loss: 0.3805, lr 9.549150281252648e-06
Step 544000 Loss: 0.3781, lr 9.549150281252648e-06
Step 544100 Loss: 0.3759, lr 9.549150281252648e-06
Step 544200 Loss: 0.3737, lr 9.549150281252648e-06
Step 544300 Loss: 0.3717, lr 9.549150281252648e-06
Step 544400 Loss: 0.3697, lr 9.549150281252648e-06
Step 544500 Loss: 0.3682, lr 9.549150281252648e-06
Step 544600 Loss: 0.3667, lr 9.549150281252648e-06
Step 544700 Loss: 0.3656, lr 9.549150281252648e-06
Step 544800 Loss: 0.3639, lr 9.549150281252648e-06
Step 544900 Loss: 0.3629, lr 9.549150281252648e-06
Step 545000 Loss: 0.3612, lr 9.549150281252648e-06
Step 545100 Loss: 0.3599, lr 9.549150281252648e-06
Step 545200 Loss: 0.3586, lr 9.549150281252648e-06
Step 545300 Loss: 0.3579, lr 9.549150281252648e-06
Step 545400 Loss: 0.3571, lr 9.549150281252648e-06
Step 545500 Loss: 0.3564, lr 9.549150281252648e-06
Step 545600 Loss: 0.3561, lr 9.549150281252648e-06
Step 545700 Loss: 0.3556, lr 9.549150281252648e-06
Step 545800 Loss: 0.3549, lr 9.549150281252648e-06
Step 545900 Loss: 0.3543, lr 9.549150281252648e-06
Step 546000 Loss: 0.3539, lr 9.549150281252648e-06
Step 546100 Loss: 0.3537, lr 9.549150281252648e-06
Step 546200 Loss: 0.3539, lr 9.549150281252648e-06
Step 546300 Loss: 0.3539, lr 9.549150281252648e-06
Step 546400 Loss: 0.3540, lr 9.549150281252648e-06
Step 546500 Loss: 0.3541, lr 9.549150281252648e-06
Step 546600 Loss: 0.3544, lr 9.549150281252648e-06
Step 546700 Loss: 0.3546, lr 9.549150281252648e-06
Step 546800 Loss: 0.3540, lr 9.549150281252648e-06
Step 546900 Loss: 0.3533, lr 9.549150281252648e-06
Step 547000 Loss: 0.3528, lr 9.549150281252648e-06
Step 547100 Loss: 0.3516, lr 9.549150281252648e-06
Step 547200 Loss: 0.3510, lr 9.549150281252648e-06
Train Epoch: [81/100] Loss: 0.3514,lr 0.000010
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6756
Step 547300 Loss: 0.7769, lr 8.645971286271927e-06
Step 547400 Loss: 0.6578, lr 8.645971286271927e-06
Step 547500 Loss: 0.6062, lr 8.645971286271927e-06
Step 547600 Loss: 0.5744, lr 8.645971286271927e-06
Step 547700 Loss: 0.5540, lr 8.645971286271927e-06
Step 547800 Loss: 0.5350, lr 8.645971286271927e-06
Step 547900 Loss: 0.5209, lr 8.645971286271927e-06
Step 548000 Loss: 0.5077, lr 8.645971286271927e-06
Step 548100 Loss: 0.4957, lr 8.645971286271927e-06
Step 548200 Loss: 0.4849, lr 8.645971286271927e-06
Step 548300 Loss: 0.4773, lr 8.645971286271927e-06
Step 548400 Loss: 0.4689, lr 8.645971286271927e-06
Step 548500 Loss: 0.4622, lr 8.645971286271927e-06
Step 548600 Loss: 0.4559, lr 8.645971286271927e-06
Step 548700 Loss: 0.4496, lr 8.645971286271927e-06
Step 548800 Loss: 0.4439, lr 8.645971286271927e-06
Step 548900 Loss: 0.4392, lr 8.645971286271927e-06
Step 549000 Loss: 0.4348, lr 8.645971286271927e-06
Step 549100 Loss: 0.4305, lr 8.645971286271927e-06
Step 549200 Loss: 0.4255, lr 8.645971286271927e-06
Step 549300 Loss: 0.4210, lr 8.645971286271927e-06
Step 549400 Loss: 0.4174, lr 8.645971286271927e-06
Step 549500 Loss: 0.4139, lr 8.645971286271927e-06
Step 549600 Loss: 0.4110, lr 8.645971286271927e-06
Step 549700 Loss: 0.4071, lr 8.645971286271927e-06
Step 549800 Loss: 0.4032, lr 8.645971286271927e-06
Step 549900 Loss: 0.4003, lr 8.645971286271927e-06
Step 550000 Loss: 0.3981, lr 8.645971286271927e-06
Step 550100 Loss: 0.3951, lr 8.645971286271927e-06
Step 550200 Loss: 0.3929, lr 8.645971286271927e-06
Step 550300 Loss: 0.3901, lr 8.645971286271927e-06
Step 550400 Loss: 0.3876, lr 8.645971286271927e-06
Step 550500 Loss: 0.3854, lr 8.645971286271927e-06
Step 550600 Loss: 0.3826, lr 8.645971286271927e-06
Step 550700 Loss: 0.3800, lr 8.645971286271927e-06
Step 550800 Loss: 0.3773, lr 8.645971286271927e-06
Step 550900 Loss: 0.3751, lr 8.645971286271927e-06
Step 551000 Loss: 0.3729, lr 8.645971286271927e-06
Step 551100 Loss: 0.3707, lr 8.645971286271927e-06
Step 551200 Loss: 0.3690, lr 8.645971286271927e-06
Step 551300 Loss: 0.3675, lr 8.645971286271927e-06
Step 551400 Loss: 0.3662, lr 8.645971286271927e-06
Step 551500 Loss: 0.3646, lr 8.645971286271927e-06
Step 551600 Loss: 0.3635, lr 8.645971286271927e-06
Step 551700 Loss: 0.3622, lr 8.645971286271927e-06
Step 551800 Loss: 0.3604, lr 8.645971286271927e-06
Step 551900 Loss: 0.3592, lr 8.645971286271927e-06
Step 552000 Loss: 0.3581, lr 8.645971286271927e-06
Step 552100 Loss: 0.3572, lr 8.645971286271927e-06
Step 552200 Loss: 0.3567, lr 8.645971286271927e-06
Step 552300 Loss: 0.3560, lr 8.645971286271927e-06
Step 552400 Loss: 0.3555, lr 8.645971286271927e-06
Step 552500 Loss: 0.3549, lr 8.645971286271927e-06
Step 552600 Loss: 0.3543, lr 8.645971286271927e-06
Step 552700 Loss: 0.3539, lr 8.645971286271927e-06
Step 552800 Loss: 0.3534, lr 8.645971286271927e-06
Step 552900 Loss: 0.3533, lr 8.645971286271927e-06
Step 553000 Loss: 0.3534, lr 8.645971286271927e-06
Step 553100 Loss: 0.3539, lr 8.645971286271927e-06
Step 553200 Loss: 0.3538, lr 8.645971286271927e-06
Step 553300 Loss: 0.3541, lr 8.645971286271927e-06
Step 553400 Loss: 0.3545, lr 8.645971286271927e-06
Step 553500 Loss: 0.3540, lr 8.645971286271927e-06
Step 553600 Loss: 0.3539, lr 8.645971286271927e-06
Step 553700 Loss: 0.3530, lr 8.645971286271927e-06
Step 553800 Loss: 0.3522, lr 8.645971286271927e-06
Step 553900 Loss: 0.3510, lr 8.645971286271927e-06
Train Epoch: [82/100] Loss: 0.3508,lr 0.000009
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6756
Step 554000 Loss: 0.9670, lr 7.78360372489927e-06
Step 554100 Loss: 0.7021, lr 7.78360372489927e-06
Step 554200 Loss: 0.6293, lr 7.78360372489927e-06
Step 554300 Loss: 0.5874, lr 7.78360372489927e-06
Step 554400 Loss: 0.5631, lr 7.78360372489927e-06
Step 554500 Loss: 0.5453, lr 7.78360372489927e-06
Step 554600 Loss: 0.5278, lr 7.78360372489927e-06
Step 554700 Loss: 0.5138, lr 7.78360372489927e-06
Step 554800 Loss: 0.5016, lr 7.78360372489927e-06
Step 554900 Loss: 0.4906, lr 7.78360372489927e-06
Step 555000 Loss: 0.4814, lr 7.78360372489927e-06
Step 555100 Loss: 0.4745, lr 7.78360372489927e-06
Step 555200 Loss: 0.4660, lr 7.78360372489927e-06
Step 555300 Loss: 0.4589, lr 7.78360372489927e-06
Step 555400 Loss: 0.4529, lr 7.78360372489927e-06
Step 555500 Loss: 0.4476, lr 7.78360372489927e-06
Step 555600 Loss: 0.4420, lr 7.78360372489927e-06
Step 555700 Loss: 0.4376, lr 7.78360372489927e-06
Step 555800 Loss: 0.4327, lr 7.78360372489927e-06
Step 555900 Loss: 0.4280, lr 7.78360372489927e-06
Step 556000 Loss: 0.4232, lr 7.78360372489927e-06
Step 556100 Loss: 0.4193, lr 7.78360372489927e-06
Step 556200 Loss: 0.4157, lr 7.78360372489927e-06
Step 556300 Loss: 0.4121, lr 7.78360372489927e-06
Step 556400 Loss: 0.4089, lr 7.78360372489927e-06
Step 556500 Loss: 0.4051, lr 7.78360372489927e-06
Step 556600 Loss: 0.4015, lr 7.78360372489927e-06
Step 556700 Loss: 0.3991, lr 7.78360372489927e-06
Step 556800 Loss: 0.3962, lr 7.78360372489927e-06
Step 556900 Loss: 0.3936, lr 7.78360372489927e-06
Step 557000 Loss: 0.3914, lr 7.78360372489927e-06
Step 557100 Loss: 0.3887, lr 7.78360372489927e-06
Step 557200 Loss: 0.3865, lr 7.78360372489927e-06
Step 557300 Loss: 0.3838, lr 7.78360372489927e-06
Step 557400 Loss: 0.3810, lr 7.78360372489927e-06
Step 557500 Loss: 0.3787, lr 7.78360372489927e-06
Step 557600 Loss: 0.3763, lr 7.78360372489927e-06
Step 557700 Loss: 0.3739, lr 7.78360372489927e-06
Step 557800 Loss: 0.3719, lr 7.78360372489927e-06
Step 557900 Loss: 0.3698, lr 7.78360372489927e-06
Step 558000 Loss: 0.3683, lr 7.78360372489927e-06
Step 558100 Loss: 0.3667, lr 7.78360372489927e-06
Step 558200 Loss: 0.3653, lr 7.78360372489927e-06
Step 558300 Loss: 0.3638, lr 7.78360372489927e-06
Step 558400 Loss: 0.3627, lr 7.78360372489927e-06
Step 558500 Loss: 0.3611, lr 7.78360372489927e-06
Step 558600 Loss: 0.3597, lr 7.78360372489927e-06
Step 558700 Loss: 0.3583, lr 7.78360372489927e-06
Step 558800 Loss: 0.3574, lr 7.78360372489927e-06
Step 558900 Loss: 0.3567, lr 7.78360372489927e-06
Step 559000 Loss: 0.3561, lr 7.78360372489927e-06
Step 559100 Loss: 0.3558, lr 7.78360372489927e-06
Step 559200 Loss: 0.3551, lr 7.78360372489927e-06
Step 559300 Loss: 0.3545, lr 7.78360372489927e-06
Step 559400 Loss: 0.3539, lr 7.78360372489927e-06
Step 559500 Loss: 0.3535, lr 7.78360372489927e-06
Step 559600 Loss: 0.3533, lr 7.78360372489927e-06
Step 559700 Loss: 0.3534, lr 7.78360372489927e-06
Step 559800 Loss: 0.3534, lr 7.78360372489927e-06
Step 559900 Loss: 0.3537, lr 7.78360372489927e-06
Step 560000 Loss: 0.3539, lr 7.78360372489927e-06
Step 560100 Loss: 0.3541, lr 7.78360372489927e-06
Step 560200 Loss: 0.3543, lr 7.78360372489927e-06
Step 560300 Loss: 0.3541, lr 7.78360372489927e-06
Step 560400 Loss: 0.3536, lr 7.78360372489927e-06
Step 560500 Loss: 0.3531, lr 7.78360372489927e-06
Step 560600 Loss: 0.3518, lr 7.78360372489927e-06
Step 560700 Loss: 0.3511, lr 7.78360372489927e-06
Train Epoch: [83/100] Loss: 0.3509,lr 0.000008
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6756
Step 560800 Loss: 0.7972, lr 6.962898649802823e-06
Step 560900 Loss: 0.6620, lr 6.962898649802823e-06
Step 561000 Loss: 0.6071, lr 6.962898649802823e-06
Step 561100 Loss: 0.5729, lr 6.962898649802823e-06
Step 561200 Loss: 0.5538, lr 6.962898649802823e-06
Step 561300 Loss: 0.5360, lr 6.962898649802823e-06
Step 561400 Loss: 0.5201, lr 6.962898649802823e-06
Step 561500 Loss: 0.5076, lr 6.962898649802823e-06
Step 561600 Loss: 0.4959, lr 6.962898649802823e-06
Step 561700 Loss: 0.4843, lr 6.962898649802823e-06
Step 561800 Loss: 0.4767, lr 6.962898649802823e-06
Step 561900 Loss: 0.4689, lr 6.962898649802823e-06
Step 562000 Loss: 0.4620, lr 6.962898649802823e-06
Step 562100 Loss: 0.4550, lr 6.962898649802823e-06
Step 562200 Loss: 0.4491, lr 6.962898649802823e-06
Step 562300 Loss: 0.4434, lr 6.962898649802823e-06
Step 562400 Loss: 0.4386, lr 6.962898649802823e-06
Step 562500 Loss: 0.4343, lr 6.962898649802823e-06
Step 562600 Loss: 0.4295, lr 6.962898649802823e-06
Step 562700 Loss: 0.4251, lr 6.962898649802823e-06
Step 562800 Loss: 0.4200, lr 6.962898649802823e-06
Step 562900 Loss: 0.4167, lr 6.962898649802823e-06
Step 563000 Loss: 0.4134, lr 6.962898649802823e-06
Step 563100 Loss: 0.4102, lr 6.962898649802823e-06
Step 563200 Loss: 0.4063, lr 6.962898649802823e-06
Step 563300 Loss: 0.4026, lr 6.962898649802823e-06
Step 563400 Loss: 0.3995, lr 6.962898649802823e-06
Step 563500 Loss: 0.3973, lr 6.962898649802823e-06
Step 563600 Loss: 0.3943, lr 6.962898649802823e-06
Step 563700 Loss: 0.3918, lr 6.962898649802823e-06
Step 563800 Loss: 0.3894, lr 6.962898649802823e-06
Step 563900 Loss: 0.3866, lr 6.962898649802823e-06
Step 564000 Loss: 0.3845, lr 6.962898649802823e-06
Step 564100 Loss: 0.3819, lr 6.962898649802823e-06
Step 564200 Loss: 0.3793, lr 6.962898649802823e-06
Step 564300 Loss: 0.3766, lr 6.962898649802823e-06
Step 564400 Loss: 0.3746, lr 6.962898649802823e-06
Step 564500 Loss: 0.3723, lr 6.962898649802823e-06
Step 564600 Loss: 0.3702, lr 6.962898649802823e-06
Step 564700 Loss: 0.3684, lr 6.962898649802823e-06
Step 564800 Loss: 0.3671, lr 6.962898649802823e-06
Step 564900 Loss: 0.3656, lr 6.962898649802823e-06
Step 565000 Loss: 0.3643, lr 6.962898649802823e-06
Step 565100 Loss: 0.3632, lr 6.962898649802823e-06
Step 565200 Loss: 0.3620, lr 6.962898649802823e-06
Step 565300 Loss: 0.3603, lr 6.962898649802823e-06
Step 565400 Loss: 0.3590, lr 6.962898649802823e-06
Step 565500 Loss: 0.3578, lr 6.962898649802823e-06
Step 565600 Loss: 0.3570, lr 6.962898649802823e-06
Step 565700 Loss: 0.3563, lr 6.962898649802823e-06
Step 565800 Loss: 0.3557, lr 6.962898649802823e-06
Step 565900 Loss: 0.3552, lr 6.962898649802823e-06
Step 566000 Loss: 0.3544, lr 6.962898649802823e-06
Step 566100 Loss: 0.3536, lr 6.962898649802823e-06
Step 566200 Loss: 0.3534, lr 6.962898649802823e-06
Step 566300 Loss: 0.3529, lr 6.962898649802823e-06
Step 566400 Loss: 0.3527, lr 6.962898649802823e-06
Step 566500 Loss: 0.3527, lr 6.962898649802823e-06
Step 566600 Loss: 0.3530, lr 6.962898649802823e-06
Step 566700 Loss: 0.3533, lr 6.962898649802823e-06
Step 566800 Loss: 0.3536, lr 6.962898649802823e-06
Step 566900 Loss: 0.3540, lr 6.962898649802823e-06
Step 567000 Loss: 0.3536, lr 6.962898649802823e-06
Step 567100 Loss: 0.3534, lr 6.962898649802823e-06
Step 567200 Loss: 0.3527, lr 6.962898649802823e-06
Step 567300 Loss: 0.3518, lr 6.962898649802823e-06
Step 567400 Loss: 0.3507, lr 6.962898649802823e-06
Train Epoch: [84/100] Loss: 0.3506,lr 0.000007
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 567500 Loss: 1.0707, lr 6.184665997806831e-06
Step 567600 Loss: 0.7158, lr 6.184665997806831e-06
Step 567700 Loss: 0.6339, lr 6.184665997806831e-06
Step 567800 Loss: 0.5894, lr 6.184665997806831e-06
Step 567900 Loss: 0.5648, lr 6.184665997806831e-06
Step 568000 Loss: 0.5465, lr 6.184665997806831e-06
Step 568100 Loss: 0.5290, lr 6.184665997806831e-06
Step 568200 Loss: 0.5155, lr 6.184665997806831e-06
Step 568300 Loss: 0.5028, lr 6.184665997806831e-06
Step 568400 Loss: 0.4907, lr 6.184665997806831e-06
Step 568500 Loss: 0.4812, lr 6.184665997806831e-06
Step 568600 Loss: 0.4744, lr 6.184665997806831e-06
Step 568700 Loss: 0.4656, lr 6.184665997806831e-06
Step 568800 Loss: 0.4589, lr 6.184665997806831e-06
Step 568900 Loss: 0.4529, lr 6.184665997806831e-06
Step 569000 Loss: 0.4474, lr 6.184665997806831e-06
Step 569100 Loss: 0.4419, lr 6.184665997806831e-06
Step 569200 Loss: 0.4375, lr 6.184665997806831e-06
Step 569300 Loss: 0.4328, lr 6.184665997806831e-06
Step 569400 Loss: 0.4282, lr 6.184665997806831e-06
Step 569500 Loss: 0.4235, lr 6.184665997806831e-06
Step 569600 Loss: 0.4196, lr 6.184665997806831e-06
Step 569700 Loss: 0.4161, lr 6.184665997806831e-06
Step 569800 Loss: 0.4125, lr 6.184665997806831e-06
Step 569900 Loss: 0.4095, lr 6.184665997806831e-06
Step 570000 Loss: 0.4052, lr 6.184665997806831e-06
Step 570100 Loss: 0.4018, lr 6.184665997806831e-06
Step 570200 Loss: 0.3993, lr 6.184665997806831e-06
Step 570300 Loss: 0.3970, lr 6.184665997806831e-06
Step 570400 Loss: 0.3941, lr 6.184665997806831e-06
Step 570500 Loss: 0.3918, lr 6.184665997806831e-06
Step 570600 Loss: 0.3889, lr 6.184665997806831e-06
Step 570700 Loss: 0.3867, lr 6.184665997806831e-06
Step 570800 Loss: 0.3840, lr 6.184665997806831e-06
Step 570900 Loss: 0.3814, lr 6.184665997806831e-06
Step 571000 Loss: 0.3788, lr 6.184665997806831e-06
Step 571100 Loss: 0.3764, lr 6.184665997806831e-06
Step 571200 Loss: 0.3742, lr 6.184665997806831e-06
Step 571300 Loss: 0.3722, lr 6.184665997806831e-06
Step 571400 Loss: 0.3702, lr 6.184665997806831e-06
Step 571500 Loss: 0.3686, lr 6.184665997806831e-06
Step 571600 Loss: 0.3672, lr 6.184665997806831e-06
Step 571700 Loss: 0.3658, lr 6.184665997806831e-06
Step 571800 Loss: 0.3645, lr 6.184665997806831e-06
Step 571900 Loss: 0.3633, lr 6.184665997806831e-06
Step 572000 Loss: 0.3617, lr 6.184665997806831e-06
Step 572100 Loss: 0.3602, lr 6.184665997806831e-06
Step 572200 Loss: 0.3588, lr 6.184665997806831e-06
Step 572300 Loss: 0.3580, lr 6.184665997806831e-06
Step 572400 Loss: 0.3572, lr 6.184665997806831e-06
Step 572500 Loss: 0.3565, lr 6.184665997806831e-06
Step 572600 Loss: 0.3560, lr 6.184665997806831e-06
Step 572700 Loss: 0.3552, lr 6.184665997806831e-06
Step 572800 Loss: 0.3548, lr 6.184665997806831e-06
Step 572900 Loss: 0.3541, lr 6.184665997806831e-06
Step 573000 Loss: 0.3537, lr 6.184665997806831e-06
Step 573100 Loss: 0.3535, lr 6.184665997806831e-06
Step 573200 Loss: 0.3532, lr 6.184665997806831e-06
Step 573300 Loss: 0.3534, lr 6.184665997806831e-06
Step 573400 Loss: 0.3539, lr 6.184665997806831e-06
Step 573500 Loss: 0.3540, lr 6.184665997806831e-06
Step 573600 Loss: 0.3541, lr 6.184665997806831e-06
Step 573700 Loss: 0.3543, lr 6.184665997806831e-06
Step 573800 Loss: 0.3541, lr 6.184665997806831e-06
Step 573900 Loss: 0.3535, lr 6.184665997806831e-06
Step 574000 Loss: 0.3530, lr 6.184665997806831e-06
Step 574100 Loss: 0.3517, lr 6.184665997806831e-06
Step 574200 Loss: 0.3511, lr 6.184665997806831e-06
Train Epoch: [85/100] Loss: 0.3510,lr 0.000006
Model Saving at epoch 85
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Step 574300 Loss: 0.8215, lr 5.449673790581619e-06
Step 574400 Loss: 0.6747, lr 5.449673790581619e-06
Step 574500 Loss: 0.6157, lr 5.449673790581619e-06
Step 574600 Loss: 0.5774, lr 5.449673790581619e-06
Step 574700 Loss: 0.5572, lr 5.449673790581619e-06
Step 574800 Loss: 0.5398, lr 5.449673790581619e-06
Step 574900 Loss: 0.5223, lr 5.449673790581619e-06
Step 575000 Loss: 0.5095, lr 5.449673790581619e-06
Step 575100 Loss: 0.4974, lr 5.449673790581619e-06
Step 575200 Loss: 0.4866, lr 5.449673790581619e-06
Step 575300 Loss: 0.4780, lr 5.449673790581619e-06
Step 575400 Loss: 0.4705, lr 5.449673790581619e-06
Step 575500 Loss: 0.4631, lr 5.449673790581619e-06
Step 575600 Loss: 0.4560, lr 5.449673790581619e-06
Step 575700 Loss: 0.4502, lr 5.449673790581619e-06
Step 575800 Loss: 0.4448, lr 5.449673790581619e-06
Step 575900 Loss: 0.4395, lr 5.449673790581619e-06
Step 576000 Loss: 0.4348, lr 5.449673790581619e-06
Step 576100 Loss: 0.4306, lr 5.449673790581619e-06
Step 576200 Loss: 0.4257, lr 5.449673790581619e-06
Step 576300 Loss: 0.4207, lr 5.449673790581619e-06
Step 576400 Loss: 0.4174, lr 5.449673790581619e-06
Step 576500 Loss: 0.4140, lr 5.449673790581619e-06
Step 576600 Loss: 0.4107, lr 5.449673790581619e-06
Step 576700 Loss: 0.4069, lr 5.449673790581619e-06
Step 576800 Loss: 0.4033, lr 5.449673790581619e-06
Step 576900 Loss: 0.4000, lr 5.449673790581619e-06
Step 577000 Loss: 0.3979, lr 5.449673790581619e-06
Step 577100 Loss: 0.3949, lr 5.449673790581619e-06
Step 577200 Loss: 0.3929, lr 5.449673790581619e-06
Step 577300 Loss: 0.3901, lr 5.449673790581619e-06
Step 577400 Loss: 0.3871, lr 5.449673790581619e-06
Step 577500 Loss: 0.3852, lr 5.449673790581619e-06
Step 577600 Loss: 0.3823, lr 5.449673790581619e-06
Step 577700 Loss: 0.3798, lr 5.449673790581619e-06
Step 577800 Loss: 0.3772, lr 5.449673790581619e-06
Step 577900 Loss: 0.3751, lr 5.449673790581619e-06
Step 578000 Loss: 0.3728, lr 5.449673790581619e-06
Step 578100 Loss: 0.3706, lr 5.449673790581619e-06
Step 578200 Loss: 0.3687, lr 5.449673790581619e-06
Step 578300 Loss: 0.3672, lr 5.449673790581619e-06
Step 578400 Loss: 0.3657, lr 5.449673790581619e-06
Step 578500 Loss: 0.3645, lr 5.449673790581619e-06
Step 578600 Loss: 0.3631, lr 5.449673790581619e-06
Step 578700 Loss: 0.3619, lr 5.449673790581619e-06
Step 578800 Loss: 0.3603, lr 5.449673790581619e-06
Step 578900 Loss: 0.3587, lr 5.449673790581619e-06
Step 579000 Loss: 0.3575, lr 5.449673790581619e-06
Step 579100 Loss: 0.3570, lr 5.449673790581619e-06
Step 579200 Loss: 0.3562, lr 5.449673790581619e-06
Step 579300 Loss: 0.3555, lr 5.449673790581619e-06
Step 579400 Loss: 0.3550, lr 5.449673790581619e-06
Step 579500 Loss: 0.3544, lr 5.449673790581619e-06
Step 579600 Loss: 0.3537, lr 5.449673790581619e-06
Step 579700 Loss: 0.3533, lr 5.449673790581619e-06
Step 579800 Loss: 0.3528, lr 5.449673790581619e-06
Step 579900 Loss: 0.3525, lr 5.449673790581619e-06
Step 580000 Loss: 0.3526, lr 5.449673790581619e-06
Step 580100 Loss: 0.3530, lr 5.449673790581619e-06
Step 580200 Loss: 0.3532, lr 5.449673790581619e-06
Step 580300 Loss: 0.3533, lr 5.449673790581619e-06
Step 580400 Loss: 0.3536, lr 5.449673790581619e-06
Step 580500 Loss: 0.3533, lr 5.449673790581619e-06
Step 580600 Loss: 0.3531, lr 5.449673790581619e-06
Step 580700 Loss: 0.3522, lr 5.449673790581619e-06
Step 580800 Loss: 0.3517, lr 5.449673790581619e-06
Step 580900 Loss: 0.3502, lr 5.449673790581619e-06
Train Epoch: [86/100] Loss: 0.3501,lr 0.000005
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 581000 Loss: 1.3624, lr 4.75864737669904e-06
Step 581100 Loss: 0.7367, lr 4.75864737669904e-06
Step 581200 Loss: 0.6439, lr 4.75864737669904e-06
Step 581300 Loss: 0.5948, lr 4.75864737669904e-06
Step 581400 Loss: 0.5677, lr 4.75864737669904e-06
Step 581500 Loss: 0.5492, lr 4.75864737669904e-06
Step 581600 Loss: 0.5303, lr 4.75864737669904e-06
Step 581700 Loss: 0.5176, lr 4.75864737669904e-06
Step 581800 Loss: 0.5049, lr 4.75864737669904e-06
Step 581900 Loss: 0.4925, lr 4.75864737669904e-06
Step 582000 Loss: 0.4818, lr 4.75864737669904e-06
Step 582100 Loss: 0.4751, lr 4.75864737669904e-06
Step 582200 Loss: 0.4667, lr 4.75864737669904e-06
Step 582300 Loss: 0.4597, lr 4.75864737669904e-06
Step 582400 Loss: 0.4535, lr 4.75864737669904e-06
Step 582500 Loss: 0.4474, lr 4.75864737669904e-06
Step 582600 Loss: 0.4417, lr 4.75864737669904e-06
Step 582700 Loss: 0.4377, lr 4.75864737669904e-06
Step 582800 Loss: 0.4331, lr 4.75864737669904e-06
Step 582900 Loss: 0.4285, lr 4.75864737669904e-06
Step 583000 Loss: 0.4238, lr 4.75864737669904e-06
Step 583100 Loss: 0.4194, lr 4.75864737669904e-06
Step 583200 Loss: 0.4159, lr 4.75864737669904e-06
Step 583300 Loss: 0.4124, lr 4.75864737669904e-06
Step 583400 Loss: 0.4093, lr 4.75864737669904e-06
Step 583500 Loss: 0.4052, lr 4.75864737669904e-06
Step 583600 Loss: 0.4015, lr 4.75864737669904e-06
Step 583700 Loss: 0.3986, lr 4.75864737669904e-06
Step 583800 Loss: 0.3964, lr 4.75864737669904e-06
Step 583900 Loss: 0.3935, lr 4.75864737669904e-06
Step 584000 Loss: 0.3914, lr 4.75864737669904e-06
Step 584100 Loss: 0.3884, lr 4.75864737669904e-06
Step 584200 Loss: 0.3862, lr 4.75864737669904e-06
Step 584300 Loss: 0.3836, lr 4.75864737669904e-06
Step 584400 Loss: 0.3811, lr 4.75864737669904e-06
Step 584500 Loss: 0.3785, lr 4.75864737669904e-06
Step 584600 Loss: 0.3759, lr 4.75864737669904e-06
Step 584700 Loss: 0.3737, lr 4.75864737669904e-06
Step 584800 Loss: 0.3717, lr 4.75864737669904e-06
Step 584900 Loss: 0.3695, lr 4.75864737669904e-06
Step 585000 Loss: 0.3679, lr 4.75864737669904e-06
Step 585100 Loss: 0.3664, lr 4.75864737669904e-06
Step 585200 Loss: 0.3651, lr 4.75864737669904e-06
Step 585300 Loss: 0.3636, lr 4.75864737669904e-06
Step 585400 Loss: 0.3625, lr 4.75864737669904e-06
Step 585500 Loss: 0.3610, lr 4.75864737669904e-06
Step 585600 Loss: 0.3594, lr 4.75864737669904e-06
Step 585700 Loss: 0.3581, lr 4.75864737669904e-06
Step 585800 Loss: 0.3572, lr 4.75864737669904e-06
Step 585900 Loss: 0.3562, lr 4.75864737669904e-06
Step 586000 Loss: 0.3556, lr 4.75864737669904e-06
Step 586100 Loss: 0.3551, lr 4.75864737669904e-06
Step 586200 Loss: 0.3545, lr 4.75864737669904e-06
Step 586300 Loss: 0.3539, lr 4.75864737669904e-06
Step 586400 Loss: 0.3531, lr 4.75864737669904e-06
Step 586500 Loss: 0.3525, lr 4.75864737669904e-06
Step 586600 Loss: 0.3523, lr 4.75864737669904e-06
Step 586700 Loss: 0.3522, lr 4.75864737669904e-06
Step 586800 Loss: 0.3521, lr 4.75864737669904e-06
Step 586900 Loss: 0.3525, lr 4.75864737669904e-06
Step 587000 Loss: 0.3524, lr 4.75864737669904e-06
Step 587100 Loss: 0.3532, lr 4.75864737669904e-06
Step 587200 Loss: 0.3531, lr 4.75864737669904e-06
Step 587300 Loss: 0.3529, lr 4.75864737669904e-06
Step 587400 Loss: 0.3526, lr 4.75864737669904e-06
Step 587500 Loss: 0.3520, lr 4.75864737669904e-06
Step 587600 Loss: 0.3509, lr 4.75864737669904e-06
Step 587700 Loss: 0.3500, lr 4.75864737669904e-06
Train Epoch: [87/100] Loss: 0.3501,lr 0.000005
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Step 587800 Loss: 0.8645, lr 4.112268715800961e-06
Step 587900 Loss: 0.6793, lr 4.112268715800961e-06
Step 588000 Loss: 0.6201, lr 4.112268715800961e-06
Step 588100 Loss: 0.5791, lr 4.112268715800961e-06
Step 588200 Loss: 0.5604, lr 4.112268715800961e-06
Step 588300 Loss: 0.5422, lr 4.112268715800961e-06
Step 588400 Loss: 0.5239, lr 4.112268715800961e-06
Step 588500 Loss: 0.5097, lr 4.112268715800961e-06
Step 588600 Loss: 0.4981, lr 4.112268715800961e-06
Step 588700 Loss: 0.4869, lr 4.112268715800961e-06
Step 588800 Loss: 0.4791, lr 4.112268715800961e-06
Step 588900 Loss: 0.4708, lr 4.112268715800961e-06
Step 589000 Loss: 0.4636, lr 4.112268715800961e-06
Step 589100 Loss: 0.4563, lr 4.112268715800961e-06
Step 589200 Loss: 0.4507, lr 4.112268715800961e-06
Step 589300 Loss: 0.4452, lr 4.112268715800961e-06
Step 589400 Loss: 0.4399, lr 4.112268715800961e-06
Step 589500 Loss: 0.4356, lr 4.112268715800961e-06
Step 589600 Loss: 0.4312, lr 4.112268715800961e-06
Step 589700 Loss: 0.4263, lr 4.112268715800961e-06
Step 589800 Loss: 0.4216, lr 4.112268715800961e-06
Step 589900 Loss: 0.4181, lr 4.112268715800961e-06
Step 590000 Loss: 0.4145, lr 4.112268715800961e-06
Step 590100 Loss: 0.4108, lr 4.112268715800961e-06
Step 590200 Loss: 0.4073, lr 4.112268715800961e-06
Step 590300 Loss: 0.4036, lr 4.112268715800961e-06
Step 590400 Loss: 0.4005, lr 4.112268715800961e-06
Step 590500 Loss: 0.3983, lr 4.112268715800961e-06
Step 590600 Loss: 0.3952, lr 4.112268715800961e-06
Step 590700 Loss: 0.3928, lr 4.112268715800961e-06
Step 590800 Loss: 0.3904, lr 4.112268715800961e-06
Step 590900 Loss: 0.3875, lr 4.112268715800961e-06
Step 591000 Loss: 0.3853, lr 4.112268715800961e-06
Step 591100 Loss: 0.3823, lr 4.112268715800961e-06
Step 591200 Loss: 0.3800, lr 4.112268715800961e-06
Step 591300 Loss: 0.3772, lr 4.112268715800961e-06
Step 591400 Loss: 0.3750, lr 4.112268715800961e-06
Step 591500 Loss: 0.3726, lr 4.112268715800961e-06
Step 591600 Loss: 0.3707, lr 4.112268715800961e-06
Step 591700 Loss: 0.3687, lr 4.112268715800961e-06
Step 591800 Loss: 0.3673, lr 4.112268715800961e-06
Step 591900 Loss: 0.3657, lr 4.112268715800961e-06
Step 592000 Loss: 0.3646, lr 4.112268715800961e-06
Step 592100 Loss: 0.3629, lr 4.112268715800961e-06
Step 592200 Loss: 0.3618, lr 4.112268715800961e-06
Step 592300 Loss: 0.3602, lr 4.112268715800961e-06
Step 592400 Loss: 0.3586, lr 4.112268715800961e-06
Step 592500 Loss: 0.3575, lr 4.112268715800961e-06
Step 592600 Loss: 0.3566, lr 4.112268715800961e-06
Step 592700 Loss: 0.3561, lr 4.112268715800961e-06
Step 592800 Loss: 0.3554, lr 4.112268715800961e-06
Step 592900 Loss: 0.3549, lr 4.112268715800961e-06
Step 593000 Loss: 0.3543, lr 4.112268715800961e-06
Step 593100 Loss: 0.3536, lr 4.112268715800961e-06
Step 593200 Loss: 0.3532, lr 4.112268715800961e-06
Step 593300 Loss: 0.3526, lr 4.112268715800961e-06
Step 593400 Loss: 0.3523, lr 4.112268715800961e-06
Step 593500 Loss: 0.3525, lr 4.112268715800961e-06
Step 593600 Loss: 0.3526, lr 4.112268715800961e-06
Step 593700 Loss: 0.3530, lr 4.112268715800961e-06
Step 593800 Loss: 0.3532, lr 4.112268715800961e-06
Step 593900 Loss: 0.3531, lr 4.112268715800961e-06
Step 594000 Loss: 0.3534, lr 4.112268715800961e-06
Step 594100 Loss: 0.3530, lr 4.112268715800961e-06
Step 594200 Loss: 0.3523, lr 4.112268715800961e-06
Step 594300 Loss: 0.3516, lr 4.112268715800961e-06
Step 594400 Loss: 0.3503, lr 4.112268715800961e-06
Train Epoch: [88/100] Loss: 0.3498,lr 0.000004
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 594500 Loss: 1.7632, lr 3.5111757055874383e-06
Step 594600 Loss: 0.7598, lr 3.5111757055874383e-06
Step 594700 Loss: 0.6534, lr 3.5111757055874383e-06
Step 594800 Loss: 0.6000, lr 3.5111757055874383e-06
Step 594900 Loss: 0.5704, lr 3.5111757055874383e-06
Step 595000 Loss: 0.5501, lr 3.5111757055874383e-06
Step 595100 Loss: 0.5314, lr 3.5111757055874383e-06
Step 595200 Loss: 0.5179, lr 3.5111757055874383e-06
Step 595300 Loss: 0.5047, lr 3.5111757055874383e-06
Step 595400 Loss: 0.4934, lr 3.5111757055874383e-06
Step 595500 Loss: 0.4828, lr 3.5111757055874383e-06
Step 595600 Loss: 0.4754, lr 3.5111757055874383e-06
Step 595700 Loss: 0.4671, lr 3.5111757055874383e-06
Step 595800 Loss: 0.4608, lr 3.5111757055874383e-06
Step 595900 Loss: 0.4542, lr 3.5111757055874383e-06
Step 596000 Loss: 0.4481, lr 3.5111757055874383e-06
Step 596100 Loss: 0.4425, lr 3.5111757055874383e-06
Step 596200 Loss: 0.4380, lr 3.5111757055874383e-06
Step 596300 Loss: 0.4336, lr 3.5111757055874383e-06
Step 596400 Loss: 0.4289, lr 3.5111757055874383e-06
Step 596500 Loss: 0.4242, lr 3.5111757055874383e-06
Step 596600 Loss: 0.4196, lr 3.5111757055874383e-06
Step 596700 Loss: 0.4161, lr 3.5111757055874383e-06
Step 596800 Loss: 0.4125, lr 3.5111757055874383e-06
Step 596900 Loss: 0.4095, lr 3.5111757055874383e-06
Step 597000 Loss: 0.4056, lr 3.5111757055874383e-06
Step 597100 Loss: 0.4019, lr 3.5111757055874383e-06
Step 597200 Loss: 0.3991, lr 3.5111757055874383e-06
Step 597300 Loss: 0.3969, lr 3.5111757055874383e-06
Step 597400 Loss: 0.3940, lr 3.5111757055874383e-06
Step 597500 Loss: 0.3918, lr 3.5111757055874383e-06
Step 597600 Loss: 0.3889, lr 3.5111757055874383e-06
Step 597700 Loss: 0.3867, lr 3.5111757055874383e-06
Step 597800 Loss: 0.3842, lr 3.5111757055874383e-06
Step 597900 Loss: 0.3815, lr 3.5111757055874383e-06
Step 598000 Loss: 0.3789, lr 3.5111757055874383e-06
Step 598100 Loss: 0.3761, lr 3.5111757055874383e-06
Step 598200 Loss: 0.3740, lr 3.5111757055874383e-06
Step 598300 Loss: 0.3720, lr 3.5111757055874383e-06
Step 598400 Loss: 0.3698, lr 3.5111757055874383e-06
Step 598500 Loss: 0.3681, lr 3.5111757055874383e-06
Step 598600 Loss: 0.3665, lr 3.5111757055874383e-06
Step 598700 Loss: 0.3652, lr 3.5111757055874383e-06
Step 598800 Loss: 0.3639, lr 3.5111757055874383e-06
Step 598900 Loss: 0.3627, lr 3.5111757055874383e-06
Step 599000 Loss: 0.3613, lr 3.5111757055874383e-06
Step 599100 Loss: 0.3596, lr 3.5111757055874383e-06
Step 599200 Loss: 0.3582, lr 3.5111757055874383e-06
Step 599300 Loss: 0.3573, lr 3.5111757055874383e-06
Step 599400 Loss: 0.3563, lr 3.5111757055874383e-06
Step 599500 Loss: 0.3557, lr 3.5111757055874383e-06
Step 599600 Loss: 0.3552, lr 3.5111757055874383e-06
Step 599700 Loss: 0.3544, lr 3.5111757055874383e-06
Step 599800 Loss: 0.3538, lr 3.5111757055874383e-06
Step 599900 Loss: 0.3530, lr 3.5111757055874383e-06
Step 600000 Loss: 0.3526, lr 3.5111757055874383e-06
Step 600100 Loss: 0.3522, lr 3.5111757055874383e-06
Step 600200 Loss: 0.3520, lr 3.5111757055874383e-06
Step 600300 Loss: 0.3523, lr 3.5111757055874383e-06
Step 600400 Loss: 0.3527, lr 3.5111757055874383e-06
Step 600500 Loss: 0.3525, lr 3.5111757055874383e-06
Step 600600 Loss: 0.3529, lr 3.5111757055874383e-06
Step 600700 Loss: 0.3531, lr 3.5111757055874383e-06
Step 600800 Loss: 0.3526, lr 3.5111757055874383e-06
Step 600900 Loss: 0.3525, lr 3.5111757055874383e-06
Step 601000 Loss: 0.3519, lr 3.5111757055874383e-06
Step 601100 Loss: 0.3510, lr 3.5111757055874383e-06
Step 601200 Loss: 0.3499, lr 3.5111757055874383e-06
Train Epoch: [89/100] Loss: 0.3503,lr 0.000004
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 601300 Loss: 0.9238, lr 2.9559615522887324e-06
Step 601400 Loss: 0.6921, lr 2.9559615522887324e-06
Step 601500 Loss: 0.6247, lr 2.9559615522887324e-06
Step 601600 Loss: 0.5831, lr 2.9559615522887324e-06
Step 601700 Loss: 0.5606, lr 2.9559615522887324e-06
Step 601800 Loss: 0.5435, lr 2.9559615522887324e-06
Step 601900 Loss: 0.5259, lr 2.9559615522887324e-06
Step 602000 Loss: 0.5113, lr 2.9559615522887324e-06
Step 602100 Loss: 0.5002, lr 2.9559615522887324e-06
Step 602200 Loss: 0.4886, lr 2.9559615522887324e-06
Step 602300 Loss: 0.4797, lr 2.9559615522887324e-06
Step 602400 Loss: 0.4720, lr 2.9559615522887324e-06
Step 602500 Loss: 0.4641, lr 2.9559615522887324e-06
Step 602600 Loss: 0.4572, lr 2.9559615522887324e-06
Step 602700 Loss: 0.4516, lr 2.9559615522887324e-06
Step 602800 Loss: 0.4462, lr 2.9559615522887324e-06
Step 602900 Loss: 0.4405, lr 2.9559615522887324e-06
Step 603000 Loss: 0.4363, lr 2.9559615522887324e-06
Step 603100 Loss: 0.4315, lr 2.9559615522887324e-06
Step 603200 Loss: 0.4269, lr 2.9559615522887324e-06
Step 603300 Loss: 0.4221, lr 2.9559615522887324e-06
Step 603400 Loss: 0.4184, lr 2.9559615522887324e-06
Step 603500 Loss: 0.4148, lr 2.9559615522887324e-06
Step 603600 Loss: 0.4113, lr 2.9559615522887324e-06
Step 603700 Loss: 0.4081, lr 2.9559615522887324e-06
Step 603800 Loss: 0.4043, lr 2.9559615522887324e-06
Step 603900 Loss: 0.4010, lr 2.9559615522887324e-06
Step 604000 Loss: 0.3986, lr 2.9559615522887324e-06
Step 604100 Loss: 0.3958, lr 2.9559615522887324e-06
Step 604200 Loss: 0.3930, lr 2.9559615522887324e-06
Step 604300 Loss: 0.3906, lr 2.9559615522887324e-06
Step 604400 Loss: 0.3880, lr 2.9559615522887324e-06
Step 604500 Loss: 0.3855, lr 2.9559615522887324e-06
Step 604600 Loss: 0.3828, lr 2.9559615522887324e-06
Step 604700 Loss: 0.3800, lr 2.9559615522887324e-06
Step 604800 Loss: 0.3776, lr 2.9559615522887324e-06
Step 604900 Loss: 0.3754, lr 2.9559615522887324e-06
Step 605000 Loss: 0.3728, lr 2.9559615522887324e-06
Step 605100 Loss: 0.3709, lr 2.9559615522887324e-06
Step 605200 Loss: 0.3688, lr 2.9559615522887324e-06
Step 605300 Loss: 0.3673, lr 2.9559615522887324e-06
Step 605400 Loss: 0.3658, lr 2.9559615522887324e-06
Step 605500 Loss: 0.3646, lr 2.9559615522887324e-06
Step 605600 Loss: 0.3630, lr 2.9559615522887324e-06
Step 605700 Loss: 0.3620, lr 2.9559615522887324e-06
Step 605800 Loss: 0.3603, lr 2.9559615522887324e-06
Step 605900 Loss: 0.3588, lr 2.9559615522887324e-06
Step 606000 Loss: 0.3575, lr 2.9559615522887324e-06
Step 606100 Loss: 0.3568, lr 2.9559615522887324e-06
Step 606200 Loss: 0.3561, lr 2.9559615522887324e-06
Step 606300 Loss: 0.3554, lr 2.9559615522887324e-06
Step 606400 Loss: 0.3550, lr 2.9559615522887324e-06
Step 606500 Loss: 0.3542, lr 2.9559615522887324e-06
Step 606600 Loss: 0.3537, lr 2.9559615522887324e-06
Step 606700 Loss: 0.3530, lr 2.9559615522887324e-06
Step 606800 Loss: 0.3525, lr 2.9559615522887324e-06
Step 606900 Loss: 0.3522, lr 2.9559615522887324e-06
Step 607000 Loss: 0.3523, lr 2.9559615522887324e-06
Step 607100 Loss: 0.3524, lr 2.9559615522887324e-06
Step 607200 Loss: 0.3527, lr 2.9559615522887324e-06
Step 607300 Loss: 0.3528, lr 2.9559615522887324e-06
Step 607400 Loss: 0.3530, lr 2.9559615522887324e-06
Step 607500 Loss: 0.3531, lr 2.9559615522887324e-06
Step 607600 Loss: 0.3527, lr 2.9559615522887324e-06
Step 607700 Loss: 0.3521, lr 2.9559615522887324e-06
Step 607800 Loss: 0.3517, lr 2.9559615522887324e-06
Step 607900 Loss: 0.3504, lr 2.9559615522887324e-06
Step 608000 Loss: 0.3499, lr 2.9559615522887324e-06
Train Epoch: [90/100] Loss: 0.3500,lr 0.000003
Model Saving at epoch 90
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Step 608100 Loss: 0.7790, lr 2.4471741852423276e-06
Step 608200 Loss: 0.6569, lr 2.4471741852423276e-06
Step 608300 Loss: 0.6033, lr 2.4471741852423276e-06
Step 608400 Loss: 0.5719, lr 2.4471741852423276e-06
Step 608500 Loss: 0.5530, lr 2.4471741852423276e-06
Step 608600 Loss: 0.5350, lr 2.4471741852423276e-06
Step 608700 Loss: 0.5200, lr 2.4471741852423276e-06
Step 608800 Loss: 0.5071, lr 2.4471741852423276e-06
Step 608900 Loss: 0.4948, lr 2.4471741852423276e-06
Step 609000 Loss: 0.4843, lr 2.4471741852423276e-06
Step 609100 Loss: 0.4766, lr 2.4471741852423276e-06
Step 609200 Loss: 0.4681, lr 2.4471741852423276e-06
Step 609300 Loss: 0.4616, lr 2.4471741852423276e-06
Step 609400 Loss: 0.4547, lr 2.4471741852423276e-06
Step 609500 Loss: 0.4488, lr 2.4471741852423276e-06
Step 609600 Loss: 0.4430, lr 2.4471741852423276e-06
Step 609700 Loss: 0.4382, lr 2.4471741852423276e-06
Step 609800 Loss: 0.4336, lr 2.4471741852423276e-06
Step 609900 Loss: 0.4289, lr 2.4471741852423276e-06
Step 610000 Loss: 0.4243, lr 2.4471741852423276e-06
Step 610100 Loss: 0.4196, lr 2.4471741852423276e-06
Step 610200 Loss: 0.4160, lr 2.4471741852423276e-06
Step 610300 Loss: 0.4127, lr 2.4471741852423276e-06
Step 610400 Loss: 0.4096, lr 2.4471741852423276e-06
Step 610500 Loss: 0.4057, lr 2.4471741852423276e-06
Step 610600 Loss: 0.4021, lr 2.4471741852423276e-06
Step 610700 Loss: 0.3990, lr 2.4471741852423276e-06
Step 610800 Loss: 0.3965, lr 2.4471741852423276e-06
Step 610900 Loss: 0.3936, lr 2.4471741852423276e-06
Step 611000 Loss: 0.3913, lr 2.4471741852423276e-06
Step 611100 Loss: 0.3887, lr 2.4471741852423276e-06
Step 611200 Loss: 0.3862, lr 2.4471741852423276e-06
Step 611300 Loss: 0.3839, lr 2.4471741852423276e-06
Step 611400 Loss: 0.3812, lr 2.4471741852423276e-06
Step 611500 Loss: 0.3786, lr 2.4471741852423276e-06
Step 611600 Loss: 0.3760, lr 2.4471741852423276e-06
Step 611700 Loss: 0.3739, lr 2.4471741852423276e-06
Step 611800 Loss: 0.3717, lr 2.4471741852423276e-06
Step 611900 Loss: 0.3697, lr 2.4471741852423276e-06
Step 612000 Loss: 0.3680, lr 2.4471741852423276e-06
Step 612100 Loss: 0.3666, lr 2.4471741852423276e-06
Step 612200 Loss: 0.3652, lr 2.4471741852423276e-06
Step 612300 Loss: 0.3637, lr 2.4471741852423276e-06
Step 612400 Loss: 0.3626, lr 2.4471741852423276e-06
Step 612500 Loss: 0.3613, lr 2.4471741852423276e-06
Step 612600 Loss: 0.3596, lr 2.4471741852423276e-06
Step 612700 Loss: 0.3581, lr 2.4471741852423276e-06
Step 612800 Loss: 0.3570, lr 2.4471741852423276e-06
Step 612900 Loss: 0.3560, lr 2.4471741852423276e-06
Step 613000 Loss: 0.3556, lr 2.4471741852423276e-06
Step 613100 Loss: 0.3548, lr 2.4471741852423276e-06
Step 613200 Loss: 0.3544, lr 2.4471741852423276e-06
Step 613300 Loss: 0.3537, lr 2.4471741852423276e-06
Step 613400 Loss: 0.3529, lr 2.4471741852423276e-06
Step 613500 Loss: 0.3525, lr 2.4471741852423276e-06
Step 613600 Loss: 0.3519, lr 2.4471741852423276e-06
Step 613700 Loss: 0.3518, lr 2.4471741852423276e-06
Step 613800 Loss: 0.3519, lr 2.4471741852423276e-06
Step 613900 Loss: 0.3524, lr 2.4471741852423276e-06
Step 614000 Loss: 0.3524, lr 2.4471741852423276e-06
Step 614100 Loss: 0.3528, lr 2.4471741852423276e-06
Step 614200 Loss: 0.3530, lr 2.4471741852423276e-06
Step 614300 Loss: 0.3525, lr 2.4471741852423276e-06
Step 614400 Loss: 0.3524, lr 2.4471741852423276e-06
Step 614500 Loss: 0.3516, lr 2.4471741852423276e-06
Step 614600 Loss: 0.3508, lr 2.4471741852423276e-06
Step 614700 Loss: 0.3496, lr 2.4471741852423276e-06
Train Epoch: [91/100] Loss: 0.3498,lr 0.000002
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6756
Step 614800 Loss: 0.9990, lr 1.9853157161528557e-06
Step 614900 Loss: 0.7045, lr 1.9853157161528557e-06
Step 615000 Loss: 0.6319, lr 1.9853157161528557e-06
Step 615100 Loss: 0.5882, lr 1.9853157161528557e-06
Step 615200 Loss: 0.5640, lr 1.9853157161528557e-06
Step 615300 Loss: 0.5456, lr 1.9853157161528557e-06
Step 615400 Loss: 0.5282, lr 1.9853157161528557e-06
Step 615500 Loss: 0.5141, lr 1.9853157161528557e-06
Step 615600 Loss: 0.5018, lr 1.9853157161528557e-06
Step 615700 Loss: 0.4901, lr 1.9853157161528557e-06
Step 615800 Loss: 0.4805, lr 1.9853157161528557e-06
Step 615900 Loss: 0.4736, lr 1.9853157161528557e-06
Step 616000 Loss: 0.4649, lr 1.9853157161528557e-06
Step 616100 Loss: 0.4581, lr 1.9853157161528557e-06
Step 616200 Loss: 0.4521, lr 1.9853157161528557e-06
Step 616300 Loss: 0.4465, lr 1.9853157161528557e-06
Step 616400 Loss: 0.4408, lr 1.9853157161528557e-06
Step 616500 Loss: 0.4366, lr 1.9853157161528557e-06
Step 616600 Loss: 0.4316, lr 1.9853157161528557e-06
Step 616700 Loss: 0.4272, lr 1.9853157161528557e-06
Step 616800 Loss: 0.4224, lr 1.9853157161528557e-06
Step 616900 Loss: 0.4187, lr 1.9853157161528557e-06
Step 617000 Loss: 0.4150, lr 1.9853157161528557e-06
Step 617100 Loss: 0.4115, lr 1.9853157161528557e-06
Step 617200 Loss: 0.4085, lr 1.9853157161528557e-06
Step 617300 Loss: 0.4042, lr 1.9853157161528557e-06
Step 617400 Loss: 0.4008, lr 1.9853157161528557e-06
Step 617500 Loss: 0.3981, lr 1.9853157161528557e-06
Step 617600 Loss: 0.3957, lr 1.9853157161528557e-06
Step 617700 Loss: 0.3928, lr 1.9853157161528557e-06
Step 617800 Loss: 0.3905, lr 1.9853157161528557e-06
Step 617900 Loss: 0.3878, lr 1.9853157161528557e-06
Step 618000 Loss: 0.3855, lr 1.9853157161528557e-06
Step 618100 Loss: 0.3828, lr 1.9853157161528557e-06
Step 618200 Loss: 0.3802, lr 1.9853157161528557e-06
Step 618300 Loss: 0.3779, lr 1.9853157161528557e-06
Step 618400 Loss: 0.3756, lr 1.9853157161528557e-06
Step 618500 Loss: 0.3733, lr 1.9853157161528557e-06
Step 618600 Loss: 0.3712, lr 1.9853157161528557e-06
Step 618700 Loss: 0.3693, lr 1.9853157161528557e-06
Step 618800 Loss: 0.3678, lr 1.9853157161528557e-06
Step 618900 Loss: 0.3662, lr 1.9853157161528557e-06
Step 619000 Loss: 0.3649, lr 1.9853157161528557e-06
Step 619100 Loss: 0.3634, lr 1.9853157161528557e-06
Step 619200 Loss: 0.3623, lr 1.9853157161528557e-06
Step 619300 Loss: 0.3607, lr 1.9853157161528557e-06
Step 619400 Loss: 0.3591, lr 1.9853157161528557e-06
Step 619500 Loss: 0.3577, lr 1.9853157161528557e-06
Step 619600 Loss: 0.3568, lr 1.9853157161528557e-06
Step 619700 Loss: 0.3561, lr 1.9853157161528557e-06
Step 619800 Loss: 0.3554, lr 1.9853157161528557e-06
Step 619900 Loss: 0.3550, lr 1.9853157161528557e-06
Step 620000 Loss: 0.3542, lr 1.9853157161528557e-06
Step 620100 Loss: 0.3537, lr 1.9853157161528557e-06
Step 620200 Loss: 0.3531, lr 1.9853157161528557e-06
Step 620300 Loss: 0.3524, lr 1.9853157161528557e-06
Step 620400 Loss: 0.3521, lr 1.9853157161528557e-06
Step 620500 Loss: 0.3522, lr 1.9853157161528557e-06
Step 620600 Loss: 0.3523, lr 1.9853157161528557e-06
Step 620700 Loss: 0.3526, lr 1.9853157161528557e-06
Step 620800 Loss: 0.3527, lr 1.9853157161528557e-06
Step 620900 Loss: 0.3528, lr 1.9853157161528557e-06
Step 621000 Loss: 0.3532, lr 1.9853157161528557e-06
Step 621100 Loss: 0.3528, lr 1.9853157161528557e-06
Step 621200 Loss: 0.3523, lr 1.9853157161528557e-06
Step 621300 Loss: 0.3517, lr 1.9853157161528557e-06
Step 621400 Loss: 0.3505, lr 1.9853157161528557e-06
Step 621500 Loss: 0.3499, lr 1.9853157161528557e-06
Train Epoch: [92/100] Loss: 0.3499,lr 0.000002
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 621600 Loss: 0.8021, lr 1.5708419435684545e-06
Step 621700 Loss: 0.6644, lr 1.5708419435684545e-06
Step 621800 Loss: 0.6098, lr 1.5708419435684545e-06
Step 621900 Loss: 0.5731, lr 1.5708419435684545e-06
Step 622000 Loss: 0.5556, lr 1.5708419435684545e-06
Step 622100 Loss: 0.5373, lr 1.5708419435684545e-06
Step 622200 Loss: 0.5208, lr 1.5708419435684545e-06
Step 622300 Loss: 0.5082, lr 1.5708419435684545e-06
Step 622400 Loss: 0.4960, lr 1.5708419435684545e-06
Step 622500 Loss: 0.4852, lr 1.5708419435684545e-06
Step 622600 Loss: 0.4773, lr 1.5708419435684545e-06
Step 622700 Loss: 0.4693, lr 1.5708419435684545e-06
Step 622800 Loss: 0.4627, lr 1.5708419435684545e-06
Step 622900 Loss: 0.4554, lr 1.5708419435684545e-06
Step 623000 Loss: 0.4496, lr 1.5708419435684545e-06
Step 623100 Loss: 0.4442, lr 1.5708419435684545e-06
Step 623200 Loss: 0.4390, lr 1.5708419435684545e-06
Step 623300 Loss: 0.4346, lr 1.5708419435684545e-06
Step 623400 Loss: 0.4300, lr 1.5708419435684545e-06
Step 623500 Loss: 0.4254, lr 1.5708419435684545e-06
Step 623600 Loss: 0.4203, lr 1.5708419435684545e-06
Step 623700 Loss: 0.4168, lr 1.5708419435684545e-06
Step 623800 Loss: 0.4133, lr 1.5708419435684545e-06
Step 623900 Loss: 0.4101, lr 1.5708419435684545e-06
Step 624000 Loss: 0.4061, lr 1.5708419435684545e-06
Step 624100 Loss: 0.4024, lr 1.5708419435684545e-06
Step 624200 Loss: 0.3994, lr 1.5708419435684545e-06
Step 624300 Loss: 0.3971, lr 1.5708419435684545e-06
Step 624400 Loss: 0.3942, lr 1.5708419435684545e-06
Step 624500 Loss: 0.3918, lr 1.5708419435684545e-06
Step 624600 Loss: 0.3893, lr 1.5708419435684545e-06
Step 624700 Loss: 0.3865, lr 1.5708419435684545e-06
Step 624800 Loss: 0.3845, lr 1.5708419435684545e-06
Step 624900 Loss: 0.3817, lr 1.5708419435684545e-06
Step 625000 Loss: 0.3792, lr 1.5708419435684545e-06
Step 625100 Loss: 0.3764, lr 1.5708419435684545e-06
Step 625200 Loss: 0.3744, lr 1.5708419435684545e-06
Step 625300 Loss: 0.3720, lr 1.5708419435684545e-06
Step 625400 Loss: 0.3700, lr 1.5708419435684545e-06
Step 625500 Loss: 0.3682, lr 1.5708419435684545e-06
Step 625600 Loss: 0.3667, lr 1.5708419435684545e-06
Step 625700 Loss: 0.3654, lr 1.5708419435684545e-06
Step 625800 Loss: 0.3640, lr 1.5708419435684545e-06
Step 625900 Loss: 0.3626, lr 1.5708419435684545e-06
Step 626000 Loss: 0.3614, lr 1.5708419435684545e-06
Step 626100 Loss: 0.3597, lr 1.5708419435684545e-06
Step 626200 Loss: 0.3582, lr 1.5708419435684545e-06
Step 626300 Loss: 0.3570, lr 1.5708419435684545e-06
Step 626400 Loss: 0.3563, lr 1.5708419435684545e-06
Step 626500 Loss: 0.3557, lr 1.5708419435684545e-06
Step 626600 Loss: 0.3550, lr 1.5708419435684545e-06
Step 626700 Loss: 0.3546, lr 1.5708419435684545e-06
Step 626800 Loss: 0.3538, lr 1.5708419435684545e-06
Step 626900 Loss: 0.3529, lr 1.5708419435684545e-06
Step 627000 Loss: 0.3526, lr 1.5708419435684545e-06
Step 627100 Loss: 0.3521, lr 1.5708419435684545e-06
Step 627200 Loss: 0.3518, lr 1.5708419435684545e-06
Step 627300 Loss: 0.3518, lr 1.5708419435684545e-06
Step 627400 Loss: 0.3523, lr 1.5708419435684545e-06
Step 627500 Loss: 0.3524, lr 1.5708419435684545e-06
Step 627600 Loss: 0.3526, lr 1.5708419435684545e-06
Step 627700 Loss: 0.3530, lr 1.5708419435684545e-06
Step 627800 Loss: 0.3528, lr 1.5708419435684545e-06
Step 627900 Loss: 0.3525, lr 1.5708419435684545e-06
Step 628000 Loss: 0.3519, lr 1.5708419435684545e-06
Step 628100 Loss: 0.3514, lr 1.5708419435684545e-06
Step 628200 Loss: 0.3500, lr 1.5708419435684545e-06
Train Epoch: [93/100] Loss: 0.3500,lr 0.000002
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Step 628300 Loss: 1.1215, lr 1.2041619030626358e-06
Step 628400 Loss: 0.7223, lr 1.2041619030626358e-06
Step 628500 Loss: 0.6358, lr 1.2041619030626358e-06
Step 628600 Loss: 0.5913, lr 1.2041619030626358e-06
Step 628700 Loss: 0.5650, lr 1.2041619030626358e-06
Step 628800 Loss: 0.5468, lr 1.2041619030626358e-06
Step 628900 Loss: 0.5288, lr 1.2041619030626358e-06
Step 629000 Loss: 0.5153, lr 1.2041619030626358e-06
Step 629100 Loss: 0.5034, lr 1.2041619030626358e-06
Step 629200 Loss: 0.4908, lr 1.2041619030626358e-06
Step 629300 Loss: 0.4812, lr 1.2041619030626358e-06
Step 629400 Loss: 0.4745, lr 1.2041619030626358e-06
Step 629500 Loss: 0.4658, lr 1.2041619030626358e-06
Step 629600 Loss: 0.4590, lr 1.2041619030626358e-06
Step 629700 Loss: 0.4531, lr 1.2041619030626358e-06
Step 629800 Loss: 0.4474, lr 1.2041619030626358e-06
Step 629900 Loss: 0.4417, lr 1.2041619030626358e-06
Step 630000 Loss: 0.4375, lr 1.2041619030626358e-06
Step 630100 Loss: 0.4328, lr 1.2041619030626358e-06
Step 630200 Loss: 0.4284, lr 1.2041619030626358e-06
Step 630300 Loss: 0.4238, lr 1.2041619030626358e-06
Step 630400 Loss: 0.4194, lr 1.2041619030626358e-06
Step 630500 Loss: 0.4158, lr 1.2041619030626358e-06
Step 630600 Loss: 0.4123, lr 1.2041619030626358e-06
Step 630700 Loss: 0.4095, lr 1.2041619030626358e-06
Step 630800 Loss: 0.4052, lr 1.2041619030626358e-06
Step 630900 Loss: 0.4017, lr 1.2041619030626358e-06
Step 631000 Loss: 0.3990, lr 1.2041619030626358e-06
Step 631100 Loss: 0.3968, lr 1.2041619030626358e-06
Step 631200 Loss: 0.3937, lr 1.2041619030626358e-06
Step 631300 Loss: 0.3916, lr 1.2041619030626358e-06
Step 631400 Loss: 0.3887, lr 1.2041619030626358e-06
Step 631500 Loss: 0.3865, lr 1.2041619030626358e-06
Step 631600 Loss: 0.3838, lr 1.2041619030626358e-06
Step 631700 Loss: 0.3810, lr 1.2041619030626358e-06
Step 631800 Loss: 0.3786, lr 1.2041619030626358e-06
Step 631900 Loss: 0.3761, lr 1.2041619030626358e-06
Step 632000 Loss: 0.3736, lr 1.2041619030626358e-06
Step 632100 Loss: 0.3716, lr 1.2041619030626358e-06
Step 632200 Loss: 0.3697, lr 1.2041619030626358e-06
Step 632300 Loss: 0.3680, lr 1.2041619030626358e-06
Step 632400 Loss: 0.3664, lr 1.2041619030626358e-06
Step 632500 Loss: 0.3651, lr 1.2041619030626358e-06
Step 632600 Loss: 0.3637, lr 1.2041619030626358e-06
Step 632700 Loss: 0.3626, lr 1.2041619030626358e-06
Step 632800 Loss: 0.3613, lr 1.2041619030626358e-06
Step 632900 Loss: 0.3598, lr 1.2041619030626358e-06
Step 633000 Loss: 0.3582, lr 1.2041619030626358e-06
Step 633100 Loss: 0.3573, lr 1.2041619030626358e-06
Step 633200 Loss: 0.3564, lr 1.2041619030626358e-06
Step 633300 Loss: 0.3558, lr 1.2041619030626358e-06
Step 633400 Loss: 0.3553, lr 1.2041619030626358e-06
Step 633500 Loss: 0.3546, lr 1.2041619030626358e-06
Step 633600 Loss: 0.3540, lr 1.2041619030626358e-06
Step 633700 Loss: 0.3532, lr 1.2041619030626358e-06
Step 633800 Loss: 0.3528, lr 1.2041619030626358e-06
Step 633900 Loss: 0.3524, lr 1.2041619030626358e-06
Step 634000 Loss: 0.3524, lr 1.2041619030626358e-06
Step 634100 Loss: 0.3523, lr 1.2041619030626358e-06
Step 634200 Loss: 0.3530, lr 1.2041619030626358e-06
Step 634300 Loss: 0.3530, lr 1.2041619030626358e-06
Step 634400 Loss: 0.3535, lr 1.2041619030626358e-06
Step 634500 Loss: 0.3538, lr 1.2041619030626358e-06
Step 634600 Loss: 0.3534, lr 1.2041619030626358e-06
Step 634700 Loss: 0.3529, lr 1.2041619030626358e-06
Step 634800 Loss: 0.3522, lr 1.2041619030626358e-06
Step 634900 Loss: 0.3509, lr 1.2041619030626358e-06
Step 635000 Loss: 0.3500, lr 1.2041619030626358e-06
Train Epoch: [94/100] Loss: 0.3501,lr 0.000001
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 635100 Loss: 0.8365, lr 8.856374635655655e-07
Step 635200 Loss: 0.6743, lr 8.856374635655655e-07
Step 635300 Loss: 0.6158, lr 8.856374635655655e-07
Step 635400 Loss: 0.5764, lr 8.856374635655655e-07
Step 635500 Loss: 0.5574, lr 8.856374635655655e-07
Step 635600 Loss: 0.5403, lr 8.856374635655655e-07
Step 635700 Loss: 0.5234, lr 8.856374635655655e-07
Step 635800 Loss: 0.5094, lr 8.856374635655655e-07
Step 635900 Loss: 0.4979, lr 8.856374635655655e-07
Step 636000 Loss: 0.4867, lr 8.856374635655655e-07
Step 636100 Loss: 0.4790, lr 8.856374635655655e-07
Step 636200 Loss: 0.4706, lr 8.856374635655655e-07
Step 636300 Loss: 0.4637, lr 8.856374635655655e-07
Step 636400 Loss: 0.4561, lr 8.856374635655655e-07
Step 636500 Loss: 0.4503, lr 8.856374635655655e-07
Step 636600 Loss: 0.4448, lr 8.856374635655655e-07
Step 636700 Loss: 0.4398, lr 8.856374635655655e-07
Step 636800 Loss: 0.4353, lr 8.856374635655655e-07
Step 636900 Loss: 0.4309, lr 8.856374635655655e-07
Step 637000 Loss: 0.4262, lr 8.856374635655655e-07
Step 637100 Loss: 0.4217, lr 8.856374635655655e-07
Step 637200 Loss: 0.4179, lr 8.856374635655655e-07
Step 637300 Loss: 0.4143, lr 8.856374635655655e-07
Step 637400 Loss: 0.4109, lr 8.856374635655655e-07
Step 637500 Loss: 0.4071, lr 8.856374635655655e-07
Step 637600 Loss: 0.4035, lr 8.856374635655655e-07
Step 637700 Loss: 0.4001, lr 8.856374635655655e-07
Step 637800 Loss: 0.3980, lr 8.856374635655655e-07
Step 637900 Loss: 0.3950, lr 8.856374635655655e-07
Step 638000 Loss: 0.3927, lr 8.856374635655655e-07
Step 638100 Loss: 0.3901, lr 8.856374635655655e-07
Step 638200 Loss: 0.3873, lr 8.856374635655655e-07
Step 638300 Loss: 0.3852, lr 8.856374635655655e-07
Step 638400 Loss: 0.3824, lr 8.856374635655655e-07
Step 638500 Loss: 0.3800, lr 8.856374635655655e-07
Step 638600 Loss: 0.3771, lr 8.856374635655655e-07
Step 638700 Loss: 0.3750, lr 8.856374635655655e-07
Step 638800 Loss: 0.3725, lr 8.856374635655655e-07
Step 638900 Loss: 0.3705, lr 8.856374635655655e-07
Step 639000 Loss: 0.3686, lr 8.856374635655655e-07
Step 639100 Loss: 0.3671, lr 8.856374635655655e-07
Step 639200 Loss: 0.3656, lr 8.856374635655655e-07
Step 639300 Loss: 0.3643, lr 8.856374635655655e-07
Step 639400 Loss: 0.3628, lr 8.856374635655655e-07
Step 639500 Loss: 0.3617, lr 8.856374635655655e-07
Step 639600 Loss: 0.3601, lr 8.856374635655655e-07
Step 639700 Loss: 0.3584, lr 8.856374635655655e-07
Step 639800 Loss: 0.3572, lr 8.856374635655655e-07
Step 639900 Loss: 0.3566, lr 8.856374635655655e-07
Step 640000 Loss: 0.3558, lr 8.856374635655655e-07
Step 640100 Loss: 0.3551, lr 8.856374635655655e-07
Step 640200 Loss: 0.3547, lr 8.856374635655655e-07
Step 640300 Loss: 0.3541, lr 8.856374635655655e-07
Step 640400 Loss: 0.3533, lr 8.856374635655655e-07
Step 640500 Loss: 0.3529, lr 8.856374635655655e-07
Step 640600 Loss: 0.3523, lr 8.856374635655655e-07
Step 640700 Loss: 0.3521, lr 8.856374635655655e-07
Step 640800 Loss: 0.3520, lr 8.856374635655655e-07
Step 640900 Loss: 0.3523, lr 8.856374635655655e-07
Step 641000 Loss: 0.3524, lr 8.856374635655655e-07
Step 641100 Loss: 0.3525, lr 8.856374635655655e-07
Step 641200 Loss: 0.3530, lr 8.856374635655655e-07
Step 641300 Loss: 0.3529, lr 8.856374635655655e-07
Step 641400 Loss: 0.3526, lr 8.856374635655655e-07
Step 641500 Loss: 0.3519, lr 8.856374635655655e-07
Step 641600 Loss: 0.3513, lr 8.856374635655655e-07
Step 641700 Loss: 0.3500, lr 8.856374635655655e-07
Train Epoch: [95/100] Loss: 0.3496,lr 0.000001
Model Saving at epoch 95
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Step 641800 Loss: 1.4819, lr 6.155829702431181e-07
Step 641900 Loss: 0.7411, lr 6.155829702431181e-07
Step 642000 Loss: 0.6435, lr 6.155829702431181e-07
Step 642100 Loss: 0.5953, lr 6.155829702431181e-07
Step 642200 Loss: 0.5676, lr 6.155829702431181e-07
Step 642300 Loss: 0.5470, lr 6.155829702431181e-07
Step 642400 Loss: 0.5287, lr 6.155829702431181e-07
Step 642500 Loss: 0.5157, lr 6.155829702431181e-07
Step 642600 Loss: 0.5031, lr 6.155829702431181e-07
Step 642700 Loss: 0.4915, lr 6.155829702431181e-07
Step 642800 Loss: 0.4814, lr 6.155829702431181e-07
Step 642900 Loss: 0.4744, lr 6.155829702431181e-07
Step 643000 Loss: 0.4657, lr 6.155829702431181e-07
Step 643100 Loss: 0.4594, lr 6.155829702431181e-07
Step 643200 Loss: 0.4531, lr 6.155829702431181e-07
Step 643300 Loss: 0.4472, lr 6.155829702431181e-07
Step 643400 Loss: 0.4419, lr 6.155829702431181e-07
Step 643500 Loss: 0.4376, lr 6.155829702431181e-07
Step 643600 Loss: 0.4328, lr 6.155829702431181e-07
Step 643700 Loss: 0.4282, lr 6.155829702431181e-07
Step 643800 Loss: 0.4234, lr 6.155829702431181e-07
Step 643900 Loss: 0.4191, lr 6.155829702431181e-07
Step 644000 Loss: 0.4155, lr 6.155829702431181e-07
Step 644100 Loss: 0.4119, lr 6.155829702431181e-07
Step 644200 Loss: 0.4092, lr 6.155829702431181e-07
Step 644300 Loss: 0.4049, lr 6.155829702431181e-07
Step 644400 Loss: 0.4013, lr 6.155829702431181e-07
Step 644500 Loss: 0.3983, lr 6.155829702431181e-07
Step 644600 Loss: 0.3961, lr 6.155829702431181e-07
Step 644700 Loss: 0.3931, lr 6.155829702431181e-07
Step 644800 Loss: 0.3909, lr 6.155829702431181e-07
Step 644900 Loss: 0.3877, lr 6.155829702431181e-07
Step 645000 Loss: 0.3856, lr 6.155829702431181e-07
Step 645100 Loss: 0.3831, lr 6.155829702431181e-07
Step 645200 Loss: 0.3806, lr 6.155829702431181e-07
Step 645300 Loss: 0.3779, lr 6.155829702431181e-07
Step 645400 Loss: 0.3753, lr 6.155829702431181e-07
Step 645500 Loss: 0.3731, lr 6.155829702431181e-07
Step 645600 Loss: 0.3710, lr 6.155829702431181e-07
Step 645700 Loss: 0.3689, lr 6.155829702431181e-07
Step 645800 Loss: 0.3673, lr 6.155829702431181e-07
Step 645900 Loss: 0.3658, lr 6.155829702431181e-07
Step 646000 Loss: 0.3646, lr 6.155829702431181e-07
Step 646100 Loss: 0.3633, lr 6.155829702431181e-07
Step 646200 Loss: 0.3620, lr 6.155829702431181e-07
Step 646300 Loss: 0.3605, lr 6.155829702431181e-07
Step 646400 Loss: 0.3589, lr 6.155829702431181e-07
Step 646500 Loss: 0.3578, lr 6.155829702431181e-07
Step 646600 Loss: 0.3567, lr 6.155829702431181e-07
Step 646700 Loss: 0.3557, lr 6.155829702431181e-07
Step 646800 Loss: 0.3551, lr 6.155829702431181e-07
Step 646900 Loss: 0.3544, lr 6.155829702431181e-07
Step 647000 Loss: 0.3539, lr 6.155829702431181e-07
Step 647100 Loss: 0.3532, lr 6.155829702431181e-07
Step 647200 Loss: 0.3526, lr 6.155829702431181e-07
Step 647300 Loss: 0.3521, lr 6.155829702431181e-07
Step 647400 Loss: 0.3516, lr 6.155829702431181e-07
Step 647500 Loss: 0.3516, lr 6.155829702431181e-07
Step 647600 Loss: 0.3516, lr 6.155829702431181e-07
Step 647700 Loss: 0.3521, lr 6.155829702431181e-07
Step 647800 Loss: 0.3519, lr 6.155829702431181e-07
Step 647900 Loss: 0.3523, lr 6.155829702431181e-07
Step 648000 Loss: 0.3526, lr 6.155829702431181e-07
Step 648100 Loss: 0.3521, lr 6.155829702431181e-07
Step 648200 Loss: 0.3520, lr 6.155829702431181e-07
Step 648300 Loss: 0.3511, lr 6.155829702431181e-07
Step 648400 Loss: 0.3501, lr 6.155829702431181e-07
Step 648500 Loss: 0.3491, lr 6.155829702431181e-07
Train Epoch: [96/100] Loss: 0.3491,lr 0.000001
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 648600 Loss: 0.8719, lr 3.9426493427611245e-07
Step 648700 Loss: 0.6775, lr 3.9426493427611245e-07
Step 648800 Loss: 0.6190, lr 3.9426493427611245e-07
Step 648900 Loss: 0.5772, lr 3.9426493427611245e-07
Step 649000 Loss: 0.5576, lr 3.9426493427611245e-07
Step 649100 Loss: 0.5403, lr 3.9426493427611245e-07
Step 649200 Loss: 0.5228, lr 3.9426493427611245e-07
Step 649300 Loss: 0.5087, lr 3.9426493427611245e-07
Step 649400 Loss: 0.4975, lr 3.9426493427611245e-07
Step 649500 Loss: 0.4859, lr 3.9426493427611245e-07
Step 649600 Loss: 0.4778, lr 3.9426493427611245e-07
Step 649700 Loss: 0.4696, lr 3.9426493427611245e-07
Step 649800 Loss: 0.4621, lr 3.9426493427611245e-07
Step 649900 Loss: 0.4552, lr 3.9426493427611245e-07
Step 650000 Loss: 0.4494, lr 3.9426493427611245e-07
Step 650100 Loss: 0.4440, lr 3.9426493427611245e-07
Step 650200 Loss: 0.4385, lr 3.9426493427611245e-07
Step 650300 Loss: 0.4344, lr 3.9426493427611245e-07
Step 650400 Loss: 0.4299, lr 3.9426493427611245e-07
Step 650500 Loss: 0.4251, lr 3.9426493427611245e-07
Step 650600 Loss: 0.4204, lr 3.9426493427611245e-07
Step 650700 Loss: 0.4168, lr 3.9426493427611245e-07
Step 650800 Loss: 0.4133, lr 3.9426493427611245e-07
Step 650900 Loss: 0.4096, lr 3.9426493427611245e-07
Step 651000 Loss: 0.4062, lr 3.9426493427611245e-07
Step 651100 Loss: 0.4025, lr 3.9426493427611245e-07
Step 651200 Loss: 0.3992, lr 3.9426493427611245e-07
Step 651300 Loss: 0.3968, lr 3.9426493427611245e-07
Step 651400 Loss: 0.3940, lr 3.9426493427611245e-07
Step 651500 Loss: 0.3916, lr 3.9426493427611245e-07
Step 651600 Loss: 0.3891, lr 3.9426493427611245e-07
Step 651700 Loss: 0.3863, lr 3.9426493427611245e-07
Step 651800 Loss: 0.3842, lr 3.9426493427611245e-07
Step 651900 Loss: 0.3813, lr 3.9426493427611245e-07
Step 652000 Loss: 0.3791, lr 3.9426493427611245e-07
Step 652100 Loss: 0.3765, lr 3.9426493427611245e-07
Step 652200 Loss: 0.3743, lr 3.9426493427611245e-07
Step 652300 Loss: 0.3719, lr 3.9426493427611245e-07
Step 652400 Loss: 0.3700, lr 3.9426493427611245e-07
Step 652500 Loss: 0.3679, lr 3.9426493427611245e-07
Step 652600 Loss: 0.3663, lr 3.9426493427611245e-07
Step 652700 Loss: 0.3649, lr 3.9426493427611245e-07
Step 652800 Loss: 0.3637, lr 3.9426493427611245e-07
Step 652900 Loss: 0.3620, lr 3.9426493427611245e-07
Step 653000 Loss: 0.3610, lr 3.9426493427611245e-07
Step 653100 Loss: 0.3594, lr 3.9426493427611245e-07
Step 653200 Loss: 0.3578, lr 3.9426493427611245e-07
Step 653300 Loss: 0.3567, lr 3.9426493427611245e-07
Step 653400 Loss: 0.3558, lr 3.9426493427611245e-07
Step 653500 Loss: 0.3554, lr 3.9426493427611245e-07
Step 653600 Loss: 0.3547, lr 3.9426493427611245e-07
Step 653700 Loss: 0.3540, lr 3.9426493427611245e-07
Step 653800 Loss: 0.3533, lr 3.9426493427611245e-07
Step 653900 Loss: 0.3529, lr 3.9426493427611245e-07
Step 654000 Loss: 0.3522, lr 3.9426493427611245e-07
Step 654100 Loss: 0.3518, lr 3.9426493427611245e-07
Step 654200 Loss: 0.3516, lr 3.9426493427611245e-07
Step 654300 Loss: 0.3517, lr 3.9426493427611245e-07
Step 654400 Loss: 0.3519, lr 3.9426493427611245e-07
Step 654500 Loss: 0.3523, lr 3.9426493427611245e-07
Step 654600 Loss: 0.3524, lr 3.9426493427611245e-07
Step 654700 Loss: 0.3526, lr 3.9426493427611245e-07
Step 654800 Loss: 0.3528, lr 3.9426493427611245e-07
Step 654900 Loss: 0.3523, lr 3.9426493427611245e-07
Step 655000 Loss: 0.3517, lr 3.9426493427611245e-07
Step 655100 Loss: 0.3512, lr 3.9426493427611245e-07
Step 655200 Loss: 0.3499, lr 3.9426493427611245e-07
Step 655300 Loss: 0.3494, lr 3.9426493427611245e-07
Train Epoch: [97/100] Loss: 0.3494,lr 0.000000
Calling G2SDataset.batch()
Done, time:  1.99 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 655400 Loss: 0.7598, lr 2.2190176984600058e-07
Step 655500 Loss: 0.6511, lr 2.2190176984600058e-07
Step 655600 Loss: 0.5982, lr 2.2190176984600058e-07
Step 655700 Loss: 0.5683, lr 2.2190176984600058e-07
Step 655800 Loss: 0.5499, lr 2.2190176984600058e-07
Step 655900 Loss: 0.5314, lr 2.2190176984600058e-07
Step 656000 Loss: 0.5185, lr 2.2190176984600058e-07
Step 656100 Loss: 0.5051, lr 2.2190176984600058e-07
Step 656200 Loss: 0.4936, lr 2.2190176984600058e-07
Step 656300 Loss: 0.4827, lr 2.2190176984600058e-07
Step 656400 Loss: 0.4752, lr 2.2190176984600058e-07
Step 656500 Loss: 0.4673, lr 2.2190176984600058e-07
Step 656600 Loss: 0.4611, lr 2.2190176984600058e-07
Step 656700 Loss: 0.4544, lr 2.2190176984600058e-07
Step 656800 Loss: 0.4480, lr 2.2190176984600058e-07
Step 656900 Loss: 0.4425, lr 2.2190176984600058e-07
Step 657000 Loss: 0.4380, lr 2.2190176984600058e-07
Step 657100 Loss: 0.4336, lr 2.2190176984600058e-07
Step 657200 Loss: 0.4292, lr 2.2190176984600058e-07
Step 657300 Loss: 0.4240, lr 2.2190176984600058e-07
Step 657400 Loss: 0.4198, lr 2.2190176984600058e-07
Step 657500 Loss: 0.4160, lr 2.2190176984600058e-07
Step 657600 Loss: 0.4124, lr 2.2190176984600058e-07
Step 657700 Loss: 0.4093, lr 2.2190176984600058e-07
Step 657800 Loss: 0.4053, lr 2.2190176984600058e-07
Step 657900 Loss: 0.4015, lr 2.2190176984600058e-07
Step 658000 Loss: 0.3987, lr 2.2190176984600058e-07
Step 658100 Loss: 0.3964, lr 2.2190176984600058e-07
Step 658200 Loss: 0.3933, lr 2.2190176984600058e-07
Step 658300 Loss: 0.3912, lr 2.2190176984600058e-07
Step 658400 Loss: 0.3883, lr 2.2190176984600058e-07
Step 658500 Loss: 0.3859, lr 2.2190176984600058e-07
Step 658600 Loss: 0.3834, lr 2.2190176984600058e-07
Step 658700 Loss: 0.3809, lr 2.2190176984600058e-07
Step 658800 Loss: 0.3782, lr 2.2190176984600058e-07
Step 658900 Loss: 0.3755, lr 2.2190176984600058e-07
Step 659000 Loss: 0.3734, lr 2.2190176984600058e-07
Step 659100 Loss: 0.3712, lr 2.2190176984600058e-07
Step 659200 Loss: 0.3691, lr 2.2190176984600058e-07
Step 659300 Loss: 0.3674, lr 2.2190176984600058e-07
Step 659400 Loss: 0.3658, lr 2.2190176984600058e-07
Step 659500 Loss: 0.3645, lr 2.2190176984600058e-07
Step 659600 Loss: 0.3632, lr 2.2190176984600058e-07
Step 659700 Loss: 0.3619, lr 2.2190176984600058e-07
Step 659800 Loss: 0.3605, lr 2.2190176984600058e-07
Step 659900 Loss: 0.3588, lr 2.2190176984600058e-07
Step 660000 Loss: 0.3575, lr 2.2190176984600058e-07
Step 660100 Loss: 0.3564, lr 2.2190176984600058e-07
Step 660200 Loss: 0.3554, lr 2.2190176984600058e-07
Step 660300 Loss: 0.3550, lr 2.2190176984600058e-07
Step 660400 Loss: 0.3543, lr 2.2190176984600058e-07
Step 660500 Loss: 0.3538, lr 2.2190176984600058e-07
Step 660600 Loss: 0.3531, lr 2.2190176984600058e-07
Step 660700 Loss: 0.3524, lr 2.2190176984600058e-07
Step 660800 Loss: 0.3521, lr 2.2190176984600058e-07
Step 660900 Loss: 0.3516, lr 2.2190176984600058e-07
Step 661000 Loss: 0.3515, lr 2.2190176984600058e-07
Step 661100 Loss: 0.3516, lr 2.2190176984600058e-07
Step 661200 Loss: 0.3520, lr 2.2190176984600058e-07
Step 661300 Loss: 0.3520, lr 2.2190176984600058e-07
Step 661400 Loss: 0.3524, lr 2.2190176984600058e-07
Step 661500 Loss: 0.3525, lr 2.2190176984600058e-07
Step 661600 Loss: 0.3520, lr 2.2190176984600058e-07
Step 661700 Loss: 0.3519, lr 2.2190176984600058e-07
Step 661800 Loss: 0.3511, lr 2.2190176984600058e-07
Step 661900 Loss: 0.3502, lr 2.2190176984600058e-07
Step 662000 Loss: 0.3492, lr 2.2190176984600058e-07
Train Epoch: [98/100] Loss: 0.3491,lr 0.000000
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6756
Step 662100 Loss: 0.9415, lr 9.866357858642222e-08
Step 662200 Loss: 0.6965, lr 9.866357858642222e-08
Step 662300 Loss: 0.6239, lr 9.866357858642222e-08
Step 662400 Loss: 0.5837, lr 9.866357858642222e-08
Step 662500 Loss: 0.5611, lr 9.866357858642222e-08
Step 662600 Loss: 0.5432, lr 9.866357858642222e-08
Step 662700 Loss: 0.5264, lr 9.866357858642222e-08
Step 662800 Loss: 0.5118, lr 9.866357858642222e-08
Step 662900 Loss: 0.4999, lr 9.866357858642222e-08
Step 663000 Loss: 0.4881, lr 9.866357858642222e-08
Step 663100 Loss: 0.4796, lr 9.866357858642222e-08
Step 663200 Loss: 0.4723, lr 9.866357858642222e-08
Step 663300 Loss: 0.4643, lr 9.866357858642222e-08
Step 663400 Loss: 0.4575, lr 9.866357858642222e-08
Step 663500 Loss: 0.4515, lr 9.866357858642222e-08
Step 663600 Loss: 0.4461, lr 9.866357858642222e-08
Step 663700 Loss: 0.4405, lr 9.866357858642222e-08
Step 663800 Loss: 0.4365, lr 9.866357858642222e-08
Step 663900 Loss: 0.4315, lr 9.866357858642222e-08
Step 664000 Loss: 0.4269, lr 9.866357858642222e-08
Step 664100 Loss: 0.4222, lr 9.866357858642222e-08
Step 664200 Loss: 0.4185, lr 9.866357858642222e-08
Step 664300 Loss: 0.4147, lr 9.866357858642222e-08
Step 664400 Loss: 0.4112, lr 9.866357858642222e-08
Step 664500 Loss: 0.4081, lr 9.866357858642222e-08
Step 664600 Loss: 0.4040, lr 9.866357858642222e-08
Step 664700 Loss: 0.4005, lr 9.866357858642222e-08
Step 664800 Loss: 0.3982, lr 9.866357858642222e-08
Step 664900 Loss: 0.3955, lr 9.866357858642222e-08
Step 665000 Loss: 0.3929, lr 9.866357858642222e-08
Step 665100 Loss: 0.3906, lr 9.866357858642222e-08
Step 665200 Loss: 0.3878, lr 9.866357858642222e-08
Step 665300 Loss: 0.3854, lr 9.866357858642222e-08
Step 665400 Loss: 0.3827, lr 9.866357858642222e-08
Step 665500 Loss: 0.3800, lr 9.866357858642222e-08
Step 665600 Loss: 0.3774, lr 9.866357858642222e-08
Step 665700 Loss: 0.3750, lr 9.866357858642222e-08
Step 665800 Loss: 0.3726, lr 9.866357858642222e-08
Step 665900 Loss: 0.3706, lr 9.866357858642222e-08
Step 666000 Loss: 0.3687, lr 9.866357858642222e-08
Step 666100 Loss: 0.3670, lr 9.866357858642222e-08
Step 666200 Loss: 0.3655, lr 9.866357858642222e-08
Step 666300 Loss: 0.3642, lr 9.866357858642222e-08
Step 666400 Loss: 0.3626, lr 9.866357858642222e-08
Step 666500 Loss: 0.3616, lr 9.866357858642222e-08
Step 666600 Loss: 0.3599, lr 9.866357858642222e-08
Step 666700 Loss: 0.3584, lr 9.866357858642222e-08
Step 666800 Loss: 0.3570, lr 9.866357858642222e-08
Step 666900 Loss: 0.3562, lr 9.866357858642222e-08
Step 667000 Loss: 0.3556, lr 9.866357858642222e-08
Step 667100 Loss: 0.3548, lr 9.866357858642222e-08
Step 667200 Loss: 0.3542, lr 9.866357858642222e-08
Step 667300 Loss: 0.3536, lr 9.866357858642222e-08
Step 667400 Loss: 0.3531, lr 9.866357858642222e-08
Step 667500 Loss: 0.3525, lr 9.866357858642222e-08
Step 667600 Loss: 0.3519, lr 9.866357858642222e-08
Step 667700 Loss: 0.3518, lr 9.866357858642222e-08
Step 667800 Loss: 0.3519, lr 9.866357858642222e-08
Step 667900 Loss: 0.3520, lr 9.866357858642222e-08
Step 668000 Loss: 0.3523, lr 9.866357858642222e-08
Step 668100 Loss: 0.3525, lr 9.866357858642222e-08
Step 668200 Loss: 0.3528, lr 9.866357858642222e-08
Step 668300 Loss: 0.3529, lr 9.866357858642222e-08
Step 668400 Loss: 0.3525, lr 9.866357858642222e-08
Step 668500 Loss: 0.3522, lr 9.866357858642222e-08
Step 668600 Loss: 0.3515, lr 9.866357858642222e-08
Step 668700 Loss: 0.3501, lr 9.866357858642222e-08
Step 668800 Loss: 0.3494, lr 9.866357858642222e-08
Train Epoch: [99/100] Loss: 0.3494,lr 0.000000
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 668900 Loss: 0.7834, lr 2.467198171342004e-08
Step 669000 Loss: 0.6582, lr 2.467198171342004e-08
Step 669100 Loss: 0.6049, lr 2.467198171342004e-08
Step 669200 Loss: 0.5720, lr 2.467198171342004e-08
Step 669300 Loss: 0.5525, lr 2.467198171342004e-08
Step 669400 Loss: 0.5351, lr 2.467198171342004e-08
Step 669500 Loss: 0.5198, lr 2.467198171342004e-08
Step 669600 Loss: 0.5067, lr 2.467198171342004e-08
Step 669700 Loss: 0.4951, lr 2.467198171342004e-08
Step 669800 Loss: 0.4844, lr 2.467198171342004e-08
Step 669900 Loss: 0.4767, lr 2.467198171342004e-08
Step 670000 Loss: 0.4684, lr 2.467198171342004e-08
Step 670100 Loss: 0.4621, lr 2.467198171342004e-08
Step 670200 Loss: 0.4551, lr 2.467198171342004e-08
Step 670300 Loss: 0.4493, lr 2.467198171342004e-08
Step 670400 Loss: 0.4437, lr 2.467198171342004e-08
Step 670500 Loss: 0.4389, lr 2.467198171342004e-08
Step 670600 Loss: 0.4346, lr 2.467198171342004e-08
Step 670700 Loss: 0.4299, lr 2.467198171342004e-08
Step 670800 Loss: 0.4252, lr 2.467198171342004e-08
Step 670900 Loss: 0.4205, lr 2.467198171342004e-08
Step 671000 Loss: 0.4170, lr 2.467198171342004e-08
Step 671100 Loss: 0.4135, lr 2.467198171342004e-08
Step 671200 Loss: 0.4103, lr 2.467198171342004e-08
Step 671300 Loss: 0.4066, lr 2.467198171342004e-08
Step 671400 Loss: 0.4026, lr 2.467198171342004e-08
Step 671500 Loss: 0.3996, lr 2.467198171342004e-08
Step 671600 Loss: 0.3974, lr 2.467198171342004e-08
Step 671700 Loss: 0.3944, lr 2.467198171342004e-08
Step 671800 Loss: 0.3920, lr 2.467198171342004e-08
Step 671900 Loss: 0.3892, lr 2.467198171342004e-08
Step 672000 Loss: 0.3867, lr 2.467198171342004e-08
Step 672100 Loss: 0.3842, lr 2.467198171342004e-08
Step 672200 Loss: 0.3814, lr 2.467198171342004e-08
Step 672300 Loss: 0.3786, lr 2.467198171342004e-08
Step 672400 Loss: 0.3758, lr 2.467198171342004e-08
Step 672500 Loss: 0.3737, lr 2.467198171342004e-08
Step 672600 Loss: 0.3716, lr 2.467198171342004e-08
Step 672700 Loss: 0.3695, lr 2.467198171342004e-08
Step 672800 Loss: 0.3677, lr 2.467198171342004e-08
Step 672900 Loss: 0.3662, lr 2.467198171342004e-08
Step 673000 Loss: 0.3648, lr 2.467198171342004e-08
Step 673100 Loss: 0.3634, lr 2.467198171342004e-08
Step 673200 Loss: 0.3624, lr 2.467198171342004e-08
Step 673300 Loss: 0.3610, lr 2.467198171342004e-08
Step 673400 Loss: 0.3593, lr 2.467198171342004e-08
Step 673500 Loss: 0.3579, lr 2.467198171342004e-08
Step 673600 Loss: 0.3567, lr 2.467198171342004e-08
Step 673700 Loss: 0.3559, lr 2.467198171342004e-08
Step 673800 Loss: 0.3553, lr 2.467198171342004e-08
Step 673900 Loss: 0.3546, lr 2.467198171342004e-08
Step 674000 Loss: 0.3540, lr 2.467198171342004e-08
Step 674100 Loss: 0.3533, lr 2.467198171342004e-08
Step 674200 Loss: 0.3527, lr 2.467198171342004e-08
Step 674300 Loss: 0.3523, lr 2.467198171342004e-08
Step 674400 Loss: 0.3517, lr 2.467198171342004e-08
Step 674500 Loss: 0.3515, lr 2.467198171342004e-08
Step 674600 Loss: 0.3517, lr 2.467198171342004e-08
Step 674700 Loss: 0.3522, lr 2.467198171342004e-08
Step 674800 Loss: 0.3524, lr 2.467198171342004e-08
Step 674900 Loss: 0.3527, lr 2.467198171342004e-08
Step 675000 Loss: 0.3530, lr 2.467198171342004e-08
Step 675100 Loss: 0.3525, lr 2.467198171342004e-08
Step 675200 Loss: 0.3523, lr 2.467198171342004e-08
Step 675300 Loss: 0.3518, lr 2.467198171342004e-08
Step 675400 Loss: 0.3511, lr 2.467198171342004e-08
Step 675500 Loss: 0.3498, lr 2.467198171342004e-08
Train Epoch: [100/100] Loss: 0.3499,lr 0.000000
Model Saving at epoch 100
