Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = **
**** embed_size = *1024*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *1024*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *1024*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *1024*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *200*
**** max_steps = *300000*
**** warmup_steps = *8000*
**** lr = *4.0*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=1129, out_features=1024, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=1138, out_features=1024, bias=True)
        (W_r): Linear(in_features=114, out_features=1024, bias=False)
        (U_r): Linear(in_features=1024, out_features=1024, bias=True)
        (W_h): Linear(in_features=1138, out_features=1024, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=1024, bias=True)
        (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
        (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=1024, bias=True)
      (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
      (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (g): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=256, bias=False)
    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 9774736
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 100 Loss: 5.1990, lr 1.7469281074217108e-07
Step 200 Loss: 5.1492, lr 1.7469281074217108e-07
Step 300 Loss: 5.1021, lr 1.7469281074217108e-07
Step 400 Loss: 5.0805, lr 1.7469281074217108e-07
Step 500 Loss: 5.0683, lr 1.7469281074217108e-07
Step 600 Loss: 5.0358, lr 1.7469281074217108e-07
Step 700 Loss: 5.0097, lr 1.7469281074217108e-07
Step 800 Loss: 4.9901, lr 1.7469281074217108e-07
Step 900 Loss: 4.9739, lr 1.7469281074217108e-07
Step 1000 Loss: 4.9610, lr 1.7469281074217108e-07
Step 1100 Loss: 4.9489, lr 1.7469281074217108e-07
Step 1200 Loss: 4.9310, lr 1.7469281074217108e-07
Step 1300 Loss: 4.9144, lr 1.7469281074217108e-07
Step 1400 Loss: 4.9015, lr 1.7469281074217108e-07
Step 1500 Loss: 4.8895, lr 1.7469281074217108e-07
Step 1600 Loss: 4.8789, lr 1.7469281074217108e-07
Step 1700 Loss: 4.8691, lr 1.7469281074217108e-07
Step 1800 Loss: 4.8598, lr 1.7469281074217108e-07
Step 1900 Loss: 4.8458, lr 1.7469281074217108e-07
Step 2000 Loss: 4.8276, lr 1.7469281074217108e-07
Step 2100 Loss: 4.8104, lr 1.7469281074217108e-07
Step 2200 Loss: 4.7952, lr 1.7469281074217108e-07
Step 2300 Loss: 4.7812, lr 1.7469281074217108e-07
Step 2400 Loss: 4.7683, lr 1.7469281074217108e-07
Step 2500 Loss: 4.7562, lr 1.7469281074217108e-07
Step 2600 Loss: 4.7449, lr 1.7469281074217108e-07
Step 2700 Loss: 4.7346, lr 1.7469281074217108e-07
Step 2800 Loss: 4.7223, lr 1.7469281074217108e-07
Step 2900 Loss: 4.7090, lr 1.7469281074217108e-07
Step 3000 Loss: 4.6966, lr 1.7469281074217108e-07
Step 3100 Loss: 4.6848, lr 1.7469281074217108e-07
Step 3200 Loss: 4.6737, lr 1.7469281074217108e-07
Step 3300 Loss: 4.6633, lr 1.7469281074217108e-07
Step 3400 Loss: 4.6537, lr 1.7469281074217108e-07
Step 3500 Loss: 4.6444, lr 1.7469281074217108e-07
Step 3600 Loss: 4.6357, lr 1.7469281074217108e-07
Step 3700 Loss: 4.6276, lr 1.7469281074217108e-07
Step 3800 Loss: 4.6199, lr 1.7469281074217108e-07
Step 3900 Loss: 4.6127, lr 1.7469281074217108e-07
Step 4000 Loss: 4.6061, lr 1.7469281074217108e-07
Step 4100 Loss: 4.5997, lr 1.7469281074217108e-07
Step 4200 Loss: 4.5940, lr 1.7469281074217108e-07
Step 4300 Loss: 4.5871, lr 1.7469281074217108e-07
Step 4400 Loss: 4.5782, lr 1.7469281074217108e-07
Step 4500 Loss: 4.5699, lr 1.7469281074217108e-07
Step 4600 Loss: 4.5622, lr 1.7469281074217108e-07
Step 4700 Loss: 4.5550, lr 1.7469281074217108e-07
Step 4800 Loss: 4.5482, lr 1.7469281074217108e-07
Step 4900 Loss: 4.5422, lr 1.7469281074217108e-07
Step 5000 Loss: 4.5365, lr 1.7469281074217108e-07
Step 5100 Loss: 4.5311, lr 1.7469281074217108e-07
Step 5200 Loss: 4.5263, lr 1.7469281074217108e-07
Step 5300 Loss: 4.5190, lr 1.7469281074217108e-07
Step 5400 Loss: 4.5115, lr 1.7469281074217108e-07
Step 5500 Loss: 4.5043, lr 1.7469281074217108e-07
Step 5600 Loss: 4.4974, lr 1.7469281074217108e-07
Step 5700 Loss: 4.4911, lr 1.7469281074217108e-07
Step 5800 Loss: 4.4852, lr 1.7469281074217108e-07
Step 5900 Loss: 4.4794, lr 1.7469281074217108e-07
Step 6000 Loss: 4.4707, lr 1.7469281074217108e-07
Step 6100 Loss: 4.4619, lr 1.7469281074217108e-07
Step 6200 Loss: 4.4530, lr 1.7469281074217108e-07
Step 6300 Loss: 4.4456, lr 1.7469281074217108e-07
Step 6400 Loss: 4.4380, lr 1.7469281074217108e-07
Step 6500 Loss: 4.4295, lr 1.7469281074217108e-07
Step 6600 Loss: 4.4197, lr 1.7469281074217108e-07
Step 6700 Loss: 4.4084, lr 1.7469281074217108e-07
Train Epoch: [1/200] Loss: 4.3987,lr 0.000000
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Step 6800 Loss: 5.0974, lr 3.4938562148434215e-07
Step 6900 Loss: 5.0250, lr 3.4938562148434215e-07
Step 7000 Loss: 4.9450, lr 3.4938562148434215e-07
Step 7100 Loss: 4.9097, lr 3.4938562148434215e-07
Step 7200 Loss: 4.8906, lr 3.4938562148434215e-07
Step 7300 Loss: 4.8678, lr 3.4938562148434215e-07
Step 7400 Loss: 4.8345, lr 3.4938562148434215e-07
Step 7500 Loss: 4.8097, lr 3.4938562148434215e-07
Step 7600 Loss: 4.7901, lr 3.4938562148434215e-07
Step 7700 Loss: 4.7737, lr 3.4938562148434215e-07
Step 7800 Loss: 4.7601, lr 3.4938562148434215e-07
Step 7900 Loss: 4.7445, lr 3.4938562148434215e-07
Step 8000 Loss: 4.7268, lr 3.4938562148434215e-07
Step 8100 Loss: 4.7111, lr 3.4938562148434215e-07
Step 8200 Loss: 4.6981, lr 3.4938562148434215e-07
Step 8300 Loss: 4.6862, lr 3.4938562148434215e-07
Step 8400 Loss: 4.6757, lr 3.4938562148434215e-07
Step 8500 Loss: 4.6657, lr 3.4938562148434215e-07
Step 8600 Loss: 4.6569, lr 3.4938562148434215e-07
Step 8700 Loss: 4.6388, lr 3.4938562148434215e-07
Step 8800 Loss: 4.6210, lr 3.4938562148434215e-07
Step 8900 Loss: 4.6050, lr 3.4938562148434215e-07
Step 9000 Loss: 4.5907, lr 3.4938562148434215e-07
Step 9100 Loss: 4.5775, lr 3.4938562148434215e-07
Step 9200 Loss: 4.5654, lr 3.4938562148434215e-07
Step 9300 Loss: 4.5540, lr 3.4938562148434215e-07
Step 9400 Loss: 4.5434, lr 3.4938562148434215e-07
Step 9500 Loss: 4.5339, lr 3.4938562148434215e-07
Step 9600 Loss: 4.5203, lr 3.4938562148434215e-07
Step 9700 Loss: 4.5080, lr 3.4938562148434215e-07
Step 9800 Loss: 4.4961, lr 3.4938562148434215e-07
Step 9900 Loss: 4.4853, lr 3.4938562148434215e-07
Step 10000 Loss: 4.4751, lr 3.4938562148434215e-07
Step 10100 Loss: 4.4655, lr 3.4938562148434215e-07
Step 10200 Loss: 4.4565, lr 3.4938562148434215e-07
Step 10300 Loss: 4.4480, lr 3.4938562148434215e-07
Step 10400 Loss: 4.4402, lr 3.4938562148434215e-07
Step 10500 Loss: 4.4327, lr 3.4938562148434215e-07
Step 10600 Loss: 4.4256, lr 3.4938562148434215e-07
Step 10700 Loss: 4.4192, lr 3.4938562148434215e-07
Step 10800 Loss: 4.4133, lr 3.4938562148434215e-07
Step 10900 Loss: 4.4077, lr 3.4938562148434215e-07
Step 11000 Loss: 4.4026, lr 3.4938562148434215e-07
Step 11100 Loss: 4.3946, lr 3.4938562148434215e-07
Step 11200 Loss: 4.3866, lr 3.4938562148434215e-07
Step 11300 Loss: 4.3791, lr 3.4938562148434215e-07
Step 11400 Loss: 4.3720, lr 3.4938562148434215e-07
Step 11500 Loss: 4.3655, lr 3.4938562148434215e-07
Step 11600 Loss: 4.3594, lr 3.4938562148434215e-07
Step 11700 Loss: 4.3539, lr 3.4938562148434215e-07
Step 11800 Loss: 4.3486, lr 3.4938562148434215e-07
Step 11900 Loss: 4.3438, lr 3.4938562148434215e-07
Step 12000 Loss: 4.3387, lr 3.4938562148434215e-07
Step 12100 Loss: 4.3314, lr 3.4938562148434215e-07
Step 12200 Loss: 4.3243, lr 3.4938562148434215e-07
Step 12300 Loss: 4.3177, lr 3.4938562148434215e-07
Step 12400 Loss: 4.3116, lr 3.4938562148434215e-07
Step 12500 Loss: 4.3059, lr 3.4938562148434215e-07
Step 12600 Loss: 4.3004, lr 3.4938562148434215e-07
Step 12700 Loss: 4.2945, lr 3.4938562148434215e-07
Step 12800 Loss: 4.2860, lr 3.4938562148434215e-07
Step 12900 Loss: 4.2777, lr 3.4938562148434215e-07
Step 13000 Loss: 4.2697, lr 3.4938562148434215e-07
Step 13100 Loss: 4.2632, lr 3.4938562148434215e-07
Step 13200 Loss: 4.2543, lr 3.4938562148434215e-07
Step 13300 Loss: 4.2452, lr 3.4938562148434215e-07
Step 13400 Loss: 4.2355, lr 3.4938562148434215e-07
Step 13500 Loss: 4.2222, lr 3.4938562148434215e-07
Train Epoch: [2/200] Loss: 4.2200,lr 0.000001
Calling G2SDataset.batch()
Done, time:  2.45 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Step 13600 Loss: 4.9348, lr 5.240784322265132e-07
Step 13700 Loss: 4.8792, lr 5.240784322265132e-07
Step 13800 Loss: 4.8175, lr 5.240784322265132e-07
Step 13900 Loss: 4.7877, lr 5.240784322265132e-07
Step 14000 Loss: 4.7701, lr 5.240784322265132e-07
Step 14100 Loss: 4.7351, lr 5.240784322265132e-07
Step 14200 Loss: 4.7039, lr 5.240784322265132e-07
Step 14300 Loss: 4.6800, lr 5.240784322265132e-07
Step 14400 Loss: 4.6606, lr 5.240784322265132e-07
Step 14500 Loss: 4.6437, lr 5.240784322265132e-07
Step 14600 Loss: 4.6296, lr 5.240784322265132e-07
Step 14700 Loss: 4.6104, lr 5.240784322265132e-07
Step 14800 Loss: 4.5923, lr 5.240784322265132e-07
Step 14900 Loss: 4.5773, lr 5.240784322265132e-07
Step 15000 Loss: 4.5640, lr 5.240784322265132e-07
Step 15100 Loss: 4.5521, lr 5.240784322265132e-07
Step 15200 Loss: 4.5413, lr 5.240784322265132e-07
Step 15300 Loss: 4.5315, lr 5.240784322265132e-07
Step 15400 Loss: 4.5182, lr 5.240784322265132e-07
Step 15500 Loss: 4.4993, lr 5.240784322265132e-07
Step 15600 Loss: 4.4819, lr 5.240784322265132e-07
Step 15700 Loss: 4.4663, lr 5.240784322265132e-07
Step 15800 Loss: 4.4523, lr 5.240784322265132e-07
Step 15900 Loss: 4.4392, lr 5.240784322265132e-07
Step 16000 Loss: 4.4271, lr 5.240784322265132e-07
Step 16100 Loss: 4.4160, lr 5.240784322265132e-07
Step 16200 Loss: 4.4055, lr 5.240784322265132e-07
Step 16300 Loss: 4.3940, lr 5.240784322265132e-07
Step 16400 Loss: 4.3807, lr 5.240784322265132e-07
Step 16500 Loss: 4.3682, lr 5.240784322265132e-07
Step 16600 Loss: 4.3566, lr 5.240784322265132e-07
Step 16700 Loss: 4.3460, lr 5.240784322265132e-07
Step 16800 Loss: 4.3356, lr 5.240784322265132e-07
Step 16900 Loss: 4.3262, lr 5.240784322265132e-07
Step 17000 Loss: 4.3172, lr 5.240784322265132e-07
Step 17100 Loss: 4.3089, lr 5.240784322265132e-07
Step 17200 Loss: 4.3011, lr 5.240784322265132e-07
Step 17300 Loss: 4.2936, lr 5.240784322265132e-07
Step 17400 Loss: 4.2865, lr 5.240784322265132e-07
Step 17500 Loss: 4.2801, lr 5.240784322265132e-07
Step 17600 Loss: 4.2741, lr 5.240784322265132e-07
Step 17700 Loss: 4.2688, lr 5.240784322265132e-07
Step 17800 Loss: 4.2626, lr 5.240784322265132e-07
Step 17900 Loss: 4.2540, lr 5.240784322265132e-07
Step 18000 Loss: 4.2461, lr 5.240784322265132e-07
Step 18100 Loss: 4.2387, lr 5.240784322265132e-07
Step 18200 Loss: 4.2316, lr 5.240784322265132e-07
Step 18300 Loss: 4.2251, lr 5.240784322265132e-07
Step 18400 Loss: 4.2193, lr 5.240784322265132e-07
Step 18500 Loss: 4.2136, lr 5.240784322265132e-07
Step 18600 Loss: 4.2084, lr 5.240784322265132e-07
Step 18700 Loss: 4.2037, lr 5.240784322265132e-07
Step 18800 Loss: 4.1970, lr 5.240784322265132e-07
Step 18900 Loss: 4.1899, lr 5.240784322265132e-07
Step 19000 Loss: 4.1829, lr 5.240784322265132e-07
Step 19100 Loss: 4.1766, lr 5.240784322265132e-07
Step 19200 Loss: 4.1707, lr 5.240784322265132e-07
Step 19300 Loss: 4.1652, lr 5.240784322265132e-07
Step 19400 Loss: 4.1599, lr 5.240784322265132e-07
Step 19500 Loss: 4.1526, lr 5.240784322265132e-07
Step 19600 Loss: 4.1445, lr 5.240784322265132e-07
Step 19700 Loss: 4.1365, lr 5.240784322265132e-07
Step 19800 Loss: 4.1293, lr 5.240784322265132e-07
Step 19900 Loss: 4.1222, lr 5.240784322265132e-07
Step 20000 Loss: 4.1131, lr 5.240784322265132e-07
Step 20100 Loss: 4.1035, lr 5.240784322265132e-07
Step 20200 Loss: 4.0940, lr 5.240784322265132e-07
Train Epoch: [3/200] Loss: 4.0833,lr 0.000001
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6755
Step 20300 Loss: 4.9873, lr 6.987712429686843e-07
Step 20400 Loss: 4.8414, lr 6.987712429686843e-07
Step 20500 Loss: 4.7564, lr 6.987712429686843e-07
Step 20600 Loss: 4.7092, lr 6.987712429686843e-07
Step 20700 Loss: 4.6824, lr 6.987712429686843e-07
Step 20800 Loss: 4.6577, lr 6.987712429686843e-07
Step 20900 Loss: 4.6185, lr 6.987712429686843e-07
Step 21000 Loss: 4.5893, lr 6.987712429686843e-07
Step 21100 Loss: 4.5664, lr 6.987712429686843e-07
Step 21200 Loss: 4.5476, lr 6.987712429686843e-07
Step 21300 Loss: 4.5317, lr 6.987712429686843e-07
Step 21400 Loss: 4.5151, lr 6.987712429686843e-07
Step 21500 Loss: 4.4958, lr 6.987712429686843e-07
Step 21600 Loss: 4.4786, lr 6.987712429686843e-07
Step 21700 Loss: 4.4640, lr 6.987712429686843e-07
Step 21800 Loss: 4.4509, lr 6.987712429686843e-07
Step 21900 Loss: 4.4393, lr 6.987712429686843e-07
Step 22000 Loss: 4.4285, lr 6.987712429686843e-07
Step 22100 Loss: 4.4191, lr 6.987712429686843e-07
Step 22200 Loss: 4.4014, lr 6.987712429686843e-07
Step 22300 Loss: 4.3828, lr 6.987712429686843e-07
Step 22400 Loss: 4.3661, lr 6.987712429686843e-07
Step 22500 Loss: 4.3509, lr 6.987712429686843e-07
Step 22600 Loss: 4.3374, lr 6.987712429686843e-07
Step 22700 Loss: 4.3244, lr 6.987712429686843e-07
Step 22800 Loss: 4.3124, lr 6.987712429686843e-07
Step 22900 Loss: 4.3016, lr 6.987712429686843e-07
Step 23000 Loss: 4.2914, lr 6.987712429686843e-07
Step 23100 Loss: 4.2779, lr 6.987712429686843e-07
Step 23200 Loss: 4.2651, lr 6.987712429686843e-07
Step 23300 Loss: 4.2527, lr 6.987712429686843e-07
Step 23400 Loss: 4.2413, lr 6.987712429686843e-07
Step 23500 Loss: 4.2306, lr 6.987712429686843e-07
Step 23600 Loss: 4.2206, lr 6.987712429686843e-07
Step 23700 Loss: 4.2113, lr 6.987712429686843e-07
Step 23800 Loss: 4.2024, lr 6.987712429686843e-07
Step 23900 Loss: 4.1942, lr 6.987712429686843e-07
Step 24000 Loss: 4.1864, lr 6.987712429686843e-07
Step 24100 Loss: 4.1789, lr 6.987712429686843e-07
Step 24200 Loss: 4.1720, lr 6.987712429686843e-07
Step 24300 Loss: 4.1657, lr 6.987712429686843e-07
Step 24400 Loss: 4.1597, lr 6.987712429686843e-07
Step 24500 Loss: 4.1544, lr 6.987712429686843e-07
Step 24600 Loss: 4.1467, lr 6.987712429686843e-07
Step 24700 Loss: 4.1384, lr 6.987712429686843e-07
Step 24800 Loss: 4.1306, lr 6.987712429686843e-07
Step 24900 Loss: 4.1232, lr 6.987712429686843e-07
Step 25000 Loss: 4.1162, lr 6.987712429686843e-07
Step 25100 Loss: 4.1099, lr 6.987712429686843e-07
Step 25200 Loss: 4.1039, lr 6.987712429686843e-07
Step 25300 Loss: 4.0983, lr 6.987712429686843e-07
Step 25400 Loss: 4.0931, lr 6.987712429686843e-07
Step 25500 Loss: 4.0879, lr 6.987712429686843e-07
Step 25600 Loss: 4.0803, lr 6.987712429686843e-07
Step 25700 Loss: 4.0733, lr 6.987712429686843e-07
Step 25800 Loss: 4.0664, lr 6.987712429686843e-07
Step 25900 Loss: 4.0605, lr 6.987712429686843e-07
Step 26000 Loss: 4.0547, lr 6.987712429686843e-07
Step 26100 Loss: 4.0493, lr 6.987712429686843e-07
Step 26200 Loss: 4.0441, lr 6.987712429686843e-07
Step 26300 Loss: 4.0360, lr 6.987712429686843e-07
Step 26400 Loss: 4.0282, lr 6.987712429686843e-07
Step 26500 Loss: 4.0207, lr 6.987712429686843e-07
Step 26600 Loss: 4.0144, lr 6.987712429686843e-07
Step 26700 Loss: 4.0065, lr 6.987712429686843e-07
Step 26800 Loss: 3.9976, lr 6.987712429686843e-07
Step 26900 Loss: 3.9882, lr 6.987712429686843e-07
Step 27000 Loss: 3.9760, lr 6.987712429686843e-07
Train Epoch: [4/200] Loss: 3.9724,lr 0.000001
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6755
Step 27100 Loss: 4.8161, lr 8.734640537108554e-07
Step 27200 Loss: 4.7400, lr 8.734640537108554e-07
Step 27300 Loss: 4.6603, lr 8.734640537108554e-07
Step 27400 Loss: 4.6203, lr 8.734640537108554e-07
Step 27500 Loss: 4.5970, lr 8.734640537108554e-07
Step 27600 Loss: 4.5605, lr 8.734640537108554e-07
Step 27700 Loss: 4.5250, lr 8.734640537108554e-07
Step 27800 Loss: 4.4985, lr 8.734640537108554e-07
Step 27900 Loss: 4.4768, lr 8.734640537108554e-07
Step 28000 Loss: 4.4585, lr 8.734640537108554e-07
Step 28100 Loss: 4.4434, lr 8.734640537108554e-07
Step 28200 Loss: 4.4242, lr 8.734640537108554e-07
Step 28300 Loss: 4.4053, lr 8.734640537108554e-07
Step 28400 Loss: 4.3892, lr 8.734640537108554e-07
Step 28500 Loss: 4.3754, lr 8.734640537108554e-07
Step 28600 Loss: 4.3630, lr 8.734640537108554e-07
Step 28700 Loss: 4.3518, lr 8.734640537108554e-07
Step 28800 Loss: 4.3417, lr 8.734640537108554e-07
Step 28900 Loss: 4.3294, lr 8.734640537108554e-07
Step 29000 Loss: 4.3099, lr 8.734640537108554e-07
Step 29100 Loss: 4.2922, lr 8.734640537108554e-07
Step 29200 Loss: 4.2762, lr 8.734640537108554e-07
Step 29300 Loss: 4.2617, lr 8.734640537108554e-07
Step 29400 Loss: 4.2483, lr 8.734640537108554e-07
Step 29500 Loss: 4.2358, lr 8.734640537108554e-07
Step 29600 Loss: 4.2244, lr 8.734640537108554e-07
Step 29700 Loss: 4.2138, lr 8.734640537108554e-07
Step 29800 Loss: 4.2023, lr 8.734640537108554e-07
Step 29900 Loss: 4.1888, lr 8.734640537108554e-07
Step 30000 Loss: 4.1763, lr 8.734640537108554e-07
Step 30100 Loss: 4.1643, lr 8.734640537108554e-07
Step 30200 Loss: 4.1534, lr 8.734640537108554e-07
Step 30300 Loss: 4.1428, lr 8.734640537108554e-07
Step 30400 Loss: 4.1332, lr 8.734640537108554e-07
Step 30500 Loss: 4.1240, lr 8.734640537108554e-07
Step 30600 Loss: 4.1155, lr 8.734640537108554e-07
Step 30700 Loss: 4.1073, lr 8.734640537108554e-07
Step 30800 Loss: 4.0997, lr 8.734640537108554e-07
Step 30900 Loss: 4.0923, lr 8.734640537108554e-07
Step 31000 Loss: 4.0857, lr 8.734640537108554e-07
Step 31100 Loss: 4.0796, lr 8.734640537108554e-07
Step 31200 Loss: 4.0741, lr 8.734640537108554e-07
Step 31300 Loss: 4.0681, lr 8.734640537108554e-07
Step 31400 Loss: 4.0594, lr 8.734640537108554e-07
Step 31500 Loss: 4.0514, lr 8.734640537108554e-07
Step 31600 Loss: 4.0437, lr 8.734640537108554e-07
Step 31700 Loss: 4.0365, lr 8.734640537108554e-07
Step 31800 Loss: 4.0297, lr 8.734640537108554e-07
Step 31900 Loss: 4.0236, lr 8.734640537108554e-07
Step 32000 Loss: 4.0175, lr 8.734640537108554e-07
Step 32100 Loss: 4.0122, lr 8.734640537108554e-07
Step 32200 Loss: 4.0070, lr 8.734640537108554e-07
Step 32300 Loss: 4.0006, lr 8.734640537108554e-07
Step 32400 Loss: 3.9932, lr 8.734640537108554e-07
Step 32500 Loss: 3.9863, lr 8.734640537108554e-07
Step 32600 Loss: 3.9798, lr 8.734640537108554e-07
Step 32700 Loss: 3.9738, lr 8.734640537108554e-07
Step 32800 Loss: 3.9683, lr 8.734640537108554e-07
Step 32900 Loss: 3.9631, lr 8.734640537108554e-07
Step 33000 Loss: 3.9564, lr 8.734640537108554e-07
Step 33100 Loss: 3.9484, lr 8.734640537108554e-07
Step 33200 Loss: 3.9406, lr 8.734640537108554e-07
Step 33300 Loss: 3.9336, lr 8.734640537108554e-07
Step 33400 Loss: 3.9273, lr 8.734640537108554e-07
Step 33500 Loss: 3.9188, lr 8.734640537108554e-07
Step 33600 Loss: 3.9091, lr 8.734640537108554e-07
Step 33700 Loss: 3.8997, lr 8.734640537108554e-07
Train Epoch: [5/200] Loss: 3.8880,lr 0.000001
Model Saving at epoch 5
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Step 33800 Loss: 5.0157, lr 1.0481568644530265e-06
Step 33900 Loss: 4.7381, lr 1.0481568644530265e-06
Step 34000 Loss: 4.6414, lr 1.0481568644530265e-06
Step 34100 Loss: 4.5834, lr 1.0481568644530265e-06
Step 34200 Loss: 4.5500, lr 1.0481568644530265e-06
Step 34300 Loss: 4.5244, lr 1.0481568644530265e-06
Step 34400 Loss: 4.4810, lr 1.0481568644530265e-06
Step 34500 Loss: 4.4496, lr 1.0481568644530265e-06
Step 34600 Loss: 4.4251, lr 1.0481568644530265e-06
Step 34700 Loss: 4.4048, lr 1.0481568644530265e-06
Step 34800 Loss: 4.3881, lr 1.0481568644530265e-06
Step 34900 Loss: 4.3717, lr 1.0481568644530265e-06
Step 35000 Loss: 4.3516, lr 1.0481568644530265e-06
Step 35100 Loss: 4.3339, lr 1.0481568644530265e-06
Step 35200 Loss: 4.3191, lr 1.0481568644530265e-06
Step 35300 Loss: 4.3057, lr 1.0481568644530265e-06
Step 35400 Loss: 4.2940, lr 1.0481568644530265e-06
Step 35500 Loss: 4.2830, lr 1.0481568644530265e-06
Step 35600 Loss: 4.2736, lr 1.0481568644530265e-06
Step 35700 Loss: 4.2566, lr 1.0481568644530265e-06
