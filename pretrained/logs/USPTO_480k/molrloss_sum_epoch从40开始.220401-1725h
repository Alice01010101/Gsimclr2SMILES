Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1/molrloss_sum_epoch40.pt*
**** embed_size = *1024*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *1024*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *1024*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *1024*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *100*
**** max_steps = *300000*
**** warmup_steps = *100*
**** lr = *6.545084971874744e-05*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loaded pretrained state_dict from ./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1/molrloss_sum_epoch40.pt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=1129, out_features=1024, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=1138, out_features=1024, bias=True)
        (W_r): Linear(in_features=114, out_features=1024, bias=False)
        (U_r): Linear(in_features=1024, out_features=1024, bias=True)
        (W_h): Linear(in_features=1138, out_features=1024, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=1024, bias=True)
        (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
        (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=1024, bias=True)
      (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
      (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (g): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=256, bias=False)
    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 9774736
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Step 100 Loss: 0.5470, lr 6.545084971874744e-05
Step 200 Loss: 0.4529, lr 6.545084971874744e-05
Step 300 Loss: 0.4105, lr 6.545084971874744e-05
Step 400 Loss: 0.3854, lr 6.545084971874744e-05
Step 500 Loss: 0.3686, lr 6.545084971874744e-05
Step 600 Loss: 0.3520, lr 6.545084971874744e-05
Step 700 Loss: 0.3395, lr 6.545084971874744e-05
Step 800 Loss: 0.3302, lr 6.545084971874744e-05
Step 900 Loss: 0.3207, lr 6.545084971874744e-05
Step 1000 Loss: 0.3125, lr 6.545084971874744e-05
Step 1100 Loss: 0.3061, lr 6.545084971874744e-05
Step 1200 Loss: 0.2999, lr 6.545084971874744e-05
Step 1300 Loss: 0.2947, lr 6.545084971874744e-05
Step 1400 Loss: 0.2886, lr 6.545084971874744e-05
Step 1500 Loss: 0.2853, lr 6.545084971874744e-05
Step 1600 Loss: 0.2812, lr 6.545084971874744e-05
Step 1700 Loss: 0.2777, lr 6.545084971874744e-05
Step 1800 Loss: 0.2743, lr 6.545084971874744e-05
Step 1900 Loss: 0.2714, lr 6.545084971874744e-05
Step 2000 Loss: 0.2678, lr 6.545084971874744e-05
Step 2100 Loss: 0.2640, lr 6.545084971874744e-05
Step 2200 Loss: 0.2615, lr 6.545084971874744e-05
Step 2300 Loss: 0.2592, lr 6.545084971874744e-05
Step 2400 Loss: 0.2566, lr 6.545084971874744e-05
Step 2500 Loss: 0.2541, lr 6.545084971874744e-05
Step 2600 Loss: 0.2515, lr 6.545084971874744e-05
Step 2700 Loss: 0.2492, lr 6.545084971874744e-05
Step 2800 Loss: 0.2472, lr 6.545084971874744e-05
Step 2900 Loss: 0.2449, lr 6.545084971874744e-05
Step 3000 Loss: 0.2430, lr 6.545084971874744e-05
Step 3100 Loss: 0.2409, lr 6.545084971874744e-05
Step 3200 Loss: 0.2387, lr 6.545084971874744e-05
Step 3300 Loss: 0.2366, lr 6.545084971874744e-05
Step 3400 Loss: 0.2347, lr 6.545084971874744e-05
Step 3500 Loss: 0.2330, lr 6.545084971874744e-05
Step 3600 Loss: 0.2309, lr 6.545084971874744e-05
Step 3700 Loss: 0.2293, lr 6.545084971874744e-05
Step 3800 Loss: 0.2280, lr 6.545084971874744e-05
Step 3900 Loss: 0.2266, lr 6.545084971874744e-05
Step 4000 Loss: 0.2256, lr 6.545084971874744e-05
Step 4100 Loss: 0.2243, lr 6.545084971874744e-05
Step 4200 Loss: 0.2235, lr 6.545084971874744e-05
Step 4300 Loss: 0.2225, lr 6.545084971874744e-05
Step 4400 Loss: 0.2216, lr 6.545084971874744e-05
Step 4500 Loss: 0.2208, lr 6.545084971874744e-05
Step 4600 Loss: 0.2196, lr 6.545084971874744e-05
Step 4700 Loss: 0.2187, lr 6.545084971874744e-05
Step 4800 Loss: 0.2183, lr 6.545084971874744e-05
Step 4900 Loss: 0.2179, lr 6.545084971874744e-05
Step 5000 Loss: 0.2177, lr 6.545084971874744e-05
Step 5100 Loss: 0.2174, lr 6.545084971874744e-05
Step 5200 Loss: 0.2171, lr 6.545084971874744e-05
Step 5300 Loss: 0.2170, lr 6.545084971874744e-05
Step 5400 Loss: 0.2169, lr 6.545084971874744e-05
Step 5500 Loss: 0.2169, lr 6.545084971874744e-05
Step 5600 Loss: 0.2166, lr 6.545084971874744e-05
Step 5700 Loss: 0.2168, lr 6.545084971874744e-05
Step 5800 Loss: 0.2171, lr 6.545084971874744e-05
Step 5900 Loss: 0.2173, lr 6.545084971874744e-05
Step 6000 Loss: 0.2180, lr 6.545084971874744e-05
Step 6100 Loss: 0.2185, lr 6.545084971874744e-05
Step 6200 Loss: 0.2191, lr 6.545084971874744e-05
Step 6300 Loss: 0.2199, lr 6.545084971874744e-05
Step 6400 Loss: 0.2212, lr 6.545084971874744e-05
Step 6500 Loss: 0.2223, lr 6.545084971874744e-05
Step 6600 Loss: 0.2223, lr 6.545084971874744e-05
Step 6700 Loss: 0.2220, lr 6.545084971874744e-05
Train Epoch: [41/100] Loss: 0.2223,lr 0.000065
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 6800 Loss: 0.7205, lr 6.543470169707354e-05
Step 6900 Loss: 0.4894, lr 6.543470169707354e-05
Step 7000 Loss: 0.4281, lr 6.543470169707354e-05
Step 7100 Loss: 0.3948, lr 6.543470169707354e-05
Step 7200 Loss: 0.3740, lr 6.543470169707354e-05
Step 7300 Loss: 0.3594, lr 6.543470169707354e-05
Step 7400 Loss: 0.3435, lr 6.543470169707354e-05
Step 7500 Loss: 0.3319, lr 6.543470169707354e-05
Step 7600 Loss: 0.3241, lr 6.543470169707354e-05
Step 7700 Loss: 0.3151, lr 6.543470169707354e-05
Step 7800 Loss: 0.3074, lr 6.543470169707354e-05
Step 7900 Loss: 0.3015, lr 6.543470169707354e-05
Step 8000 Loss: 0.2959, lr 6.543470169707354e-05
Step 8100 Loss: 0.2906, lr 6.543470169707354e-05
Step 8200 Loss: 0.2860, lr 6.543470169707354e-05
Step 8300 Loss: 0.2827, lr 6.543470169707354e-05
Step 8400 Loss: 0.2788, lr 6.543470169707354e-05
Step 8500 Loss: 0.2751, lr 6.543470169707354e-05
Step 8600 Loss: 0.2717, lr 6.543470169707354e-05
Step 8700 Loss: 0.2686, lr 6.543470169707354e-05
Step 8800 Loss: 0.2647, lr 6.543470169707354e-05
Step 8900 Loss: 0.2616, lr 6.543470169707354e-05
Step 9000 Loss: 0.2589, lr 6.543470169707354e-05
Step 9100 Loss: 0.2560, lr 6.543470169707354e-05
Step 9200 Loss: 0.2534, lr 6.543470169707354e-05
Step 9300 Loss: 0.2508, lr 6.543470169707354e-05
Step 9400 Loss: 0.2483, lr 6.543470169707354e-05
Step 9500 Loss: 0.2465, lr 6.543470169707354e-05
Step 9600 Loss: 0.2441, lr 6.543470169707354e-05
Step 9700 Loss: 0.2424, lr 6.543470169707354e-05
Step 9800 Loss: 0.2403, lr 6.543470169707354e-05
Step 9900 Loss: 0.2382, lr 6.543470169707354e-05
Step 10000 Loss: 0.2362, lr 6.543470169707354e-05
Step 10100 Loss: 0.2340, lr 6.543470169707354e-05
Step 10200 Loss: 0.2320, lr 6.543470169707354e-05
Step 10300 Loss: 0.2304, lr 6.543470169707354e-05
Step 10400 Loss: 0.2286, lr 6.543470169707354e-05
Step 10500 Loss: 0.2269, lr 6.543470169707354e-05
Step 10600 Loss: 0.2257, lr 6.543470169707354e-05
Step 10700 Loss: 0.2244, lr 6.543470169707354e-05
Step 10800 Loss: 0.2234, lr 6.543470169707354e-05
Step 10900 Loss: 0.2222, lr 6.543470169707354e-05
Step 11000 Loss: 0.2215, lr 6.543470169707354e-05
Step 11100 Loss: 0.2204, lr 6.543470169707354e-05
Step 11200 Loss: 0.2195, lr 6.543470169707354e-05
Step 11300 Loss: 0.2184, lr 6.543470169707354e-05
Step 11400 Loss: 0.2175, lr 6.543470169707354e-05
Step 11500 Loss: 0.2167, lr 6.543470169707354e-05
Step 11600 Loss: 0.2163, lr 6.543470169707354e-05
Step 11700 Loss: 0.2160, lr 6.543470169707354e-05
Step 11800 Loss: 0.2154, lr 6.543470169707354e-05
Step 11900 Loss: 0.2155, lr 6.543470169707354e-05
Step 12000 Loss: 0.2150, lr 6.543470169707354e-05
Step 12100 Loss: 0.2150, lr 6.543470169707354e-05
Step 12200 Loss: 0.2147, lr 6.543470169707354e-05
Step 12300 Loss: 0.2147, lr 6.543470169707354e-05
Step 12400 Loss: 0.2145, lr 6.543470169707354e-05
Step 12500 Loss: 0.2149, lr 6.543470169707354e-05
Step 12600 Loss: 0.2151, lr 6.543470169707354e-05
Step 12700 Loss: 0.2154, lr 6.543470169707354e-05
Step 12800 Loss: 0.2159, lr 6.543470169707354e-05
Step 12900 Loss: 0.2163, lr 6.543470169707354e-05
Step 13000 Loss: 0.2172, lr 6.543470169707354e-05
Step 13100 Loss: 0.2181, lr 6.543470169707354e-05
Step 13200 Loss: 0.2196, lr 6.543470169707354e-05
Step 13300 Loss: 0.2205, lr 6.543470169707354e-05
Step 13400 Loss: 0.2200, lr 6.543470169707354e-05
Step 13500 Loss: 0.2198, lr 6.543470169707354e-05
Train Epoch: [42/100] Loss: 0.2207,lr 0.000065
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Step 13600 Loss: 0.6132, lr 6.53862735681997e-05
Step 13700 Loss: 0.4801, lr 6.53862735681997e-05
Step 13800 Loss: 0.4225, lr 6.53862735681997e-05
Step 13900 Loss: 0.3922, lr 6.53862735681997e-05
Step 14000 Loss: 0.3733, lr 6.53862735681997e-05
Step 14100 Loss: 0.3565, lr 6.53862735681997e-05
Step 14200 Loss: 0.3427, lr 6.53862735681997e-05
Step 14300 Loss: 0.3319, lr 6.53862735681997e-05
Step 14400 Loss: 0.3225, lr 6.53862735681997e-05
Step 14500 Loss: 0.3141, lr 6.53862735681997e-05
Step 14600 Loss: 0.3067, lr 6.53862735681997e-05
Step 14700 Loss: 0.3010, lr 6.53862735681997e-05
Step 14800 Loss: 0.2957, lr 6.53862735681997e-05
Step 14900 Loss: 0.2899, lr 6.53862735681997e-05
Step 15000 Loss: 0.2860, lr 6.53862735681997e-05
Step 15100 Loss: 0.2820, lr 6.53862735681997e-05
Step 15200 Loss: 0.2784, lr 6.53862735681997e-05
Step 15300 Loss: 0.2749, lr 6.53862735681997e-05
Step 15400 Loss: 0.2713, lr 6.53862735681997e-05
Step 15500 Loss: 0.2675, lr 6.53862735681997e-05
Step 15600 Loss: 0.2636, lr 6.53862735681997e-05
Step 15700 Loss: 0.2608, lr 6.53862735681997e-05
Step 15800 Loss: 0.2583, lr 6.53862735681997e-05
Step 15900 Loss: 0.2552, lr 6.53862735681997e-05
Step 16000 Loss: 0.2521, lr 6.53862735681997e-05
Step 16100 Loss: 0.2496, lr 6.53862735681997e-05
Step 16200 Loss: 0.2471, lr 6.53862735681997e-05
Step 16300 Loss: 0.2451, lr 6.53862735681997e-05
Step 16400 Loss: 0.2432, lr 6.53862735681997e-05
Step 16500 Loss: 0.2409, lr 6.53862735681997e-05
Step 16600 Loss: 0.2389, lr 6.53862735681997e-05
Step 16700 Loss: 0.2368, lr 6.53862735681997e-05
Step 16800 Loss: 0.2348, lr 6.53862735681997e-05
Step 16900 Loss: 0.2327, lr 6.53862735681997e-05
Step 17000 Loss: 0.2309, lr 6.53862735681997e-05
Step 17100 Loss: 0.2291, lr 6.53862735681997e-05
Step 17200 Loss: 0.2275, lr 6.53862735681997e-05
Step 17300 Loss: 0.2260, lr 6.53862735681997e-05
Step 17400 Loss: 0.2245, lr 6.53862735681997e-05
Step 17500 Loss: 0.2235, lr 6.53862735681997e-05
Step 17600 Loss: 0.2222, lr 6.53862735681997e-05
Step 17700 Loss: 0.2216, lr 6.53862735681997e-05
Step 17800 Loss: 0.2205, lr 6.53862735681997e-05
Step 17900 Loss: 0.2195, lr 6.53862735681997e-05
Step 18000 Loss: 0.2185, lr 6.53862735681997e-05
Step 18100 Loss: 0.2173, lr 6.53862735681997e-05
Step 18200 Loss: 0.2164, lr 6.53862735681997e-05
Step 18300 Loss: 0.2159, lr 6.53862735681997e-05
Step 18400 Loss: 0.2154, lr 6.53862735681997e-05
Step 18500 Loss: 0.2151, lr 6.53862735681997e-05
Step 18600 Loss: 0.2147, lr 6.53862735681997e-05
Step 18700 Loss: 0.2145, lr 6.53862735681997e-05
Step 18800 Loss: 0.2141, lr 6.53862735681997e-05
Step 18900 Loss: 0.2139, lr 6.53862735681997e-05
Step 19000 Loss: 0.2137, lr 6.53862735681997e-05
Step 19100 Loss: 0.2138, lr 6.53862735681997e-05
Step 19200 Loss: 0.2140, lr 6.53862735681997e-05
Step 19300 Loss: 0.2142, lr 6.53862735681997e-05
Step 19400 Loss: 0.2145, lr 6.53862735681997e-05
Step 19500 Loss: 0.2151, lr 6.53862735681997e-05
Step 19600 Loss: 0.2155, lr 6.53862735681997e-05
Step 19700 Loss: 0.2162, lr 6.53862735681997e-05
Step 19800 Loss: 0.2172, lr 6.53862735681997e-05
Step 19900 Loss: 0.2183, lr 6.53862735681997e-05
Step 20000 Loss: 0.2193, lr 6.53862735681997e-05
Step 20100 Loss: 0.2193, lr 6.53862735681997e-05
Step 20200 Loss: 0.2188, lr 6.53862735681997e-05
Train Epoch: [43/100] Loss: 0.2191,lr 0.000065
Calling G2SDataset.batch()
Done, time:  2.12 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.46 s, total batches: 6755
Step 20300 Loss: 0.8126, lr 6.530561312484229e-05
Step 20400 Loss: 0.5025, lr 6.530561312484229e-05
Step 20500 Loss: 0.4330, lr 6.530561312484229e-05
Step 20600 Loss: 0.3950, lr 6.530561312484229e-05
Step 20700 Loss: 0.3724, lr 6.530561312484229e-05
Step 20800 Loss: 0.3578, lr 6.530561312484229e-05
Step 20900 Loss: 0.3425, lr 6.530561312484229e-05
Step 21000 Loss: 0.3302, lr 6.530561312484229e-05
Step 21100 Loss: 0.3218, lr 6.530561312484229e-05
Step 21200 Loss: 0.3132, lr 6.530561312484229e-05
Step 21300 Loss: 0.3056, lr 6.530561312484229e-05
Step 21400 Loss: 0.2998, lr 6.530561312484229e-05
Step 21500 Loss: 0.2943, lr 6.530561312484229e-05
Step 21600 Loss: 0.2891, lr 6.530561312484229e-05
Step 21700 Loss: 0.2843, lr 6.530561312484229e-05
Step 21800 Loss: 0.2807, lr 6.530561312484229e-05
Step 21900 Loss: 0.2768, lr 6.530561312484229e-05
Step 22000 Loss: 0.2732, lr 6.530561312484229e-05
Step 22100 Loss: 0.2703, lr 6.530561312484229e-05
Step 22200 Loss: 0.2671, lr 6.530561312484229e-05
Step 22300 Loss: 0.2635, lr 6.530561312484229e-05
Step 22400 Loss: 0.2603, lr 6.530561312484229e-05
Step 22500 Loss: 0.2576, lr 6.530561312484229e-05
Step 22600 Loss: 0.2544, lr 6.530561312484229e-05
Step 22700 Loss: 0.2519, lr 6.530561312484229e-05
Step 22800 Loss: 0.2493, lr 6.530561312484229e-05
Step 22900 Loss: 0.2468, lr 6.530561312484229e-05
Step 23000 Loss: 0.2449, lr 6.530561312484229e-05
Step 23100 Loss: 0.2428, lr 6.530561312484229e-05
Step 23200 Loss: 0.2407, lr 6.530561312484229e-05
Step 23300 Loss: 0.2387, lr 6.530561312484229e-05
Step 23400 Loss: 0.2367, lr 6.530561312484229e-05
Step 23500 Loss: 0.2345, lr 6.530561312484229e-05
Step 23600 Loss: 0.2324, lr 6.530561312484229e-05
Step 23700 Loss: 0.2305, lr 6.530561312484229e-05
Step 23800 Loss: 0.2289, lr 6.530561312484229e-05
Step 23900 Loss: 0.2271, lr 6.530561312484229e-05
Step 24000 Loss: 0.2253, lr 6.530561312484229e-05
Step 24100 Loss: 0.2240, lr 6.530561312484229e-05
Step 24200 Loss: 0.2227, lr 6.530561312484229e-05
Step 24300 Loss: 0.2218, lr 6.530561312484229e-05
Step 24400 Loss: 0.2207, lr 6.530561312484229e-05
Step 24500 Loss: 0.2197, lr 6.530561312484229e-05
Step 24600 Loss: 0.2188, lr 6.530561312484229e-05
Step 24700 Loss: 0.2177, lr 6.530561312484229e-05
Step 24800 Loss: 0.2167, lr 6.530561312484229e-05
Step 24900 Loss: 0.2158, lr 6.530561312484229e-05
Step 25000 Loss: 0.2149, lr 6.530561312484229e-05
Step 25100 Loss: 0.2143, lr 6.530561312484229e-05
Step 25200 Loss: 0.2141, lr 6.530561312484229e-05
Step 25300 Loss: 0.2135, lr 6.530561312484229e-05
Step 25400 Loss: 0.2134, lr 6.530561312484229e-05
Step 25500 Loss: 0.2128, lr 6.530561312484229e-05
Step 25600 Loss: 0.2128, lr 6.530561312484229e-05
Step 25700 Loss: 0.2125, lr 6.530561312484229e-05
Step 25800 Loss: 0.2125, lr 6.530561312484229e-05
Step 25900 Loss: 0.2123, lr 6.530561312484229e-05
Step 26000 Loss: 0.2123, lr 6.530561312484229e-05
Step 26100 Loss: 0.2124, lr 6.530561312484229e-05
Step 26200 Loss: 0.2128, lr 6.530561312484229e-05
Step 26300 Loss: 0.2134, lr 6.530561312484229e-05
Step 26400 Loss: 0.2137, lr 6.530561312484229e-05
Step 26500 Loss: 0.2144, lr 6.530561312484229e-05
Step 26600 Loss: 0.2155, lr 6.530561312484229e-05
Step 26700 Loss: 0.2167, lr 6.530561312484229e-05
Step 26800 Loss: 0.2176, lr 6.530561312484229e-05
Step 26900 Loss: 0.2173, lr 6.530561312484229e-05
Step 27000 Loss: 0.2171, lr 6.530561312484229e-05
Train Epoch: [44/100] Loss: 0.2173,lr 0.000065
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6755
Step 27100 Loss: 0.5648, lr 6.519279996912067e-05
Step 27200 Loss: 0.4546, lr 6.519279996912067e-05
Step 27300 Loss: 0.4076, lr 6.519279996912067e-05
Step 27400 Loss: 0.3794, lr 6.519279996912067e-05
Step 27500 Loss: 0.3636, lr 6.519279996912067e-05
Step 27600 Loss: 0.3488, lr 6.519279996912067e-05
Step 27700 Loss: 0.3347, lr 6.519279996912067e-05
Step 27800 Loss: 0.3255, lr 6.519279996912067e-05
Step 27900 Loss: 0.3166, lr 6.519279996912067e-05
Step 28000 Loss: 0.3085, lr 6.519279996912067e-05
Step 28100 Loss: 0.3015, lr 6.519279996912067e-05
Step 28200 Loss: 0.2958, lr 6.519279996912067e-05
Step 28300 Loss: 0.2911, lr 6.519279996912067e-05
Step 28400 Loss: 0.2852, lr 6.519279996912067e-05
Step 28500 Loss: 0.2811, lr 6.519279996912067e-05
Step 28600 Loss: 0.2776, lr 6.519279996912067e-05
Step 28700 Loss: 0.2742, lr 6.519279996912067e-05
Step 28800 Loss: 0.2707, lr 6.519279996912067e-05
Step 28900 Loss: 0.2674, lr 6.519279996912067e-05
Step 29000 Loss: 0.2638, lr 6.519279996912067e-05
Step 29100 Loss: 0.2599, lr 6.519279996912067e-05
Step 29200 Loss: 0.2570, lr 6.519279996912067e-05
Step 29300 Loss: 0.2544, lr 6.519279996912067e-05
Step 29400 Loss: 0.2515, lr 6.519279996912067e-05
Step 29500 Loss: 0.2485, lr 6.519279996912067e-05
Step 29600 Loss: 0.2463, lr 6.519279996912067e-05
Step 29700 Loss: 0.2441, lr 6.519279996912067e-05
Step 29800 Loss: 0.2421, lr 6.519279996912067e-05
Step 29900 Loss: 0.2399, lr 6.519279996912067e-05
Step 30000 Loss: 0.2382, lr 6.519279996912067e-05
Step 30100 Loss: 0.2361, lr 6.519279996912067e-05
Step 30200 Loss: 0.2340, lr 6.519279996912067e-05
Step 30300 Loss: 0.2322, lr 6.519279996912067e-05
Step 30400 Loss: 0.2300, lr 6.519279996912067e-05
Step 30500 Loss: 0.2283, lr 6.519279996912067e-05
Step 30600 Loss: 0.2265, lr 6.519279996912067e-05
Step 30700 Loss: 0.2248, lr 6.519279996912067e-05
Step 30800 Loss: 0.2234, lr 6.519279996912067e-05
Step 30900 Loss: 0.2219, lr 6.519279996912067e-05
Step 31000 Loss: 0.2209, lr 6.519279996912067e-05
Step 31100 Loss: 0.2199, lr 6.519279996912067e-05
Step 31200 Loss: 0.2189, lr 6.519279996912067e-05
Step 31300 Loss: 0.2179, lr 6.519279996912067e-05
Step 31400 Loss: 0.2170, lr 6.519279996912067e-05
Step 31500 Loss: 0.2162, lr 6.519279996912067e-05
Step 31600 Loss: 0.2149, lr 6.519279996912067e-05
Step 31700 Loss: 0.2140, lr 6.519279996912067e-05
Step 31800 Loss: 0.2135, lr 6.519279996912067e-05
Step 31900 Loss: 0.2131, lr 6.519279996912067e-05
Step 32000 Loss: 0.2128, lr 6.519279996912067e-05
Step 32100 Loss: 0.2123, lr 6.519279996912067e-05
Step 32200 Loss: 0.2122, lr 6.519279996912067e-05
Step 32300 Loss: 0.2118, lr 6.519279996912067e-05
Step 32400 Loss: 0.2118, lr 6.519279996912067e-05
Step 32500 Loss: 0.2116, lr 6.519279996912067e-05
Step 32600 Loss: 0.2115, lr 6.519279996912067e-05
Step 32700 Loss: 0.2116, lr 6.519279996912067e-05
Step 32800 Loss: 0.2118, lr 6.519279996912067e-05
Step 32900 Loss: 0.2121, lr 6.519279996912067e-05
Step 33000 Loss: 0.2124, lr 6.519279996912067e-05
Step 33100 Loss: 0.2129, lr 6.519279996912067e-05
Step 33200 Loss: 0.2135, lr 6.519279996912067e-05
Step 33300 Loss: 0.2144, lr 6.519279996912067e-05
Step 33400 Loss: 0.2154, lr 6.519279996912067e-05
Step 33500 Loss: 0.2163, lr 6.519279996912067e-05
Step 33600 Loss: 0.2169, lr 6.519279996912067e-05
Step 33700 Loss: 0.2163, lr 6.519279996912067e-05
Train Epoch: [45/100] Loss: 0.2169,lr 0.000065
Model Saving at epoch 45
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6755
Step 33800 Loss: 0.9086, lr 6.504794543399942e-05
Step 33900 Loss: 0.5089, lr 6.504794543399942e-05
Step 34000 Loss: 0.4315, lr 6.504794543399942e-05
Step 34100 Loss: 0.3925, lr 6.504794543399942e-05
Step 34200 Loss: 0.3719, lr 6.504794543399942e-05
Step 34300 Loss: 0.3567, lr 6.504794543399942e-05
Step 34400 Loss: 0.3407, lr 6.504794543399942e-05
Step 34500 Loss: 0.3286, lr 6.504794543399942e-05
Step 34600 Loss: 0.3211, lr 6.504794543399942e-05
Step 34700 Loss: 0.3119, lr 6.504794543399942e-05
Step 34800 Loss: 0.3048, lr 6.504794543399942e-05
Step 34900 Loss: 0.2988, lr 6.504794543399942e-05
Step 35000 Loss: 0.2928, lr 6.504794543399942e-05
Step 35100 Loss: 0.2873, lr 6.504794543399942e-05
Step 35200 Loss: 0.2821, lr 6.504794543399942e-05
Step 35300 Loss: 0.2789, lr 6.504794543399942e-05
Step 35400 Loss: 0.2747, lr 6.504794543399942e-05
Step 35500 Loss: 0.2714, lr 6.504794543399942e-05
Step 35600 Loss: 0.2683, lr 6.504794543399942e-05
Step 35700 Loss: 0.2653, lr 6.504794543399942e-05
Step 35800 Loss: 0.2614, lr 6.504794543399942e-05
Step 35900 Loss: 0.2579, lr 6.504794543399942e-05
Step 36000 Loss: 0.2555, lr 6.504794543399942e-05
Step 36100 Loss: 0.2523, lr 6.504794543399942e-05
Step 36200 Loss: 0.2499, lr 6.504794543399942e-05
Step 36300 Loss: 0.2472, lr 6.504794543399942e-05
Step 36400 Loss: 0.2444, lr 6.504794543399942e-05
Step 36500 Loss: 0.2426, lr 6.504794543399942e-05
Step 36600 Loss: 0.2405, lr 6.504794543399942e-05
Step 36700 Loss: 0.2385, lr 6.504794543399942e-05
Step 36800 Loss: 0.2366, lr 6.504794543399942e-05
Step 36900 Loss: 0.2345, lr 6.504794543399942e-05
Step 37000 Loss: 0.2324, lr 6.504794543399942e-05
Step 37100 Loss: 0.2304, lr 6.504794543399942e-05
Step 37200 Loss: 0.2285, lr 6.504794543399942e-05
Step 37300 Loss: 0.2267, lr 6.504794543399942e-05
Step 37400 Loss: 0.2251, lr 6.504794543399942e-05
Step 37500 Loss: 0.2233, lr 6.504794543399942e-05
Step 37600 Loss: 0.2221, lr 6.504794543399942e-05
Step 37700 Loss: 0.2207, lr 6.504794543399942e-05
Step 37800 Loss: 0.2197, lr 6.504794543399942e-05
Step 37900 Loss: 0.2187, lr 6.504794543399942e-05
Step 38000 Loss: 0.2178, lr 6.504794543399942e-05
Step 38100 Loss: 0.2169, lr 6.504794543399942e-05
Step 38200 Loss: 0.2158, lr 6.504794543399942e-05
Step 38300 Loss: 0.2149, lr 6.504794543399942e-05
Step 38400 Loss: 0.2138, lr 6.504794543399942e-05
Step 38500 Loss: 0.2130, lr 6.504794543399942e-05
Step 38600 Loss: 0.2127, lr 6.504794543399942e-05
Step 38700 Loss: 0.2125, lr 6.504794543399942e-05
Step 38800 Loss: 0.2119, lr 6.504794543399942e-05
Step 38900 Loss: 0.2116, lr 6.504794543399942e-05
Step 39000 Loss: 0.2112, lr 6.504794543399942e-05
Step 39100 Loss: 0.2111, lr 6.504794543399942e-05
Step 39200 Loss: 0.2107, lr 6.504794543399942e-05
Step 39300 Loss: 0.2105, lr 6.504794543399942e-05
Step 39400 Loss: 0.2104, lr 6.504794543399942e-05
Step 39500 Loss: 0.2104, lr 6.504794543399942e-05
Step 39600 Loss: 0.2106, lr 6.504794543399942e-05
Step 39700 Loss: 0.2110, lr 6.504794543399942e-05
Step 39800 Loss: 0.2113, lr 6.504794543399942e-05
Step 39900 Loss: 0.2117, lr 6.504794543399942e-05
Step 40000 Loss: 0.2125, lr 6.504794543399942e-05
Step 40100 Loss: 0.2134, lr 6.504794543399942e-05
Step 40200 Loss: 0.2145, lr 6.504794543399942e-05
Step 40300 Loss: 0.2154, lr 6.504794543399942e-05
Step 40400 Loss: 0.2151, lr 6.504794543399942e-05
Step 40500 Loss: 0.2147, lr 6.504794543399942e-05
Train Epoch: [46/100] Loss: 0.2153,lr 0.000065
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Step 40600 Loss: 0.5842, lr 6.487119247341622e-05
Step 40700 Loss: 0.4543, lr 6.487119247341622e-05
Step 40800 Loss: 0.4047, lr 6.487119247341622e-05
Step 40900 Loss: 0.3748, lr 6.487119247341622e-05
Step 41000 Loss: 0.3586, lr 6.487119247341622e-05
Step 41100 Loss: 0.3448, lr 6.487119247341622e-05
Step 41200 Loss: 0.3303, lr 6.487119247341622e-05
Step 41300 Loss: 0.3209, lr 6.487119247341622e-05
Step 41400 Loss: 0.3133, lr 6.487119247341622e-05
Step 41500 Loss: 0.3051, lr 6.487119247341622e-05
Step 41600 Loss: 0.2980, lr 6.487119247341622e-05
Step 41700 Loss: 0.2923, lr 6.487119247341622e-05
Step 41800 Loss: 0.2874, lr 6.487119247341622e-05
Step 41900 Loss: 0.2815, lr 6.487119247341622e-05
Step 42000 Loss: 0.2774, lr 6.487119247341622e-05
Step 42100 Loss: 0.2741, lr 6.487119247341622e-05
Step 42200 Loss: 0.2706, lr 6.487119247341622e-05
Step 42300 Loss: 0.2671, lr 6.487119247341622e-05
Step 42400 Loss: 0.2643, lr 6.487119247341622e-05
Step 42500 Loss: 0.2608, lr 6.487119247341622e-05
Step 42600 Loss: 0.2572, lr 6.487119247341622e-05
Step 42700 Loss: 0.2545, lr 6.487119247341622e-05
Step 42800 Loss: 0.2521, lr 6.487119247341622e-05
Step 42900 Loss: 0.2492, lr 6.487119247341622e-05
Step 43000 Loss: 0.2465, lr 6.487119247341622e-05
Step 43100 Loss: 0.2443, lr 6.487119247341622e-05
Step 43200 Loss: 0.2419, lr 6.487119247341622e-05
Step 43300 Loss: 0.2403, lr 6.487119247341622e-05
Step 43400 Loss: 0.2381, lr 6.487119247341622e-05
Step 43500 Loss: 0.2364, lr 6.487119247341622e-05
Step 43600 Loss: 0.2345, lr 6.487119247341622e-05
Step 43700 Loss: 0.2323, lr 6.487119247341622e-05
Step 43800 Loss: 0.2305, lr 6.487119247341622e-05
Step 43900 Loss: 0.2283, lr 6.487119247341622e-05
Step 44000 Loss: 0.2265, lr 6.487119247341622e-05
Step 44100 Loss: 0.2247, lr 6.487119247341622e-05
Step 44200 Loss: 0.2231, lr 6.487119247341622e-05
Step 44300 Loss: 0.2216, lr 6.487119247341622e-05
Step 44400 Loss: 0.2202, lr 6.487119247341622e-05
Step 44500 Loss: 0.2190, lr 6.487119247341622e-05
Step 44600 Loss: 0.2180, lr 6.487119247341622e-05
Step 44700 Loss: 0.2171, lr 6.487119247341622e-05
Step 44800 Loss: 0.2160, lr 6.487119247341622e-05
Step 44900 Loss: 0.2150, lr 6.487119247341622e-05
Step 45000 Loss: 0.2142, lr 6.487119247341622e-05
Step 45100 Loss: 0.2131, lr 6.487119247341622e-05
Step 45200 Loss: 0.2120, lr 6.487119247341622e-05
Step 45300 Loss: 0.2114, lr 6.487119247341622e-05
Step 45400 Loss: 0.2112, lr 6.487119247341622e-05
Step 45500 Loss: 0.2108, lr 6.487119247341622e-05
Step 45600 Loss: 0.2103, lr 6.487119247341622e-05
Step 45700 Loss: 0.2101, lr 6.487119247341622e-05
Step 45800 Loss: 0.2096, lr 6.487119247341622e-05
Step 45900 Loss: 0.2095, lr 6.487119247341622e-05
Step 46000 Loss: 0.2093, lr 6.487119247341622e-05
Step 46100 Loss: 0.2090, lr 6.487119247341622e-05
Step 46200 Loss: 0.2089, lr 6.487119247341622e-05
Step 46300 Loss: 0.2090, lr 6.487119247341622e-05
Step 46400 Loss: 0.2094, lr 6.487119247341622e-05
Step 46500 Loss: 0.2098, lr 6.487119247341622e-05
Step 46600 Loss: 0.2101, lr 6.487119247341622e-05
Step 46700 Loss: 0.2106, lr 6.487119247341622e-05
Step 46800 Loss: 0.2116, lr 6.487119247341622e-05
Step 46900 Loss: 0.2123, lr 6.487119247341622e-05
Step 47000 Loss: 0.2134, lr 6.487119247341622e-05
Step 47100 Loss: 0.2140, lr 6.487119247341622e-05
Step 47200 Loss: 0.2136, lr 6.487119247341622e-05
Train Epoch: [47/100] Loss: 0.2136,lr 0.000065
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6756
Step 47300 Loss: 1.3037, lr 6.466271552120351e-05
Step 47400 Loss: 0.5353, lr 6.466271552120351e-05
Step 47500 Loss: 0.4427, lr 6.466271552120351e-05
Step 47600 Loss: 0.4001, lr 6.466271552120351e-05
Step 47700 Loss: 0.3771, lr 6.466271552120351e-05
Step 47800 Loss: 0.3597, lr 6.466271552120351e-05
Step 47900 Loss: 0.3430, lr 6.466271552120351e-05
Step 48000 Loss: 0.3302, lr 6.466271552120351e-05
Step 48100 Loss: 0.3219, lr 6.466271552120351e-05
Step 48200 Loss: 0.3125, lr 6.466271552120351e-05
Step 48300 Loss: 0.3044, lr 6.466271552120351e-05
Step 48400 Loss: 0.2982, lr 6.466271552120351e-05
Step 48500 Loss: 0.2921, lr 6.466271552120351e-05
Step 48600 Loss: 0.2865, lr 6.466271552120351e-05
Step 48700 Loss: 0.2812, lr 6.466271552120351e-05
Step 48800 Loss: 0.2777, lr 6.466271552120351e-05
Step 48900 Loss: 0.2732, lr 6.466271552120351e-05
Step 49000 Loss: 0.2702, lr 6.466271552120351e-05
Step 49100 Loss: 0.2671, lr 6.466271552120351e-05
Step 49200 Loss: 0.2641, lr 6.466271552120351e-05
Step 49300 Loss: 0.2603, lr 6.466271552120351e-05
Step 49400 Loss: 0.2568, lr 6.466271552120351e-05
Step 49500 Loss: 0.2544, lr 6.466271552120351e-05
Step 49600 Loss: 0.2518, lr 6.466271552120351e-05
Step 49700 Loss: 0.2491, lr 6.466271552120351e-05
Step 49800 Loss: 0.2463, lr 6.466271552120351e-05
Step 49900 Loss: 0.2438, lr 6.466271552120351e-05
Step 50000 Loss: 0.2418, lr 6.466271552120351e-05
Step 50100 Loss: 0.2398, lr 6.466271552120351e-05
Step 50200 Loss: 0.2377, lr 6.466271552120351e-05
Step 50300 Loss: 0.2357, lr 6.466271552120351e-05
Step 50400 Loss: 0.2335, lr 6.466271552120351e-05
Step 50500 Loss: 0.2313, lr 6.466271552120351e-05
Step 50600 Loss: 0.2293, lr 6.466271552120351e-05
Step 50700 Loss: 0.2274, lr 6.466271552120351e-05
Step 50800 Loss: 0.2256, lr 6.466271552120351e-05
Step 50900 Loss: 0.2237, lr 6.466271552120351e-05
Step 51000 Loss: 0.2221, lr 6.466271552120351e-05
Step 51100 Loss: 0.2208, lr 6.466271552120351e-05
Step 51200 Loss: 0.2195, lr 6.466271552120351e-05
Step 51300 Loss: 0.2186, lr 6.466271552120351e-05
Step 51400 Loss: 0.2173, lr 6.466271552120351e-05
Step 51500 Loss: 0.2166, lr 6.466271552120351e-05
Step 51600 Loss: 0.2156, lr 6.466271552120351e-05
Step 51700 Loss: 0.2144, lr 6.466271552120351e-05
Step 51800 Loss: 0.2136, lr 6.466271552120351e-05
Step 51900 Loss: 0.2125, lr 6.466271552120351e-05
Step 52000 Loss: 0.2116, lr 6.466271552120351e-05
Step 52100 Loss: 0.2112, lr 6.466271552120351e-05
Step 52200 Loss: 0.2108, lr 6.466271552120351e-05
Step 52300 Loss: 0.2104, lr 6.466271552120351e-05
Step 52400 Loss: 0.2100, lr 6.466271552120351e-05
Step 52500 Loss: 0.2097, lr 6.466271552120351e-05
Step 52600 Loss: 0.2094, lr 6.466271552120351e-05
Step 52700 Loss: 0.2090, lr 6.466271552120351e-05
Step 52800 Loss: 0.2090, lr 6.466271552120351e-05
Step 52900 Loss: 0.2087, lr 6.466271552120351e-05
Step 53000 Loss: 0.2086, lr 6.466271552120351e-05
Step 53100 Loss: 0.2088, lr 6.466271552120351e-05
Step 53200 Loss: 0.2091, lr 6.466271552120351e-05
Step 53300 Loss: 0.2095, lr 6.466271552120351e-05
Step 53400 Loss: 0.2100, lr 6.466271552120351e-05
Step 53500 Loss: 0.2107, lr 6.466271552120351e-05
Step 53600 Loss: 0.2116, lr 6.466271552120351e-05
Step 53700 Loss: 0.2125, lr 6.466271552120351e-05
Step 53800 Loss: 0.2132, lr 6.466271552120351e-05
Step 53900 Loss: 0.2132, lr 6.466271552120351e-05
Step 54000 Loss: 0.2126, lr 6.466271552120351e-05
Train Epoch: [48/100] Loss: 0.2129,lr 0.000065
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Step 54100 Loss: 0.6545, lr 6.442272031894341e-05
Step 54200 Loss: 0.4738, lr 6.442272031894341e-05
Step 54300 Loss: 0.4152, lr 6.442272031894341e-05
Step 54400 Loss: 0.3825, lr 6.442272031894341e-05
Step 54500 Loss: 0.3657, lr 6.442272031894341e-05
Step 54600 Loss: 0.3507, lr 6.442272031894341e-05
Step 54700 Loss: 0.3356, lr 6.442272031894341e-05
Step 54800 Loss: 0.3243, lr 6.442272031894341e-05
Step 54900 Loss: 0.3162, lr 6.442272031894341e-05
Step 55000 Loss: 0.3080, lr 6.442272031894341e-05
Step 55100 Loss: 0.3004, lr 6.442272031894341e-05
Step 55200 Loss: 0.2950, lr 6.442272031894341e-05
Step 55300 Loss: 0.2895, lr 6.442272031894341e-05
Step 55400 Loss: 0.2839, lr 6.442272031894341e-05
Step 55500 Loss: 0.2793, lr 6.442272031894341e-05
Step 55600 Loss: 0.2756, lr 6.442272031894341e-05
Step 55700 Loss: 0.2718, lr 6.442272031894341e-05
Step 55800 Loss: 0.2683, lr 6.442272031894341e-05
Step 55900 Loss: 0.2651, lr 6.442272031894341e-05
Step 56000 Loss: 0.2618, lr 6.442272031894341e-05
Step 56100 Loss: 0.2579, lr 6.442272031894341e-05
Step 56200 Loss: 0.2549, lr 6.442272031894341e-05
Step 56300 Loss: 0.2523, lr 6.442272031894341e-05
Step 56400 Loss: 0.2492, lr 6.442272031894341e-05
Step 56500 Loss: 0.2465, lr 6.442272031894341e-05
Step 56600 Loss: 0.2441, lr 6.442272031894341e-05
Step 56700 Loss: 0.2418, lr 6.442272031894341e-05
Step 56800 Loss: 0.2399, lr 6.442272031894341e-05
Step 56900 Loss: 0.2380, lr 6.442272031894341e-05
Step 57000 Loss: 0.2361, lr 6.442272031894341e-05
Step 57100 Loss: 0.2341, lr 6.442272031894341e-05
Step 57200 Loss: 0.2321, lr 6.442272031894341e-05
Step 57300 Loss: 0.2304, lr 6.442272031894341e-05
Step 57400 Loss: 0.2281, lr 6.442272031894341e-05
Step 57500 Loss: 0.2262, lr 6.442272031894341e-05
Step 57600 Loss: 0.2245, lr 6.442272031894341e-05
Step 57700 Loss: 0.2228, lr 6.442272031894341e-05
Step 57800 Loss: 0.2211, lr 6.442272031894341e-05
Step 57900 Loss: 0.2197, lr 6.442272031894341e-05
Step 58000 Loss: 0.2184, lr 6.442272031894341e-05
Step 58100 Loss: 0.2175, lr 6.442272031894341e-05
Step 58200 Loss: 0.2164, lr 6.442272031894341e-05
Step 58300 Loss: 0.2155, lr 6.442272031894341e-05
Step 58400 Loss: 0.2143, lr 6.442272031894341e-05
Step 58500 Loss: 0.2136, lr 6.442272031894341e-05
Step 58600 Loss: 0.2124, lr 6.442272031894341e-05
Step 58700 Loss: 0.2114, lr 6.442272031894341e-05
Step 58800 Loss: 0.2108, lr 6.442272031894341e-05
Step 58900 Loss: 0.2103, lr 6.442272031894341e-05
Step 59000 Loss: 0.2100, lr 6.442272031894341e-05
Step 59100 Loss: 0.2094, lr 6.442272031894341e-05
Step 59200 Loss: 0.2093, lr 6.442272031894341e-05
Step 59300 Loss: 0.2089, lr 6.442272031894341e-05
Step 59400 Loss: 0.2086, lr 6.442272031894341e-05
Step 59500 Loss: 0.2083, lr 6.442272031894341e-05
Step 59600 Loss: 0.2082, lr 6.442272031894341e-05
Step 59700 Loss: 0.2080, lr 6.442272031894341e-05
Step 59800 Loss: 0.2082, lr 6.442272031894341e-05
Step 59900 Loss: 0.2083, lr 6.442272031894341e-05
Step 60000 Loss: 0.2086, lr 6.442272031894341e-05
Step 60100 Loss: 0.2090, lr 6.442272031894341e-05
Step 60200 Loss: 0.2095, lr 6.442272031894341e-05
Step 60300 Loss: 0.2105, lr 6.442272031894341e-05
Step 60400 Loss: 0.2113, lr 6.442272031894341e-05
Step 60500 Loss: 0.2123, lr 6.442272031894341e-05
Step 60600 Loss: 0.2129, lr 6.442272031894341e-05
Step 60700 Loss: 0.2125, lr 6.442272031894341e-05
Step 60800 Loss: 0.2123, lr 6.442272031894341e-05
Train Epoch: [49/100] Loss: 0.2125,lr 0.000064
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 60900 Loss: 0.5047, lr 6.415144371292558e-05
Step 61000 Loss: 0.4203, lr 6.415144371292558e-05
Step 61100 Loss: 0.3837, lr 6.415144371292558e-05
Step 61200 Loss: 0.3621, lr 6.415144371292558e-05
Step 61300 Loss: 0.3482, lr 6.415144371292558e-05
Step 61400 Loss: 0.3335, lr 6.415144371292558e-05
Step 61500 Loss: 0.3222, lr 6.415144371292558e-05
Step 61600 Loss: 0.3142, lr 6.415144371292558e-05
Step 61700 Loss: 0.3052, lr 6.415144371292558e-05
Step 61800 Loss: 0.2976, lr 6.415144371292558e-05
Step 61900 Loss: 0.2913, lr 6.415144371292558e-05
Step 62000 Loss: 0.2856, lr 6.415144371292558e-05
Step 62100 Loss: 0.2807, lr 6.415144371292558e-05
Step 62200 Loss: 0.2753, lr 6.415144371292558e-05
Step 62300 Loss: 0.2721, lr 6.415144371292558e-05
Step 62400 Loss: 0.2680, lr 6.415144371292558e-05
Step 62500 Loss: 0.2650, lr 6.415144371292558e-05
Step 62600 Loss: 0.2618, lr 6.415144371292558e-05
Step 62700 Loss: 0.2589, lr 6.415144371292558e-05
Step 62800 Loss: 0.2554, lr 6.415144371292558e-05
Step 62900 Loss: 0.2521, lr 6.415144371292558e-05
Step 63000 Loss: 0.2496, lr 6.415144371292558e-05
Step 63100 Loss: 0.2470, lr 6.415144371292558e-05
Step 63200 Loss: 0.2445, lr 6.415144371292558e-05
Step 63300 Loss: 0.2419, lr 6.415144371292558e-05
Step 63400 Loss: 0.2393, lr 6.415144371292558e-05
Step 63500 Loss: 0.2373, lr 6.415144371292558e-05
Step 63600 Loss: 0.2354, lr 6.415144371292558e-05
Step 63700 Loss: 0.2335, lr 6.415144371292558e-05
Step 63800 Loss: 0.2316, lr 6.415144371292558e-05
Step 63900 Loss: 0.2295, lr 6.415144371292558e-05
Step 64000 Loss: 0.2274, lr 6.415144371292558e-05
Step 64100 Loss: 0.2256, lr 6.415144371292558e-05
Step 64200 Loss: 0.2237, lr 6.415144371292558e-05
Step 64300 Loss: 0.2220, lr 6.415144371292558e-05
Step 64400 Loss: 0.2201, lr 6.415144371292558e-05
Step 64500 Loss: 0.2187, lr 6.415144371292558e-05
Step 64600 Loss: 0.2173, lr 6.415144371292558e-05
Step 64700 Loss: 0.2160, lr 6.415144371292558e-05
Step 64800 Loss: 0.2150, lr 6.415144371292558e-05
Step 64900 Loss: 0.2138, lr 6.415144371292558e-05
Step 65000 Loss: 0.2131, lr 6.415144371292558e-05
Step 65100 Loss: 0.2122, lr 6.415144371292558e-05
Step 65200 Loss: 0.2112, lr 6.415144371292558e-05
Step 65300 Loss: 0.2103, lr 6.415144371292558e-05
Step 65400 Loss: 0.2092, lr 6.415144371292558e-05
Step 65500 Loss: 0.2083, lr 6.415144371292558e-05
Step 65600 Loss: 0.2078, lr 6.415144371292558e-05
Step 65700 Loss: 0.2075, lr 6.415144371292558e-05
Step 65800 Loss: 0.2071, lr 6.415144371292558e-05
Step 65900 Loss: 0.2067, lr 6.415144371292558e-05
Step 66000 Loss: 0.2063, lr 6.415144371292558e-05
Step 66100 Loss: 0.2060, lr 6.415144371292558e-05
Step 66200 Loss: 0.2057, lr 6.415144371292558e-05
Step 66300 Loss: 0.2055, lr 6.415144371292558e-05
Step 66400 Loss: 0.2053, lr 6.415144371292558e-05
Step 66500 Loss: 0.2055, lr 6.415144371292558e-05
Step 66600 Loss: 0.2057, lr 6.415144371292558e-05
Step 66700 Loss: 0.2061, lr 6.415144371292558e-05
Step 66800 Loss: 0.2064, lr 6.415144371292558e-05
Step 66900 Loss: 0.2071, lr 6.415144371292558e-05
Step 67000 Loss: 0.2076, lr 6.415144371292558e-05
Step 67100 Loss: 0.2085, lr 6.415144371292558e-05
Step 67200 Loss: 0.2095, lr 6.415144371292558e-05
Step 67300 Loss: 0.2104, lr 6.415144371292558e-05
Step 67400 Loss: 0.2105, lr 6.415144371292558e-05
Step 67500 Loss: 0.2100, lr 6.415144371292558e-05
Train Epoch: [50/100] Loss: 0.2103,lr 0.000064
Model Saving at epoch 50
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6755
Step 67600 Loss: 0.6849, lr 6.384915342040852e-05
Step 67700 Loss: 0.4638, lr 6.384915342040852e-05
Step 67800 Loss: 0.4052, lr 6.384915342040852e-05
Step 67900 Loss: 0.3741, lr 6.384915342040852e-05
Step 68000 Loss: 0.3546, lr 6.384915342040852e-05
Step 68100 Loss: 0.3420, lr 6.384915342040852e-05
Step 68200 Loss: 0.3274, lr 6.384915342040852e-05
Step 68300 Loss: 0.3165, lr 6.384915342040852e-05
Step 68400 Loss: 0.3095, lr 6.384915342040852e-05
Step 68500 Loss: 0.3019, lr 6.384915342040852e-05
Step 68600 Loss: 0.2953, lr 6.384915342040852e-05
Step 68700 Loss: 0.2896, lr 6.384915342040852e-05
Step 68800 Loss: 0.2847, lr 6.384915342040852e-05
Step 68900 Loss: 0.2797, lr 6.384915342040852e-05
Step 69000 Loss: 0.2749, lr 6.384915342040852e-05
Step 69100 Loss: 0.2718, lr 6.384915342040852e-05
Step 69200 Loss: 0.2680, lr 6.384915342040852e-05
Step 69300 Loss: 0.2646, lr 6.384915342040852e-05
Step 69400 Loss: 0.2615, lr 6.384915342040852e-05
Step 69500 Loss: 0.2583, lr 6.384915342040852e-05
Step 69600 Loss: 0.2546, lr 6.384915342040852e-05
Step 69700 Loss: 0.2520, lr 6.384915342040852e-05
Step 69800 Loss: 0.2493, lr 6.384915342040852e-05
Step 69900 Loss: 0.2465, lr 6.384915342040852e-05
Step 70000 Loss: 0.2440, lr 6.384915342040852e-05
Step 70100 Loss: 0.2416, lr 6.384915342040852e-05
Step 70200 Loss: 0.2395, lr 6.384915342040852e-05
Step 70300 Loss: 0.2377, lr 6.384915342040852e-05
Step 70400 Loss: 0.2354, lr 6.384915342040852e-05
Step 70500 Loss: 0.2334, lr 6.384915342040852e-05
Step 70600 Loss: 0.2316, lr 6.384915342040852e-05
Step 70700 Loss: 0.2295, lr 6.384915342040852e-05
Step 70800 Loss: 0.2274, lr 6.384915342040852e-05
Step 70900 Loss: 0.2252, lr 6.384915342040852e-05
Step 71000 Loss: 0.2234, lr 6.384915342040852e-05
Step 71100 Loss: 0.2218, lr 6.384915342040852e-05
Step 71200 Loss: 0.2200, lr 6.384915342040852e-05
Step 71300 Loss: 0.2183, lr 6.384915342040852e-05
Step 71400 Loss: 0.2172, lr 6.384915342040852e-05
Step 71500 Loss: 0.2158, lr 6.384915342040852e-05
Step 71600 Loss: 0.2148, lr 6.384915342040852e-05
Step 71700 Loss: 0.2137, lr 6.384915342040852e-05
Step 71800 Loss: 0.2130, lr 6.384915342040852e-05
Step 71900 Loss: 0.2119, lr 6.384915342040852e-05
Step 72000 Loss: 0.2109, lr 6.384915342040852e-05
Step 72100 Loss: 0.2099, lr 6.384915342040852e-05
Step 72200 Loss: 0.2090, lr 6.384915342040852e-05
Step 72300 Loss: 0.2082, lr 6.384915342040852e-05
Step 72400 Loss: 0.2078, lr 6.384915342040852e-05
Step 72500 Loss: 0.2075, lr 6.384915342040852e-05
Step 72600 Loss: 0.2070, lr 6.384915342040852e-05
Step 72700 Loss: 0.2069, lr 6.384915342040852e-05
Step 72800 Loss: 0.2063, lr 6.384915342040852e-05
Step 72900 Loss: 0.2062, lr 6.384915342040852e-05
Step 73000 Loss: 0.2058, lr 6.384915342040852e-05
Step 73100 Loss: 0.2056, lr 6.384915342040852e-05
Step 73200 Loss: 0.2054, lr 6.384915342040852e-05
Step 73300 Loss: 0.2056, lr 6.384915342040852e-05
Step 73400 Loss: 0.2058, lr 6.384915342040852e-05
Step 73500 Loss: 0.2061, lr 6.384915342040852e-05
Step 73600 Loss: 0.2063, lr 6.384915342040852e-05
Step 73700 Loss: 0.2068, lr 6.384915342040852e-05
Step 73800 Loss: 0.2076, lr 6.384915342040852e-05
Step 73900 Loss: 0.2084, lr 6.384915342040852e-05
Step 74000 Loss: 0.2096, lr 6.384915342040852e-05
Step 74100 Loss: 0.2103, lr 6.384915342040852e-05
Step 74200 Loss: 0.2099, lr 6.384915342040852e-05
Step 74300 Loss: 0.2096, lr 6.384915342040852e-05
Train Epoch: [51/100] Loss: 0.2098,lr 0.000064
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6756
Step 74400 Loss: 0.5341, lr 6.351614776541501e-05
Step 74500 Loss: 0.4362, lr 6.351614776541501e-05
Step 74600 Loss: 0.3937, lr 6.351614776541501e-05
Step 74700 Loss: 0.3693, lr 6.351614776541501e-05
Step 74800 Loss: 0.3536, lr 6.351614776541501e-05
Step 74900 Loss: 0.3388, lr 6.351614776541501e-05
Step 75000 Loss: 0.3262, lr 6.351614776541501e-05
Step 75100 Loss: 0.3171, lr 6.351614776541501e-05
Step 75200 Loss: 0.3080, lr 6.351614776541501e-05
Step 75300 Loss: 0.3002, lr 6.351614776541501e-05
Step 75400 Loss: 0.2933, lr 6.351614776541501e-05
Step 75500 Loss: 0.2880, lr 6.351614776541501e-05
Step 75600 Loss: 0.2827, lr 6.351614776541501e-05
Step 75700 Loss: 0.2771, lr 6.351614776541501e-05
Step 75800 Loss: 0.2736, lr 6.351614776541501e-05
Step 75900 Loss: 0.2699, lr 6.351614776541501e-05
Step 76000 Loss: 0.2666, lr 6.351614776541501e-05
Step 76100 Loss: 0.2634, lr 6.351614776541501e-05
Step 76200 Loss: 0.2602, lr 6.351614776541501e-05
Step 76300 Loss: 0.2562, lr 6.351614776541501e-05
Step 76400 Loss: 0.2526, lr 6.351614776541501e-05
Step 76500 Loss: 0.2498, lr 6.351614776541501e-05
Step 76600 Loss: 0.2474, lr 6.351614776541501e-05
Step 76700 Loss: 0.2447, lr 6.351614776541501e-05
Step 76800 Loss: 0.2419, lr 6.351614776541501e-05
Step 76900 Loss: 0.2396, lr 6.351614776541501e-05
Step 77000 Loss: 0.2375, lr 6.351614776541501e-05
Step 77100 Loss: 0.2356, lr 6.351614776541501e-05
Step 77200 Loss: 0.2336, lr 6.351614776541501e-05
Step 77300 Loss: 0.2318, lr 6.351614776541501e-05
Step 77400 Loss: 0.2299, lr 6.351614776541501e-05
Step 77500 Loss: 0.2278, lr 6.351614776541501e-05
Step 77600 Loss: 0.2259, lr 6.351614776541501e-05
Step 77700 Loss: 0.2240, lr 6.351614776541501e-05
Step 77800 Loss: 0.2221, lr 6.351614776541501e-05
Step 77900 Loss: 0.2201, lr 6.351614776541501e-05
Step 78000 Loss: 0.2186, lr 6.351614776541501e-05
Step 78100 Loss: 0.2171, lr 6.351614776541501e-05
Step 78200 Loss: 0.2157, lr 6.351614776541501e-05
Step 78300 Loss: 0.2148, lr 6.351614776541501e-05
Step 78400 Loss: 0.2136, lr 6.351614776541501e-05
Step 78500 Loss: 0.2129, lr 6.351614776541501e-05
Step 78600 Loss: 0.2117, lr 6.351614776541501e-05
Step 78700 Loss: 0.2108, lr 6.351614776541501e-05
Step 78800 Loss: 0.2099, lr 6.351614776541501e-05
Step 78900 Loss: 0.2088, lr 6.351614776541501e-05
Step 79000 Loss: 0.2078, lr 6.351614776541501e-05
Step 79100 Loss: 0.2073, lr 6.351614776541501e-05
Step 79200 Loss: 0.2068, lr 6.351614776541501e-05
Step 79300 Loss: 0.2063, lr 6.351614776541501e-05
Step 79400 Loss: 0.2059, lr 6.351614776541501e-05
Step 79500 Loss: 0.2056, lr 6.351614776541501e-05
Step 79600 Loss: 0.2052, lr 6.351614776541501e-05
Step 79700 Loss: 0.2048, lr 6.351614776541501e-05
Step 79800 Loss: 0.2047, lr 6.351614776541501e-05
Step 79900 Loss: 0.2044, lr 6.351614776541501e-05
Step 80000 Loss: 0.2044, lr 6.351614776541501e-05
Step 80100 Loss: 0.2045, lr 6.351614776541501e-05
Step 80200 Loss: 0.2047, lr 6.351614776541501e-05
Step 80300 Loss: 0.2052, lr 6.351614776541501e-05
Step 80400 Loss: 0.2056, lr 6.351614776541501e-05
Step 80500 Loss: 0.2063, lr 6.351614776541501e-05
Step 80600 Loss: 0.2071, lr 6.351614776541501e-05
Step 80700 Loss: 0.2082, lr 6.351614776541501e-05
Step 80800 Loss: 0.2093, lr 6.351614776541501e-05
Step 80900 Loss: 0.2093, lr 6.351614776541501e-05
Step 81000 Loss: 0.2088, lr 6.351614776541501e-05
Train Epoch: [52/100] Loss: 0.2091,lr 0.000064
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 81100 Loss: 0.8127, lr 6.315275538432225e-05
Step 81200 Loss: 0.4888, lr 6.315275538432225e-05
Step 81300 Loss: 0.4205, lr 6.315275538432225e-05
Step 81400 Loss: 0.3842, lr 6.315275538432225e-05
Step 81500 Loss: 0.3636, lr 6.315275538432225e-05
Step 81600 Loss: 0.3493, lr 6.315275538432225e-05
Step 81700 Loss: 0.3341, lr 6.315275538432225e-05
Step 81800 Loss: 0.3223, lr 6.315275538432225e-05
Step 81900 Loss: 0.3143, lr 6.315275538432225e-05
Step 82000 Loss: 0.3051, lr 6.315275538432225e-05
Step 82100 Loss: 0.2977, lr 6.315275538432225e-05
Step 82200 Loss: 0.2915, lr 6.315275538432225e-05
Step 82300 Loss: 0.2859, lr 6.315275538432225e-05
Step 82400 Loss: 0.2805, lr 6.315275538432225e-05
Step 82500 Loss: 0.2754, lr 6.315275538432225e-05
Step 82600 Loss: 0.2721, lr 6.315275538432225e-05
Step 82700 Loss: 0.2683, lr 6.315275538432225e-05
Step 82800 Loss: 0.2648, lr 6.315275538432225e-05
Step 82900 Loss: 0.2616, lr 6.315275538432225e-05
Step 83000 Loss: 0.2586, lr 6.315275538432225e-05
Step 83100 Loss: 0.2548, lr 6.315275538432225e-05
Step 83200 Loss: 0.2517, lr 6.315275538432225e-05
Step 83300 Loss: 0.2491, lr 6.315275538432225e-05
Step 83400 Loss: 0.2461, lr 6.315275538432225e-05
Step 83500 Loss: 0.2434, lr 6.315275538432225e-05
Step 83600 Loss: 0.2408, lr 6.315275538432225e-05
Step 83700 Loss: 0.2385, lr 6.315275538432225e-05
Step 83800 Loss: 0.2368, lr 6.315275538432225e-05
Step 83900 Loss: 0.2347, lr 6.315275538432225e-05
Step 84000 Loss: 0.2328, lr 6.315275538432225e-05
Step 84100 Loss: 0.2307, lr 6.315275538432225e-05
Step 84200 Loss: 0.2288, lr 6.315275538432225e-05
Step 84300 Loss: 0.2267, lr 6.315275538432225e-05
Step 84400 Loss: 0.2245, lr 6.315275538432225e-05
Step 84500 Loss: 0.2228, lr 6.315275538432225e-05
Step 84600 Loss: 0.2210, lr 6.315275538432225e-05
Step 84700 Loss: 0.2193, lr 6.315275538432225e-05
Step 84800 Loss: 0.2176, lr 6.315275538432225e-05
Step 84900 Loss: 0.2163, lr 6.315275538432225e-05
Step 85000 Loss: 0.2152, lr 6.315275538432225e-05
Step 85100 Loss: 0.2142, lr 6.315275538432225e-05
Step 85200 Loss: 0.2131, lr 6.315275538432225e-05
Step 85300 Loss: 0.2123, lr 6.315275538432225e-05
Step 85400 Loss: 0.2112, lr 6.315275538432225e-05
Step 85500 Loss: 0.2104, lr 6.315275538432225e-05
Step 85600 Loss: 0.2094, lr 6.315275538432225e-05
Step 85700 Loss: 0.2083, lr 6.315275538432225e-05
Step 85800 Loss: 0.2076, lr 6.315275538432225e-05
Step 85900 Loss: 0.2071, lr 6.315275538432225e-05
Step 86000 Loss: 0.2069, lr 6.315275538432225e-05
Step 86100 Loss: 0.2062, lr 6.315275538432225e-05
Step 86200 Loss: 0.2060, lr 6.315275538432225e-05
Step 86300 Loss: 0.2054, lr 6.315275538432225e-05
Step 86400 Loss: 0.2053, lr 6.315275538432225e-05
Step 86500 Loss: 0.2048, lr 6.315275538432225e-05
Step 86600 Loss: 0.2047, lr 6.315275538432225e-05
Step 86700 Loss: 0.2044, lr 6.315275538432225e-05
Step 86800 Loss: 0.2043, lr 6.315275538432225e-05
Step 86900 Loss: 0.2045, lr 6.315275538432225e-05
Step 87000 Loss: 0.2048, lr 6.315275538432225e-05
Step 87100 Loss: 0.2053, lr 6.315275538432225e-05
Step 87200 Loss: 0.2057, lr 6.315275538432225e-05
Step 87300 Loss: 0.2064, lr 6.315275538432225e-05
Step 87400 Loss: 0.2072, lr 6.315275538432225e-05
Step 87500 Loss: 0.2085, lr 6.315275538432225e-05
Step 87600 Loss: 0.2092, lr 6.315275538432225e-05
Step 87700 Loss: 0.2088, lr 6.315275538432225e-05
Step 87800 Loss: 0.2085, lr 6.315275538432225e-05
Train Epoch: [53/100] Loss: 0.2095,lr 0.000063
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Step 87900 Loss: 0.5652, lr 6.27593349015375e-05
Step 88000 Loss: 0.4483, lr 6.27593349015375e-05
Step 88100 Loss: 0.4003, lr 6.27593349015375e-05
Step 88200 Loss: 0.3716, lr 6.27593349015375e-05
Step 88300 Loss: 0.3551, lr 6.27593349015375e-05
Step 88400 Loss: 0.3406, lr 6.27593349015375e-05
Step 88500 Loss: 0.3267, lr 6.27593349015375e-05
Step 88600 Loss: 0.3173, lr 6.27593349015375e-05
Step 88700 Loss: 0.3087, lr 6.27593349015375e-05
Step 88800 Loss: 0.3009, lr 6.27593349015375e-05
Step 88900 Loss: 0.2941, lr 6.27593349015375e-05
Step 89000 Loss: 0.2880, lr 6.27593349015375e-05
Step 89100 Loss: 0.2831, lr 6.27593349015375e-05
Step 89200 Loss: 0.2770, lr 6.27593349015375e-05
Step 89300 Loss: 0.2727, lr 6.27593349015375e-05
Step 89400 Loss: 0.2693, lr 6.27593349015375e-05
Step 89500 Loss: 0.2659, lr 6.27593349015375e-05
Step 89600 Loss: 0.2626, lr 6.27593349015375e-05
Step 89700 Loss: 0.2594, lr 6.27593349015375e-05
Step 89800 Loss: 0.2559, lr 6.27593349015375e-05
Step 89900 Loss: 0.2520, lr 6.27593349015375e-05
Step 90000 Loss: 0.2494, lr 6.27593349015375e-05
Step 90100 Loss: 0.2467, lr 6.27593349015375e-05
Step 90200 Loss: 0.2439, lr 6.27593349015375e-05
Step 90300 Loss: 0.2408, lr 6.27593349015375e-05
Step 90400 Loss: 0.2384, lr 6.27593349015375e-05
Step 90500 Loss: 0.2362, lr 6.27593349015375e-05
Step 90600 Loss: 0.2342, lr 6.27593349015375e-05
Step 90700 Loss: 0.2321, lr 6.27593349015375e-05
Step 90800 Loss: 0.2304, lr 6.27593349015375e-05
Step 90900 Loss: 0.2282, lr 6.27593349015375e-05
Step 91000 Loss: 0.2261, lr 6.27593349015375e-05
Step 91100 Loss: 0.2242, lr 6.27593349015375e-05
Step 91200 Loss: 0.2220, lr 6.27593349015375e-05
Step 91300 Loss: 0.2202, lr 6.27593349015375e-05
Step 91400 Loss: 0.2184, lr 6.27593349015375e-05
Step 91500 Loss: 0.2168, lr 6.27593349015375e-05
Step 91600 Loss: 0.2154, lr 6.27593349015375e-05
Step 91700 Loss: 0.2139, lr 6.27593349015375e-05
Step 91800 Loss: 0.2129, lr 6.27593349015375e-05
Step 91900 Loss: 0.2119, lr 6.27593349015375e-05
Step 92000 Loss: 0.2110, lr 6.27593349015375e-05
Step 92100 Loss: 0.2099, lr 6.27593349015375e-05
Step 92200 Loss: 0.2090, lr 6.27593349015375e-05
Step 92300 Loss: 0.2083, lr 6.27593349015375e-05
Step 92400 Loss: 0.2073, lr 6.27593349015375e-05
Step 92500 Loss: 0.2063, lr 6.27593349015375e-05
Step 92600 Loss: 0.2056, lr 6.27593349015375e-05
Step 92700 Loss: 0.2054, lr 6.27593349015375e-05
Step 92800 Loss: 0.2049, lr 6.27593349015375e-05
Step 92900 Loss: 0.2044, lr 6.27593349015375e-05
Step 93000 Loss: 0.2043, lr 6.27593349015375e-05
Step 93100 Loss: 0.2037, lr 6.27593349015375e-05
Step 93200 Loss: 0.2035, lr 6.27593349015375e-05
Step 93300 Loss: 0.2033, lr 6.27593349015375e-05
Step 93400 Loss: 0.2029, lr 6.27593349015375e-05
Step 93500 Loss: 0.2029, lr 6.27593349015375e-05
Step 93600 Loss: 0.2027, lr 6.27593349015375e-05
Step 93700 Loss: 0.2030, lr 6.27593349015375e-05
Step 93800 Loss: 0.2032, lr 6.27593349015375e-05
Step 93900 Loss: 0.2037, lr 6.27593349015375e-05
Step 94000 Loss: 0.2042, lr 6.27593349015375e-05
Step 94100 Loss: 0.2050, lr 6.27593349015375e-05
Step 94200 Loss: 0.2057, lr 6.27593349015375e-05
Step 94300 Loss: 0.2067, lr 6.27593349015375e-05
Step 94400 Loss: 0.2071, lr 6.27593349015375e-05
Step 94500 Loss: 0.2065, lr 6.27593349015375e-05
Train Epoch: [54/100] Loss: 0.2067,lr 0.000063
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Step 94600 Loss: 0.9153, lr 6.233627457557906e-05
Step 94700 Loss: 0.4937, lr 6.233627457557906e-05
Step 94800 Loss: 0.4206, lr 6.233627457557906e-05
Step 94900 Loss: 0.3835, lr 6.233627457557906e-05
Step 95000 Loss: 0.3633, lr 6.233627457557906e-05
Step 95100 Loss: 0.3488, lr 6.233627457557906e-05
Step 95200 Loss: 0.3333, lr 6.233627457557906e-05
Step 95300 Loss: 0.3205, lr 6.233627457557906e-05
Step 95400 Loss: 0.3133, lr 6.233627457557906e-05
Step 95500 Loss: 0.3038, lr 6.233627457557906e-05
Step 95600 Loss: 0.2963, lr 6.233627457557906e-05
Step 95700 Loss: 0.2902, lr 6.233627457557906e-05
Step 95800 Loss: 0.2840, lr 6.233627457557906e-05
Step 95900 Loss: 0.2785, lr 6.233627457557906e-05
Step 96000 Loss: 0.2735, lr 6.233627457557906e-05
Step 96100 Loss: 0.2700, lr 6.233627457557906e-05
Step 96200 Loss: 0.2659, lr 6.233627457557906e-05
Step 96300 Loss: 0.2629, lr 6.233627457557906e-05
Step 96400 Loss: 0.2598, lr 6.233627457557906e-05
Step 96500 Loss: 0.2566, lr 6.233627457557906e-05
Step 96600 Loss: 0.2528, lr 6.233627457557906e-05
Step 96700 Loss: 0.2494, lr 6.233627457557906e-05
Step 96800 Loss: 0.2471, lr 6.233627457557906e-05
Step 96900 Loss: 0.2443, lr 6.233627457557906e-05
Step 97000 Loss: 0.2418, lr 6.233627457557906e-05
Step 97100 Loss: 0.2390, lr 6.233627457557906e-05
Step 97200 Loss: 0.2364, lr 6.233627457557906e-05
Step 97300 Loss: 0.2346, lr 6.233627457557906e-05
Step 97400 Loss: 0.2328, lr 6.233627457557906e-05
Step 97500 Loss: 0.2309, lr 6.233627457557906e-05
Step 97600 Loss: 0.2290, lr 6.233627457557906e-05
Step 97700 Loss: 0.2269, lr 6.233627457557906e-05
Step 97800 Loss: 0.2248, lr 6.233627457557906e-05
Step 97900 Loss: 0.2229, lr 6.233627457557906e-05
Step 98000 Loss: 0.2210, lr 6.233627457557906e-05
Step 98100 Loss: 0.2191, lr 6.233627457557906e-05
Step 98200 Loss: 0.2173, lr 6.233627457557906e-05
Step 98300 Loss: 0.2157, lr 6.233627457557906e-05
Step 98400 Loss: 0.2145, lr 6.233627457557906e-05
Step 98500 Loss: 0.2131, lr 6.233627457557906e-05
Step 98600 Loss: 0.2122, lr 6.233627457557906e-05
Step 98700 Loss: 0.2110, lr 6.233627457557906e-05
Step 98800 Loss: 0.2102, lr 6.233627457557906e-05
Step 98900 Loss: 0.2092, lr 6.233627457557906e-05
Step 99000 Loss: 0.2081, lr 6.233627457557906e-05
Step 99100 Loss: 0.2072, lr 6.233627457557906e-05
Step 99200 Loss: 0.2061, lr 6.233627457557906e-05
Step 99300 Loss: 0.2052, lr 6.233627457557906e-05
Step 99400 Loss: 0.2046, lr 6.233627457557906e-05
Step 99500 Loss: 0.2043, lr 6.233627457557906e-05
Step 99600 Loss: 0.2040, lr 6.233627457557906e-05
Step 99700 Loss: 0.2035, lr 6.233627457557906e-05
Step 99800 Loss: 0.2031, lr 6.233627457557906e-05
Step 99900 Loss: 0.2029, lr 6.233627457557906e-05
Step 100000 Loss: 0.2025, lr 6.233627457557906e-05
Step 100100 Loss: 0.2023, lr 6.233627457557906e-05
Step 100200 Loss: 0.2020, lr 6.233627457557906e-05
Step 100300 Loss: 0.2019, lr 6.233627457557906e-05
Step 100400 Loss: 0.2021, lr 6.233627457557906e-05
Step 100500 Loss: 0.2025, lr 6.233627457557906e-05
Step 100600 Loss: 0.2029, lr 6.233627457557906e-05
Step 100700 Loss: 0.2032, lr 6.233627457557906e-05
Step 100800 Loss: 0.2038, lr 6.233627457557906e-05
Step 100900 Loss: 0.2048, lr 6.233627457557906e-05
Step 101000 Loss: 0.2060, lr 6.233627457557906e-05
Step 101100 Loss: 0.2067, lr 6.233627457557906e-05
Step 101200 Loss: 0.2064, lr 6.233627457557906e-05
Step 101300 Loss: 0.2059, lr 6.233627457557906e-05
Train Epoch: [55/100] Loss: 0.2063,lr 0.000062
Model Saving at epoch 55
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6755
Step 101400 Loss: 0.5857, lr 6.188399191591192e-05
Step 101500 Loss: 0.4479, lr 6.188399191591192e-05
Step 101600 Loss: 0.3986, lr 6.188399191591192e-05
Step 101700 Loss: 0.3681, lr 6.188399191591192e-05
Step 101800 Loss: 0.3520, lr 6.188399191591192e-05
Step 101900 Loss: 0.3387, lr 6.188399191591192e-05
Step 102000 Loss: 0.3232, lr 6.188399191591192e-05
Step 102100 Loss: 0.3137, lr 6.188399191591192e-05
Step 102200 Loss: 0.3062, lr 6.188399191591192e-05
Step 102300 Loss: 0.2980, lr 6.188399191591192e-05
Step 102400 Loss: 0.2909, lr 6.188399191591192e-05
Step 102500 Loss: 0.2852, lr 6.188399191591192e-05
Step 102600 Loss: 0.2801, lr 6.188399191591192e-05
Step 102700 Loss: 0.2744, lr 6.188399191591192e-05
Step 102800 Loss: 0.2702, lr 6.188399191591192e-05
Step 102900 Loss: 0.2670, lr 6.188399191591192e-05
Step 103000 Loss: 0.2637, lr 6.188399191591192e-05
Step 103100 Loss: 0.2603, lr 6.188399191591192e-05
Step 103200 Loss: 0.2573, lr 6.188399191591192e-05
Step 103300 Loss: 0.2537, lr 6.188399191591192e-05
Step 103400 Loss: 0.2502, lr 6.188399191591192e-05
Step 103500 Loss: 0.2476, lr 6.188399191591192e-05
Step 103600 Loss: 0.2450, lr 6.188399191591192e-05
Step 103700 Loss: 0.2421, lr 6.188399191591192e-05
Step 103800 Loss: 0.2392, lr 6.188399191591192e-05
Step 103900 Loss: 0.2371, lr 6.188399191591192e-05
Step 104000 Loss: 0.2347, lr 6.188399191591192e-05
Step 104100 Loss: 0.2330, lr 6.188399191591192e-05
Step 104200 Loss: 0.2308, lr 6.188399191591192e-05
Step 104300 Loss: 0.2290, lr 6.188399191591192e-05
Step 104400 Loss: 0.2270, lr 6.188399191591192e-05
Step 104500 Loss: 0.2249, lr 6.188399191591192e-05
Step 104600 Loss: 0.2231, lr 6.188399191591192e-05
Step 104700 Loss: 0.2208, lr 6.188399191591192e-05
Step 104800 Loss: 0.2189, lr 6.188399191591192e-05
Step 104900 Loss: 0.2171, lr 6.188399191591192e-05
Step 105000 Loss: 0.2157, lr 6.188399191591192e-05
Step 105100 Loss: 0.2142, lr 6.188399191591192e-05
Step 105200 Loss: 0.2127, lr 6.188399191591192e-05
Step 105300 Loss: 0.2115, lr 6.188399191591192e-05
Step 105400 Loss: 0.2105, lr 6.188399191591192e-05
Step 105500 Loss: 0.2096, lr 6.188399191591192e-05
Step 105600 Loss: 0.2087, lr 6.188399191591192e-05
Step 105700 Loss: 0.2075, lr 6.188399191591192e-05
Step 105800 Loss: 0.2068, lr 6.188399191591192e-05
Step 105900 Loss: 0.2057, lr 6.188399191591192e-05
Step 106000 Loss: 0.2045, lr 6.188399191591192e-05
Step 106100 Loss: 0.2039, lr 6.188399191591192e-05
Step 106200 Loss: 0.2035, lr 6.188399191591192e-05
Step 106300 Loss: 0.2031, lr 6.188399191591192e-05
Step 106400 Loss: 0.2026, lr 6.188399191591192e-05
Step 106500 Loss: 0.2024, lr 6.188399191591192e-05
Step 106600 Loss: 0.2019, lr 6.188399191591192e-05
Step 106700 Loss: 0.2017, lr 6.188399191591192e-05
Step 106800 Loss: 0.2014, lr 6.188399191591192e-05
Step 106900 Loss: 0.2014, lr 6.188399191591192e-05
Step 107000 Loss: 0.2011, lr 6.188399191591192e-05
Step 107100 Loss: 0.2012, lr 6.188399191591192e-05
Step 107200 Loss: 0.2013, lr 6.188399191591192e-05
Step 107300 Loss: 0.2015, lr 6.188399191591192e-05
Step 107400 Loss: 0.2018, lr 6.188399191591192e-05
Step 107500 Loss: 0.2022, lr 6.188399191591192e-05
Step 107600 Loss: 0.2033, lr 6.188399191591192e-05
Step 107700 Loss: 0.2041, lr 6.188399191591192e-05
Step 107800 Loss: 0.2050, lr 6.188399191591192e-05
Step 107900 Loss: 0.2055, lr 6.188399191591192e-05
Step 108000 Loss: 0.2051, lr 6.188399191591192e-05
Train Epoch: [56/100] Loss: 0.2053,lr 0.000062
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6756
Step 108100 Loss: 1.2368, lr 6.140293327091643e-05
Step 108200 Loss: 0.4895, lr 6.140293327091643e-05
Step 108300 Loss: 0.4125, lr 6.140293327091643e-05
Step 108400 Loss: 0.3754, lr 6.140293327091643e-05
Step 108500 Loss: 0.3559, lr 6.140293327091643e-05
Step 108600 Loss: 0.3415, lr 6.140293327091643e-05
Step 108700 Loss: 0.3272, lr 6.140293327091643e-05
Step 108800 Loss: 0.3155, lr 6.140293327091643e-05
Step 108900 Loss: 0.3084, lr 6.140293327091643e-05
Step 109000 Loss: 0.3001, lr 6.140293327091643e-05
Step 109100 Loss: 0.2930, lr 6.140293327091643e-05
Step 109200 Loss: 0.2874, lr 6.140293327091643e-05
Step 109300 Loss: 0.2813, lr 6.140293327091643e-05
Step 109400 Loss: 0.2767, lr 6.140293327091643e-05
Step 109500 Loss: 0.2715, lr 6.140293327091643e-05
Step 109600 Loss: 0.2681, lr 6.140293327091643e-05
Step 109700 Loss: 0.2640, lr 6.140293327091643e-05
Step 109800 Loss: 0.2610, lr 6.140293327091643e-05
Step 109900 Loss: 0.2579, lr 6.140293327091643e-05
Step 110000 Loss: 0.2549, lr 6.140293327091643e-05
Step 110100 Loss: 0.2510, lr 6.140293327091643e-05
Step 110200 Loss: 0.2474, lr 6.140293327091643e-05
Step 110300 Loss: 0.2450, lr 6.140293327091643e-05
Step 110400 Loss: 0.2422, lr 6.140293327091643e-05
Step 110500 Loss: 0.2396, lr 6.140293327091643e-05
Step 110600 Loss: 0.2369, lr 6.140293327091643e-05
Step 110700 Loss: 0.2345, lr 6.140293327091643e-05
Step 110800 Loss: 0.2326, lr 6.140293327091643e-05
Step 110900 Loss: 0.2305, lr 6.140293327091643e-05
Step 111000 Loss: 0.2286, lr 6.140293327091643e-05
Step 111100 Loss: 0.2267, lr 6.140293327091643e-05
Step 111200 Loss: 0.2246, lr 6.140293327091643e-05
Step 111300 Loss: 0.2226, lr 6.140293327091643e-05
Step 111400 Loss: 0.2206, lr 6.140293327091643e-05
Step 111500 Loss: 0.2187, lr 6.140293327091643e-05
Step 111600 Loss: 0.2171, lr 6.140293327091643e-05
Step 111700 Loss: 0.2151, lr 6.140293327091643e-05
Step 111800 Loss: 0.2136, lr 6.140293327091643e-05
Step 111900 Loss: 0.2123, lr 6.140293327091643e-05
Step 112000 Loss: 0.2111, lr 6.140293327091643e-05
Step 112100 Loss: 0.2102, lr 6.140293327091643e-05
Step 112200 Loss: 0.2090, lr 6.140293327091643e-05
Step 112300 Loss: 0.2082, lr 6.140293327091643e-05
Step 112400 Loss: 0.2073, lr 6.140293327091643e-05
Step 112500 Loss: 0.2061, lr 6.140293327091643e-05
Step 112600 Loss: 0.2053, lr 6.140293327091643e-05
Step 112700 Loss: 0.2040, lr 6.140293327091643e-05
Step 112800 Loss: 0.2032, lr 6.140293327091643e-05
Step 112900 Loss: 0.2027, lr 6.140293327091643e-05
Step 113000 Loss: 0.2022, lr 6.140293327091643e-05
Step 113100 Loss: 0.2017, lr 6.140293327091643e-05
Step 113200 Loss: 0.2014, lr 6.140293327091643e-05
Step 113300 Loss: 0.2010, lr 6.140293327091643e-05
Step 113400 Loss: 0.2006, lr 6.140293327091643e-05
Step 113500 Loss: 0.2002, lr 6.140293327091643e-05
Step 113600 Loss: 0.2001, lr 6.140293327091643e-05
Step 113700 Loss: 0.1998, lr 6.140293327091643e-05
Step 113800 Loss: 0.1996, lr 6.140293327091643e-05
Step 113900 Loss: 0.1998, lr 6.140293327091643e-05
Step 114000 Loss: 0.1998, lr 6.140293327091643e-05
Step 114100 Loss: 0.2004, lr 6.140293327091643e-05
Step 114200 Loss: 0.2007, lr 6.140293327091643e-05
Step 114300 Loss: 0.2014, lr 6.140293327091643e-05
Step 114400 Loss: 0.2025, lr 6.140293327091643e-05
Step 114500 Loss: 0.2036, lr 6.140293327091643e-05
Step 114600 Loss: 0.2046, lr 6.140293327091643e-05
Step 114700 Loss: 0.2043, lr 6.140293327091643e-05
Step 114800 Loss: 0.2039, lr 6.140293327091643e-05
Train Epoch: [57/100] Loss: 0.2043,lr 0.000061
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6755
Step 114900 Loss: 0.6516, lr 6.0893573387396314e-05
Step 115000 Loss: 0.4593, lr 6.0893573387396314e-05
Step 115100 Loss: 0.4026, lr 6.0893573387396314e-05
Step 115200 Loss: 0.3709, lr 6.0893573387396314e-05
Step 115300 Loss: 0.3531, lr 6.0893573387396314e-05
Step 115400 Loss: 0.3385, lr 6.0893573387396314e-05
Step 115500 Loss: 0.3239, lr 6.0893573387396314e-05
Step 115600 Loss: 0.3138, lr 6.0893573387396314e-05
Step 115700 Loss: 0.3058, lr 6.0893573387396314e-05
Step 115800 Loss: 0.2978, lr 6.0893573387396314e-05
Step 115900 Loss: 0.2912, lr 6.0893573387396314e-05
Step 116000 Loss: 0.2853, lr 6.0893573387396314e-05
Step 116100 Loss: 0.2799, lr 6.0893573387396314e-05
Step 116200 Loss: 0.2747, lr 6.0893573387396314e-05
Step 116300 Loss: 0.2696, lr 6.0893573387396314e-05
Step 116400 Loss: 0.2663, lr 6.0893573387396314e-05
Step 116500 Loss: 0.2629, lr 6.0893573387396314e-05
Step 116600 Loss: 0.2593, lr 6.0893573387396314e-05
Step 116700 Loss: 0.2561, lr 6.0893573387396314e-05
Step 116800 Loss: 0.2526, lr 6.0893573387396314e-05
Step 116900 Loss: 0.2493, lr 6.0893573387396314e-05
Step 117000 Loss: 0.2464, lr 6.0893573387396314e-05
Step 117100 Loss: 0.2438, lr 6.0893573387396314e-05
Step 117200 Loss: 0.2408, lr 6.0893573387396314e-05
Step 117300 Loss: 0.2382, lr 6.0893573387396314e-05
Step 117400 Loss: 0.2359, lr 6.0893573387396314e-05
Step 117500 Loss: 0.2337, lr 6.0893573387396314e-05
Step 117600 Loss: 0.2318, lr 6.0893573387396314e-05
Step 117700 Loss: 0.2298, lr 6.0893573387396314e-05
Step 117800 Loss: 0.2281, lr 6.0893573387396314e-05
Step 117900 Loss: 0.2261, lr 6.0893573387396314e-05
Step 118000 Loss: 0.2239, lr 6.0893573387396314e-05
Step 118100 Loss: 0.2220, lr 6.0893573387396314e-05
Step 118200 Loss: 0.2197, lr 6.0893573387396314e-05
Step 118300 Loss: 0.2179, lr 6.0893573387396314e-05
Step 118400 Loss: 0.2161, lr 6.0893573387396314e-05
Step 118500 Loss: 0.2145, lr 6.0893573387396314e-05
Step 118600 Loss: 0.2128, lr 6.0893573387396314e-05
Step 118700 Loss: 0.2115, lr 6.0893573387396314e-05
Step 118800 Loss: 0.2103, lr 6.0893573387396314e-05
Step 118900 Loss: 0.2093, lr 6.0893573387396314e-05
Step 119000 Loss: 0.2083, lr 6.0893573387396314e-05
Step 119100 Loss: 0.2075, lr 6.0893573387396314e-05
Step 119200 Loss: 0.2064, lr 6.0893573387396314e-05
Step 119300 Loss: 0.2056, lr 6.0893573387396314e-05
Step 119400 Loss: 0.2044, lr 6.0893573387396314e-05
Step 119500 Loss: 0.2033, lr 6.0893573387396314e-05
Step 119600 Loss: 0.2027, lr 6.0893573387396314e-05
Step 119700 Loss: 0.2022, lr 6.0893573387396314e-05
Step 119800 Loss: 0.2019, lr 6.0893573387396314e-05
Step 119900 Loss: 0.2013, lr 6.0893573387396314e-05
Step 120000 Loss: 0.2013, lr 6.0893573387396314e-05
Step 120100 Loss: 0.2007, lr 6.0893573387396314e-05
Step 120200 Loss: 0.2003, lr 6.0893573387396314e-05
Step 120300 Loss: 0.1999, lr 6.0893573387396314e-05
Step 120400 Loss: 0.1996, lr 6.0893573387396314e-05
Step 120500 Loss: 0.1994, lr 6.0893573387396314e-05
Step 120600 Loss: 0.1996, lr 6.0893573387396314e-05
Step 120700 Loss: 0.1997, lr 6.0893573387396314e-05
Step 120800 Loss: 0.1999, lr 6.0893573387396314e-05
Step 120900 Loss: 0.2003, lr 6.0893573387396314e-05
Step 121000 Loss: 0.2006, lr 6.0893573387396314e-05
Step 121100 Loss: 0.2015, lr 6.0893573387396314e-05
Step 121200 Loss: 0.2021, lr 6.0893573387396314e-05
Step 121300 Loss: 0.2031, lr 6.0893573387396314e-05
Step 121400 Loss: 0.2035, lr 6.0893573387396314e-05
Step 121500 Loss: 0.2031, lr 6.0893573387396314e-05
Step 121600 Loss: 0.2031, lr 6.0893573387396314e-05
Train Epoch: [58/100] Loss: 0.2034,lr 0.000061
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 121700 Loss: 0.5186, lr 6.035641494206082e-05
Step 121800 Loss: 0.4263, lr 6.035641494206082e-05
Step 121900 Loss: 0.3828, lr 6.035641494206082e-05
Step 122000 Loss: 0.3605, lr 6.035641494206082e-05
Step 122100 Loss: 0.3459, lr 6.035641494206082e-05
Step 122200 Loss: 0.3307, lr 6.035641494206082e-05
Step 122300 Loss: 0.3187, lr 6.035641494206082e-05
Step 122400 Loss: 0.3099, lr 6.035641494206082e-05
Step 122500 Loss: 0.3014, lr 6.035641494206082e-05
Step 122600 Loss: 0.2935, lr 6.035641494206082e-05
Step 122700 Loss: 0.2873, lr 6.035641494206082e-05
Step 122800 Loss: 0.2813, lr 6.035641494206082e-05
Step 122900 Loss: 0.2761, lr 6.035641494206082e-05
Step 123000 Loss: 0.2701, lr 6.035641494206082e-05
Step 123100 Loss: 0.2668, lr 6.035641494206082e-05
Step 123200 Loss: 0.2629, lr 6.035641494206082e-05
Step 123300 Loss: 0.2598, lr 6.035641494206082e-05
Step 123400 Loss: 0.2566, lr 6.035641494206082e-05
Step 123500 Loss: 0.2533, lr 6.035641494206082e-05
Step 123600 Loss: 0.2499, lr 6.035641494206082e-05
Step 123700 Loss: 0.2464, lr 6.035641494206082e-05
Step 123800 Loss: 0.2438, lr 6.035641494206082e-05
Step 123900 Loss: 0.2411, lr 6.035641494206082e-05
Step 124000 Loss: 0.2385, lr 6.035641494206082e-05
Step 124100 Loss: 0.2356, lr 6.035641494206082e-05
Step 124200 Loss: 0.2333, lr 6.035641494206082e-05
Step 124300 Loss: 0.2313, lr 6.035641494206082e-05
Step 124400 Loss: 0.2297, lr 6.035641494206082e-05
Step 124500 Loss: 0.2277, lr 6.035641494206082e-05
Step 124600 Loss: 0.2259, lr 6.035641494206082e-05
Step 124700 Loss: 0.2239, lr 6.035641494206082e-05
Step 124800 Loss: 0.2218, lr 6.035641494206082e-05
Step 124900 Loss: 0.2199, lr 6.035641494206082e-05
Step 125000 Loss: 0.2180, lr 6.035641494206082e-05
Step 125100 Loss: 0.2163, lr 6.035641494206082e-05
Step 125200 Loss: 0.2145, lr 6.035641494206082e-05
Step 125300 Loss: 0.2129, lr 6.035641494206082e-05
Step 125400 Loss: 0.2117, lr 6.035641494206082e-05
Step 125500 Loss: 0.2103, lr 6.035641494206082e-05
Step 125600 Loss: 0.2094, lr 6.035641494206082e-05
Step 125700 Loss: 0.2082, lr 6.035641494206082e-05
Step 125800 Loss: 0.2075, lr 6.035641494206082e-05
Step 125900 Loss: 0.2066, lr 6.035641494206082e-05
Step 126000 Loss: 0.2055, lr 6.035641494206082e-05
Step 126100 Loss: 0.2046, lr 6.035641494206082e-05
Step 126200 Loss: 0.2034, lr 6.035641494206082e-05
Step 126300 Loss: 0.2025, lr 6.035641494206082e-05
Step 126400 Loss: 0.2019, lr 6.035641494206082e-05
Step 126500 Loss: 0.2014, lr 6.035641494206082e-05
Step 126600 Loss: 0.2010, lr 6.035641494206082e-05
Step 126700 Loss: 0.2006, lr 6.035641494206082e-05
Step 126800 Loss: 0.2003, lr 6.035641494206082e-05
Step 126900 Loss: 0.1997, lr 6.035641494206082e-05
Step 127000 Loss: 0.1994, lr 6.035641494206082e-05
Step 127100 Loss: 0.1991, lr 6.035641494206082e-05
Step 127200 Loss: 0.1988, lr 6.035641494206082e-05
Step 127300 Loss: 0.1987, lr 6.035641494206082e-05
Step 127400 Loss: 0.1986, lr 6.035641494206082e-05
Step 127500 Loss: 0.1988, lr 6.035641494206082e-05
Step 127600 Loss: 0.1991, lr 6.035641494206082e-05
Step 127700 Loss: 0.1995, lr 6.035641494206082e-05
Step 127800 Loss: 0.1998, lr 6.035641494206082e-05
Step 127900 Loss: 0.2006, lr 6.035641494206082e-05
Step 128000 Loss: 0.2015, lr 6.035641494206082e-05
Step 128100 Loss: 0.2024, lr 6.035641494206082e-05
Step 128200 Loss: 0.2024, lr 6.035641494206082e-05
Step 128300 Loss: 0.2019, lr 6.035641494206082e-05
Train Epoch: [59/100] Loss: 0.2026,lr 0.000060
Calling G2SDataset.batch()
Done, time:  2.31 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6756
Step 128400 Loss: 0.6259, lr 5.979198804544357e-05
Step 128500 Loss: 0.4486, lr 5.979198804544357e-05
Step 128600 Loss: 0.3932, lr 5.979198804544357e-05
Step 128700 Loss: 0.3639, lr 5.979198804544357e-05
Step 128800 Loss: 0.3456, lr 5.979198804544357e-05
Step 128900 Loss: 0.3331, lr 5.979198804544357e-05
Step 129000 Loss: 0.3197, lr 5.979198804544357e-05
Step 129100 Loss: 0.3084, lr 5.979198804544357e-05
Step 129200 Loss: 0.3012, lr 5.979198804544357e-05
Step 129300 Loss: 0.2928, lr 5.979198804544357e-05
Step 129400 Loss: 0.2856, lr 5.979198804544357e-05
Step 129500 Loss: 0.2802, lr 5.979198804544357e-05
Step 129600 Loss: 0.2749, lr 5.979198804544357e-05
Step 129700 Loss: 0.2699, lr 5.979198804544357e-05
Step 129800 Loss: 0.2650, lr 5.979198804544357e-05
Step 129900 Loss: 0.2619, lr 5.979198804544357e-05
Step 130000 Loss: 0.2584, lr 5.979198804544357e-05
Step 130100 Loss: 0.2550, lr 5.979198804544357e-05
Step 130200 Loss: 0.2521, lr 5.979198804544357e-05
Step 130300 Loss: 0.2488, lr 5.979198804544357e-05
Step 130400 Loss: 0.2451, lr 5.979198804544357e-05
Step 130500 Loss: 0.2423, lr 5.979198804544357e-05
Step 130600 Loss: 0.2398, lr 5.979198804544357e-05
Step 130700 Loss: 0.2369, lr 5.979198804544357e-05
Step 130800 Loss: 0.2342, lr 5.979198804544357e-05
Step 130900 Loss: 0.2318, lr 5.979198804544357e-05
Step 131000 Loss: 0.2295, lr 5.979198804544357e-05
Step 131100 Loss: 0.2278, lr 5.979198804544357e-05
Step 131200 Loss: 0.2257, lr 5.979198804544357e-05
Step 131300 Loss: 0.2240, lr 5.979198804544357e-05
Step 131400 Loss: 0.2219, lr 5.979198804544357e-05
Step 131500 Loss: 0.2198, lr 5.979198804544357e-05
Step 131600 Loss: 0.2180, lr 5.979198804544357e-05
Step 131700 Loss: 0.2159, lr 5.979198804544357e-05
Step 131800 Loss: 0.2141, lr 5.979198804544357e-05
Step 131900 Loss: 0.2126, lr 5.979198804544357e-05
Step 132000 Loss: 0.2109, lr 5.979198804544357e-05
Step 132100 Loss: 0.2094, lr 5.979198804544357e-05
Step 132200 Loss: 0.2082, lr 5.979198804544357e-05
Step 132300 Loss: 0.2071, lr 5.979198804544357e-05
Step 132400 Loss: 0.2062, lr 5.979198804544357e-05
Step 132500 Loss: 0.2052, lr 5.979198804544357e-05
Step 132600 Loss: 0.2044, lr 5.979198804544357e-05
Step 132700 Loss: 0.2033, lr 5.979198804544357e-05
Step 132800 Loss: 0.2023, lr 5.979198804544357e-05
Step 132900 Loss: 0.2013, lr 5.979198804544357e-05
Step 133000 Loss: 0.2002, lr 5.979198804544357e-05
Step 133100 Loss: 0.1993, lr 5.979198804544357e-05
Step 133200 Loss: 0.1988, lr 5.979198804544357e-05
Step 133300 Loss: 0.1986, lr 5.979198804544357e-05
Step 133400 Loss: 0.1980, lr 5.979198804544357e-05
Step 133500 Loss: 0.1978, lr 5.979198804544357e-05
Step 133600 Loss: 0.1972, lr 5.979198804544357e-05
Step 133700 Loss: 0.1970, lr 5.979198804544357e-05
Step 133800 Loss: 0.1965, lr 5.979198804544357e-05
Step 133900 Loss: 0.1964, lr 5.979198804544357e-05
Step 134000 Loss: 0.1961, lr 5.979198804544357e-05
Step 134100 Loss: 0.1962, lr 5.979198804544357e-05
Step 134200 Loss: 0.1962, lr 5.979198804544357e-05
Step 134300 Loss: 0.1964, lr 5.979198804544357e-05
Step 134400 Loss: 0.1968, lr 5.979198804544357e-05
Step 134500 Loss: 0.1971, lr 5.979198804544357e-05
Step 134600 Loss: 0.1978, lr 5.979198804544357e-05
Step 134700 Loss: 0.1985, lr 5.979198804544357e-05
Step 134800 Loss: 0.1997, lr 5.979198804544357e-05
Step 134900 Loss: 0.2003, lr 5.979198804544357e-05
Step 135000 Loss: 0.1999, lr 5.979198804544357e-05
Step 135100 Loss: 0.1995, lr 5.979198804544357e-05
Train Epoch: [60/100] Loss: 0.1998,lr 0.000060
Model Saving at epoch 60
Calling G2SDataset.batch()
Done, time:  2.42 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.23 s, total batches: 6756
Step 135200 Loss: 0.4994, lr 5.920084971874745e-05
Step 135300 Loss: 0.4140, lr 5.920084971874745e-05
Step 135400 Loss: 0.3750, lr 5.920084971874745e-05
Step 135500 Loss: 0.3527, lr 5.920084971874745e-05
Step 135600 Loss: 0.3383, lr 5.920084971874745e-05
Step 135700 Loss: 0.3249, lr 5.920084971874745e-05
Step 135800 Loss: 0.3123, lr 5.920084971874745e-05
Step 135900 Loss: 0.3042, lr 5.920084971874745e-05
Step 136000 Loss: 0.2958, lr 5.920084971874745e-05
Step 136100 Loss: 0.2887, lr 5.920084971874745e-05
Step 136200 Loss: 0.2825, lr 5.920084971874745e-05
Step 136300 Loss: 0.2769, lr 5.920084971874745e-05
Step 136400 Loss: 0.2721, lr 5.920084971874745e-05
Step 136500 Loss: 0.2665, lr 5.920084971874745e-05
Step 136600 Loss: 0.2630, lr 5.920084971874745e-05
Step 136700 Loss: 0.2596, lr 5.920084971874745e-05
Step 136800 Loss: 0.2568, lr 5.920084971874745e-05
Step 136900 Loss: 0.2533, lr 5.920084971874745e-05
Step 137000 Loss: 0.2502, lr 5.920084971874745e-05
Step 137100 Loss: 0.2466, lr 5.920084971874745e-05
Step 137200 Loss: 0.2431, lr 5.920084971874745e-05
Step 137300 Loss: 0.2404, lr 5.920084971874745e-05
Step 137400 Loss: 0.2380, lr 5.920084971874745e-05
Step 137500 Loss: 0.2353, lr 5.920084971874745e-05
Step 137600 Loss: 0.2325, lr 5.920084971874745e-05
Step 137700 Loss: 0.2303, lr 5.920084971874745e-05
Step 137800 Loss: 0.2283, lr 5.920084971874745e-05
Step 137900 Loss: 0.2263, lr 5.920084971874745e-05
Step 138000 Loss: 0.2245, lr 5.920084971874745e-05
Step 138100 Loss: 0.2226, lr 5.920084971874745e-05
Step 138200 Loss: 0.2205, lr 5.920084971874745e-05
Step 138300 Loss: 0.2187, lr 5.920084971874745e-05
Step 138400 Loss: 0.2169, lr 5.920084971874745e-05
Step 138500 Loss: 0.2148, lr 5.920084971874745e-05
Step 138600 Loss: 0.2130, lr 5.920084971874745e-05
Step 138700 Loss: 0.2112, lr 5.920084971874745e-05
Step 138800 Loss: 0.2096, lr 5.920084971874745e-05
Step 138900 Loss: 0.2084, lr 5.920084971874745e-05
Step 139000 Loss: 0.2070, lr 5.920084971874745e-05
Step 139100 Loss: 0.2061, lr 5.920084971874745e-05
Step 139200 Loss: 0.2049, lr 5.920084971874745e-05
Step 139300 Loss: 0.2043, lr 5.920084971874745e-05
Step 139400 Loss: 0.2031, lr 5.920084971874745e-05
Step 139500 Loss: 0.2023, lr 5.920084971874745e-05
Step 139600 Loss: 0.2013, lr 5.920084971874745e-05
Step 139700 Loss: 0.2002, lr 5.920084971874745e-05
Step 139800 Loss: 0.1992, lr 5.920084971874745e-05
Step 139900 Loss: 0.1987, lr 5.920084971874745e-05
Step 140000 Loss: 0.1981, lr 5.920084971874745e-05
Step 140100 Loss: 0.1976, lr 5.920084971874745e-05
Step 140200 Loss: 0.1973, lr 5.920084971874745e-05
Step 140300 Loss: 0.1969, lr 5.920084971874745e-05
Step 140400 Loss: 0.1964, lr 5.920084971874745e-05
Step 140500 Loss: 0.1960, lr 5.920084971874745e-05
Step 140600 Loss: 0.1958, lr 5.920084971874745e-05
Step 140700 Loss: 0.1956, lr 5.920084971874745e-05
Step 140800 Loss: 0.1954, lr 5.920084971874745e-05
Step 140900 Loss: 0.1955, lr 5.920084971874745e-05
Step 141000 Loss: 0.1957, lr 5.920084971874745e-05
Step 141100 Loss: 0.1960, lr 5.920084971874745e-05
Step 141200 Loss: 0.1962, lr 5.920084971874745e-05
Step 141300 Loss: 0.1969, lr 5.920084971874745e-05
Step 141400 Loss: 0.1976, lr 5.920084971874745e-05
Step 141500 Loss: 0.1984, lr 5.920084971874745e-05
Step 141600 Loss: 0.1993, lr 5.920084971874745e-05
Step 141700 Loss: 0.1994, lr 5.920084971874745e-05
Step 141800 Loss: 0.1989, lr 5.920084971874745e-05
Train Epoch: [61/100] Loss: 0.1990,lr 0.000059
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Step 141900 Loss: 0.7449, lr 5.8583583344131895e-05
Step 142000 Loss: 0.4597, lr 5.8583583344131895e-05
Step 142100 Loss: 0.4000, lr 5.8583583344131895e-05
Step 142200 Loss: 0.3675, lr 5.8583583344131895e-05
Step 142300 Loss: 0.3484, lr 5.8583583344131895e-05
Step 142400 Loss: 0.3367, lr 5.8583583344131895e-05
Step 142500 Loss: 0.3215, lr 5.8583583344131895e-05
Step 142600 Loss: 0.3103, lr 5.8583583344131895e-05
Step 142700 Loss: 0.3036, lr 5.8583583344131895e-05
Step 142800 Loss: 0.2962, lr 5.8583583344131895e-05
Step 142900 Loss: 0.2895, lr 5.8583583344131895e-05
Step 143000 Loss: 0.2842, lr 5.8583583344131895e-05
Step 143100 Loss: 0.2783, lr 5.8583583344131895e-05
Step 143200 Loss: 0.2732, lr 5.8583583344131895e-05
Step 143300 Loss: 0.2683, lr 5.8583583344131895e-05
Step 143400 Loss: 0.2650, lr 5.8583583344131895e-05
Step 143500 Loss: 0.2614, lr 5.8583583344131895e-05
Step 143600 Loss: 0.2577, lr 5.8583583344131895e-05
Step 143700 Loss: 0.2545, lr 5.8583583344131895e-05
Step 143800 Loss: 0.2513, lr 5.8583583344131895e-05
Step 143900 Loss: 0.2474, lr 5.8583583344131895e-05
Step 144000 Loss: 0.2442, lr 5.8583583344131895e-05
Step 144100 Loss: 0.2415, lr 5.8583583344131895e-05
Step 144200 Loss: 0.2384, lr 5.8583583344131895e-05
Step 144300 Loss: 0.2360, lr 5.8583583344131895e-05
Step 144400 Loss: 0.2332, lr 5.8583583344131895e-05
Step 144500 Loss: 0.2307, lr 5.8583583344131895e-05
Step 144600 Loss: 0.2290, lr 5.8583583344131895e-05
Step 144700 Loss: 0.2271, lr 5.8583583344131895e-05
Step 144800 Loss: 0.2251, lr 5.8583583344131895e-05
Step 144900 Loss: 0.2232, lr 5.8583583344131895e-05
Step 145000 Loss: 0.2211, lr 5.8583583344131895e-05
Step 145100 Loss: 0.2192, lr 5.8583583344131895e-05
Step 145200 Loss: 0.2172, lr 5.8583583344131895e-05
Step 145300 Loss: 0.2153, lr 5.8583583344131895e-05
Step 145400 Loss: 0.2136, lr 5.8583583344131895e-05
Step 145500 Loss: 0.2119, lr 5.8583583344131895e-05
Step 145600 Loss: 0.2102, lr 5.8583583344131895e-05
Step 145700 Loss: 0.2090, lr 5.8583583344131895e-05
Step 145800 Loss: 0.2077, lr 5.8583583344131895e-05
Step 145900 Loss: 0.2068, lr 5.8583583344131895e-05
Step 146000 Loss: 0.2056, lr 5.8583583344131895e-05
Step 146100 Loss: 0.2047, lr 5.8583583344131895e-05
Step 146200 Loss: 0.2037, lr 5.8583583344131895e-05
Step 146300 Loss: 0.2026, lr 5.8583583344131895e-05
Step 146400 Loss: 0.2016, lr 5.8583583344131895e-05
Step 146500 Loss: 0.2004, lr 5.8583583344131895e-05
Step 146600 Loss: 0.1997, lr 5.8583583344131895e-05
Step 146700 Loss: 0.1992, lr 5.8583583344131895e-05
Step 146800 Loss: 0.1988, lr 5.8583583344131895e-05
Step 146900 Loss: 0.1981, lr 5.8583583344131895e-05
Step 147000 Loss: 0.1978, lr 5.8583583344131895e-05
Step 147100 Loss: 0.1973, lr 5.8583583344131895e-05
Step 147200 Loss: 0.1969, lr 5.8583583344131895e-05
Step 147300 Loss: 0.1965, lr 5.8583583344131895e-05
Step 147400 Loss: 0.1962, lr 5.8583583344131895e-05
Step 147500 Loss: 0.1959, lr 5.8583583344131895e-05
Step 147600 Loss: 0.1958, lr 5.8583583344131895e-05
Step 147700 Loss: 0.1959, lr 5.8583583344131895e-05
Step 147800 Loss: 0.1959, lr 5.8583583344131895e-05
Step 147900 Loss: 0.1962, lr 5.8583583344131895e-05
Step 148000 Loss: 0.1964, lr 5.8583583344131895e-05
Step 148100 Loss: 0.1970, lr 5.8583583344131895e-05
Step 148200 Loss: 0.1980, lr 5.8583583344131895e-05
Step 148300 Loss: 0.1990, lr 5.8583583344131895e-05
Step 148400 Loss: 0.1998, lr 5.8583583344131895e-05
Step 148500 Loss: 0.1994, lr 5.8583583344131895e-05
Step 148600 Loss: 0.1989, lr 5.8583583344131895e-05
Train Epoch: [62/100] Loss: 0.1995,lr 0.000059
Calling G2SDataset.batch()
Done, time:  2.32 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 148700 Loss: 0.5387, lr 5.79407980889852e-05
Step 148800 Loss: 0.4298, lr 5.79407980889852e-05
Step 148900 Loss: 0.3861, lr 5.79407980889852e-05
Step 149000 Loss: 0.3596, lr 5.79407980889852e-05
Step 149100 Loss: 0.3449, lr 5.79407980889852e-05
Step 149200 Loss: 0.3312, lr 5.79407980889852e-05
Step 149300 Loss: 0.3171, lr 5.79407980889852e-05
Step 149400 Loss: 0.3087, lr 5.79407980889852e-05
Step 149500 Loss: 0.3003, lr 5.79407980889852e-05
Step 149600 Loss: 0.2925, lr 5.79407980889852e-05
Step 149700 Loss: 0.2859, lr 5.79407980889852e-05
Step 149800 Loss: 0.2806, lr 5.79407980889852e-05
Step 149900 Loss: 0.2755, lr 5.79407980889852e-05
Step 150000 Loss: 0.2695, lr 5.79407980889852e-05
Step 150100 Loss: 0.2652, lr 5.79407980889852e-05
Step 150200 Loss: 0.2620, lr 5.79407980889852e-05
Step 150300 Loss: 0.2588, lr 5.79407980889852e-05
Step 150400 Loss: 0.2551, lr 5.79407980889852e-05
Step 150500 Loss: 0.2521, lr 5.79407980889852e-05
Step 150600 Loss: 0.2485, lr 5.79407980889852e-05
Step 150700 Loss: 0.2449, lr 5.79407980889852e-05
Step 150800 Loss: 0.2423, lr 5.79407980889852e-05
Step 150900 Loss: 0.2396, lr 5.79407980889852e-05
Step 151000 Loss: 0.2368, lr 5.79407980889852e-05
Step 151100 Loss: 0.2340, lr 5.79407980889852e-05
Step 151200 Loss: 0.2319, lr 5.79407980889852e-05
Step 151300 Loss: 0.2296, lr 5.79407980889852e-05
Step 151400 Loss: 0.2276, lr 5.79407980889852e-05
Step 151500 Loss: 0.2257, lr 5.79407980889852e-05
Step 151600 Loss: 0.2241, lr 5.79407980889852e-05
Step 151700 Loss: 0.2219, lr 5.79407980889852e-05
Step 151800 Loss: 0.2198, lr 5.79407980889852e-05
Step 151900 Loss: 0.2180, lr 5.79407980889852e-05
Step 152000 Loss: 0.2159, lr 5.79407980889852e-05
Step 152100 Loss: 0.2141, lr 5.79407980889852e-05
Step 152200 Loss: 0.2123, lr 5.79407980889852e-05
Step 152300 Loss: 0.2108, lr 5.79407980889852e-05
Step 152400 Loss: 0.2095, lr 5.79407980889852e-05
Step 152500 Loss: 0.2080, lr 5.79407980889852e-05
Step 152600 Loss: 0.2070, lr 5.79407980889852e-05
Step 152700 Loss: 0.2060, lr 5.79407980889852e-05
Step 152800 Loss: 0.2052, lr 5.79407980889852e-05
Step 152900 Loss: 0.2043, lr 5.79407980889852e-05
Step 153000 Loss: 0.2033, lr 5.79407980889852e-05
Step 153100 Loss: 0.2024, lr 5.79407980889852e-05
Step 153200 Loss: 0.2012, lr 5.79407980889852e-05
Step 153300 Loss: 0.2002, lr 5.79407980889852e-05
Step 153400 Loss: 0.1994, lr 5.79407980889852e-05
Step 153500 Loss: 0.1991, lr 5.79407980889852e-05
Step 153600 Loss: 0.1986, lr 5.79407980889852e-05
Step 153700 Loss: 0.1980, lr 5.79407980889852e-05
Step 153800 Loss: 0.1976, lr 5.79407980889852e-05
Step 153900 Loss: 0.1970, lr 5.79407980889852e-05
Step 154000 Loss: 0.1966, lr 5.79407980889852e-05
Step 154100 Loss: 0.1963, lr 5.79407980889852e-05
Step 154200 Loss: 0.1961, lr 5.79407980889852e-05
Step 154300 Loss: 0.1961, lr 5.79407980889852e-05
Step 154400 Loss: 0.1961, lr 5.79407980889852e-05
Step 154500 Loss: 0.1963, lr 5.79407980889852e-05
Step 154600 Loss: 0.1964, lr 5.79407980889852e-05
Step 154700 Loss: 0.1966, lr 5.79407980889852e-05
Step 154800 Loss: 0.1969, lr 5.79407980889852e-05
Step 154900 Loss: 0.1977, lr 5.79407980889852e-05
Step 155000 Loss: 0.1982, lr 5.79407980889852e-05
Step 155100 Loss: 0.1991, lr 5.79407980889852e-05
Step 155200 Loss: 0.1993, lr 5.79407980889852e-05
Step 155300 Loss: 0.1988, lr 5.79407980889852e-05
Train Epoch: [63/100] Loss: 0.1989,lr 0.000058
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6756
Step 155400 Loss: 1.0169, lr 5.7273128304749785e-05
Step 155500 Loss: 0.4895, lr 5.7273128304749785e-05
Step 155600 Loss: 0.4138, lr 5.7273128304749785e-05
Step 155700 Loss: 0.3756, lr 5.7273128304749785e-05
Step 155800 Loss: 0.3545, lr 5.7273128304749785e-05
Step 155900 Loss: 0.3403, lr 5.7273128304749785e-05
Step 156000 Loss: 0.3246, lr 5.7273128304749785e-05
Step 156100 Loss: 0.3125, lr 5.7273128304749785e-05
Step 156200 Loss: 0.3048, lr 5.7273128304749785e-05
Step 156300 Loss: 0.2957, lr 5.7273128304749785e-05
Step 156400 Loss: 0.2882, lr 5.7273128304749785e-05
Step 156500 Loss: 0.2822, lr 5.7273128304749785e-05
Step 156600 Loss: 0.2761, lr 5.7273128304749785e-05
Step 156700 Loss: 0.2709, lr 5.7273128304749785e-05
Step 156800 Loss: 0.2660, lr 5.7273128304749785e-05
Step 156900 Loss: 0.2625, lr 5.7273128304749785e-05
Step 157000 Loss: 0.2583, lr 5.7273128304749785e-05
Step 157100 Loss: 0.2557, lr 5.7273128304749785e-05
Step 157200 Loss: 0.2522, lr 5.7273128304749785e-05
Step 157300 Loss: 0.2491, lr 5.7273128304749785e-05
Step 157400 Loss: 0.2455, lr 5.7273128304749785e-05
Step 157500 Loss: 0.2421, lr 5.7273128304749785e-05
Step 157600 Loss: 0.2394, lr 5.7273128304749785e-05
Step 157700 Loss: 0.2365, lr 5.7273128304749785e-05
Step 157800 Loss: 0.2342, lr 5.7273128304749785e-05
Step 157900 Loss: 0.2315, lr 5.7273128304749785e-05
Step 158000 Loss: 0.2289, lr 5.7273128304749785e-05
Step 158100 Loss: 0.2270, lr 5.7273128304749785e-05
Step 158200 Loss: 0.2252, lr 5.7273128304749785e-05
Step 158300 Loss: 0.2231, lr 5.7273128304749785e-05
Step 158400 Loss: 0.2212, lr 5.7273128304749785e-05
Step 158500 Loss: 0.2191, lr 5.7273128304749785e-05
Step 158600 Loss: 0.2171, lr 5.7273128304749785e-05
Step 158700 Loss: 0.2151, lr 5.7273128304749785e-05
Step 158800 Loss: 0.2133, lr 5.7273128304749785e-05
Step 158900 Loss: 0.2117, lr 5.7273128304749785e-05
Step 159000 Loss: 0.2099, lr 5.7273128304749785e-05
Step 159100 Loss: 0.2083, lr 5.7273128304749785e-05
Step 159200 Loss: 0.2070, lr 5.7273128304749785e-05
Step 159300 Loss: 0.2058, lr 5.7273128304749785e-05
Step 159400 Loss: 0.2049, lr 5.7273128304749785e-05
Step 159500 Loss: 0.2038, lr 5.7273128304749785e-05
Step 159600 Loss: 0.2029, lr 5.7273128304749785e-05
Step 159700 Loss: 0.2020, lr 5.7273128304749785e-05
Step 159800 Loss: 0.2009, lr 5.7273128304749785e-05
Step 159900 Loss: 0.2001, lr 5.7273128304749785e-05
Step 160000 Loss: 0.1989, lr 5.7273128304749785e-05
Step 160100 Loss: 0.1980, lr 5.7273128304749785e-05
Step 160200 Loss: 0.1975, lr 5.7273128304749785e-05
Step 160300 Loss: 0.1971, lr 5.7273128304749785e-05
Step 160400 Loss: 0.1964, lr 5.7273128304749785e-05
Step 160500 Loss: 0.1961, lr 5.7273128304749785e-05
Step 160600 Loss: 0.1956, lr 5.7273128304749785e-05
Step 160700 Loss: 0.1953, lr 5.7273128304749785e-05
Step 160800 Loss: 0.1948, lr 5.7273128304749785e-05
Step 160900 Loss: 0.1945, lr 5.7273128304749785e-05
Step 161000 Loss: 0.1942, lr 5.7273128304749785e-05
Step 161100 Loss: 0.1940, lr 5.7273128304749785e-05
Step 161200 Loss: 0.1940, lr 5.7273128304749785e-05
Step 161300 Loss: 0.1942, lr 5.7273128304749785e-05
Step 161400 Loss: 0.1943, lr 5.7273128304749785e-05
Step 161500 Loss: 0.1946, lr 5.7273128304749785e-05
Step 161600 Loss: 0.1950, lr 5.7273128304749785e-05
Step 161700 Loss: 0.1959, lr 5.7273128304749785e-05
Step 161800 Loss: 0.1968, lr 5.7273128304749785e-05
Step 161900 Loss: 0.1977, lr 5.7273128304749785e-05
Step 162000 Loss: 0.1973, lr 5.7273128304749785e-05
Step 162100 Loss: 0.1969, lr 5.7273128304749785e-05
Train Epoch: [64/100] Loss: 0.1971,lr 0.000057
Calling G2SDataset.batch()
Done, time:  2.35 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Step 162200 Loss: 0.5438, lr 5.658123290089393e-05
Step 162300 Loss: 0.4201, lr 5.658123290089393e-05
Step 162400 Loss: 0.3774, lr 5.658123290089393e-05
Step 162500 Loss: 0.3520, lr 5.658123290089393e-05
Step 162600 Loss: 0.3377, lr 5.658123290089393e-05
Step 162700 Loss: 0.3254, lr 5.658123290089393e-05
Step 162800 Loss: 0.3113, lr 5.658123290089393e-05
Step 162900 Loss: 0.3026, lr 5.658123290089393e-05
Step 163000 Loss: 0.2957, lr 5.658123290089393e-05
Step 163100 Loss: 0.2881, lr 5.658123290089393e-05
Step 163200 Loss: 0.2816, lr 5.658123290089393e-05
Step 163300 Loss: 0.2760, lr 5.658123290089393e-05
Step 163400 Loss: 0.2711, lr 5.658123290089393e-05
Step 163500 Loss: 0.2659, lr 5.658123290089393e-05
Step 163600 Loss: 0.2617, lr 5.658123290089393e-05
Step 163700 Loss: 0.2587, lr 5.658123290089393e-05
Step 163800 Loss: 0.2554, lr 5.658123290089393e-05
Step 163900 Loss: 0.2520, lr 5.658123290089393e-05
Step 164000 Loss: 0.2488, lr 5.658123290089393e-05
Step 164100 Loss: 0.2453, lr 5.658123290089393e-05
Step 164200 Loss: 0.2418, lr 5.658123290089393e-05
Step 164300 Loss: 0.2390, lr 5.658123290089393e-05
Step 164400 Loss: 0.2364, lr 5.658123290089393e-05
Step 164500 Loss: 0.2336, lr 5.658123290089393e-05
Step 164600 Loss: 0.2310, lr 5.658123290089393e-05
Step 164700 Loss: 0.2288, lr 5.658123290089393e-05
Step 164800 Loss: 0.2264, lr 5.658123290089393e-05
Step 164900 Loss: 0.2247, lr 5.658123290089393e-05
Step 165000 Loss: 0.2226, lr 5.658123290089393e-05
Step 165100 Loss: 0.2209, lr 5.658123290089393e-05
Step 165200 Loss: 0.2191, lr 5.658123290089393e-05
Step 165300 Loss: 0.2171, lr 5.658123290089393e-05
Step 165400 Loss: 0.2153, lr 5.658123290089393e-05
Step 165500 Loss: 0.2130, lr 5.658123290089393e-05
Step 165600 Loss: 0.2114, lr 5.658123290089393e-05
Step 165700 Loss: 0.2096, lr 5.658123290089393e-05
Step 165800 Loss: 0.2081, lr 5.658123290089393e-05
Step 165900 Loss: 0.2067, lr 5.658123290089393e-05
Step 166000 Loss: 0.2053, lr 5.658123290089393e-05
Step 166100 Loss: 0.2040, lr 5.658123290089393e-05
Step 166200 Loss: 0.2031, lr 5.658123290089393e-05
Step 166300 Loss: 0.2021, lr 5.658123290089393e-05
Step 166400 Loss: 0.2012, lr 5.658123290089393e-05
Step 166500 Loss: 0.2002, lr 5.658123290089393e-05
Step 166600 Loss: 0.1994, lr 5.658123290089393e-05
Step 166700 Loss: 0.1981, lr 5.658123290089393e-05
Step 166800 Loss: 0.1971, lr 5.658123290089393e-05
Step 166900 Loss: 0.1964, lr 5.658123290089393e-05
Step 167000 Loss: 0.1958, lr 5.658123290089393e-05
Step 167100 Loss: 0.1954, lr 5.658123290089393e-05
Step 167200 Loss: 0.1950, lr 5.658123290089393e-05
Step 167300 Loss: 0.1949, lr 5.658123290089393e-05
Step 167400 Loss: 0.1942, lr 5.658123290089393e-05
Step 167500 Loss: 0.1940, lr 5.658123290089393e-05
Step 167600 Loss: 0.1936, lr 5.658123290089393e-05
Step 167700 Loss: 0.1935, lr 5.658123290089393e-05
Step 167800 Loss: 0.1932, lr 5.658123290089393e-05
Step 167900 Loss: 0.1931, lr 5.658123290089393e-05
Step 168000 Loss: 0.1932, lr 5.658123290089393e-05
Step 168100 Loss: 0.1932, lr 5.658123290089393e-05
Step 168200 Loss: 0.1935, lr 5.658123290089393e-05
Step 168300 Loss: 0.1938, lr 5.658123290089393e-05
Step 168400 Loss: 0.1946, lr 5.658123290089393e-05
Step 168500 Loss: 0.1952, lr 5.658123290089393e-05
Step 168600 Loss: 0.1960, lr 5.658123290089393e-05
Step 168700 Loss: 0.1965, lr 5.658123290089393e-05
Step 168800 Loss: 0.1959, lr 5.658123290089393e-05
Train Epoch: [65/100] Loss: 0.1960,lr 0.000057
Model Saving at epoch 65
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6756
Step 168900 Loss: 1.3014, lr 5.58657946946477e-05
Step 169000 Loss: 0.4803, lr 5.58657946946477e-05
Step 169100 Loss: 0.4026, lr 5.58657946946477e-05
Step 169200 Loss: 0.3668, lr 5.58657946946477e-05
Step 169300 Loss: 0.3488, lr 5.58657946946477e-05
Step 169400 Loss: 0.3339, lr 5.58657946946477e-05
Step 169500 Loss: 0.3196, lr 5.58657946946477e-05
Step 169600 Loss: 0.3085, lr 5.58657946946477e-05
Step 169700 Loss: 0.3015, lr 5.58657946946477e-05
Step 169800 Loss: 0.2926, lr 5.58657946946477e-05
Step 169900 Loss: 0.2851, lr 5.58657946946477e-05
Step 170000 Loss: 0.2798, lr 5.58657946946477e-05
Step 170100 Loss: 0.2742, lr 5.58657946946477e-05
Step 170200 Loss: 0.2689, lr 5.58657946946477e-05
Step 170300 Loss: 0.2635, lr 5.58657946946477e-05
Step 170400 Loss: 0.2605, lr 5.58657946946477e-05
Step 170500 Loss: 0.2564, lr 5.58657946946477e-05
Step 170600 Loss: 0.2536, lr 5.58657946946477e-05
Step 170700 Loss: 0.2503, lr 5.58657946946477e-05
Step 170800 Loss: 0.2470, lr 5.58657946946477e-05
Step 170900 Loss: 0.2433, lr 5.58657946946477e-05
Step 171000 Loss: 0.2397, lr 5.58657946946477e-05
Step 171100 Loss: 0.2372, lr 5.58657946946477e-05
Step 171200 Loss: 0.2344, lr 5.58657946946477e-05
Step 171300 Loss: 0.2318, lr 5.58657946946477e-05
Step 171400 Loss: 0.2290, lr 5.58657946946477e-05
Step 171500 Loss: 0.2266, lr 5.58657946946477e-05
Step 171600 Loss: 0.2247, lr 5.58657946946477e-05
Step 171700 Loss: 0.2230, lr 5.58657946946477e-05
Step 171800 Loss: 0.2210, lr 5.58657946946477e-05
Step 171900 Loss: 0.2192, lr 5.58657946946477e-05
Step 172000 Loss: 0.2170, lr 5.58657946946477e-05
Step 172100 Loss: 0.2151, lr 5.58657946946477e-05
Step 172200 Loss: 0.2131, lr 5.58657946946477e-05
Step 172300 Loss: 0.2114, lr 5.58657946946477e-05
Step 172400 Loss: 0.2097, lr 5.58657946946477e-05
Step 172500 Loss: 0.2079, lr 5.58657946946477e-05
Step 172600 Loss: 0.2064, lr 5.58657946946477e-05
Step 172700 Loss: 0.2052, lr 5.58657946946477e-05
Step 172800 Loss: 0.2038, lr 5.58657946946477e-05
Step 172900 Loss: 0.2029, lr 5.58657946946477e-05
Step 173000 Loss: 0.2017, lr 5.58657946946477e-05
Step 173100 Loss: 0.2009, lr 5.58657946946477e-05
Step 173200 Loss: 0.2000, lr 5.58657946946477e-05
Step 173300 Loss: 0.1988, lr 5.58657946946477e-05
Step 173400 Loss: 0.1978, lr 5.58657946946477e-05
Step 173500 Loss: 0.1967, lr 5.58657946946477e-05
Step 173600 Loss: 0.1958, lr 5.58657946946477e-05
Step 173700 Loss: 0.1953, lr 5.58657946946477e-05
Step 173800 Loss: 0.1948, lr 5.58657946946477e-05
Step 173900 Loss: 0.1941, lr 5.58657946946477e-05
Step 174000 Loss: 0.1938, lr 5.58657946946477e-05
Step 174100 Loss: 0.1933, lr 5.58657946946477e-05
Step 174200 Loss: 0.1929, lr 5.58657946946477e-05
Step 174300 Loss: 0.1925, lr 5.58657946946477e-05
Step 174400 Loss: 0.1921, lr 5.58657946946477e-05
Step 174500 Loss: 0.1918, lr 5.58657946946477e-05
Step 174600 Loss: 0.1916, lr 5.58657946946477e-05
Step 174700 Loss: 0.1915, lr 5.58657946946477e-05
Step 174800 Loss: 0.1917, lr 5.58657946946477e-05
Step 174900 Loss: 0.1919, lr 5.58657946946477e-05
Step 175000 Loss: 0.1921, lr 5.58657946946477e-05
Step 175100 Loss: 0.1925, lr 5.58657946946477e-05
Step 175200 Loss: 0.1933, lr 5.58657946946477e-05
Step 175300 Loss: 0.1941, lr 5.58657946946477e-05
Step 175400 Loss: 0.1950, lr 5.58657946946477e-05
Step 175500 Loss: 0.1946, lr 5.58657946946477e-05
Step 175600 Loss: 0.1943, lr 5.58657946946477e-05
Train Epoch: [66/100] Loss: 0.1947,lr 0.000056
Calling G2SDataset.batch()
Done, time:  2.34 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Step 175700 Loss: 0.6200, lr 5.512751973714478e-05
Step 175800 Loss: 0.4387, lr 5.512751973714478e-05
Step 175900 Loss: 0.3855, lr 5.512751973714478e-05
Step 176000 Loss: 0.3572, lr 5.512751973714478e-05
Step 176100 Loss: 0.3407, lr 5.512751973714478e-05
Step 176200 Loss: 0.3279, lr 5.512751973714478e-05
Step 176300 Loss: 0.3133, lr 5.512751973714478e-05
Step 176400 Loss: 0.3036, lr 5.512751973714478e-05
Step 176500 Loss: 0.2960, lr 5.512751973714478e-05
Step 176600 Loss: 0.2881, lr 5.512751973714478e-05
Step 176700 Loss: 0.2811, lr 5.512751973714478e-05
Step 176800 Loss: 0.2757, lr 5.512751973714478e-05
Step 176900 Loss: 0.2705, lr 5.512751973714478e-05
Step 177000 Loss: 0.2652, lr 5.512751973714478e-05
Step 177100 Loss: 0.2605, lr 5.512751973714478e-05
Step 177200 Loss: 0.2576, lr 5.512751973714478e-05
Step 177300 Loss: 0.2543, lr 5.512751973714478e-05
Step 177400 Loss: 0.2512, lr 5.512751973714478e-05
Step 177500 Loss: 0.2481, lr 5.512751973714478e-05
Step 177600 Loss: 0.2448, lr 5.512751973714478e-05
Step 177700 Loss: 0.2414, lr 5.512751973714478e-05
Step 177800 Loss: 0.2386, lr 5.512751973714478e-05
Step 177900 Loss: 0.2360, lr 5.512751973714478e-05
Step 178000 Loss: 0.2330, lr 5.512751973714478e-05
Step 178100 Loss: 0.2306, lr 5.512751973714478e-05
Step 178200 Loss: 0.2281, lr 5.512751973714478e-05
Step 178300 Loss: 0.2258, lr 5.512751973714478e-05
Step 178400 Loss: 0.2241, lr 5.512751973714478e-05
Step 178500 Loss: 0.2220, lr 5.512751973714478e-05
Step 178600 Loss: 0.2202, lr 5.512751973714478e-05
Step 178700 Loss: 0.2186, lr 5.512751973714478e-05
Step 178800 Loss: 0.2164, lr 5.512751973714478e-05
Step 178900 Loss: 0.2146, lr 5.512751973714478e-05
Step 179000 Loss: 0.2124, lr 5.512751973714478e-05
Step 179100 Loss: 0.2107, lr 5.512751973714478e-05
Step 179200 Loss: 0.2090, lr 5.512751973714478e-05
Step 179300 Loss: 0.2073, lr 5.512751973714478e-05
Step 179400 Loss: 0.2057, lr 5.512751973714478e-05
Step 179500 Loss: 0.2045, lr 5.512751973714478e-05
Step 179600 Loss: 0.2032, lr 5.512751973714478e-05
Step 179700 Loss: 0.2023, lr 5.512751973714478e-05
Step 179800 Loss: 0.2012, lr 5.512751973714478e-05
Step 179900 Loss: 0.2004, lr 5.512751973714478e-05
Step 180000 Loss: 0.1993, lr 5.512751973714478e-05
Step 180100 Loss: 0.1983, lr 5.512751973714478e-05
Step 180200 Loss: 0.1971, lr 5.512751973714478e-05
Step 180300 Loss: 0.1961, lr 5.512751973714478e-05
Step 180400 Loss: 0.1953, lr 5.512751973714478e-05
Step 180500 Loss: 0.1948, lr 5.512751973714478e-05
Step 180600 Loss: 0.1944, lr 5.512751973714478e-05
Step 180700 Loss: 0.1937, lr 5.512751973714478e-05
Step 180800 Loss: 0.1935, lr 5.512751973714478e-05
Step 180900 Loss: 0.1928, lr 5.512751973714478e-05
Step 181000 Loss: 0.1924, lr 5.512751973714478e-05
Step 181100 Loss: 0.1920, lr 5.512751973714478e-05
Step 181200 Loss: 0.1918, lr 5.512751973714478e-05
Step 181300 Loss: 0.1914, lr 5.512751973714478e-05
Step 181400 Loss: 0.1914, lr 5.512751973714478e-05
Step 181500 Loss: 0.1914, lr 5.512751973714478e-05
Step 181600 Loss: 0.1915, lr 5.512751973714478e-05
Step 181700 Loss: 0.1917, lr 5.512751973714478e-05
Step 181800 Loss: 0.1920, lr 5.512751973714478e-05
Step 181900 Loss: 0.1925, lr 5.512751973714478e-05
Step 182000 Loss: 0.1931, lr 5.512751973714478e-05
Step 182100 Loss: 0.1941, lr 5.512751973714478e-05
Step 182200 Loss: 0.1945, lr 5.512751973714478e-05
Step 182300 Loss: 0.1939, lr 5.512751973714478e-05
Step 182400 Loss: 0.1938, lr 5.512751973714478e-05
Train Epoch: [67/100] Loss: 0.1943,lr 0.000055
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Step 182500 Loss: 0.5031, lr 5.4367136616635176e-05
Step 182600 Loss: 0.4157, lr 5.4367136616635176e-05
Step 182700 Loss: 0.3741, lr 5.4367136616635176e-05
Step 182800 Loss: 0.3515, lr 5.4367136616635176e-05
Step 182900 Loss: 0.3365, lr 5.4367136616635176e-05
Step 183000 Loss: 0.3209, lr 5.4367136616635176e-05
Step 183100 Loss: 0.3103, lr 5.4367136616635176e-05
Step 183200 Loss: 0.3017, lr 5.4367136616635176e-05
Step 183300 Loss: 0.2927, lr 5.4367136616635176e-05
Step 183400 Loss: 0.2850, lr 5.4367136616635176e-05
Step 183500 Loss: 0.2789, lr 5.4367136616635176e-05
Step 183600 Loss: 0.2730, lr 5.4367136616635176e-05
Step 183700 Loss: 0.2680, lr 5.4367136616635176e-05
Step 183800 Loss: 0.2623, lr 5.4367136616635176e-05
Step 183900 Loss: 0.2588, lr 5.4367136616635176e-05
Step 184000 Loss: 0.2553, lr 5.4367136616635176e-05
Step 184100 Loss: 0.2523, lr 5.4367136616635176e-05
Step 184200 Loss: 0.2490, lr 5.4367136616635176e-05
Step 184300 Loss: 0.2458, lr 5.4367136616635176e-05
Step 184400 Loss: 0.2420, lr 5.4367136616635176e-05
Step 184500 Loss: 0.2384, lr 5.4367136616635176e-05
Step 184600 Loss: 0.2360, lr 5.4367136616635176e-05
Step 184700 Loss: 0.2335, lr 5.4367136616635176e-05
Step 184800 Loss: 0.2308, lr 5.4367136616635176e-05
Step 184900 Loss: 0.2281, lr 5.4367136616635176e-05
Step 185000 Loss: 0.2256, lr 5.4367136616635176e-05
Step 185100 Loss: 0.2235, lr 5.4367136616635176e-05
Step 185200 Loss: 0.2217, lr 5.4367136616635176e-05
Step 185300 Loss: 0.2197, lr 5.4367136616635176e-05
Step 185400 Loss: 0.2178, lr 5.4367136616635176e-05
Step 185500 Loss: 0.2157, lr 5.4367136616635176e-05
Step 185600 Loss: 0.2138, lr 5.4367136616635176e-05
Step 185700 Loss: 0.2119, lr 5.4367136616635176e-05
Step 185800 Loss: 0.2101, lr 5.4367136616635176e-05
Step 185900 Loss: 0.2084, lr 5.4367136616635176e-05
Step 186000 Loss: 0.2066, lr 5.4367136616635176e-05
Step 186100 Loss: 0.2051, lr 5.4367136616635176e-05
Step 186200 Loss: 0.2039, lr 5.4367136616635176e-05
Step 186300 Loss: 0.2025, lr 5.4367136616635176e-05
Step 186400 Loss: 0.2018, lr 5.4367136616635176e-05
Step 186500 Loss: 0.2006, lr 5.4367136616635176e-05
Step 186600 Loss: 0.1998, lr 5.4367136616635176e-05
Step 186700 Loss: 0.1988, lr 5.4367136616635176e-05
Step 186800 Loss: 0.1978, lr 5.4367136616635176e-05
Step 186900 Loss: 0.1970, lr 5.4367136616635176e-05
Step 187000 Loss: 0.1958, lr 5.4367136616635176e-05
Step 187100 Loss: 0.1950, lr 5.4367136616635176e-05
Step 187200 Loss: 0.1944, lr 5.4367136616635176e-05
Step 187300 Loss: 0.1940, lr 5.4367136616635176e-05
Step 187400 Loss: 0.1934, lr 5.4367136616635176e-05
Step 187500 Loss: 0.1930, lr 5.4367136616635176e-05
Step 187600 Loss: 0.1927, lr 5.4367136616635176e-05
Step 187700 Loss: 0.1920, lr 5.4367136616635176e-05
Step 187800 Loss: 0.1915, lr 5.4367136616635176e-05
Step 187900 Loss: 0.1913, lr 5.4367136616635176e-05
Step 188000 Loss: 0.1908, lr 5.4367136616635176e-05
Step 188100 Loss: 0.1906, lr 5.4367136616635176e-05
Step 188200 Loss: 0.1906, lr 5.4367136616635176e-05
Step 188300 Loss: 0.1905, lr 5.4367136616635176e-05
Step 188400 Loss: 0.1907, lr 5.4367136616635176e-05
Step 188500 Loss: 0.1907, lr 5.4367136616635176e-05
Step 188600 Loss: 0.1913, lr 5.4367136616635176e-05
Step 188700 Loss: 0.1920, lr 5.4367136616635176e-05
Step 188800 Loss: 0.1931, lr 5.4367136616635176e-05
Step 188900 Loss: 0.1940, lr 5.4367136616635176e-05
Step 189000 Loss: 0.1939, lr 5.4367136616635176e-05
Step 189100 Loss: 0.1934, lr 5.4367136616635176e-05
Train Epoch: [68/100] Loss: 0.1939,lr 0.000054
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 6755
Step 189200 Loss: 0.6702, lr 5.358539573945668e-05
Step 189300 Loss: 0.4450, lr 5.358539573945668e-05
Step 189400 Loss: 0.3881, lr 5.358539573945668e-05
Step 189500 Loss: 0.3572, lr 5.358539573945668e-05
Step 189600 Loss: 0.3383, lr 5.358539573945668e-05
Step 189700 Loss: 0.3263, lr 5.358539573945668e-05
Step 189800 Loss: 0.3123, lr 5.358539573945668e-05
Step 189900 Loss: 0.3013, lr 5.358539573945668e-05
Step 190000 Loss: 0.2945, lr 5.358539573945668e-05
Step 190100 Loss: 0.2865, lr 5.358539573945668e-05
Step 190200 Loss: 0.2791, lr 5.358539573945668e-05
Step 190300 Loss: 0.2736, lr 5.358539573945668e-05
Step 190400 Loss: 0.2685, lr 5.358539573945668e-05
Step 190500 Loss: 0.2637, lr 5.358539573945668e-05
Step 190600 Loss: 0.2590, lr 5.358539573945668e-05
Step 190700 Loss: 0.2560, lr 5.358539573945668e-05
Step 190800 Loss: 0.2526, lr 5.358539573945668e-05
Step 190900 Loss: 0.2494, lr 5.358539573945668e-05
Step 191000 Loss: 0.2463, lr 5.358539573945668e-05
Step 191100 Loss: 0.2431, lr 5.358539573945668e-05
Step 191200 Loss: 0.2392, lr 5.358539573945668e-05
Step 191300 Loss: 0.2364, lr 5.358539573945668e-05
Step 191400 Loss: 0.2338, lr 5.358539573945668e-05
Step 191500 Loss: 0.2310, lr 5.358539573945668e-05
Step 191600 Loss: 0.2285, lr 5.358539573945668e-05
Step 191700 Loss: 0.2260, lr 5.358539573945668e-05
Step 191800 Loss: 0.2237, lr 5.358539573945668e-05
Step 191900 Loss: 0.2221, lr 5.358539573945668e-05
Step 192000 Loss: 0.2198, lr 5.358539573945668e-05
Step 192100 Loss: 0.2180, lr 5.358539573945668e-05
Step 192200 Loss: 0.2160, lr 5.358539573945668e-05
Step 192300 Loss: 0.2140, lr 5.358539573945668e-05
Step 192400 Loss: 0.2120, lr 5.358539573945668e-05
Step 192500 Loss: 0.2101, lr 5.358539573945668e-05
Step 192600 Loss: 0.2084, lr 5.358539573945668e-05
Step 192700 Loss: 0.2068, lr 5.358539573945668e-05
Step 192800 Loss: 0.2051, lr 5.358539573945668e-05
Step 192900 Loss: 0.2035, lr 5.358539573945668e-05
Step 193000 Loss: 0.2025, lr 5.358539573945668e-05
Step 193100 Loss: 0.2014, lr 5.358539573945668e-05
Step 193200 Loss: 0.2004, lr 5.358539573945668e-05
Step 193300 Loss: 0.1993, lr 5.358539573945668e-05
Step 193400 Loss: 0.1985, lr 5.358539573945668e-05
Step 193500 Loss: 0.1975, lr 5.358539573945668e-05
Step 193600 Loss: 0.1965, lr 5.358539573945668e-05
Step 193700 Loss: 0.1954, lr 5.358539573945668e-05
Step 193800 Loss: 0.1945, lr 5.358539573945668e-05
Step 193900 Loss: 0.1936, lr 5.358539573945668e-05
Step 194000 Loss: 0.1929, lr 5.358539573945668e-05
Step 194100 Loss: 0.1925, lr 5.358539573945668e-05
Step 194200 Loss: 0.1919, lr 5.358539573945668e-05
Step 194300 Loss: 0.1917, lr 5.358539573945668e-05
Step 194400 Loss: 0.1910, lr 5.358539573945668e-05
Step 194500 Loss: 0.1907, lr 5.358539573945668e-05
Step 194600 Loss: 0.1904, lr 5.358539573945668e-05
Step 194700 Loss: 0.1901, lr 5.358539573945668e-05
Step 194800 Loss: 0.1896, lr 5.358539573945668e-05
Step 194900 Loss: 0.1895, lr 5.358539573945668e-05
Step 195000 Loss: 0.1894, lr 5.358539573945668e-05
Step 195100 Loss: 0.1896, lr 5.358539573945668e-05
Step 195200 Loss: 0.1899, lr 5.358539573945668e-05
Step 195300 Loss: 0.1901, lr 5.358539573945668e-05
Step 195400 Loss: 0.1907, lr 5.358539573945668e-05
Step 195500 Loss: 0.1915, lr 5.358539573945668e-05
Step 195600 Loss: 0.1925, lr 5.358539573945668e-05
Step 195700 Loss: 0.1930, lr 5.358539573945668e-05
Step 195800 Loss: 0.1925, lr 5.358539573945668e-05
Step 195900 Loss: 0.1921, lr 5.358539573945668e-05
Train Epoch: [69/100] Loss: 0.1923,lr 0.000054
Calling G2SDataset.batch()
Done, time:  2.76 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.71 s, total batches: 6756
Step 196000 Loss: 0.5455, lr 5.278306858947436e-05
Step 196100 Loss: 0.4299, lr 5.278306858947436e-05
Step 196200 Loss: 0.3824, lr 5.278306858947436e-05
Step 196300 Loss: 0.3552, lr 5.278306858947436e-05
Step 196400 Loss: 0.3392, lr 5.278306858947436e-05
Step 196500 Loss: 0.3232, lr 5.278306858947436e-05
Step 196600 Loss: 0.3102, lr 5.278306858947436e-05
Step 196700 Loss: 0.3018, lr 5.278306858947436e-05
Step 196800 Loss: 0.2929, lr 5.278306858947436e-05
Step 196900 Loss: 0.2853, lr 5.278306858947436e-05
Step 197000 Loss: 0.2788, lr 5.278306858947436e-05
Step 197100 Loss: 0.2727, lr 5.278306858947436e-05
Step 197200 Loss: 0.2678, lr 5.278306858947436e-05
Step 197300 Loss: 0.2621, lr 5.278306858947436e-05
Step 197400 Loss: 0.2589, lr 5.278306858947436e-05
Step 197500 Loss: 0.2555, lr 5.278306858947436e-05
Step 197600 Loss: 0.2525, lr 5.278306858947436e-05
Step 197700 Loss: 0.2492, lr 5.278306858947436e-05
Step 197800 Loss: 0.2458, lr 5.278306858947436e-05
Step 197900 Loss: 0.2422, lr 5.278306858947436e-05
Step 198000 Loss: 0.2384, lr 5.278306858947436e-05
Step 198100 Loss: 0.2360, lr 5.278306858947436e-05
Step 198200 Loss: 0.2334, lr 5.278306858947436e-05
Step 198300 Loss: 0.2306, lr 5.278306858947436e-05
Step 198400 Loss: 0.2276, lr 5.278306858947436e-05
Step 198500 Loss: 0.2252, lr 5.278306858947436e-05
Step 198600 Loss: 0.2231, lr 5.278306858947436e-05
Step 198700 Loss: 0.2210, lr 5.278306858947436e-05
Step 198800 Loss: 0.2190, lr 5.278306858947436e-05
Step 198900 Loss: 0.2171, lr 5.278306858947436e-05
Step 199000 Loss: 0.2150, lr 5.278306858947436e-05
Step 199100 Loss: 0.2131, lr 5.278306858947436e-05
Step 199200 Loss: 0.2115, lr 5.278306858947436e-05
Step 199300 Loss: 0.2095, lr 5.278306858947436e-05
Step 199400 Loss: 0.2078, lr 5.278306858947436e-05
Step 199500 Loss: 0.2059, lr 5.278306858947436e-05
Step 199600 Loss: 0.2043, lr 5.278306858947436e-05
Step 199700 Loss: 0.2031, lr 5.278306858947436e-05
Step 199800 Loss: 0.2016, lr 5.278306858947436e-05
Step 199900 Loss: 0.2006, lr 5.278306858947436e-05
Step 200000 Loss: 0.1996, lr 5.278306858947436e-05
Step 200100 Loss: 0.1987, lr 5.278306858947436e-05
Step 200200 Loss: 0.1976, lr 5.278306858947436e-05
Step 200300 Loss: 0.1968, lr 5.278306858947436e-05
Step 200400 Loss: 0.1957, lr 5.278306858947436e-05
Step 200500 Loss: 0.1945, lr 5.278306858947436e-05
Step 200600 Loss: 0.1936, lr 5.278306858947436e-05
Step 200700 Loss: 0.1929, lr 5.278306858947436e-05
Step 200800 Loss: 0.1924, lr 5.278306858947436e-05
Step 200900 Loss: 0.1919, lr 5.278306858947436e-05
Step 201000 Loss: 0.1913, lr 5.278306858947436e-05
Step 201100 Loss: 0.1909, lr 5.278306858947436e-05
Step 201200 Loss: 0.1904, lr 5.278306858947436e-05
Step 201300 Loss: 0.1900, lr 5.278306858947436e-05
Step 201400 Loss: 0.1896, lr 5.278306858947436e-05
Step 201500 Loss: 0.1892, lr 5.278306858947436e-05
Step 201600 Loss: 0.1890, lr 5.278306858947436e-05
Step 201700 Loss: 0.1889, lr 5.278306858947436e-05
Step 201800 Loss: 0.1890, lr 5.278306858947436e-05
Step 201900 Loss: 0.1890, lr 5.278306858947436e-05
Step 202000 Loss: 0.1891, lr 5.278306858947436e-05
Step 202100 Loss: 0.1896, lr 5.278306858947436e-05
Step 202200 Loss: 0.1904, lr 5.278306858947436e-05
Step 202300 Loss: 0.1914, lr 5.278306858947436e-05
Step 202400 Loss: 0.1921, lr 5.278306858947436e-05
Step 202500 Loss: 0.1921, lr 5.278306858947436e-05
Step 202600 Loss: 0.1915, lr 5.278306858947436e-05
Train Epoch: [70/100] Loss: 0.1922,lr 0.000053
Model Saving at epoch 70
Calling G2SDataset.batch()
Done, time:  2.75 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.22 s, total batches: 6756
Step 202700 Loss: 0.7425, lr 5.1960946966719094e-05
Step 202800 Loss: 0.4566, lr 5.1960946966719094e-05
Step 202900 Loss: 0.3956, lr 5.1960946966719094e-05
Step 203000 Loss: 0.3618, lr 5.1960946966719094e-05
Step 203100 Loss: 0.3424, lr 5.1960946966719094e-05
Step 203200 Loss: 0.3295, lr 5.1960946966719094e-05
Step 203300 Loss: 0.3147, lr 5.1960946966719094e-05
Step 203400 Loss: 0.3043, lr 5.1960946966719094e-05
Step 203500 Loss: 0.2967, lr 5.1960946966719094e-05
Step 203600 Loss: 0.2886, lr 5.1960946966719094e-05
Step 203700 Loss: 0.2818, lr 5.1960946966719094e-05
Step 203800 Loss: 0.2759, lr 5.1960946966719094e-05
Step 203900 Loss: 0.2704, lr 5.1960946966719094e-05
Step 204000 Loss: 0.2655, lr 5.1960946966719094e-05
Step 204100 Loss: 0.2606, lr 5.1960946966719094e-05
Step 204200 Loss: 0.2573, lr 5.1960946966719094e-05
Step 204300 Loss: 0.2539, lr 5.1960946966719094e-05
Step 204400 Loss: 0.2505, lr 5.1960946966719094e-05
Step 204500 Loss: 0.2474, lr 5.1960946966719094e-05
Step 204600 Loss: 0.2443, lr 5.1960946966719094e-05
Step 204700 Loss: 0.2404, lr 5.1960946966719094e-05
Step 204800 Loss: 0.2372, lr 5.1960946966719094e-05
Step 204900 Loss: 0.2344, lr 5.1960946966719094e-05
Step 205000 Loss: 0.2315, lr 5.1960946966719094e-05
Step 205100 Loss: 0.2289, lr 5.1960946966719094e-05
Step 205200 Loss: 0.2262, lr 5.1960946966719094e-05
Step 205300 Loss: 0.2239, lr 5.1960946966719094e-05
Step 205400 Loss: 0.2223, lr 5.1960946966719094e-05
Step 205500 Loss: 0.2204, lr 5.1960946966719094e-05
Step 205600 Loss: 0.2184, lr 5.1960946966719094e-05
Step 205700 Loss: 0.2165, lr 5.1960946966719094e-05
Step 205800 Loss: 0.2144, lr 5.1960946966719094e-05
Step 205900 Loss: 0.2123, lr 5.1960946966719094e-05
Step 206000 Loss: 0.2104, lr 5.1960946966719094e-05
Step 206100 Loss: 0.2087, lr 5.1960946966719094e-05
Step 206200 Loss: 0.2069, lr 5.1960946966719094e-05
Step 206300 Loss: 0.2052, lr 5.1960946966719094e-05
Step 206400 Loss: 0.2036, lr 5.1960946966719094e-05
Step 206500 Loss: 0.2024, lr 5.1960946966719094e-05
Step 206600 Loss: 0.2011, lr 5.1960946966719094e-05
Step 206700 Loss: 0.2001, lr 5.1960946966719094e-05
Step 206800 Loss: 0.1989, lr 5.1960946966719094e-05
Step 206900 Loss: 0.1980, lr 5.1960946966719094e-05
Step 207000 Loss: 0.1970, lr 5.1960946966719094e-05
Step 207100 Loss: 0.1959, lr 5.1960946966719094e-05
Step 207200 Loss: 0.1947, lr 5.1960946966719094e-05
Step 207300 Loss: 0.1935, lr 5.1960946966719094e-05
Step 207400 Loss: 0.1927, lr 5.1960946966719094e-05
Step 207500 Loss: 0.1921, lr 5.1960946966719094e-05
Step 207600 Loss: 0.1918, lr 5.1960946966719094e-05
Step 207700 Loss: 0.1910, lr 5.1960946966719094e-05
Step 207800 Loss: 0.1908, lr 5.1960946966719094e-05
Step 207900 Loss: 0.1901, lr 5.1960946966719094e-05
Step 208000 Loss: 0.1898, lr 5.1960946966719094e-05
Step 208100 Loss: 0.1893, lr 5.1960946966719094e-05
Step 208200 Loss: 0.1891, lr 5.1960946966719094e-05
Step 208300 Loss: 0.1887, lr 5.1960946966719094e-05
Step 208400 Loss: 0.1885, lr 5.1960946966719094e-05
Step 208500 Loss: 0.1885, lr 5.1960946966719094e-05
Step 208600 Loss: 0.1886, lr 5.1960946966719094e-05
Step 208700 Loss: 0.1886, lr 5.1960946966719094e-05
Step 208800 Loss: 0.1887, lr 5.1960946966719094e-05
Step 208900 Loss: 0.1892, lr 5.1960946966719094e-05
Step 209000 Loss: 0.1901, lr 5.1960946966719094e-05
Step 209100 Loss: 0.1909, lr 5.1960946966719094e-05
Step 209200 Loss: 0.1915, lr 5.1960946966719094e-05
Step 209300 Loss: 0.1909, lr 5.1960946966719094e-05
Step 209400 Loss: 0.1904, lr 5.1960946966719094e-05
Train Epoch: [71/100] Loss: 0.1913,lr 0.000052
Calling G2SDataset.batch()
Done, time:  2.43 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Step 209500 Loss: 0.5535, lr 5.1119842205976606e-05
Step 209600 Loss: 0.4283, lr 5.1119842205976606e-05
Step 209700 Loss: 0.3802, lr 5.1119842205976606e-05
Step 209800 Loss: 0.3523, lr 5.1119842205976606e-05
Step 209900 Loss: 0.3364, lr 5.1119842205976606e-05
Step 210000 Loss: 0.3228, lr 5.1119842205976606e-05
Step 210100 Loss: 0.3087, lr 5.1119842205976606e-05
Step 210200 Loss: 0.3002, lr 5.1119842205976606e-05
Step 210300 Loss: 0.2916, lr 5.1119842205976606e-05
Step 210400 Loss: 0.2837, lr 5.1119842205976606e-05
Step 210500 Loss: 0.2770, lr 5.1119842205976606e-05
Step 210600 Loss: 0.2714, lr 5.1119842205976606e-05
Step 210700 Loss: 0.2662, lr 5.1119842205976606e-05
Step 210800 Loss: 0.2607, lr 5.1119842205976606e-05
Step 210900 Loss: 0.2570, lr 5.1119842205976606e-05
Step 211000 Loss: 0.2538, lr 5.1119842205976606e-05
Step 211100 Loss: 0.2510, lr 5.1119842205976606e-05
Step 211200 Loss: 0.2477, lr 5.1119842205976606e-05
Step 211300 Loss: 0.2446, lr 5.1119842205976606e-05
Step 211400 Loss: 0.2410, lr 5.1119842205976606e-05
Step 211500 Loss: 0.2373, lr 5.1119842205976606e-05
Step 211600 Loss: 0.2348, lr 5.1119842205976606e-05
Step 211700 Loss: 0.2321, lr 5.1119842205976606e-05
Step 211800 Loss: 0.2293, lr 5.1119842205976606e-05
Step 211900 Loss: 0.2266, lr 5.1119842205976606e-05
Step 212000 Loss: 0.2243, lr 5.1119842205976606e-05
Step 212100 Loss: 0.2221, lr 5.1119842205976606e-05
Step 212200 Loss: 0.2203, lr 5.1119842205976606e-05
Step 212300 Loss: 0.2182, lr 5.1119842205976606e-05
Step 212400 Loss: 0.2164, lr 5.1119842205976606e-05
Step 212500 Loss: 0.2144, lr 5.1119842205976606e-05
Step 212600 Loss: 0.2122, lr 5.1119842205976606e-05
Step 212700 Loss: 0.2105, lr 5.1119842205976606e-05
Step 212800 Loss: 0.2084, lr 5.1119842205976606e-05
Step 212900 Loss: 0.2067, lr 5.1119842205976606e-05
Step 213000 Loss: 0.2050, lr 5.1119842205976606e-05
Step 213100 Loss: 0.2036, lr 5.1119842205976606e-05
Step 213200 Loss: 0.2022, lr 5.1119842205976606e-05
Step 213300 Loss: 0.2008, lr 5.1119842205976606e-05
Step 213400 Loss: 0.1997, lr 5.1119842205976606e-05
Step 213500 Loss: 0.1988, lr 5.1119842205976606e-05
Step 213600 Loss: 0.1978, lr 5.1119842205976606e-05
Step 213700 Loss: 0.1968, lr 5.1119842205976606e-05
Step 213800 Loss: 0.1958, lr 5.1119842205976606e-05
Step 213900 Loss: 0.1948, lr 5.1119842205976606e-05
Step 214000 Loss: 0.1936, lr 5.1119842205976606e-05
Step 214100 Loss: 0.1927, lr 5.1119842205976606e-05
Step 214200 Loss: 0.1919, lr 5.1119842205976606e-05
Step 214300 Loss: 0.1914, lr 5.1119842205976606e-05
Step 214400 Loss: 0.1909, lr 5.1119842205976606e-05
Step 214500 Loss: 0.1904, lr 5.1119842205976606e-05
Step 214600 Loss: 0.1902, lr 5.1119842205976606e-05
Step 214700 Loss: 0.1895, lr 5.1119842205976606e-05
Step 214800 Loss: 0.1891, lr 5.1119842205976606e-05
Step 214900 Loss: 0.1887, lr 5.1119842205976606e-05
Step 215000 Loss: 0.1884, lr 5.1119842205976606e-05
Step 215100 Loss: 0.1880, lr 5.1119842205976606e-05
Step 215200 Loss: 0.1878, lr 5.1119842205976606e-05
Step 215300 Loss: 0.1879, lr 5.1119842205976606e-05
Step 215400 Loss: 0.1879, lr 5.1119842205976606e-05
Step 215500 Loss: 0.1881, lr 5.1119842205976606e-05
Step 215600 Loss: 0.1883, lr 5.1119842205976606e-05
Step 215700 Loss: 0.1890, lr 5.1119842205976606e-05
Step 215800 Loss: 0.1900, lr 5.1119842205976606e-05
Step 215900 Loss: 0.1911, lr 5.1119842205976606e-05
Step 216000 Loss: 0.1911, lr 5.1119842205976606e-05
Step 216100 Loss: 0.1906, lr 5.1119842205976606e-05
Train Epoch: [72/100] Loss: 0.1906,lr 0.000051
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Step 216200 Loss: 1.0392, lr 5.026058437609794e-05
Step 216300 Loss: 0.4833, lr 5.026058437609794e-05
Step 216400 Loss: 0.4019, lr 5.026058437609794e-05
Step 216500 Loss: 0.3644, lr 5.026058437609794e-05
Step 216600 Loss: 0.3448, lr 5.026058437609794e-05
Step 216700 Loss: 0.3301, lr 5.026058437609794e-05
Step 216800 Loss: 0.3155, lr 5.026058437609794e-05
Step 216900 Loss: 0.3040, lr 5.026058437609794e-05
Step 217000 Loss: 0.2971, lr 5.026058437609794e-05
Step 217100 Loss: 0.2879, lr 5.026058437609794e-05
Step 217200 Loss: 0.2809, lr 5.026058437609794e-05
Step 217300 Loss: 0.2753, lr 5.026058437609794e-05
Step 217400 Loss: 0.2695, lr 5.026058437609794e-05
Step 217500 Loss: 0.2640, lr 5.026058437609794e-05
Step 217600 Loss: 0.2592, lr 5.026058437609794e-05
Step 217700 Loss: 0.2558, lr 5.026058437609794e-05
Step 217800 Loss: 0.2519, lr 5.026058437609794e-05
Step 217900 Loss: 0.2490, lr 5.026058437609794e-05
Step 218000 Loss: 0.2456, lr 5.026058437609794e-05
Step 218100 Loss: 0.2427, lr 5.026058437609794e-05
Step 218200 Loss: 0.2390, lr 5.026058437609794e-05
Step 218300 Loss: 0.2355, lr 5.026058437609794e-05
Step 218400 Loss: 0.2328, lr 5.026058437609794e-05
Step 218500 Loss: 0.2302, lr 5.026058437609794e-05
Step 218600 Loss: 0.2278, lr 5.026058437609794e-05
Step 218700 Loss: 0.2249, lr 5.026058437609794e-05
Step 218800 Loss: 0.2227, lr 5.026058437609794e-05
Step 218900 Loss: 0.2209, lr 5.026058437609794e-05
Step 219000 Loss: 0.2191, lr 5.026058437609794e-05
Step 219100 Loss: 0.2171, lr 5.026058437609794e-05
Step 219200 Loss: 0.2153, lr 5.026058437609794e-05
Step 219300 Loss: 0.2131, lr 5.026058437609794e-05
Step 219400 Loss: 0.2113, lr 5.026058437609794e-05
Step 219500 Loss: 0.2092, lr 5.026058437609794e-05
Step 219600 Loss: 0.2076, lr 5.026058437609794e-05
Step 219700 Loss: 0.2059, lr 5.026058437609794e-05
Step 219800 Loss: 0.2041, lr 5.026058437609794e-05
Step 219900 Loss: 0.2025, lr 5.026058437609794e-05
Step 220000 Loss: 0.2013, lr 5.026058437609794e-05
Step 220100 Loss: 0.2000, lr 5.026058437609794e-05
Step 220200 Loss: 0.1991, lr 5.026058437609794e-05
Step 220300 Loss: 0.1979, lr 5.026058437609794e-05
Step 220400 Loss: 0.1970, lr 5.026058437609794e-05
Step 220500 Loss: 0.1960, lr 5.026058437609794e-05
Step 220600 Loss: 0.1948, lr 5.026058437609794e-05
Step 220700 Loss: 0.1940, lr 5.026058437609794e-05
Step 220800 Loss: 0.1928, lr 5.026058437609794e-05
Step 220900 Loss: 0.1919, lr 5.026058437609794e-05
Step 221000 Loss: 0.1912, lr 5.026058437609794e-05
Step 221100 Loss: 0.1908, lr 5.026058437609794e-05
Step 221200 Loss: 0.1902, lr 5.026058437609794e-05
Step 221300 Loss: 0.1898, lr 5.026058437609794e-05
Step 221400 Loss: 0.1892, lr 5.026058437609794e-05
Step 221500 Loss: 0.1887, lr 5.026058437609794e-05
Step 221600 Loss: 0.1881, lr 5.026058437609794e-05
Step 221700 Loss: 0.1877, lr 5.026058437609794e-05
Step 221800 Loss: 0.1873, lr 5.026058437609794e-05
Step 221900 Loss: 0.1870, lr 5.026058437609794e-05
Step 222000 Loss: 0.1868, lr 5.026058437609794e-05
Step 222100 Loss: 0.1868, lr 5.026058437609794e-05
Step 222200 Loss: 0.1870, lr 5.026058437609794e-05
Step 222300 Loss: 0.1872, lr 5.026058437609794e-05
Step 222400 Loss: 0.1876, lr 5.026058437609794e-05
Step 222500 Loss: 0.1885, lr 5.026058437609794e-05
Step 222600 Loss: 0.1896, lr 5.026058437609794e-05
Step 222700 Loss: 0.1904, lr 5.026058437609794e-05
Step 222800 Loss: 0.1902, lr 5.026058437609794e-05
Step 222900 Loss: 0.1897, lr 5.026058437609794e-05
Train Epoch: [73/100] Loss: 0.1904,lr 0.000050
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Step 223000 Loss: 0.6133, lr 4.9384021460821733e-05
Step 223100 Loss: 0.4492, lr 4.9384021460821733e-05
Step 223200 Loss: 0.3922, lr 4.9384021460821733e-05
Step 223300 Loss: 0.3597, lr 4.9384021460821733e-05
Step 223400 Loss: 0.3420, lr 4.9384021460821733e-05
Step 223500 Loss: 0.3281, lr 4.9384021460821733e-05
Step 223600 Loss: 0.3126, lr 4.9384021460821733e-05
Step 223700 Loss: 0.3027, lr 4.9384021460821733e-05
Step 223800 Loss: 0.2943, lr 4.9384021460821733e-05
Step 223900 Loss: 0.2865, lr 4.9384021460821733e-05
Step 224000 Loss: 0.2791, lr 4.9384021460821733e-05
Step 224100 Loss: 0.2737, lr 4.9384021460821733e-05
Step 224200 Loss: 0.2685, lr 4.9384021460821733e-05
Step 224300 Loss: 0.2630, lr 4.9384021460821733e-05
Step 224400 Loss: 0.2586, lr 4.9384021460821733e-05
Step 224500 Loss: 0.2553, lr 4.9384021460821733e-05
Step 224600 Loss: 0.2519, lr 4.9384021460821733e-05
Step 224700 Loss: 0.2484, lr 4.9384021460821733e-05
Step 224800 Loss: 0.2455, lr 4.9384021460821733e-05
Step 224900 Loss: 0.2418, lr 4.9384021460821733e-05
Step 225000 Loss: 0.2381, lr 4.9384021460821733e-05
Step 225100 Loss: 0.2351, lr 4.9384021460821733e-05
Step 225200 Loss: 0.2324, lr 4.9384021460821733e-05
Step 225300 Loss: 0.2295, lr 4.9384021460821733e-05
Step 225400 Loss: 0.2268, lr 4.9384021460821733e-05
Step 225500 Loss: 0.2247, lr 4.9384021460821733e-05
Step 225600 Loss: 0.2222, lr 4.9384021460821733e-05
Step 225700 Loss: 0.2205, lr 4.9384021460821733e-05
Step 225800 Loss: 0.2184, lr 4.9384021460821733e-05
Step 225900 Loss: 0.2166, lr 4.9384021460821733e-05
Step 226000 Loss: 0.2146, lr 4.9384021460821733e-05
Step 226100 Loss: 0.2123, lr 4.9384021460821733e-05
Step 226200 Loss: 0.2106, lr 4.9384021460821733e-05
Step 226300 Loss: 0.2084, lr 4.9384021460821733e-05
Step 226400 Loss: 0.2068, lr 4.9384021460821733e-05
Step 226500 Loss: 0.2049, lr 4.9384021460821733e-05
Step 226600 Loss: 0.2034, lr 4.9384021460821733e-05
Step 226700 Loss: 0.2017, lr 4.9384021460821733e-05
Step 226800 Loss: 0.2004, lr 4.9384021460821733e-05
Step 226900 Loss: 0.1992, lr 4.9384021460821733e-05
Step 227000 Loss: 0.1983, lr 4.9384021460821733e-05
Step 227100 Loss: 0.1972, lr 4.9384021460821733e-05
Step 227200 Loss: 0.1963, lr 4.9384021460821733e-05
Step 227300 Loss: 0.1952, lr 4.9384021460821733e-05
Step 227400 Loss: 0.1943, lr 4.9384021460821733e-05
Step 227500 Loss: 0.1931, lr 4.9384021460821733e-05
Step 227600 Loss: 0.1919, lr 4.9384021460821733e-05
Step 227700 Loss: 0.1912, lr 4.9384021460821733e-05
Step 227800 Loss: 0.1906, lr 4.9384021460821733e-05
Step 227900 Loss: 0.1902, lr 4.9384021460821733e-05
Step 228000 Loss: 0.1894, lr 4.9384021460821733e-05
Step 228100 Loss: 0.1892, lr 4.9384021460821733e-05
Step 228200 Loss: 0.1885, lr 4.9384021460821733e-05
Step 228300 Loss: 0.1880, lr 4.9384021460821733e-05
Step 228400 Loss: 0.1876, lr 4.9384021460821733e-05
Step 228500 Loss: 0.1872, lr 4.9384021460821733e-05
Step 228600 Loss: 0.1869, lr 4.9384021460821733e-05
Step 228700 Loss: 0.1867, lr 4.9384021460821733e-05
Step 228800 Loss: 0.1866, lr 4.9384021460821733e-05
Step 228900 Loss: 0.1867, lr 4.9384021460821733e-05
Step 229000 Loss: 0.1868, lr 4.9384021460821733e-05
Step 229100 Loss: 0.1870, lr 4.9384021460821733e-05
Step 229200 Loss: 0.1877, lr 4.9384021460821733e-05
Step 229300 Loss: 0.1883, lr 4.9384021460821733e-05
Step 229400 Loss: 0.1892, lr 4.9384021460821733e-05
Step 229500 Loss: 0.1894, lr 4.9384021460821733e-05
Step 229600 Loss: 0.1889, lr 4.9384021460821733e-05
Train Epoch: [74/100] Loss: 0.1888,lr 0.000049
Calling G2SDataset.batch()
Done, time:  1.99 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6755
Step 229700 Loss: 1.3308, lr 4.849101852191663e-05
Step 229800 Loss: 0.4835, lr 4.849101852191663e-05
Step 229900 Loss: 0.4012, lr 4.849101852191663e-05
Step 230000 Loss: 0.3639, lr 4.849101852191663e-05
Step 230100 Loss: 0.3441, lr 4.849101852191663e-05
Step 230200 Loss: 0.3295, lr 4.849101852191663e-05
Step 230300 Loss: 0.3152, lr 4.849101852191663e-05
Step 230400 Loss: 0.3042, lr 4.849101852191663e-05
Step 230500 Loss: 0.2960, lr 4.849101852191663e-05
Step 230600 Loss: 0.2872, lr 4.849101852191663e-05
Step 230700 Loss: 0.2798, lr 4.849101852191663e-05
Step 230800 Loss: 0.2743, lr 4.849101852191663e-05
Step 230900 Loss: 0.2684, lr 4.849101852191663e-05
Step 231000 Loss: 0.2636, lr 4.849101852191663e-05
Step 231100 Loss: 0.2583, lr 4.849101852191663e-05
Step 231200 Loss: 0.2551, lr 4.849101852191663e-05
Step 231300 Loss: 0.2514, lr 4.849101852191663e-05
Step 231400 Loss: 0.2483, lr 4.849101852191663e-05
Step 231500 Loss: 0.2453, lr 4.849101852191663e-05
Step 231600 Loss: 0.2422, lr 4.849101852191663e-05
Step 231700 Loss: 0.2386, lr 4.849101852191663e-05
Step 231800 Loss: 0.2349, lr 4.849101852191663e-05
Step 231900 Loss: 0.2323, lr 4.849101852191663e-05
Step 232000 Loss: 0.2296, lr 4.849101852191663e-05
Step 232100 Loss: 0.2270, lr 4.849101852191663e-05
Step 232200 Loss: 0.2242, lr 4.849101852191663e-05
Step 232300 Loss: 0.2218, lr 4.849101852191663e-05
Step 232400 Loss: 0.2197, lr 4.849101852191663e-05
Step 232500 Loss: 0.2180, lr 4.849101852191663e-05
Step 232600 Loss: 0.2160, lr 4.849101852191663e-05
Step 232700 Loss: 0.2142, lr 4.849101852191663e-05
Step 232800 Loss: 0.2120, lr 4.849101852191663e-05
Step 232900 Loss: 0.2100, lr 4.849101852191663e-05
Step 233000 Loss: 0.2080, lr 4.849101852191663e-05
Step 233100 Loss: 0.2062, lr 4.849101852191663e-05
Step 233200 Loss: 0.2045, lr 4.849101852191663e-05
Step 233300 Loss: 0.2028, lr 4.849101852191663e-05
Step 233400 Loss: 0.2013, lr 4.849101852191663e-05
Step 233500 Loss: 0.2000, lr 4.849101852191663e-05
Step 233600 Loss: 0.1986, lr 4.849101852191663e-05
Step 233700 Loss: 0.1977, lr 4.849101852191663e-05
Step 233800 Loss: 0.1965, lr 4.849101852191663e-05
Step 233900 Loss: 0.1957, lr 4.849101852191663e-05
Step 234000 Loss: 0.1947, lr 4.849101852191663e-05
Step 234100 Loss: 0.1936, lr 4.849101852191663e-05
Step 234200 Loss: 0.1926, lr 4.849101852191663e-05
Step 234300 Loss: 0.1915, lr 4.849101852191663e-05
Step 234400 Loss: 0.1906, lr 4.849101852191663e-05
Step 234500 Loss: 0.1899, lr 4.849101852191663e-05
Step 234600 Loss: 0.1894, lr 4.849101852191663e-05
Step 234700 Loss: 0.1889, lr 4.849101852191663e-05
Step 234800 Loss: 0.1884, lr 4.849101852191663e-05
Step 234900 Loss: 0.1879, lr 4.849101852191663e-05
Step 235000 Loss: 0.1873, lr 4.849101852191663e-05
Step 235100 Loss: 0.1868, lr 4.849101852191663e-05
Step 235200 Loss: 0.1866, lr 4.849101852191663e-05
Step 235300 Loss: 0.1861, lr 4.849101852191663e-05
Step 235400 Loss: 0.1859, lr 4.849101852191663e-05
Step 235500 Loss: 0.1858, lr 4.849101852191663e-05
Step 235600 Loss: 0.1857, lr 4.849101852191663e-05
Step 235700 Loss: 0.1857, lr 4.849101852191663e-05
Step 235800 Loss: 0.1856, lr 4.849101852191663e-05
Step 235900 Loss: 0.1860, lr 4.849101852191663e-05
Step 236000 Loss: 0.1868, lr 4.849101852191663e-05
Step 236100 Loss: 0.1875, lr 4.849101852191663e-05
Step 236200 Loss: 0.1885, lr 4.849101852191663e-05
Step 236300 Loss: 0.1884, lr 4.849101852191663e-05
Step 236400 Loss: 0.1882, lr 4.849101852191663e-05
Train Epoch: [75/100] Loss: 0.1883,lr 0.000048
Model Saving at epoch 75
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6755
Step 236500 Loss: 0.6920, lr 4.75824568454698e-05
Step 236600 Loss: 0.4638, lr 4.75824568454698e-05
Step 236700 Loss: 0.4003, lr 4.75824568454698e-05
Step 236800 Loss: 0.3666, lr 4.75824568454698e-05
Step 236900 Loss: 0.3460, lr 4.75824568454698e-05
Step 237000 Loss: 0.3311, lr 4.75824568454698e-05
Step 237100 Loss: 0.3156, lr 4.75824568454698e-05
Step 237200 Loss: 0.3048, lr 4.75824568454698e-05
Step 237300 Loss: 0.2965, lr 4.75824568454698e-05
Step 237400 Loss: 0.2882, lr 4.75824568454698e-05
Step 237500 Loss: 0.2810, lr 4.75824568454698e-05
Step 237600 Loss: 0.2754, lr 4.75824568454698e-05
Step 237700 Loss: 0.2699, lr 4.75824568454698e-05
Step 237800 Loss: 0.2644, lr 4.75824568454698e-05
Step 237900 Loss: 0.2593, lr 4.75824568454698e-05
Step 238000 Loss: 0.2563, lr 4.75824568454698e-05
Step 238100 Loss: 0.2531, lr 4.75824568454698e-05
Step 238200 Loss: 0.2494, lr 4.75824568454698e-05
Step 238300 Loss: 0.2464, lr 4.75824568454698e-05
Step 238400 Loss: 0.2430, lr 4.75824568454698e-05
Step 238500 Loss: 0.2394, lr 4.75824568454698e-05
Step 238600 Loss: 0.2364, lr 4.75824568454698e-05
Step 238700 Loss: 0.2335, lr 4.75824568454698e-05
Step 238800 Loss: 0.2307, lr 4.75824568454698e-05
Step 238900 Loss: 0.2280, lr 4.75824568454698e-05
Step 239000 Loss: 0.2254, lr 4.75824568454698e-05
Step 239100 Loss: 0.2231, lr 4.75824568454698e-05
Step 239200 Loss: 0.2212, lr 4.75824568454698e-05
Step 239300 Loss: 0.2193, lr 4.75824568454698e-05
Step 239400 Loss: 0.2173, lr 4.75824568454698e-05
Step 239500 Loss: 0.2154, lr 4.75824568454698e-05
Step 239600 Loss: 0.2131, lr 4.75824568454698e-05
Step 239700 Loss: 0.2112, lr 4.75824568454698e-05
Step 239800 Loss: 0.2090, lr 4.75824568454698e-05
Step 239900 Loss: 0.2071, lr 4.75824568454698e-05
Step 240000 Loss: 0.2054, lr 4.75824568454698e-05
Step 240100 Loss: 0.2038, lr 4.75824568454698e-05
Step 240200 Loss: 0.2021, lr 4.75824568454698e-05
Step 240300 Loss: 0.2010, lr 4.75824568454698e-05
Step 240400 Loss: 0.1997, lr 4.75824568454698e-05
Step 240500 Loss: 0.1986, lr 4.75824568454698e-05
Step 240600 Loss: 0.1975, lr 4.75824568454698e-05
Step 240700 Loss: 0.1965, lr 4.75824568454698e-05
Step 240800 Loss: 0.1955, lr 4.75824568454698e-05
Step 240900 Loss: 0.1944, lr 4.75824568454698e-05
Step 241000 Loss: 0.1932, lr 4.75824568454698e-05
Step 241100 Loss: 0.1921, lr 4.75824568454698e-05
Step 241200 Loss: 0.1912, lr 4.75824568454698e-05
Step 241300 Loss: 0.1908, lr 4.75824568454698e-05
Step 241400 Loss: 0.1903, lr 4.75824568454698e-05
Step 241500 Loss: 0.1895, lr 4.75824568454698e-05
Step 241600 Loss: 0.1894, lr 4.75824568454698e-05
Step 241700 Loss: 0.1885, lr 4.75824568454698e-05
Step 241800 Loss: 0.1881, lr 4.75824568454698e-05
Step 241900 Loss: 0.1876, lr 4.75824568454698e-05
Step 242000 Loss: 0.1873, lr 4.75824568454698e-05
Step 242100 Loss: 0.1868, lr 4.75824568454698e-05
Step 242200 Loss: 0.1867, lr 4.75824568454698e-05
Step 242300 Loss: 0.1865, lr 4.75824568454698e-05
Step 242400 Loss: 0.1865, lr 4.75824568454698e-05
Step 242500 Loss: 0.1866, lr 4.75824568454698e-05
Step 242600 Loss: 0.1866, lr 4.75824568454698e-05
Step 242700 Loss: 0.1870, lr 4.75824568454698e-05
Step 242800 Loss: 0.1875, lr 4.75824568454698e-05
Step 242900 Loss: 0.1884, lr 4.75824568454698e-05
Step 243000 Loss: 0.1888, lr 4.75824568454698e-05
Step 243100 Loss: 0.1885, lr 4.75824568454698e-05
Step 243200 Loss: 0.1882, lr 4.75824568454698e-05
Train Epoch: [76/100] Loss: 0.1885,lr 0.000048
Calling G2SDataset.batch()
Done, time:  2.31 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6755
Step 243300 Loss: 0.4810, lr 4.66592330721639e-05
Step 243400 Loss: 0.4011, lr 4.66592330721639e-05
Step 243500 Loss: 0.3625, lr 4.66592330721639e-05
Step 243600 Loss: 0.3416, lr 4.66592330721639e-05
Step 243700 Loss: 0.3282, lr 4.66592330721639e-05
Step 243800 Loss: 0.3136, lr 4.66592330721639e-05
Step 243900 Loss: 0.3028, lr 4.66592330721639e-05
Step 244000 Loss: 0.2949, lr 4.66592330721639e-05
Step 244100 Loss: 0.2865, lr 4.66592330721639e-05
Step 244200 Loss: 0.2792, lr 4.66592330721639e-05
Step 244300 Loss: 0.2735, lr 4.66592330721639e-05
Step 244400 Loss: 0.2677, lr 4.66592330721639e-05
Step 244500 Loss: 0.2630, lr 4.66592330721639e-05
Step 244600 Loss: 0.2572, lr 4.66592330721639e-05
Step 244700 Loss: 0.2540, lr 4.66592330721639e-05
Step 244800 Loss: 0.2505, lr 4.66592330721639e-05
Step 244900 Loss: 0.2477, lr 4.66592330721639e-05
Step 245000 Loss: 0.2446, lr 4.66592330721639e-05
Step 245100 Loss: 0.2415, lr 4.66592330721639e-05
Step 245200 Loss: 0.2377, lr 4.66592330721639e-05
Step 245300 Loss: 0.2342, lr 4.66592330721639e-05
Step 245400 Loss: 0.2319, lr 4.66592330721639e-05
Step 245500 Loss: 0.2293, lr 4.66592330721639e-05
Step 245600 Loss: 0.2267, lr 4.66592330721639e-05
Step 245700 Loss: 0.2237, lr 4.66592330721639e-05
Step 245800 Loss: 0.2213, lr 4.66592330721639e-05
Step 245900 Loss: 0.2193, lr 4.66592330721639e-05
Step 246000 Loss: 0.2176, lr 4.66592330721639e-05
Step 246100 Loss: 0.2156, lr 4.66592330721639e-05
Step 246200 Loss: 0.2139, lr 4.66592330721639e-05
Step 246300 Loss: 0.2117, lr 4.66592330721639e-05
Step 246400 Loss: 0.2096, lr 4.66592330721639e-05
Step 246500 Loss: 0.2077, lr 4.66592330721639e-05
Step 246600 Loss: 0.2058, lr 4.66592330721639e-05
Step 246700 Loss: 0.2041, lr 4.66592330721639e-05
Step 246800 Loss: 0.2021, lr 4.66592330721639e-05
Step 246900 Loss: 0.2007, lr 4.66592330721639e-05
Step 247000 Loss: 0.1995, lr 4.66592330721639e-05
Step 247100 Loss: 0.1981, lr 4.66592330721639e-05
Step 247200 Loss: 0.1973, lr 4.66592330721639e-05
Step 247300 Loss: 0.1960, lr 4.66592330721639e-05
Step 247400 Loss: 0.1952, lr 4.66592330721639e-05
Step 247500 Loss: 0.1942, lr 4.66592330721639e-05
Step 247600 Loss: 0.1931, lr 4.66592330721639e-05
Step 247700 Loss: 0.1922, lr 4.66592330721639e-05
Step 247800 Loss: 0.1909, lr 4.66592330721639e-05
Step 247900 Loss: 0.1899, lr 4.66592330721639e-05
Step 248000 Loss: 0.1893, lr 4.66592330721639e-05
Step 248100 Loss: 0.1887, lr 4.66592330721639e-05
Step 248200 Loss: 0.1881, lr 4.66592330721639e-05
Step 248300 Loss: 0.1876, lr 4.66592330721639e-05
Step 248400 Loss: 0.1871, lr 4.66592330721639e-05
Step 248500 Loss: 0.1864, lr 4.66592330721639e-05
Step 248600 Loss: 0.1860, lr 4.66592330721639e-05
Step 248700 Loss: 0.1857, lr 4.66592330721639e-05
Step 248800 Loss: 0.1852, lr 4.66592330721639e-05
Step 248900 Loss: 0.1850, lr 4.66592330721639e-05
Step 249000 Loss: 0.1847, lr 4.66592330721639e-05
Step 249100 Loss: 0.1846, lr 4.66592330721639e-05
Step 249200 Loss: 0.1848, lr 4.66592330721639e-05
Step 249300 Loss: 0.1848, lr 4.66592330721639e-05
Step 249400 Loss: 0.1850, lr 4.66592330721639e-05
Step 249500 Loss: 0.1856, lr 4.66592330721639e-05
Step 249600 Loss: 0.1865, lr 4.66592330721639e-05
Step 249700 Loss: 0.1873, lr 4.66592330721639e-05
Step 249800 Loss: 0.1872, lr 4.66592330721639e-05
Step 249900 Loss: 0.1866, lr 4.66592330721639e-05
Train Epoch: [77/100] Loss: 0.1868,lr 0.000047
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6756
Step 250000 Loss: 0.6911, lr 4.5722258312401024e-05
Step 250100 Loss: 0.4474, lr 4.5722258312401024e-05
Step 250200 Loss: 0.3883, lr 4.5722258312401024e-05
Step 250300 Loss: 0.3568, lr 4.5722258312401024e-05
Step 250400 Loss: 0.3361, lr 4.5722258312401024e-05
Step 250500 Loss: 0.3246, lr 4.5722258312401024e-05
Step 250600 Loss: 0.3100, lr 4.5722258312401024e-05
Step 250700 Loss: 0.2991, lr 4.5722258312401024e-05
Step 250800 Loss: 0.2922, lr 4.5722258312401024e-05
Step 250900 Loss: 0.2838, lr 4.5722258312401024e-05
Step 251000 Loss: 0.2769, lr 4.5722258312401024e-05
Step 251100 Loss: 0.2715, lr 4.5722258312401024e-05
Step 251200 Loss: 0.2661, lr 4.5722258312401024e-05
Step 251300 Loss: 0.2613, lr 4.5722258312401024e-05
Step 251400 Loss: 0.2562, lr 4.5722258312401024e-05
Step 251500 Loss: 0.2533, lr 4.5722258312401024e-05
Step 251600 Loss: 0.2500, lr 4.5722258312401024e-05
Step 251700 Loss: 0.2466, lr 4.5722258312401024e-05
Step 251800 Loss: 0.2435, lr 4.5722258312401024e-05
Step 251900 Loss: 0.2403, lr 4.5722258312401024e-05
Step 252000 Loss: 0.2364, lr 4.5722258312401024e-05
Step 252100 Loss: 0.2336, lr 4.5722258312401024e-05
Step 252200 Loss: 0.2307, lr 4.5722258312401024e-05
Step 252300 Loss: 0.2279, lr 4.5722258312401024e-05
Step 252400 Loss: 0.2252, lr 4.5722258312401024e-05
Step 252500 Loss: 0.2228, lr 4.5722258312401024e-05
Step 252600 Loss: 0.2205, lr 4.5722258312401024e-05
Step 252700 Loss: 0.2187, lr 4.5722258312401024e-05
Step 252800 Loss: 0.2165, lr 4.5722258312401024e-05
Step 252900 Loss: 0.2147, lr 4.5722258312401024e-05
Step 253000 Loss: 0.2132, lr 4.5722258312401024e-05
Step 253100 Loss: 0.2112, lr 4.5722258312401024e-05
Step 253200 Loss: 0.2092, lr 4.5722258312401024e-05
Step 253300 Loss: 0.2073, lr 4.5722258312401024e-05
Step 253400 Loss: 0.2054, lr 4.5722258312401024e-05
Step 253500 Loss: 0.2037, lr 4.5722258312401024e-05
Step 253600 Loss: 0.2021, lr 4.5722258312401024e-05
Step 253700 Loss: 0.2004, lr 4.5722258312401024e-05
Step 253800 Loss: 0.1994, lr 4.5722258312401024e-05
Step 253900 Loss: 0.1983, lr 4.5722258312401024e-05
Step 254000 Loss: 0.1972, lr 4.5722258312401024e-05
Step 254100 Loss: 0.1960, lr 4.5722258312401024e-05
Step 254200 Loss: 0.1953, lr 4.5722258312401024e-05
Step 254300 Loss: 0.1942, lr 4.5722258312401024e-05
Step 254400 Loss: 0.1931, lr 4.5722258312401024e-05
Step 254500 Loss: 0.1919, lr 4.5722258312401024e-05
Step 254600 Loss: 0.1909, lr 4.5722258312401024e-05
Step 254700 Loss: 0.1900, lr 4.5722258312401024e-05
Step 254800 Loss: 0.1894, lr 4.5722258312401024e-05
Step 254900 Loss: 0.1890, lr 4.5722258312401024e-05
Step 255000 Loss: 0.1883, lr 4.5722258312401024e-05
Step 255100 Loss: 0.1879, lr 4.5722258312401024e-05
Step 255200 Loss: 0.1871, lr 4.5722258312401024e-05
Step 255300 Loss: 0.1868, lr 4.5722258312401024e-05
Step 255400 Loss: 0.1862, lr 4.5722258312401024e-05
Step 255500 Loss: 0.1859, lr 4.5722258312401024e-05
Step 255600 Loss: 0.1855, lr 4.5722258312401024e-05
Step 255700 Loss: 0.1854, lr 4.5722258312401024e-05
Step 255800 Loss: 0.1851, lr 4.5722258312401024e-05
Step 255900 Loss: 0.1851, lr 4.5722258312401024e-05
Step 256000 Loss: 0.1851, lr 4.5722258312401024e-05
Step 256100 Loss: 0.1852, lr 4.5722258312401024e-05
Step 256200 Loss: 0.1859, lr 4.5722258312401024e-05
Step 256300 Loss: 0.1864, lr 4.5722258312401024e-05
Step 256400 Loss: 0.1873, lr 4.5722258312401024e-05
Step 256500 Loss: 0.1877, lr 4.5722258312401024e-05
Step 256600 Loss: 0.1872, lr 4.5722258312401024e-05
Step 256700 Loss: 0.1867, lr 4.5722258312401024e-05
Train Epoch: [78/100] Loss: 0.1870,lr 0.000046
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Step 256800 Loss: 0.4789, lr 4.477245724714674e-05
Step 256900 Loss: 0.3960, lr 4.477245724714674e-05
Step 257000 Loss: 0.3591, lr 4.477245724714674e-05
Step 257100 Loss: 0.3364, lr 4.477245724714674e-05
Step 257200 Loss: 0.3237, lr 4.477245724714674e-05
Step 257300 Loss: 0.3109, lr 4.477245724714674e-05
Step 257400 Loss: 0.2991, lr 4.477245724714674e-05
Step 257500 Loss: 0.2916, lr 4.477245724714674e-05
Step 257600 Loss: 0.2835, lr 4.477245724714674e-05
Step 257700 Loss: 0.2763, lr 4.477245724714674e-05
Step 257800 Loss: 0.2704, lr 4.477245724714674e-05
Step 257900 Loss: 0.2649, lr 4.477245724714674e-05
Step 258000 Loss: 0.2605, lr 4.477245724714674e-05
Step 258100 Loss: 0.2549, lr 4.477245724714674e-05
Step 258200 Loss: 0.2512, lr 4.477245724714674e-05
Step 258300 Loss: 0.2485, lr 4.477245724714674e-05
Step 258400 Loss: 0.2456, lr 4.477245724714674e-05
Step 258500 Loss: 0.2424, lr 4.477245724714674e-05
Step 258600 Loss: 0.2394, lr 4.477245724714674e-05
Step 258700 Loss: 0.2365, lr 4.477245724714674e-05
Step 258800 Loss: 0.2328, lr 4.477245724714674e-05
Step 258900 Loss: 0.2305, lr 4.477245724714674e-05
Step 259000 Loss: 0.2280, lr 4.477245724714674e-05
Step 259100 Loss: 0.2254, lr 4.477245724714674e-05
Step 259200 Loss: 0.2227, lr 4.477245724714674e-05
Step 259300 Loss: 0.2204, lr 4.477245724714674e-05
Step 259400 Loss: 0.2183, lr 4.477245724714674e-05
Step 259500 Loss: 0.2163, lr 4.477245724714674e-05
Step 259600 Loss: 0.2146, lr 4.477245724714674e-05
Step 259700 Loss: 0.2129, lr 4.477245724714674e-05
Step 259800 Loss: 0.2108, lr 4.477245724714674e-05
Step 259900 Loss: 0.2088, lr 4.477245724714674e-05
Step 260000 Loss: 0.2070, lr 4.477245724714674e-05
Step 260100 Loss: 0.2049, lr 4.477245724714674e-05
Step 260200 Loss: 0.2032, lr 4.477245724714674e-05
Step 260300 Loss: 0.2013, lr 4.477245724714674e-05
Step 260400 Loss: 0.1999, lr 4.477245724714674e-05
Step 260500 Loss: 0.1987, lr 4.477245724714674e-05
Step 260600 Loss: 0.1973, lr 4.477245724714674e-05
Step 260700 Loss: 0.1964, lr 4.477245724714674e-05
Step 260800 Loss: 0.1953, lr 4.477245724714674e-05
Step 260900 Loss: 0.1945, lr 4.477245724714674e-05
Step 261000 Loss: 0.1934, lr 4.477245724714674e-05
Step 261100 Loss: 0.1926, lr 4.477245724714674e-05
Step 261200 Loss: 0.1917, lr 4.477245724714674e-05
Step 261300 Loss: 0.1906, lr 4.477245724714674e-05
Step 261400 Loss: 0.1896, lr 4.477245724714674e-05
Step 261500 Loss: 0.1890, lr 4.477245724714674e-05
Step 261600 Loss: 0.1884, lr 4.477245724714674e-05
Step 261700 Loss: 0.1878, lr 4.477245724714674e-05
Step 261800 Loss: 0.1872, lr 4.477245724714674e-05
Step 261900 Loss: 0.1869, lr 4.477245724714674e-05
Step 262000 Loss: 0.1861, lr 4.477245724714674e-05
Step 262100 Loss: 0.1856, lr 4.477245724714674e-05
Step 262200 Loss: 0.1851, lr 4.477245724714674e-05
Step 262300 Loss: 0.1848, lr 4.477245724714674e-05
Step 262400 Loss: 0.1844, lr 4.477245724714674e-05
Step 262500 Loss: 0.1843, lr 4.477245724714674e-05
Step 262600 Loss: 0.1841, lr 4.477245724714674e-05
Step 262700 Loss: 0.1842, lr 4.477245724714674e-05
Step 262800 Loss: 0.1841, lr 4.477245724714674e-05
Step 262900 Loss: 0.1843, lr 4.477245724714674e-05
Step 263000 Loss: 0.1848, lr 4.477245724714674e-05
Step 263100 Loss: 0.1856, lr 4.477245724714674e-05
Step 263200 Loss: 0.1863, lr 4.477245724714674e-05
Step 263300 Loss: 0.1862, lr 4.477245724714674e-05
Step 263400 Loss: 0.1857, lr 4.477245724714674e-05
Train Epoch: [79/100] Loss: 0.1857,lr 0.000045
Calling G2SDataset.batch()
Done, time:  2.61 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Step 263500 Loss: 0.7206, lr 4.381076721538156e-05
Step 263600 Loss: 0.4356, lr 4.381076721538156e-05
Step 263700 Loss: 0.3787, lr 4.381076721538156e-05
Step 263800 Loss: 0.3475, lr 4.381076721538156e-05
Step 263900 Loss: 0.3295, lr 4.381076721538156e-05
Step 264000 Loss: 0.3182, lr 4.381076721538156e-05
Step 264100 Loss: 0.3043, lr 4.381076721538156e-05
Step 264200 Loss: 0.2942, lr 4.381076721538156e-05
Step 264300 Loss: 0.2869, lr 4.381076721538156e-05
Step 264400 Loss: 0.2791, lr 4.381076721538156e-05
Step 264500 Loss: 0.2728, lr 4.381076721538156e-05
Step 264600 Loss: 0.2673, lr 4.381076721538156e-05
Step 264700 Loss: 0.2617, lr 4.381076721538156e-05
Step 264800 Loss: 0.2571, lr 4.381076721538156e-05
Step 264900 Loss: 0.2525, lr 4.381076721538156e-05
Step 265000 Loss: 0.2495, lr 4.381076721538156e-05
Step 265100 Loss: 0.2463, lr 4.381076721538156e-05
Step 265200 Loss: 0.2432, lr 4.381076721538156e-05
Step 265300 Loss: 0.2402, lr 4.381076721538156e-05
Step 265400 Loss: 0.2371, lr 4.381076721538156e-05
Step 265500 Loss: 0.2333, lr 4.381076721538156e-05
Step 265600 Loss: 0.2302, lr 4.381076721538156e-05
Step 265700 Loss: 0.2274, lr 4.381076721538156e-05
Step 265800 Loss: 0.2247, lr 4.381076721538156e-05
Step 265900 Loss: 0.2222, lr 4.381076721538156e-05
Step 266000 Loss: 0.2196, lr 4.381076721538156e-05
Step 266100 Loss: 0.2172, lr 4.381076721538156e-05
Step 266200 Loss: 0.2156, lr 4.381076721538156e-05
Step 266300 Loss: 0.2135, lr 4.381076721538156e-05
Step 266400 Loss: 0.2117, lr 4.381076721538156e-05
Step 266500 Loss: 0.2099, lr 4.381076721538156e-05
Step 266600 Loss: 0.2080, lr 4.381076721538156e-05
Step 266700 Loss: 0.2061, lr 4.381076721538156e-05
Step 266800 Loss: 0.2040, lr 4.381076721538156e-05
Step 266900 Loss: 0.2023, lr 4.381076721538156e-05
Step 267000 Loss: 0.2006, lr 4.381076721538156e-05
Step 267100 Loss: 0.1991, lr 4.381076721538156e-05
Step 267200 Loss: 0.1974, lr 4.381076721538156e-05
Step 267300 Loss: 0.1963, lr 4.381076721538156e-05
Step 267400 Loss: 0.1951, lr 4.381076721538156e-05
Step 267500 Loss: 0.1942, lr 4.381076721538156e-05
Step 267600 Loss: 0.1930, lr 4.381076721538156e-05
Step 267700 Loss: 0.1921, lr 4.381076721538156e-05
Step 267800 Loss: 0.1912, lr 4.381076721538156e-05
Step 267900 Loss: 0.1901, lr 4.381076721538156e-05
Step 268000 Loss: 0.1891, lr 4.381076721538156e-05
Step 268100 Loss: 0.1879, lr 4.381076721538156e-05
Step 268200 Loss: 0.1870, lr 4.381076721538156e-05
Step 268300 Loss: 0.1867, lr 4.381076721538156e-05
Step 268400 Loss: 0.1863, lr 4.381076721538156e-05
Step 268500 Loss: 0.1855, lr 4.381076721538156e-05
Step 268600 Loss: 0.1852, lr 4.381076721538156e-05
Step 268700 Loss: 0.1846, lr 4.381076721538156e-05
Step 268800 Loss: 0.1841, lr 4.381076721538156e-05
Step 268900 Loss: 0.1836, lr 4.381076721538156e-05
Step 269000 Loss: 0.1832, lr 4.381076721538156e-05
Step 269100 Loss: 0.1828, lr 4.381076721538156e-05
Step 269200 Loss: 0.1825, lr 4.381076721538156e-05
Step 269300 Loss: 0.1824, lr 4.381076721538156e-05
Step 269400 Loss: 0.1823, lr 4.381076721538156e-05
Step 269500 Loss: 0.1824, lr 4.381076721538156e-05
Step 269600 Loss: 0.1823, lr 4.381076721538156e-05
Step 269700 Loss: 0.1827, lr 4.381076721538156e-05
Step 269800 Loss: 0.1834, lr 4.381076721538156e-05
Step 269900 Loss: 0.1842, lr 4.381076721538156e-05
Step 270000 Loss: 0.1848, lr 4.381076721538156e-05
Step 270100 Loss: 0.1841, lr 4.381076721538156e-05
Step 270200 Loss: 0.1837, lr 4.381076721538156e-05
Train Epoch: [80/100] Loss: 0.1843,lr 0.000044
Model Saving at epoch 80
Calling G2SDataset.batch()
Done, time:  2.28 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6756
Step 270300 Loss: 0.5004, lr 4.28381372890606e-05
Step 270400 Loss: 0.3970, lr 4.28381372890606e-05
Step 270500 Loss: 0.3586, lr 4.28381372890606e-05
Step 270600 Loss: 0.3336, lr 4.28381372890606e-05
Step 270700 Loss: 0.3210, lr 4.28381372890606e-05
Step 270800 Loss: 0.3089, lr 4.28381372890606e-05
Step 270900 Loss: 0.2963, lr 4.28381372890606e-05
Step 271000 Loss: 0.2886, lr 4.28381372890606e-05
Step 271100 Loss: 0.2812, lr 4.28381372890606e-05
Step 271200 Loss: 0.2739, lr 4.28381372890606e-05
Step 271300 Loss: 0.2683, lr 4.28381372890606e-05
Step 271400 Loss: 0.2631, lr 4.28381372890606e-05
Step 271500 Loss: 0.2587, lr 4.28381372890606e-05
Step 271600 Loss: 0.2529, lr 4.28381372890606e-05
Step 271700 Loss: 0.2493, lr 4.28381372890606e-05
Step 271800 Loss: 0.2466, lr 4.28381372890606e-05
Step 271900 Loss: 0.2440, lr 4.28381372890606e-05
Step 272000 Loss: 0.2408, lr 4.28381372890606e-05
Step 272100 Loss: 0.2382, lr 4.28381372890606e-05
Step 272200 Loss: 0.2348, lr 4.28381372890606e-05
Step 272300 Loss: 0.2311, lr 4.28381372890606e-05
Step 272400 Loss: 0.2284, lr 4.28381372890606e-05
Step 272500 Loss: 0.2258, lr 4.28381372890606e-05
Step 272600 Loss: 0.2231, lr 4.28381372890606e-05
Step 272700 Loss: 0.2204, lr 4.28381372890606e-05
Step 272800 Loss: 0.2183, lr 4.28381372890606e-05
Step 272900 Loss: 0.2161, lr 4.28381372890606e-05
Step 273000 Loss: 0.2143, lr 4.28381372890606e-05
Step 273100 Loss: 0.2123, lr 4.28381372890606e-05
Step 273200 Loss: 0.2107, lr 4.28381372890606e-05
Step 273300 Loss: 0.2085, lr 4.28381372890606e-05
Step 273400 Loss: 0.2064, lr 4.28381372890606e-05
Step 273500 Loss: 0.2049, lr 4.28381372890606e-05
Step 273600 Loss: 0.2027, lr 4.28381372890606e-05
Step 273700 Loss: 0.2010, lr 4.28381372890606e-05
Step 273800 Loss: 0.1993, lr 4.28381372890606e-05
Step 273900 Loss: 0.1978, lr 4.28381372890606e-05
Step 274000 Loss: 0.1964, lr 4.28381372890606e-05
Step 274100 Loss: 0.1950, lr 4.28381372890606e-05
Step 274200 Loss: 0.1940, lr 4.28381372890606e-05
Step 274300 Loss: 0.1930, lr 4.28381372890606e-05
Step 274400 Loss: 0.1920, lr 4.28381372890606e-05
Step 274500 Loss: 0.1910, lr 4.28381372890606e-05
Step 274600 Loss: 0.1900, lr 4.28381372890606e-05
Step 274700 Loss: 0.1890, lr 4.28381372890606e-05
Step 274800 Loss: 0.1879, lr 4.28381372890606e-05
Step 274900 Loss: 0.1868, lr 4.28381372890606e-05
Step 275000 Loss: 0.1862, lr 4.28381372890606e-05
Step 275100 Loss: 0.1857, lr 4.28381372890606e-05
Step 275200 Loss: 0.1851, lr 4.28381372890606e-05
Step 275300 Loss: 0.1844, lr 4.28381372890606e-05
Step 275400 Loss: 0.1841, lr 4.28381372890606e-05
Step 275500 Loss: 0.1834, lr 4.28381372890606e-05
Step 275600 Loss: 0.1830, lr 4.28381372890606e-05
Step 275700 Loss: 0.1825, lr 4.28381372890606e-05
Step 275800 Loss: 0.1821, lr 4.28381372890606e-05
Step 275900 Loss: 0.1817, lr 4.28381372890606e-05
Step 276000 Loss: 0.1815, lr 4.28381372890606e-05
Step 276100 Loss: 0.1814, lr 4.28381372890606e-05
Step 276200 Loss: 0.1813, lr 4.28381372890606e-05
Step 276300 Loss: 0.1814, lr 4.28381372890606e-05
Step 276400 Loss: 0.1815, lr 4.28381372890606e-05
Step 276500 Loss: 0.1822, lr 4.28381372890606e-05
Step 276600 Loss: 0.1827, lr 4.28381372890606e-05
Step 276700 Loss: 0.1836, lr 4.28381372890606e-05
Step 276800 Loss: 0.1838, lr 4.28381372890606e-05
Step 276900 Loss: 0.1832, lr 4.28381372890606e-05
Train Epoch: [81/100] Loss: 0.1835,lr 0.000043
Calling G2SDataset.batch()
Done, time:  2.12 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6756
Step 277000 Loss: 0.8762, lr 4.185552733649411e-05
Step 277100 Loss: 0.4400, lr 4.185552733649411e-05
Step 277200 Loss: 0.3782, lr 4.185552733649411e-05
Step 277300 Loss: 0.3448, lr 4.185552733649411e-05
Step 277400 Loss: 0.3281, lr 4.185552733649411e-05
Step 277500 Loss: 0.3157, lr 4.185552733649411e-05
Step 277600 Loss: 0.3021, lr 4.185552733649411e-05
Step 277700 Loss: 0.2916, lr 4.185552733649411e-05
Step 277800 Loss: 0.2855, lr 4.185552733649411e-05
Step 277900 Loss: 0.2773, lr 4.185552733649411e-05
Step 278000 Loss: 0.2705, lr 4.185552733649411e-05
Step 278100 Loss: 0.2656, lr 4.185552733649411e-05
Step 278200 Loss: 0.2599, lr 4.185552733649411e-05
Step 278300 Loss: 0.2548, lr 4.185552733649411e-05
Step 278400 Loss: 0.2503, lr 4.185552733649411e-05
Step 278500 Loss: 0.2472, lr 4.185552733649411e-05
Step 278600 Loss: 0.2438, lr 4.185552733649411e-05
Step 278700 Loss: 0.2412, lr 4.185552733649411e-05
Step 278800 Loss: 0.2382, lr 4.185552733649411e-05
Step 278900 Loss: 0.2352, lr 4.185552733649411e-05
Step 279000 Loss: 0.2314, lr 4.185552733649411e-05
Step 279100 Loss: 0.2280, lr 4.185552733649411e-05
Step 279200 Loss: 0.2257, lr 4.185552733649411e-05
Step 279300 Loss: 0.2230, lr 4.185552733649411e-05
Step 279400 Loss: 0.2206, lr 4.185552733649411e-05
Step 279500 Loss: 0.2179, lr 4.185552733649411e-05
Step 279600 Loss: 0.2158, lr 4.185552733649411e-05
Step 279700 Loss: 0.2140, lr 4.185552733649411e-05
Step 279800 Loss: 0.2122, lr 4.185552733649411e-05
Step 279900 Loss: 0.2103, lr 4.185552733649411e-05
Step 280000 Loss: 0.2086, lr 4.185552733649411e-05
Step 280100 Loss: 0.2065, lr 4.185552733649411e-05
Step 280200 Loss: 0.2046, lr 4.185552733649411e-05
Step 280300 Loss: 0.2028, lr 4.185552733649411e-05
Step 280400 Loss: 0.2011, lr 4.185552733649411e-05
Step 280500 Loss: 0.1994, lr 4.185552733649411e-05
Step 280600 Loss: 0.1977, lr 4.185552733649411e-05
Step 280700 Loss: 0.1963, lr 4.185552733649411e-05
Step 280800 Loss: 0.1950, lr 4.185552733649411e-05
Step 280900 Loss: 0.1937, lr 4.185552733649411e-05
Step 281000 Loss: 0.1927, lr 4.185552733649411e-05
Step 281100 Loss: 0.1917, lr 4.185552733649411e-05
Step 281200 Loss: 0.1908, lr 4.185552733649411e-05
Step 281300 Loss: 0.1899, lr 4.185552733649411e-05
Step 281400 Loss: 0.1887, lr 4.185552733649411e-05
Step 281500 Loss: 0.1877, lr 4.185552733649411e-05
Step 281600 Loss: 0.1865, lr 4.185552733649411e-05
Step 281700 Loss: 0.1856, lr 4.185552733649411e-05
Step 281800 Loss: 0.1850, lr 4.185552733649411e-05
Step 281900 Loss: 0.1845, lr 4.185552733649411e-05
Step 282000 Loss: 0.1838, lr 4.185552733649411e-05
Step 282100 Loss: 0.1834, lr 4.185552733649411e-05
Step 282200 Loss: 0.1827, lr 4.185552733649411e-05
Step 282300 Loss: 0.1821, lr 4.185552733649411e-05
Step 282400 Loss: 0.1815, lr 4.185552733649411e-05
Step 282500 Loss: 0.1811, lr 4.185552733649411e-05
Step 282600 Loss: 0.1806, lr 4.185552733649411e-05
Step 282700 Loss: 0.1803, lr 4.185552733649411e-05
Step 282800 Loss: 0.1802, lr 4.185552733649411e-05
Step 282900 Loss: 0.1801, lr 4.185552733649411e-05
Step 283000 Loss: 0.1802, lr 4.185552733649411e-05
Step 283100 Loss: 0.1801, lr 4.185552733649411e-05
Step 283200 Loss: 0.1803, lr 4.185552733649411e-05
Step 283300 Loss: 0.1809, lr 4.185552733649411e-05
Step 283400 Loss: 0.1818, lr 4.185552733649411e-05
Step 283500 Loss: 0.1826, lr 4.185552733649411e-05
Step 283600 Loss: 0.1823, lr 4.185552733649411e-05
Step 283700 Loss: 0.1820, lr 4.185552733649411e-05
Train Epoch: [82/100] Loss: 0.1824,lr 0.000042
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 6755
Step 283800 Loss: 0.6039, lr 4.086390707507332e-05
Step 283900 Loss: 0.4336, lr 4.086390707507332e-05
Step 284000 Loss: 0.3798, lr 4.086390707507332e-05
Step 284100 Loss: 0.3487, lr 4.086390707507332e-05
Step 284200 Loss: 0.3319, lr 4.086390707507332e-05
Step 284300 Loss: 0.3187, lr 4.086390707507332e-05
Step 284400 Loss: 0.3040, lr 4.086390707507332e-05
Step 284500 Loss: 0.2948, lr 4.086390707507332e-05
Step 284600 Loss: 0.2870, lr 4.086390707507332e-05
Step 284700 Loss: 0.2795, lr 4.086390707507332e-05
Step 284800 Loss: 0.2728, lr 4.086390707507332e-05
Step 284900 Loss: 0.2672, lr 4.086390707507332e-05
Step 285000 Loss: 0.2620, lr 4.086390707507332e-05
Step 285100 Loss: 0.2567, lr 4.086390707507332e-05
Step 285200 Loss: 0.2524, lr 4.086390707507332e-05
Step 285300 Loss: 0.2493, lr 4.086390707507332e-05
Step 285400 Loss: 0.2461, lr 4.086390707507332e-05
Step 285500 Loss: 0.2429, lr 4.086390707507332e-05
Step 285600 Loss: 0.2397, lr 4.086390707507332e-05
Step 285700 Loss: 0.2363, lr 4.086390707507332e-05
Step 285800 Loss: 0.2327, lr 4.086390707507332e-05
Step 285900 Loss: 0.2299, lr 4.086390707507332e-05
Step 286000 Loss: 0.2272, lr 4.086390707507332e-05
Step 286100 Loss: 0.2244, lr 4.086390707507332e-05
Step 286200 Loss: 0.2216, lr 4.086390707507332e-05
Step 286300 Loss: 0.2194, lr 4.086390707507332e-05
Step 286400 Loss: 0.2169, lr 4.086390707507332e-05
Step 286500 Loss: 0.2152, lr 4.086390707507332e-05
Step 286600 Loss: 0.2131, lr 4.086390707507332e-05
Step 286700 Loss: 0.2114, lr 4.086390707507332e-05
Step 286800 Loss: 0.2093, lr 4.086390707507332e-05
Step 286900 Loss: 0.2071, lr 4.086390707507332e-05
Step 287000 Loss: 0.2054, lr 4.086390707507332e-05
Step 287100 Loss: 0.2031, lr 4.086390707507332e-05
Step 287200 Loss: 0.2014, lr 4.086390707507332e-05
Step 287300 Loss: 0.1996, lr 4.086390707507332e-05
Step 287400 Loss: 0.1981, lr 4.086390707507332e-05
Step 287500 Loss: 0.1967, lr 4.086390707507332e-05
Step 287600 Loss: 0.1955, lr 4.086390707507332e-05
Step 287700 Loss: 0.1943, lr 4.086390707507332e-05
Step 287800 Loss: 0.1934, lr 4.086390707507332e-05
Step 287900 Loss: 0.1922, lr 4.086390707507332e-05
Step 288000 Loss: 0.1913, lr 4.086390707507332e-05
Step 288100 Loss: 0.1902, lr 4.086390707507332e-05
Step 288200 Loss: 0.1892, lr 4.086390707507332e-05
Step 288300 Loss: 0.1880, lr 4.086390707507332e-05
Step 288400 Loss: 0.1869, lr 4.086390707507332e-05
Step 288500 Loss: 0.1862, lr 4.086390707507332e-05
Step 288600 Loss: 0.1856, lr 4.086390707507332e-05
Step 288700 Loss: 0.1851, lr 4.086390707507332e-05
Step 288800 Loss: 0.1843, lr 4.086390707507332e-05
Step 288900 Loss: 0.1841, lr 4.086390707507332e-05
Step 289000 Loss: 0.1833, lr 4.086390707507332e-05
Step 289100 Loss: 0.1827, lr 4.086390707507332e-05
Step 289200 Loss: 0.1822, lr 4.086390707507332e-05
Step 289300 Loss: 0.1818, lr 4.086390707507332e-05
Step 289400 Loss: 0.1815, lr 4.086390707507332e-05
Step 289500 Loss: 0.1812, lr 4.086390707507332e-05
Step 289600 Loss: 0.1811, lr 4.086390707507332e-05
Step 289700 Loss: 0.1810, lr 4.086390707507332e-05
Step 289800 Loss: 0.1810, lr 4.086390707507332e-05
Step 289900 Loss: 0.1810, lr 4.086390707507332e-05
Step 290000 Loss: 0.1816, lr 4.086390707507332e-05
Step 290100 Loss: 0.1821, lr 4.086390707507332e-05
Step 290200 Loss: 0.1830, lr 4.086390707507332e-05
Step 290300 Loss: 0.1832, lr 4.086390707507332e-05
Step 290400 Loss: 0.1826, lr 4.086390707507332e-05
Train Epoch: [83/100] Loss: 0.1829,lr 0.000041
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.32 s, total batches: 6755
Step 290500 Loss: 1.2074, lr 3.986425511427652e-05
Step 290600 Loss: 0.4516, lr 3.986425511427652e-05
Step 290700 Loss: 0.3821, lr 3.986425511427652e-05
Step 290800 Loss: 0.3480, lr 3.986425511427652e-05
Step 290900 Loss: 0.3302, lr 3.986425511427652e-05
Step 291000 Loss: 0.3171, lr 3.986425511427652e-05
Step 291100 Loss: 0.3031, lr 3.986425511427652e-05
Step 291200 Loss: 0.2931, lr 3.986425511427652e-05
Step 291300 Loss: 0.2860, lr 3.986425511427652e-05
Step 291400 Loss: 0.2776, lr 3.986425511427652e-05
Step 291500 Loss: 0.2706, lr 3.986425511427652e-05
Step 291600 Loss: 0.2650, lr 3.986425511427652e-05
Step 291700 Loss: 0.2596, lr 3.986425511427652e-05
Step 291800 Loss: 0.2550, lr 3.986425511427652e-05
Step 291900 Loss: 0.2498, lr 3.986425511427652e-05
Step 292000 Loss: 0.2469, lr 3.986425511427652e-05
Step 292100 Loss: 0.2433, lr 3.986425511427652e-05
Step 292200 Loss: 0.2406, lr 3.986425511427652e-05
Step 292300 Loss: 0.2375, lr 3.986425511427652e-05
Step 292400 Loss: 0.2347, lr 3.986425511427652e-05
Step 292500 Loss: 0.2308, lr 3.986425511427652e-05
Step 292600 Loss: 0.2274, lr 3.986425511427652e-05
Step 292700 Loss: 0.2251, lr 3.986425511427652e-05
Step 292800 Loss: 0.2224, lr 3.986425511427652e-05
Step 292900 Loss: 0.2200, lr 3.986425511427652e-05
Step 293000 Loss: 0.2172, lr 3.986425511427652e-05
Step 293100 Loss: 0.2147, lr 3.986425511427652e-05
Step 293200 Loss: 0.2127, lr 3.986425511427652e-05
Step 293300 Loss: 0.2112, lr 3.986425511427652e-05
Step 293400 Loss: 0.2092, lr 3.986425511427652e-05
Step 293500 Loss: 0.2074, lr 3.986425511427652e-05
Step 293600 Loss: 0.2052, lr 3.986425511427652e-05
Step 293700 Loss: 0.2033, lr 3.986425511427652e-05
Step 293800 Loss: 0.2013, lr 3.986425511427652e-05
Step 293900 Loss: 0.1996, lr 3.986425511427652e-05
Step 294000 Loss: 0.1979, lr 3.986425511427652e-05
Step 294100 Loss: 0.1961, lr 3.986425511427652e-05
Step 294200 Loss: 0.1945, lr 3.986425511427652e-05
Step 294300 Loss: 0.1934, lr 3.986425511427652e-05
Step 294400 Loss: 0.1921, lr 3.986425511427652e-05
Step 294500 Loss: 0.1912, lr 3.986425511427652e-05
Step 294600 Loss: 0.1900, lr 3.986425511427652e-05
Step 294700 Loss: 0.1892, lr 3.986425511427652e-05
Step 294800 Loss: 0.1883, lr 3.986425511427652e-05
Step 294900 Loss: 0.1872, lr 3.986425511427652e-05
Step 295000 Loss: 0.1863, lr 3.986425511427652e-05
Step 295100 Loss: 0.1851, lr 3.986425511427652e-05
Step 295200 Loss: 0.1843, lr 3.986425511427652e-05
Step 295300 Loss: 0.1837, lr 3.986425511427652e-05
Step 295400 Loss: 0.1832, lr 3.986425511427652e-05
Step 295500 Loss: 0.1826, lr 3.986425511427652e-05
Step 295600 Loss: 0.1821, lr 3.986425511427652e-05
Step 295700 Loss: 0.1816, lr 3.986425511427652e-05
Step 295800 Loss: 0.1809, lr 3.986425511427652e-05
Step 295900 Loss: 0.1803, lr 3.986425511427652e-05
Step 296000 Loss: 0.1800, lr 3.986425511427652e-05
Step 296100 Loss: 0.1795, lr 3.986425511427652e-05
Step 296200 Loss: 0.1794, lr 3.986425511427652e-05
Step 296300 Loss: 0.1792, lr 3.986425511427652e-05
Step 296400 Loss: 0.1790, lr 3.986425511427652e-05
Step 296500 Loss: 0.1791, lr 3.986425511427652e-05
Step 296600 Loss: 0.1790, lr 3.986425511427652e-05
Step 296700 Loss: 0.1793, lr 3.986425511427652e-05
Step 296800 Loss: 0.1799, lr 3.986425511427652e-05
Step 296900 Loss: 0.1806, lr 3.986425511427652e-05
Step 297000 Loss: 0.1813, lr 3.986425511427652e-05
Step 297100 Loss: 0.1811, lr 3.986425511427652e-05
Step 297200 Loss: 0.1806, lr 3.986425511427652e-05
Train Epoch: [84/100] Loss: 0.1808,lr 0.000040
Calling G2SDataset.batch()
Done, time:  2.34 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.35 s, total batches: 6756
Step 297300 Loss: 0.5934, lr 3.885755798989955e-05
Step 297400 Loss: 0.4230, lr 3.885755798989955e-05
Step 297500 Loss: 0.3709, lr 3.885755798989955e-05
Step 297600 Loss: 0.3417, lr 3.885755798989955e-05
Step 297700 Loss: 0.3257, lr 3.885755798989955e-05
Step 297800 Loss: 0.3134, lr 3.885755798989955e-05
Step 297900 Loss: 0.2992, lr 3.885755798989955e-05
Step 298000 Loss: 0.2896, lr 3.885755798989955e-05
Step 298100 Loss: 0.2827, lr 3.885755798989955e-05
Step 298200 Loss: 0.2750, lr 3.885755798989955e-05
Step 298300 Loss: 0.2684, lr 3.885755798989955e-05
Step 298400 Loss: 0.2631, lr 3.885755798989955e-05
Step 298500 Loss: 0.2581, lr 3.885755798989955e-05
Step 298600 Loss: 0.2531, lr 3.885755798989955e-05
Step 298700 Loss: 0.2486, lr 3.885755798989955e-05
Step 298800 Loss: 0.2456, lr 3.885755798989955e-05
Step 298900 Loss: 0.2428, lr 3.885755798989955e-05
Step 299000 Loss: 0.2395, lr 3.885755798989955e-05
Step 299100 Loss: 0.2366, lr 3.885755798989955e-05
Step 299200 Loss: 0.2332, lr 3.885755798989955e-05
Step 299300 Loss: 0.2296, lr 3.885755798989955e-05
Step 299400 Loss: 0.2268, lr 3.885755798989955e-05
Step 299500 Loss: 0.2241, lr 3.885755798989955e-05
Step 299600 Loss: 0.2213, lr 3.885755798989955e-05
Step 299700 Loss: 0.2189, lr 3.885755798989955e-05
Step 299800 Loss: 0.2165, lr 3.885755798989955e-05
Step 299900 Loss: 0.2141, lr 3.885755798989955e-05
Step 300000 Loss: 0.2124, lr 3.885755798989955e-05
Step 300100 Loss: 0.2105, lr 3.885755798989955e-05
Step 300200 Loss: 0.2088, lr 3.885755798989955e-05
Step 300300 Loss: 0.2068, lr 3.885755798989955e-05
Step 300400 Loss: 0.2046, lr 3.885755798989955e-05
Step 300500 Loss: 0.2027, lr 3.885755798989955e-05
Step 300600 Loss: 0.2006, lr 3.885755798989955e-05
Step 300700 Loss: 0.1991, lr 3.885755798989955e-05
Step 300800 Loss: 0.1974, lr 3.885755798989955e-05
Step 300900 Loss: 0.1958, lr 3.885755798989955e-05
Step 301000 Loss: 0.1943, lr 3.885755798989955e-05
Step 301100 Loss: 0.1932, lr 3.885755798989955e-05
Step 301200 Loss: 0.1920, lr 3.885755798989955e-05
Step 301300 Loss: 0.1910, lr 3.885755798989955e-05
Step 301400 Loss: 0.1899, lr 3.885755798989955e-05
Step 301500 Loss: 0.1892, lr 3.885755798989955e-05
Step 301600 Loss: 0.1881, lr 3.885755798989955e-05
Step 301700 Loss: 0.1872, lr 3.885755798989955e-05
Step 301800 Loss: 0.1859, lr 3.885755798989955e-05
Step 301900 Loss: 0.1849, lr 3.885755798989955e-05
Step 302000 Loss: 0.1840, lr 3.885755798989955e-05
Step 302100 Loss: 0.1835, lr 3.885755798989955e-05
Step 302200 Loss: 0.1831, lr 3.885755798989955e-05
Step 302300 Loss: 0.1823, lr 3.885755798989955e-05
Step 302400 Loss: 0.1821, lr 3.885755798989955e-05
Step 302500 Loss: 0.1812, lr 3.885755798989955e-05
Step 302600 Loss: 0.1808, lr 3.885755798989955e-05
Step 302700 Loss: 0.1803, lr 3.885755798989955e-05
Step 302800 Loss: 0.1799, lr 3.885755798989955e-05
Step 302900 Loss: 0.1794, lr 3.885755798989955e-05
Step 303000 Loss: 0.1793, lr 3.885755798989955e-05
Step 303100 Loss: 0.1792, lr 3.885755798989955e-05
Step 303200 Loss: 0.1790, lr 3.885755798989955e-05
Step 303300 Loss: 0.1792, lr 3.885755798989955e-05
Step 303400 Loss: 0.1792, lr 3.885755798989955e-05
Step 303500 Loss: 0.1796, lr 3.885755798989955e-05
Step 303600 Loss: 0.1801, lr 3.885755798989955e-05
Step 303700 Loss: 0.1810, lr 3.885755798989955e-05
Step 303800 Loss: 0.1814, lr 3.885755798989955e-05
Step 303900 Loss: 0.1809, lr 3.885755798989955e-05
Step 304000 Loss: 0.1805, lr 3.885755798989955e-05
Train Epoch: [85/100] Loss: 0.1808,lr 0.000039
Model Saving at epoch 85
Calling G2SDataset.batch()
Done, time:  2.30 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.17 s, total batches: 6755
Step 304100 Loss: 0.4733, lr 3.7844809190464146e-05
Step 304200 Loss: 0.3916, lr 3.7844809190464146e-05
Step 304300 Loss: 0.3536, lr 3.7844809190464146e-05
Step 304400 Loss: 0.3327, lr 3.7844809190464146e-05
Step 304500 Loss: 0.3186, lr 3.7844809190464146e-05
Step 304600 Loss: 0.3050, lr 3.7844809190464146e-05
Step 304700 Loss: 0.2937, lr 3.7844809190464146e-05
Step 304800 Loss: 0.2861, lr 3.7844809190464146e-05
Step 304900 Loss: 0.2782, lr 3.7844809190464146e-05
Step 305000 Loss: 0.2711, lr 3.7844809190464146e-05
Step 305100 Loss: 0.2659, lr 3.7844809190464146e-05
Step 305200 Loss: 0.2602, lr 3.7844809190464146e-05
Step 305300 Loss: 0.2556, lr 3.7844809190464146e-05
Step 305400 Loss: 0.2503, lr 3.7844809190464146e-05
Step 305500 Loss: 0.2470, lr 3.7844809190464146e-05
Step 305600 Loss: 0.2439, lr 3.7844809190464146e-05
Step 305700 Loss: 0.2409, lr 3.7844809190464146e-05
Step 305800 Loss: 0.2379, lr 3.7844809190464146e-05
Step 305900 Loss: 0.2350, lr 3.7844809190464146e-05
Step 306000 Loss: 0.2310, lr 3.7844809190464146e-05
Step 306100 Loss: 0.2277, lr 3.7844809190464146e-05
Step 306200 Loss: 0.2251, lr 3.7844809190464146e-05
Step 306300 Loss: 0.2226, lr 3.7844809190464146e-05
Step 306400 Loss: 0.2198, lr 3.7844809190464146e-05
Step 306500 Loss: 0.2172, lr 3.7844809190464146e-05
Step 306600 Loss: 0.2148, lr 3.7844809190464146e-05
Step 306700 Loss: 0.2127, lr 3.7844809190464146e-05
Step 306800 Loss: 0.2112, lr 3.7844809190464146e-05
Step 306900 Loss: 0.2093, lr 3.7844809190464146e-05
Step 307000 Loss: 0.2075, lr 3.7844809190464146e-05
Step 307100 Loss: 0.2054, lr 3.7844809190464146e-05
Step 307200 Loss: 0.2033, lr 3.7844809190464146e-05
Step 307300 Loss: 0.2014, lr 3.7844809190464146e-05
Step 307400 Loss: 0.1995, lr 3.7844809190464146e-05
Step 307500 Loss: 0.1978, lr 3.7844809190464146e-05
Step 307600 Loss: 0.1959, lr 3.7844809190464146e-05
Step 307700 Loss: 0.1945, lr 3.7844809190464146e-05
Step 307800 Loss: 0.1932, lr 3.7844809190464146e-05
Step 307900 Loss: 0.1918, lr 3.7844809190464146e-05
Step 308000 Loss: 0.1910, lr 3.7844809190464146e-05
Step 308100 Loss: 0.1897, lr 3.7844809190464146e-05
Step 308200 Loss: 0.1890, lr 3.7844809190464146e-05
Step 308300 Loss: 0.1878, lr 3.7844809190464146e-05
Step 308400 Loss: 0.1868, lr 3.7844809190464146e-05
Step 308500 Loss: 0.1859, lr 3.7844809190464146e-05
Step 308600 Loss: 0.1847, lr 3.7844809190464146e-05
Step 308700 Loss: 0.1837, lr 3.7844809190464146e-05
Step 308800 Loss: 0.1831, lr 3.7844809190464146e-05
Step 308900 Loss: 0.1824, lr 3.7844809190464146e-05
Step 309000 Loss: 0.1818, lr 3.7844809190464146e-05
Step 309100 Loss: 0.1812, lr 3.7844809190464146e-05
Step 309200 Loss: 0.1807, lr 3.7844809190464146e-05
Step 309300 Loss: 0.1800, lr 3.7844809190464146e-05
Step 309400 Loss: 0.1794, lr 3.7844809190464146e-05
Step 309500 Loss: 0.1792, lr 3.7844809190464146e-05
Step 309600 Loss: 0.1787, lr 3.7844809190464146e-05
Step 309700 Loss: 0.1783, lr 3.7844809190464146e-05
Step 309800 Loss: 0.1781, lr 3.7844809190464146e-05
Step 309900 Loss: 0.1779, lr 3.7844809190464146e-05
Step 310000 Loss: 0.1779, lr 3.7844809190464146e-05
Step 310100 Loss: 0.1780, lr 3.7844809190464146e-05
Step 310200 Loss: 0.1783, lr 3.7844809190464146e-05
Step 310300 Loss: 0.1790, lr 3.7844809190464146e-05
Step 310400 Loss: 0.1796, lr 3.7844809190464146e-05
Step 310500 Loss: 0.1804, lr 3.7844809190464146e-05
Step 310600 Loss: 0.1801, lr 3.7844809190464146e-05
Step 310700 Loss: 0.1798, lr 3.7844809190464146e-05
Train Epoch: [86/100] Loss: 0.1800,lr 0.000038
Calling G2SDataset.batch()
Done, time:  2.17 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 310800 Loss: 0.7015, lr 3.6827008176764724e-05
Step 310900 Loss: 0.4526, lr 3.6827008176764724e-05
Step 311000 Loss: 0.3864, lr 3.6827008176764724e-05
Step 311100 Loss: 0.3536, lr 3.6827008176764724e-05
Step 311200 Loss: 0.3324, lr 3.6827008176764724e-05
Step 311300 Loss: 0.3197, lr 3.6827008176764724e-05
Step 311400 Loss: 0.3051, lr 3.6827008176764724e-05
Step 311500 Loss: 0.2942, lr 3.6827008176764724e-05
Step 311600 Loss: 0.2866, lr 3.6827008176764724e-05
Step 311700 Loss: 0.2787, lr 3.6827008176764724e-05
Step 311800 Loss: 0.2718, lr 3.6827008176764724e-05
Step 311900 Loss: 0.2662, lr 3.6827008176764724e-05
Step 312000 Loss: 0.2606, lr 3.6827008176764724e-05
Step 312100 Loss: 0.2558, lr 3.6827008176764724e-05
Step 312200 Loss: 0.2508, lr 3.6827008176764724e-05
Step 312300 Loss: 0.2478, lr 3.6827008176764724e-05
Step 312400 Loss: 0.2445, lr 3.6827008176764724e-05
Step 312500 Loss: 0.2414, lr 3.6827008176764724e-05
Step 312600 Loss: 0.2380, lr 3.6827008176764724e-05
Step 312700 Loss: 0.2346, lr 3.6827008176764724e-05
Step 312800 Loss: 0.2306, lr 3.6827008176764724e-05
Step 312900 Loss: 0.2276, lr 3.6827008176764724e-05
Step 313000 Loss: 0.2248, lr 3.6827008176764724e-05
Step 313100 Loss: 0.2220, lr 3.6827008176764724e-05
Step 313200 Loss: 0.2194, lr 3.6827008176764724e-05
Step 313300 Loss: 0.2169, lr 3.6827008176764724e-05
Step 313400 Loss: 0.2144, lr 3.6827008176764724e-05
Step 313500 Loss: 0.2129, lr 3.6827008176764724e-05
Step 313600 Loss: 0.2109, lr 3.6827008176764724e-05
Step 313700 Loss: 0.2089, lr 3.6827008176764724e-05
Step 313800 Loss: 0.2071, lr 3.6827008176764724e-05
Step 313900 Loss: 0.2050, lr 3.6827008176764724e-05
Step 314000 Loss: 0.2030, lr 3.6827008176764724e-05
Step 314100 Loss: 0.2009, lr 3.6827008176764724e-05
Step 314200 Loss: 0.1992, lr 3.6827008176764724e-05
Step 314300 Loss: 0.1975, lr 3.6827008176764724e-05
Step 314400 Loss: 0.1960, lr 3.6827008176764724e-05
Step 314500 Loss: 0.1944, lr 3.6827008176764724e-05
Step 314600 Loss: 0.1933, lr 3.6827008176764724e-05
Step 314700 Loss: 0.1921, lr 3.6827008176764724e-05
Step 314800 Loss: 0.1911, lr 3.6827008176764724e-05
Step 314900 Loss: 0.1899, lr 3.6827008176764724e-05
Step 315000 Loss: 0.1891, lr 3.6827008176764724e-05
Step 315100 Loss: 0.1881, lr 3.6827008176764724e-05
Step 315200 Loss: 0.1871, lr 3.6827008176764724e-05
Step 315300 Loss: 0.1859, lr 3.6827008176764724e-05
Step 315400 Loss: 0.1849, lr 3.6827008176764724e-05
Step 315500 Loss: 0.1840, lr 3.6827008176764724e-05
Step 315600 Loss: 0.1834, lr 3.6827008176764724e-05
Step 315700 Loss: 0.1829, lr 3.6827008176764724e-05
Step 315800 Loss: 0.1821, lr 3.6827008176764724e-05
Step 315900 Loss: 0.1818, lr 3.6827008176764724e-05
Step 316000 Loss: 0.1810, lr 3.6827008176764724e-05
Step 316100 Loss: 0.1804, lr 3.6827008176764724e-05
Step 316200 Loss: 0.1799, lr 3.6827008176764724e-05
Step 316300 Loss: 0.1794, lr 3.6827008176764724e-05
Step 316400 Loss: 0.1788, lr 3.6827008176764724e-05
Step 316500 Loss: 0.1788, lr 3.6827008176764724e-05
Step 316600 Loss: 0.1786, lr 3.6827008176764724e-05
Step 316700 Loss: 0.1785, lr 3.6827008176764724e-05
Step 316800 Loss: 0.1784, lr 3.6827008176764724e-05
Step 316900 Loss: 0.1784, lr 3.6827008176764724e-05
Step 317000 Loss: 0.1788, lr 3.6827008176764724e-05
Step 317100 Loss: 0.1794, lr 3.6827008176764724e-05
Step 317200 Loss: 0.1803, lr 3.6827008176764724e-05
Step 317300 Loss: 0.1808, lr 3.6827008176764724e-05
Step 317400 Loss: 0.1804, lr 3.6827008176764724e-05
Step 317500 Loss: 0.1800, lr 3.6827008176764724e-05
Train Epoch: [87/100] Loss: 0.1803,lr 0.000037
Calling G2SDataset.batch()
Done, time:  2.30 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.37 s, total batches: 6756
Step 317600 Loss: 0.4980, lr 3.580515939552118e-05
Step 317700 Loss: 0.4032, lr 3.580515939552118e-05
Step 317800 Loss: 0.3607, lr 3.580515939552118e-05
Step 317900 Loss: 0.3360, lr 3.580515939552118e-05
Step 318000 Loss: 0.3225, lr 3.580515939552118e-05
Step 318100 Loss: 0.3085, lr 3.580515939552118e-05
Step 318200 Loss: 0.2961, lr 3.580515939552118e-05
Step 318300 Loss: 0.2882, lr 3.580515939552118e-05
Step 318400 Loss: 0.2804, lr 3.580515939552118e-05
Step 318500 Loss: 0.2735, lr 3.580515939552118e-05
Step 318600 Loss: 0.2673, lr 3.580515939552118e-05
Step 318700 Loss: 0.2616, lr 3.580515939552118e-05
Step 318800 Loss: 0.2570, lr 3.580515939552118e-05
Step 318900 Loss: 0.2515, lr 3.580515939552118e-05
Step 319000 Loss: 0.2476, lr 3.580515939552118e-05
Step 319100 Loss: 0.2446, lr 3.580515939552118e-05
Step 319200 Loss: 0.2415, lr 3.580515939552118e-05
Step 319300 Loss: 0.2383, lr 3.580515939552118e-05
Step 319400 Loss: 0.2350, lr 3.580515939552118e-05
Step 319500 Loss: 0.2314, lr 3.580515939552118e-05
Step 319600 Loss: 0.2277, lr 3.580515939552118e-05
Step 319700 Loss: 0.2252, lr 3.580515939552118e-05
Step 319800 Loss: 0.2226, lr 3.580515939552118e-05
Step 319900 Loss: 0.2200, lr 3.580515939552118e-05
Step 320000 Loss: 0.2171, lr 3.580515939552118e-05
Step 320100 Loss: 0.2148, lr 3.580515939552118e-05
Step 320200 Loss: 0.2128, lr 3.580515939552118e-05
Step 320300 Loss: 0.2109, lr 3.580515939552118e-05
Step 320400 Loss: 0.2091, lr 3.580515939552118e-05
Step 320500 Loss: 0.2074, lr 3.580515939552118e-05
Step 320600 Loss: 0.2052, lr 3.580515939552118e-05
Step 320700 Loss: 0.2033, lr 3.580515939552118e-05
Step 320800 Loss: 0.2013, lr 3.580515939552118e-05
Step 320900 Loss: 0.1993, lr 3.580515939552118e-05
Step 321000 Loss: 0.1976, lr 3.580515939552118e-05
Step 321100 Loss: 0.1959, lr 3.580515939552118e-05
Step 321200 Loss: 0.1944, lr 3.580515939552118e-05
Step 321300 Loss: 0.1931, lr 3.580515939552118e-05
Step 321400 Loss: 0.1917, lr 3.580515939552118e-05
Step 321500 Loss: 0.1907, lr 3.580515939552118e-05
Step 321600 Loss: 0.1897, lr 3.580515939552118e-05
Step 321700 Loss: 0.1888, lr 3.580515939552118e-05
Step 321800 Loss: 0.1878, lr 3.580515939552118e-05
Step 321900 Loss: 0.1868, lr 3.580515939552118e-05
Step 322000 Loss: 0.1859, lr 3.580515939552118e-05
Step 322100 Loss: 0.1846, lr 3.580515939552118e-05
Step 322200 Loss: 0.1835, lr 3.580515939552118e-05
Step 322300 Loss: 0.1828, lr 3.580515939552118e-05
Step 322400 Loss: 0.1822, lr 3.580515939552118e-05
Step 322500 Loss: 0.1817, lr 3.580515939552118e-05
Step 322600 Loss: 0.1810, lr 3.580515939552118e-05
Step 322700 Loss: 0.1806, lr 3.580515939552118e-05
Step 322800 Loss: 0.1799, lr 3.580515939552118e-05
Step 322900 Loss: 0.1792, lr 3.580515939552118e-05
Step 323000 Loss: 0.1788, lr 3.580515939552118e-05
Step 323100 Loss: 0.1782, lr 3.580515939552118e-05
Step 323200 Loss: 0.1778, lr 3.580515939552118e-05
Step 323300 Loss: 0.1775, lr 3.580515939552118e-05
Step 323400 Loss: 0.1773, lr 3.580515939552118e-05
Step 323500 Loss: 0.1773, lr 3.580515939552118e-05
Step 323600 Loss: 0.1773, lr 3.580515939552118e-05
Step 323700 Loss: 0.1773, lr 3.580515939552118e-05
Step 323800 Loss: 0.1777, lr 3.580515939552118e-05
Step 323900 Loss: 0.1782, lr 3.580515939552118e-05
Step 324000 Loss: 0.1789, lr 3.580515939552118e-05
Step 324100 Loss: 0.1790, lr 3.580515939552118e-05
Step 324200 Loss: 0.1784, lr 3.580515939552118e-05
Train Epoch: [88/100] Loss: 0.1786,lr 0.000036
Calling G2SDataset.batch()
Done, time:  2.28 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6755
Step 324300 Loss: 0.7256, lr 3.478027128811133e-05
Step 324400 Loss: 0.4506, lr 3.478027128811133e-05
Step 324500 Loss: 0.3854, lr 3.478027128811133e-05
Step 324600 Loss: 0.3511, lr 3.478027128811133e-05
Step 324700 Loss: 0.3314, lr 3.478027128811133e-05
Step 324800 Loss: 0.3190, lr 3.478027128811133e-05
Step 324900 Loss: 0.3039, lr 3.478027128811133e-05
Step 325000 Loss: 0.2936, lr 3.478027128811133e-05
Step 325100 Loss: 0.2863, lr 3.478027128811133e-05
Step 325200 Loss: 0.2781, lr 3.478027128811133e-05
Step 325300 Loss: 0.2715, lr 3.478027128811133e-05
Step 325400 Loss: 0.2660, lr 3.478027128811133e-05
Step 325500 Loss: 0.2602, lr 3.478027128811133e-05
Step 325600 Loss: 0.2553, lr 3.478027128811133e-05
Step 325700 Loss: 0.2505, lr 3.478027128811133e-05
Step 325800 Loss: 0.2474, lr 3.478027128811133e-05
Step 325900 Loss: 0.2439, lr 3.478027128811133e-05
Step 326000 Loss: 0.2406, lr 3.478027128811133e-05
Step 326100 Loss: 0.2375, lr 3.478027128811133e-05
Step 326200 Loss: 0.2340, lr 3.478027128811133e-05
Step 326300 Loss: 0.2303, lr 3.478027128811133e-05
Step 326400 Loss: 0.2269, lr 3.478027128811133e-05
Step 326500 Loss: 0.2243, lr 3.478027128811133e-05
Step 326600 Loss: 0.2213, lr 3.478027128811133e-05
Step 326700 Loss: 0.2190, lr 3.478027128811133e-05
Step 326800 Loss: 0.2163, lr 3.478027128811133e-05
Step 326900 Loss: 0.2138, lr 3.478027128811133e-05
Step 327000 Loss: 0.2121, lr 3.478027128811133e-05
Step 327100 Loss: 0.2103, lr 3.478027128811133e-05
Step 327200 Loss: 0.2084, lr 3.478027128811133e-05
Step 327300 Loss: 0.2065, lr 3.478027128811133e-05
Step 327400 Loss: 0.2043, lr 3.478027128811133e-05
Step 327500 Loss: 0.2023, lr 3.478027128811133e-05
Step 327600 Loss: 0.2002, lr 3.478027128811133e-05
Step 327700 Loss: 0.1984, lr 3.478027128811133e-05
Step 327800 Loss: 0.1967, lr 3.478027128811133e-05
Step 327900 Loss: 0.1952, lr 3.478027128811133e-05
Step 328000 Loss: 0.1935, lr 3.478027128811133e-05
Step 328100 Loss: 0.1923, lr 3.478027128811133e-05
Step 328200 Loss: 0.1909, lr 3.478027128811133e-05
Step 328300 Loss: 0.1900, lr 3.478027128811133e-05
Step 328400 Loss: 0.1889, lr 3.478027128811133e-05
Step 328500 Loss: 0.1880, lr 3.478027128811133e-05
Step 328600 Loss: 0.1871, lr 3.478027128811133e-05
Step 328700 Loss: 0.1858, lr 3.478027128811133e-05
Step 328800 Loss: 0.1848, lr 3.478027128811133e-05
Step 328900 Loss: 0.1837, lr 3.478027128811133e-05
Step 329000 Loss: 0.1827, lr 3.478027128811133e-05
Step 329100 Loss: 0.1821, lr 3.478027128811133e-05
Step 329200 Loss: 0.1816, lr 3.478027128811133e-05
Step 329300 Loss: 0.1809, lr 3.478027128811133e-05
Step 329400 Loss: 0.1805, lr 3.478027128811133e-05
Step 329500 Loss: 0.1798, lr 3.478027128811133e-05
Step 329600 Loss: 0.1791, lr 3.478027128811133e-05
Step 329700 Loss: 0.1784, lr 3.478027128811133e-05
Step 329800 Loss: 0.1780, lr 3.478027128811133e-05
Step 329900 Loss: 0.1776, lr 3.478027128811133e-05
Step 330000 Loss: 0.1774, lr 3.478027128811133e-05
Step 330100 Loss: 0.1773, lr 3.478027128811133e-05
Step 330200 Loss: 0.1771, lr 3.478027128811133e-05
Step 330300 Loss: 0.1772, lr 3.478027128811133e-05
Step 330400 Loss: 0.1771, lr 3.478027128811133e-05
Step 330500 Loss: 0.1774, lr 3.478027128811133e-05
Step 330600 Loss: 0.1779, lr 3.478027128811133e-05
Step 330700 Loss: 0.1785, lr 3.478027128811133e-05
Step 330800 Loss: 0.1792, lr 3.478027128811133e-05
Step 330900 Loss: 0.1790, lr 3.478027128811133e-05
Step 331000 Loss: 0.1786, lr 3.478027128811133e-05
Train Epoch: [89/100] Loss: 0.1792,lr 0.000035
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6756
Step 331100 Loss: 0.5212, lr 3.375335529536092e-05
Step 331200 Loss: 0.4100, lr 3.375335529536092e-05
Step 331300 Loss: 0.3655, lr 3.375335529536092e-05
Step 331400 Loss: 0.3384, lr 3.375335529536092e-05
Step 331500 Loss: 0.3245, lr 3.375335529536092e-05
Step 331600 Loss: 0.3109, lr 3.375335529536092e-05
Step 331700 Loss: 0.2974, lr 3.375335529536092e-05
Step 331800 Loss: 0.2887, lr 3.375335529536092e-05
Step 331900 Loss: 0.2807, lr 3.375335529536092e-05
Step 332000 Loss: 0.2735, lr 3.375335529536092e-05
Step 332100 Loss: 0.2672, lr 3.375335529536092e-05
Step 332200 Loss: 0.2621, lr 3.375335529536092e-05
Step 332300 Loss: 0.2570, lr 3.375335529536092e-05
Step 332400 Loss: 0.2514, lr 3.375335529536092e-05
Step 332500 Loss: 0.2473, lr 3.375335529536092e-05
Step 332600 Loss: 0.2442, lr 3.375335529536092e-05
Step 332700 Loss: 0.2413, lr 3.375335529536092e-05
Step 332800 Loss: 0.2383, lr 3.375335529536092e-05
Step 332900 Loss: 0.2353, lr 3.375335529536092e-05
Step 333000 Loss: 0.2317, lr 3.375335529536092e-05
Step 333100 Loss: 0.2278, lr 3.375335529536092e-05
Step 333200 Loss: 0.2251, lr 3.375335529536092e-05
Step 333300 Loss: 0.2225, lr 3.375335529536092e-05
Step 333400 Loss: 0.2198, lr 3.375335529536092e-05
Step 333500 Loss: 0.2172, lr 3.375335529536092e-05
Step 333600 Loss: 0.2149, lr 3.375335529536092e-05
Step 333700 Loss: 0.2126, lr 3.375335529536092e-05
Step 333800 Loss: 0.2107, lr 3.375335529536092e-05
Step 333900 Loss: 0.2085, lr 3.375335529536092e-05
Step 334000 Loss: 0.2069, lr 3.375335529536092e-05
Step 334100 Loss: 0.2047, lr 3.375335529536092e-05
Step 334200 Loss: 0.2026, lr 3.375335529536092e-05
Step 334300 Loss: 0.2008, lr 3.375335529536092e-05
Step 334400 Loss: 0.1987, lr 3.375335529536092e-05
Step 334500 Loss: 0.1969, lr 3.375335529536092e-05
Step 334600 Loss: 0.1951, lr 3.375335529536092e-05
Step 334700 Loss: 0.1938, lr 3.375335529536092e-05
Step 334800 Loss: 0.1924, lr 3.375335529536092e-05
Step 334900 Loss: 0.1909, lr 3.375335529536092e-05
Step 335000 Loss: 0.1898, lr 3.375335529536092e-05
Step 335100 Loss: 0.1888, lr 3.375335529536092e-05
Step 335200 Loss: 0.1877, lr 3.375335529536092e-05
Step 335300 Loss: 0.1866, lr 3.375335529536092e-05
Step 335400 Loss: 0.1857, lr 3.375335529536092e-05
Step 335500 Loss: 0.1848, lr 3.375335529536092e-05
Step 335600 Loss: 0.1836, lr 3.375335529536092e-05
Step 335700 Loss: 0.1825, lr 3.375335529536092e-05
Step 335800 Loss: 0.1818, lr 3.375335529536092e-05
Step 335900 Loss: 0.1812, lr 3.375335529536092e-05
Step 336000 Loss: 0.1806, lr 3.375335529536092e-05
Step 336100 Loss: 0.1800, lr 3.375335529536092e-05
Step 336200 Loss: 0.1797, lr 3.375335529536092e-05
Step 336300 Loss: 0.1788, lr 3.375335529536092e-05
Step 336400 Loss: 0.1783, lr 3.375335529536092e-05
Step 336500 Loss: 0.1778, lr 3.375335529536092e-05
Step 336600 Loss: 0.1773, lr 3.375335529536092e-05
Step 336700 Loss: 0.1768, lr 3.375335529536092e-05
Step 336800 Loss: 0.1765, lr 3.375335529536092e-05
Step 336900 Loss: 0.1764, lr 3.375335529536092e-05
Step 337000 Loss: 0.1762, lr 3.375335529536092e-05
Step 337100 Loss: 0.1762, lr 3.375335529536092e-05
Step 337200 Loss: 0.1761, lr 3.375335529536092e-05
Step 337300 Loss: 0.1767, lr 3.375335529536092e-05
Step 337400 Loss: 0.1771, lr 3.375335529536092e-05
Step 337500 Loss: 0.1778, lr 3.375335529536092e-05
Step 337600 Loss: 0.1782, lr 3.375335529536092e-05
Step 337700 Loss: 0.1776, lr 3.375335529536092e-05
Train Epoch: [90/100] Loss: 0.1775,lr 0.000034
Model Saving at epoch 90
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Step 337800 Loss: 0.9023, lr 3.2725424859373745e-05
Step 337900 Loss: 0.4726, lr 3.2725424859373745e-05
Step 338000 Loss: 0.3978, lr 3.2725424859373745e-05
Step 338100 Loss: 0.3591, lr 3.2725424859373745e-05
Step 338200 Loss: 0.3389, lr 3.2725424859373745e-05
Step 338300 Loss: 0.3244, lr 3.2725424859373745e-05
Step 338400 Loss: 0.3087, lr 3.2725424859373745e-05
Step 338500 Loss: 0.2966, lr 3.2725424859373745e-05
Step 338600 Loss: 0.2894, lr 3.2725424859373745e-05
Step 338700 Loss: 0.2802, lr 3.2725424859373745e-05
Step 338800 Loss: 0.2731, lr 3.2725424859373745e-05
Step 338900 Loss: 0.2674, lr 3.2725424859373745e-05
Step 339000 Loss: 0.2613, lr 3.2725424859373745e-05
Step 339100 Loss: 0.2559, lr 3.2725424859373745e-05
Step 339200 Loss: 0.2510, lr 3.2725424859373745e-05
Step 339300 Loss: 0.2478, lr 3.2725424859373745e-05
Step 339400 Loss: 0.2442, lr 3.2725424859373745e-05
Step 339500 Loss: 0.2414, lr 3.2725424859373745e-05
Step 339600 Loss: 0.2382, lr 3.2725424859373745e-05
Step 339700 Loss: 0.2348, lr 3.2725424859373745e-05
Step 339800 Loss: 0.2311, lr 3.2725424859373745e-05
Step 339900 Loss: 0.2274, lr 3.2725424859373745e-05
Step 340000 Loss: 0.2248, lr 3.2725424859373745e-05
Step 340100 Loss: 0.2219, lr 3.2725424859373745e-05
Step 340200 Loss: 0.2194, lr 3.2725424859373745e-05
Step 340300 Loss: 0.2166, lr 3.2725424859373745e-05
Step 340400 Loss: 0.2141, lr 3.2725424859373745e-05
Step 340500 Loss: 0.2121, lr 3.2725424859373745e-05
Step 340600 Loss: 0.2103, lr 3.2725424859373745e-05
Step 340700 Loss: 0.2083, lr 3.2725424859373745e-05
Step 340800 Loss: 0.2063, lr 3.2725424859373745e-05
Step 340900 Loss: 0.2041, lr 3.2725424859373745e-05
Step 341000 Loss: 0.2021, lr 3.2725424859373745e-05
Step 341100 Loss: 0.2002, lr 3.2725424859373745e-05
Step 341200 Loss: 0.1984, lr 3.2725424859373745e-05
Step 341300 Loss: 0.1966, lr 3.2725424859373745e-05
Step 341400 Loss: 0.1950, lr 3.2725424859373745e-05
Step 341500 Loss: 0.1934, lr 3.2725424859373745e-05
Step 341600 Loss: 0.1922, lr 3.2725424859373745e-05
Step 341700 Loss: 0.1910, lr 3.2725424859373745e-05
Step 341800 Loss: 0.1899, lr 3.2725424859373745e-05
Step 341900 Loss: 0.1888, lr 3.2725424859373745e-05
Step 342000 Loss: 0.1878, lr 3.2725424859373745e-05
Step 342100 Loss: 0.1869, lr 3.2725424859373745e-05
Step 342200 Loss: 0.1857, lr 3.2725424859373745e-05
Step 342300 Loss: 0.1847, lr 3.2725424859373745e-05
Step 342400 Loss: 0.1834, lr 3.2725424859373745e-05
Step 342500 Loss: 0.1824, lr 3.2725424859373745e-05
Step 342600 Loss: 0.1817, lr 3.2725424859373745e-05
Step 342700 Loss: 0.1811, lr 3.2725424859373745e-05
Step 342800 Loss: 0.1804, lr 3.2725424859373745e-05
Step 342900 Loss: 0.1799, lr 3.2725424859373745e-05
Step 343000 Loss: 0.1793, lr 3.2725424859373745e-05
Step 343100 Loss: 0.1786, lr 3.2725424859373745e-05
Step 343200 Loss: 0.1780, lr 3.2725424859373745e-05
Step 343300 Loss: 0.1775, lr 3.2725424859373745e-05
Step 343400 Loss: 0.1769, lr 3.2725424859373745e-05
Step 343500 Loss: 0.1766, lr 3.2725424859373745e-05
Step 343600 Loss: 0.1764, lr 3.2725424859373745e-05
Step 343700 Loss: 0.1762, lr 3.2725424859373745e-05
Step 343800 Loss: 0.1762, lr 3.2725424859373745e-05
Step 343900 Loss: 0.1760, lr 3.2725424859373745e-05
Step 344000 Loss: 0.1764, lr 3.2725424859373745e-05
Step 344100 Loss: 0.1769, lr 3.2725424859373745e-05
Step 344200 Loss: 0.1775, lr 3.2725424859373745e-05
Step 344300 Loss: 0.1783, lr 3.2725424859373745e-05
Step 344400 Loss: 0.1779, lr 3.2725424859373745e-05
Step 344500 Loss: 0.1776, lr 3.2725424859373745e-05
Train Epoch: [91/100] Loss: 0.1777,lr 0.000033
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6756
Step 344600 Loss: 0.5632, lr 3.169749442338657e-05
Step 344700 Loss: 0.4210, lr 3.169749442338657e-05
Step 344800 Loss: 0.3730, lr 3.169749442338657e-05
Step 344900 Loss: 0.3441, lr 3.169749442338657e-05
Step 345000 Loss: 0.3281, lr 3.169749442338657e-05
Step 345100 Loss: 0.3155, lr 3.169749442338657e-05
Step 345200 Loss: 0.3002, lr 3.169749442338657e-05
Step 345300 Loss: 0.2908, lr 3.169749442338657e-05
Step 345400 Loss: 0.2832, lr 3.169749442338657e-05
Step 345500 Loss: 0.2754, lr 3.169749442338657e-05
Step 345600 Loss: 0.2687, lr 3.169749442338657e-05
Step 345700 Loss: 0.2632, lr 3.169749442338657e-05
Step 345800 Loss: 0.2582, lr 3.169749442338657e-05
Step 345900 Loss: 0.2526, lr 3.169749442338657e-05
Step 346000 Loss: 0.2483, lr 3.169749442338657e-05
Step 346100 Loss: 0.2451, lr 3.169749442338657e-05
Step 346200 Loss: 0.2423, lr 3.169749442338657e-05
Step 346300 Loss: 0.2388, lr 3.169749442338657e-05
Step 346400 Loss: 0.2361, lr 3.169749442338657e-05
Step 346500 Loss: 0.2325, lr 3.169749442338657e-05
Step 346600 Loss: 0.2288, lr 3.169749442338657e-05
Step 346700 Loss: 0.2259, lr 3.169749442338657e-05
Step 346800 Loss: 0.2232, lr 3.169749442338657e-05
Step 346900 Loss: 0.2202, lr 3.169749442338657e-05
Step 347000 Loss: 0.2176, lr 3.169749442338657e-05
Step 347100 Loss: 0.2152, lr 3.169749442338657e-05
Step 347200 Loss: 0.2129, lr 3.169749442338657e-05
Step 347300 Loss: 0.2110, lr 3.169749442338657e-05
Step 347400 Loss: 0.2090, lr 3.169749442338657e-05
Step 347500 Loss: 0.2073, lr 3.169749442338657e-05
Step 347600 Loss: 0.2054, lr 3.169749442338657e-05
Step 347700 Loss: 0.2032, lr 3.169749442338657e-05
Step 347800 Loss: 0.2015, lr 3.169749442338657e-05
Step 347900 Loss: 0.1993, lr 3.169749442338657e-05
Step 348000 Loss: 0.1976, lr 3.169749442338657e-05
Step 348100 Loss: 0.1959, lr 3.169749442338657e-05
Step 348200 Loss: 0.1942, lr 3.169749442338657e-05
Step 348300 Loss: 0.1928, lr 3.169749442338657e-05
Step 348400 Loss: 0.1915, lr 3.169749442338657e-05
Step 348500 Loss: 0.1902, lr 3.169749442338657e-05
Step 348600 Loss: 0.1892, lr 3.169749442338657e-05
Step 348700 Loss: 0.1882, lr 3.169749442338657e-05
Step 348800 Loss: 0.1872, lr 3.169749442338657e-05
Step 348900 Loss: 0.1861, lr 3.169749442338657e-05
Step 349000 Loss: 0.1852, lr 3.169749442338657e-05
Step 349100 Loss: 0.1840, lr 3.169749442338657e-05
Step 349200 Loss: 0.1829, lr 3.169749442338657e-05
Step 349300 Loss: 0.1821, lr 3.169749442338657e-05
Step 349400 Loss: 0.1815, lr 3.169749442338657e-05
Step 349500 Loss: 0.1810, lr 3.169749442338657e-05
Step 349600 Loss: 0.1803, lr 3.169749442338657e-05
Step 349700 Loss: 0.1799, lr 3.169749442338657e-05
Step 349800 Loss: 0.1791, lr 3.169749442338657e-05
Step 349900 Loss: 0.1786, lr 3.169749442338657e-05
Step 350000 Loss: 0.1780, lr 3.169749442338657e-05
Step 350100 Loss: 0.1776, lr 3.169749442338657e-05
Step 350200 Loss: 0.1771, lr 3.169749442338657e-05
Step 350300 Loss: 0.1768, lr 3.169749442338657e-05
Step 350400 Loss: 0.1766, lr 3.169749442338657e-05
Step 350500 Loss: 0.1764, lr 3.169749442338657e-05
Step 350600 Loss: 0.1762, lr 3.169749442338657e-05
Step 350700 Loss: 0.1761, lr 3.169749442338657e-05
Step 350800 Loss: 0.1766, lr 3.169749442338657e-05
Step 350900 Loss: 0.1769, lr 3.169749442338657e-05
Step 351000 Loss: 0.1777, lr 3.169749442338657e-05
Step 351100 Loss: 0.1779, lr 3.169749442338657e-05
Step 351200 Loss: 0.1774, lr 3.169749442338657e-05
Train Epoch: [92/100] Loss: 0.1775,lr 0.000032
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.99 s, total batches: 6755
Step 351300 Loss: 1.2196, lr 3.067057843063616e-05
Step 351400 Loss: 0.4703, lr 3.067057843063616e-05
Step 351500 Loss: 0.3924, lr 3.067057843063616e-05
Step 351600 Loss: 0.3562, lr 3.067057843063616e-05
Step 351700 Loss: 0.3360, lr 3.067057843063616e-05
Step 351800 Loss: 0.3218, lr 3.067057843063616e-05
Step 351900 Loss: 0.3064, lr 3.067057843063616e-05
Step 352000 Loss: 0.2954, lr 3.067057843063616e-05
Step 352100 Loss: 0.2878, lr 3.067057843063616e-05
Step 352200 Loss: 0.2789, lr 3.067057843063616e-05
Step 352300 Loss: 0.2715, lr 3.067057843063616e-05
Step 352400 Loss: 0.2660, lr 3.067057843063616e-05
Step 352500 Loss: 0.2603, lr 3.067057843063616e-05
Step 352600 Loss: 0.2553, lr 3.067057843063616e-05
Step 352700 Loss: 0.2500, lr 3.067057843063616e-05
Step 352800 Loss: 0.2468, lr 3.067057843063616e-05
Step 352900 Loss: 0.2429, lr 3.067057843063616e-05
Step 353000 Loss: 0.2403, lr 3.067057843063616e-05
Step 353100 Loss: 0.2370, lr 3.067057843063616e-05
Step 353200 Loss: 0.2339, lr 3.067057843063616e-05
Step 353300 Loss: 0.2300, lr 3.067057843063616e-05
Step 353400 Loss: 0.2266, lr 3.067057843063616e-05
Step 353500 Loss: 0.2239, lr 3.067057843063616e-05
Step 353600 Loss: 0.2212, lr 3.067057843063616e-05
Step 353700 Loss: 0.2186, lr 3.067057843063616e-05
Step 353800 Loss: 0.2157, lr 3.067057843063616e-05
Step 353900 Loss: 0.2131, lr 3.067057843063616e-05
Step 354000 Loss: 0.2111, lr 3.067057843063616e-05
Step 354100 Loss: 0.2095, lr 3.067057843063616e-05
Step 354200 Loss: 0.2073, lr 3.067057843063616e-05
Step 354300 Loss: 0.2056, lr 3.067057843063616e-05
Step 354400 Loss: 0.2033, lr 3.067057843063616e-05
Step 354500 Loss: 0.2014, lr 3.067057843063616e-05
Step 354600 Loss: 0.1996, lr 3.067057843063616e-05
Step 354700 Loss: 0.1977, lr 3.067057843063616e-05
Step 354800 Loss: 0.1961, lr 3.067057843063616e-05
Step 354900 Loss: 0.1942, lr 3.067057843063616e-05
Step 355000 Loss: 0.1927, lr 3.067057843063616e-05
Step 355100 Loss: 0.1915, lr 3.067057843063616e-05
Step 355200 Loss: 0.1902, lr 3.067057843063616e-05
Step 355300 Loss: 0.1892, lr 3.067057843063616e-05
Step 355400 Loss: 0.1879, lr 3.067057843063616e-05
Step 355500 Loss: 0.1871, lr 3.067057843063616e-05
Step 355600 Loss: 0.1862, lr 3.067057843063616e-05
Step 355700 Loss: 0.1850, lr 3.067057843063616e-05
Step 355800 Loss: 0.1840, lr 3.067057843063616e-05
Step 355900 Loss: 0.1827, lr 3.067057843063616e-05
Step 356000 Loss: 0.1819, lr 3.067057843063616e-05
Step 356100 Loss: 0.1812, lr 3.067057843063616e-05
Step 356200 Loss: 0.1805, lr 3.067057843063616e-05
Step 356300 Loss: 0.1799, lr 3.067057843063616e-05
Step 356400 Loss: 0.1793, lr 3.067057843063616e-05
Step 356500 Loss: 0.1787, lr 3.067057843063616e-05
Step 356600 Loss: 0.1779, lr 3.067057843063616e-05
Step 356700 Loss: 0.1773, lr 3.067057843063616e-05
Step 356800 Loss: 0.1769, lr 3.067057843063616e-05
Step 356900 Loss: 0.1763, lr 3.067057843063616e-05
Step 357000 Loss: 0.1759, lr 3.067057843063616e-05
Step 357100 Loss: 0.1757, lr 3.067057843063616e-05
Step 357200 Loss: 0.1754, lr 3.067057843063616e-05
Step 357300 Loss: 0.1755, lr 3.067057843063616e-05
Step 357400 Loss: 0.1753, lr 3.067057843063616e-05
Step 357500 Loss: 0.1754, lr 3.067057843063616e-05
Step 357600 Loss: 0.1759, lr 3.067057843063616e-05
Step 357700 Loss: 0.1765, lr 3.067057843063616e-05
Step 357800 Loss: 0.1771, lr 3.067057843063616e-05
Step 357900 Loss: 0.1768, lr 3.067057843063616e-05
Step 358000 Loss: 0.1764, lr 3.067057843063616e-05
Train Epoch: [93/100] Loss: 0.1766,lr 0.000031
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6755
Step 358100 Loss: 0.5978, lr 2.96456903232263e-05
Step 358200 Loss: 0.4262, lr 2.96456903232263e-05
Step 358300 Loss: 0.3727, lr 2.96456903232263e-05
Step 358400 Loss: 0.3439, lr 2.96456903232263e-05
Step 358500 Loss: 0.3271, lr 2.96456903232263e-05
Step 358600 Loss: 0.3150, lr 2.96456903232263e-05
Step 358700 Loss: 0.3004, lr 2.96456903232263e-05
Step 358800 Loss: 0.2907, lr 2.96456903232263e-05
Step 358900 Loss: 0.2832, lr 2.96456903232263e-05
Step 359000 Loss: 0.2751, lr 2.96456903232263e-05
Step 359100 Loss: 0.2685, lr 2.96456903232263e-05
Step 359200 Loss: 0.2633, lr 2.96456903232263e-05
Step 359300 Loss: 0.2585, lr 2.96456903232263e-05
Step 359400 Loss: 0.2537, lr 2.96456903232263e-05
Step 359500 Loss: 0.2495, lr 2.96456903232263e-05
Step 359600 Loss: 0.2464, lr 2.96456903232263e-05
Step 359700 Loss: 0.2429, lr 2.96456903232263e-05
Step 359800 Loss: 0.2396, lr 2.96456903232263e-05
Step 359900 Loss: 0.2363, lr 2.96456903232263e-05
Step 360000 Loss: 0.2329, lr 2.96456903232263e-05
Step 360100 Loss: 0.2290, lr 2.96456903232263e-05
Step 360200 Loss: 0.2261, lr 2.96456903232263e-05
Step 360300 Loss: 0.2234, lr 2.96456903232263e-05
Step 360400 Loss: 0.2206, lr 2.96456903232263e-05
Step 360500 Loss: 0.2180, lr 2.96456903232263e-05
Step 360600 Loss: 0.2156, lr 2.96456903232263e-05
Step 360700 Loss: 0.2132, lr 2.96456903232263e-05
Step 360800 Loss: 0.2114, lr 2.96456903232263e-05
Step 360900 Loss: 0.2093, lr 2.96456903232263e-05
Step 361000 Loss: 0.2075, lr 2.96456903232263e-05
Step 361100 Loss: 0.2056, lr 2.96456903232263e-05
Step 361200 Loss: 0.2033, lr 2.96456903232263e-05
Step 361300 Loss: 0.2015, lr 2.96456903232263e-05
Step 361400 Loss: 0.1993, lr 2.96456903232263e-05
Step 361500 Loss: 0.1975, lr 2.96456903232263e-05
Step 361600 Loss: 0.1959, lr 2.96456903232263e-05
Step 361700 Loss: 0.1943, lr 2.96456903232263e-05
Step 361800 Loss: 0.1926, lr 2.96456903232263e-05
Step 361900 Loss: 0.1914, lr 2.96456903232263e-05
Step 362000 Loss: 0.1901, lr 2.96456903232263e-05
Step 362100 Loss: 0.1890, lr 2.96456903232263e-05
Step 362200 Loss: 0.1878, lr 2.96456903232263e-05
Step 362300 Loss: 0.1870, lr 2.96456903232263e-05
Step 362400 Loss: 0.1859, lr 2.96456903232263e-05
Step 362500 Loss: 0.1849, lr 2.96456903232263e-05
Step 362600 Loss: 0.1837, lr 2.96456903232263e-05
Step 362700 Loss: 0.1826, lr 2.96456903232263e-05
Step 362800 Loss: 0.1817, lr 2.96456903232263e-05
Step 362900 Loss: 0.1810, lr 2.96456903232263e-05
Step 363000 Loss: 0.1805, lr 2.96456903232263e-05
Step 363100 Loss: 0.1796, lr 2.96456903232263e-05
Step 363200 Loss: 0.1793, lr 2.96456903232263e-05
Step 363300 Loss: 0.1784, lr 2.96456903232263e-05
Step 363400 Loss: 0.1777, lr 2.96456903232263e-05
Step 363500 Loss: 0.1771, lr 2.96456903232263e-05
Step 363600 Loss: 0.1767, lr 2.96456903232263e-05
Step 363700 Loss: 0.1761, lr 2.96456903232263e-05
Step 363800 Loss: 0.1757, lr 2.96456903232263e-05
Step 363900 Loss: 0.1754, lr 2.96456903232263e-05
Step 364000 Loss: 0.1753, lr 2.96456903232263e-05
Step 364100 Loss: 0.1753, lr 2.96456903232263e-05
Step 364200 Loss: 0.1752, lr 2.96456903232263e-05
Step 364300 Loss: 0.1755, lr 2.96456903232263e-05
Step 364400 Loss: 0.1759, lr 2.96456903232263e-05
Step 364500 Loss: 0.1767, lr 2.96456903232263e-05
Step 364600 Loss: 0.1768, lr 2.96456903232263e-05
Step 364700 Loss: 0.1763, lr 2.96456903232263e-05
Step 364800 Loss: 0.1760, lr 2.96456903232263e-05
Train Epoch: [94/100] Loss: 0.1763,lr 0.000030
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.99 s, total batches: 6756
Step 364900 Loss: 0.4832, lr 2.8623841541982767e-05
Step 365000 Loss: 0.3954, lr 2.8623841541982767e-05
Step 365100 Loss: 0.3567, lr 2.8623841541982767e-05
Step 365200 Loss: 0.3347, lr 2.8623841541982767e-05
Step 365300 Loss: 0.3213, lr 2.8623841541982767e-05
Step 365400 Loss: 0.3063, lr 2.8623841541982767e-05
Step 365500 Loss: 0.2953, lr 2.8623841541982767e-05
Step 365600 Loss: 0.2874, lr 2.8623841541982767e-05
Step 365700 Loss: 0.2788, lr 2.8623841541982767e-05
Step 365800 Loss: 0.2716, lr 2.8623841541982767e-05
Step 365900 Loss: 0.2658, lr 2.8623841541982767e-05
Step 366000 Loss: 0.2601, lr 2.8623841541982767e-05
Step 366100 Loss: 0.2553, lr 2.8623841541982767e-05
Step 366200 Loss: 0.2498, lr 2.8623841541982767e-05
Step 366300 Loss: 0.2465, lr 2.8623841541982767e-05
Step 366400 Loss: 0.2433, lr 2.8623841541982767e-05
Step 366500 Loss: 0.2404, lr 2.8623841541982767e-05
Step 366600 Loss: 0.2373, lr 2.8623841541982767e-05
Step 366700 Loss: 0.2341, lr 2.8623841541982767e-05
Step 366800 Loss: 0.2301, lr 2.8623841541982767e-05
Step 366900 Loss: 0.2264, lr 2.8623841541982767e-05
Step 367000 Loss: 0.2237, lr 2.8623841541982767e-05
Step 367100 Loss: 0.2212, lr 2.8623841541982767e-05
Step 367200 Loss: 0.2184, lr 2.8623841541982767e-05
Step 367300 Loss: 0.2157, lr 2.8623841541982767e-05
Step 367400 Loss: 0.2132, lr 2.8623841541982767e-05
Step 367500 Loss: 0.2111, lr 2.8623841541982767e-05
Step 367600 Loss: 0.2092, lr 2.8623841541982767e-05
Step 367700 Loss: 0.2072, lr 2.8623841541982767e-05
Step 367800 Loss: 0.2052, lr 2.8623841541982767e-05
Step 367900 Loss: 0.2031, lr 2.8623841541982767e-05
Step 368000 Loss: 0.2010, lr 2.8623841541982767e-05
Step 368100 Loss: 0.1990, lr 2.8623841541982767e-05
Step 368200 Loss: 0.1971, lr 2.8623841541982767e-05
Step 368300 Loss: 0.1954, lr 2.8623841541982767e-05
Step 368400 Loss: 0.1936, lr 2.8623841541982767e-05
Step 368500 Loss: 0.1922, lr 2.8623841541982767e-05
Step 368600 Loss: 0.1910, lr 2.8623841541982767e-05
Step 368700 Loss: 0.1895, lr 2.8623841541982767e-05
Step 368800 Loss: 0.1886, lr 2.8623841541982767e-05
Step 368900 Loss: 0.1873, lr 2.8623841541982767e-05
Step 369000 Loss: 0.1865, lr 2.8623841541982767e-05
Step 369100 Loss: 0.1854, lr 2.8623841541982767e-05
Step 369200 Loss: 0.1843, lr 2.8623841541982767e-05
Step 369300 Loss: 0.1832, lr 2.8623841541982767e-05
Step 369400 Loss: 0.1820, lr 2.8623841541982767e-05
Step 369500 Loss: 0.1810, lr 2.8623841541982767e-05
Step 369600 Loss: 0.1803, lr 2.8623841541982767e-05
Step 369700 Loss: 0.1797, lr 2.8623841541982767e-05
Step 369800 Loss: 0.1790, lr 2.8623841541982767e-05
Step 369900 Loss: 0.1784, lr 2.8623841541982767e-05
Step 370000 Loss: 0.1779, lr 2.8623841541982767e-05
Step 370100 Loss: 0.1771, lr 2.8623841541982767e-05
Step 370200 Loss: 0.1764, lr 2.8623841541982767e-05
Step 370300 Loss: 0.1760, lr 2.8623841541982767e-05
Step 370400 Loss: 0.1755, lr 2.8623841541982767e-05
Step 370500 Loss: 0.1751, lr 2.8623841541982767e-05
Step 370600 Loss: 0.1748, lr 2.8623841541982767e-05
Step 370700 Loss: 0.1745, lr 2.8623841541982767e-05
Step 370800 Loss: 0.1744, lr 2.8623841541982767e-05
Step 370900 Loss: 0.1743, lr 2.8623841541982767e-05
Step 371000 Loss: 0.1744, lr 2.8623841541982767e-05
Step 371100 Loss: 0.1749, lr 2.8623841541982767e-05
Step 371200 Loss: 0.1754, lr 2.8623841541982767e-05
Step 371300 Loss: 0.1760, lr 2.8623841541982767e-05
Step 371400 Loss: 0.1759, lr 2.8623841541982767e-05
Step 371500 Loss: 0.1752, lr 2.8623841541982767e-05
Train Epoch: [95/100] Loss: 0.1759,lr 0.000029
Model Saving at epoch 95
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6755
Step 371600 Loss: 0.6480, lr 2.760604052828334e-05
Step 371700 Loss: 0.4297, lr 2.760604052828334e-05
Step 371800 Loss: 0.3734, lr 2.760604052828334e-05
Step 371900 Loss: 0.3436, lr 2.760604052828334e-05
Step 372000 Loss: 0.3255, lr 2.760604052828334e-05
Step 372100 Loss: 0.3147, lr 2.760604052828334e-05
Step 372200 Loss: 0.3001, lr 2.760604052828334e-05
Step 372300 Loss: 0.2900, lr 2.760604052828334e-05
Step 372400 Loss: 0.2831, lr 2.760604052828334e-05
Step 372500 Loss: 0.2748, lr 2.760604052828334e-05
Step 372600 Loss: 0.2680, lr 2.760604052828334e-05
Step 372700 Loss: 0.2624, lr 2.760604052828334e-05
Step 372800 Loss: 0.2573, lr 2.760604052828334e-05
Step 372900 Loss: 0.2522, lr 2.760604052828334e-05
Step 373000 Loss: 0.2475, lr 2.760604052828334e-05
Step 373100 Loss: 0.2448, lr 2.760604052828334e-05
Step 373200 Loss: 0.2416, lr 2.760604052828334e-05
Step 373300 Loss: 0.2385, lr 2.760604052828334e-05
Step 373400 Loss: 0.2353, lr 2.760604052828334e-05
Step 373500 Loss: 0.2318, lr 2.760604052828334e-05
Step 373600 Loss: 0.2280, lr 2.760604052828334e-05
Step 373700 Loss: 0.2249, lr 2.760604052828334e-05
Step 373800 Loss: 0.2220, lr 2.760604052828334e-05
Step 373900 Loss: 0.2190, lr 2.760604052828334e-05
Step 374000 Loss: 0.2167, lr 2.760604052828334e-05
Step 374100 Loss: 0.2141, lr 2.760604052828334e-05
Step 374200 Loss: 0.2115, lr 2.760604052828334e-05
Step 374300 Loss: 0.2099, lr 2.760604052828334e-05
Step 374400 Loss: 0.2077, lr 2.760604052828334e-05
Step 374500 Loss: 0.2058, lr 2.760604052828334e-05
Step 374600 Loss: 0.2038, lr 2.760604052828334e-05
Step 374700 Loss: 0.2016, lr 2.760604052828334e-05
Step 374800 Loss: 0.1996, lr 2.760604052828334e-05
Step 374900 Loss: 0.1975, lr 2.760604052828334e-05
Step 375000 Loss: 0.1957, lr 2.760604052828334e-05
Step 375100 Loss: 0.1940, lr 2.760604052828334e-05
Step 375200 Loss: 0.1924, lr 2.760604052828334e-05
Step 375300 Loss: 0.1907, lr 2.760604052828334e-05
Step 375400 Loss: 0.1897, lr 2.760604052828334e-05
Step 375500 Loss: 0.1885, lr 2.760604052828334e-05
Step 375600 Loss: 0.1875, lr 2.760604052828334e-05
Step 375700 Loss: 0.1863, lr 2.760604052828334e-05
Step 375800 Loss: 0.1856, lr 2.760604052828334e-05
Step 375900 Loss: 0.1845, lr 2.760604052828334e-05
Step 376000 Loss: 0.1835, lr 2.760604052828334e-05
Step 376100 Loss: 0.1824, lr 2.760604052828334e-05
Step 376200 Loss: 0.1813, lr 2.760604052828334e-05
Step 376300 Loss: 0.1803, lr 2.760604052828334e-05
Step 376400 Loss: 0.1797, lr 2.760604052828334e-05
Step 376500 Loss: 0.1792, lr 2.760604052828334e-05
Step 376600 Loss: 0.1784, lr 2.760604052828334e-05
Step 376700 Loss: 0.1779, lr 2.760604052828334e-05
Step 376800 Loss: 0.1771, lr 2.760604052828334e-05
Step 376900 Loss: 0.1764, lr 2.760604052828334e-05
Step 377000 Loss: 0.1758, lr 2.760604052828334e-05
Step 377100 Loss: 0.1753, lr 2.760604052828334e-05
Step 377200 Loss: 0.1748, lr 2.760604052828334e-05
Step 377300 Loss: 0.1744, lr 2.760604052828334e-05
Step 377400 Loss: 0.1741, lr 2.760604052828334e-05
Step 377500 Loss: 0.1739, lr 2.760604052828334e-05
Step 377600 Loss: 0.1738, lr 2.760604052828334e-05
Step 377700 Loss: 0.1737, lr 2.760604052828334e-05
Step 377800 Loss: 0.1739, lr 2.760604052828334e-05
Step 377900 Loss: 0.1744, lr 2.760604052828334e-05
Step 378000 Loss: 0.1751, lr 2.760604052828334e-05
Step 378100 Loss: 0.1754, lr 2.760604052828334e-05
Step 378200 Loss: 0.1750, lr 2.760604052828334e-05
Step 378300 Loss: 0.1746, lr 2.760604052828334e-05
Train Epoch: [96/100] Loss: 0.1749,lr 0.000028
Calling G2SDataset.batch()
Done, time:  2.35 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Step 378400 Loss: 0.4833, lr 2.6593291728847938e-05
Step 378500 Loss: 0.3955, lr 2.6593291728847938e-05
Step 378600 Loss: 0.3550, lr 2.6593291728847938e-05
Step 378700 Loss: 0.3328, lr 2.6593291728847938e-05
Step 378800 Loss: 0.3187, lr 2.6593291728847938e-05
Step 378900 Loss: 0.3054, lr 2.6593291728847938e-05
Step 379000 Loss: 0.2930, lr 2.6593291728847938e-05
Step 379100 Loss: 0.2854, lr 2.6593291728847938e-05
Step 379200 Loss: 0.2771, lr 2.6593291728847938e-05
Step 379300 Loss: 0.2698, lr 2.6593291728847938e-05
Step 379400 Loss: 0.2637, lr 2.6593291728847938e-05
Step 379500 Loss: 0.2581, lr 2.6593291728847938e-05
Step 379600 Loss: 0.2537, lr 2.6593291728847938e-05
Step 379700 Loss: 0.2482, lr 2.6593291728847938e-05
Step 379800 Loss: 0.2446, lr 2.6593291728847938e-05
Step 379900 Loss: 0.2416, lr 2.6593291728847938e-05
Step 380000 Loss: 0.2387, lr 2.6593291728847938e-05
Step 380100 Loss: 0.2356, lr 2.6593291728847938e-05
Step 380200 Loss: 0.2324, lr 2.6593291728847938e-05
Step 380300 Loss: 0.2286, lr 2.6593291728847938e-05
Step 380400 Loss: 0.2249, lr 2.6593291728847938e-05
Step 380500 Loss: 0.2223, lr 2.6593291728847938e-05
Step 380600 Loss: 0.2197, lr 2.6593291728847938e-05
Step 380700 Loss: 0.2170, lr 2.6593291728847938e-05
Step 380800 Loss: 0.2141, lr 2.6593291728847938e-05
Step 380900 Loss: 0.2118, lr 2.6593291728847938e-05
Step 381000 Loss: 0.2094, lr 2.6593291728847938e-05
Step 381100 Loss: 0.2076, lr 2.6593291728847938e-05
Step 381200 Loss: 0.2056, lr 2.6593291728847938e-05
Step 381300 Loss: 0.2039, lr 2.6593291728847938e-05
Step 381400 Loss: 0.2018, lr 2.6593291728847938e-05
Step 381500 Loss: 0.1997, lr 2.6593291728847938e-05
Step 381600 Loss: 0.1979, lr 2.6593291728847938e-05
Step 381700 Loss: 0.1960, lr 2.6593291728847938e-05
Step 381800 Loss: 0.1941, lr 2.6593291728847938e-05
Step 381900 Loss: 0.1925, lr 2.6593291728847938e-05
Step 382000 Loss: 0.1910, lr 2.6593291728847938e-05
Step 382100 Loss: 0.1896, lr 2.6593291728847938e-05
Step 382200 Loss: 0.1882, lr 2.6593291728847938e-05
Step 382300 Loss: 0.1874, lr 2.6593291728847938e-05
Step 382400 Loss: 0.1861, lr 2.6593291728847938e-05
Step 382500 Loss: 0.1852, lr 2.6593291728847938e-05
Step 382600 Loss: 0.1841, lr 2.6593291728847938e-05
Step 382700 Loss: 0.1832, lr 2.6593291728847938e-05
Step 382800 Loss: 0.1822, lr 2.6593291728847938e-05
Step 382900 Loss: 0.1810, lr 2.6593291728847938e-05
Step 383000 Loss: 0.1800, lr 2.6593291728847938e-05
Step 383100 Loss: 0.1792, lr 2.6593291728847938e-05
Step 383200 Loss: 0.1786, lr 2.6593291728847938e-05
Step 383300 Loss: 0.1780, lr 2.6593291728847938e-05
Step 383400 Loss: 0.1773, lr 2.6593291728847938e-05
Step 383500 Loss: 0.1769, lr 2.6593291728847938e-05
Step 383600 Loss: 0.1761, lr 2.6593291728847938e-05
Step 383700 Loss: 0.1754, lr 2.6593291728847938e-05
Step 383800 Loss: 0.1748, lr 2.6593291728847938e-05
Step 383900 Loss: 0.1744, lr 2.6593291728847938e-05
Step 384000 Loss: 0.1740, lr 2.6593291728847938e-05
Step 384100 Loss: 0.1736, lr 2.6593291728847938e-05
Step 384200 Loss: 0.1734, lr 2.6593291728847938e-05
Step 384300 Loss: 0.1734, lr 2.6593291728847938e-05
Step 384400 Loss: 0.1734, lr 2.6593291728847938e-05
Step 384500 Loss: 0.1732, lr 2.6593291728847938e-05
Step 384600 Loss: 0.1737, lr 2.6593291728847938e-05
Step 384700 Loss: 0.1743, lr 2.6593291728847938e-05
Step 384800 Loss: 0.1749, lr 2.6593291728847938e-05
Step 384900 Loss: 0.1749, lr 2.6593291728847938e-05
Step 385000 Loss: 0.1744, lr 2.6593291728847938e-05
Train Epoch: [97/100] Loss: 0.1750,lr 0.000027
Calling G2SDataset.batch()
Done, time:  2.27 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.20 s, total batches: 6756
Step 385100 Loss: 0.6699, lr 2.5586594604470972e-05
Step 385200 Loss: 0.4240, lr 2.5586594604470972e-05
Step 385300 Loss: 0.3705, lr 2.5586594604470972e-05
Step 385400 Loss: 0.3392, lr 2.5586594604470972e-05
Step 385500 Loss: 0.3217, lr 2.5586594604470972e-05
Step 385600 Loss: 0.3100, lr 2.5586594604470972e-05
Step 385700 Loss: 0.2957, lr 2.5586594604470972e-05
Step 385800 Loss: 0.2855, lr 2.5586594604470972e-05
Step 385900 Loss: 0.2789, lr 2.5586594604470972e-05
Step 386000 Loss: 0.2711, lr 2.5586594604470972e-05
Step 386100 Loss: 0.2646, lr 2.5586594604470972e-05
Step 386200 Loss: 0.2594, lr 2.5586594604470972e-05
Step 386300 Loss: 0.2541, lr 2.5586594604470972e-05
Step 386400 Loss: 0.2493, lr 2.5586594604470972e-05
Step 386500 Loss: 0.2451, lr 2.5586594604470972e-05
Step 386600 Loss: 0.2421, lr 2.5586594604470972e-05
Step 386700 Loss: 0.2387, lr 2.5586594604470972e-05
Step 386800 Loss: 0.2357, lr 2.5586594604470972e-05
Step 386900 Loss: 0.2328, lr 2.5586594604470972e-05
Step 387000 Loss: 0.2293, lr 2.5586594604470972e-05
Step 387100 Loss: 0.2257, lr 2.5586594604470972e-05
Step 387200 Loss: 0.2225, lr 2.5586594604470972e-05
Step 387300 Loss: 0.2198, lr 2.5586594604470972e-05
Step 387400 Loss: 0.2169, lr 2.5586594604470972e-05
Step 387500 Loss: 0.2144, lr 2.5586594604470972e-05
Step 387600 Loss: 0.2119, lr 2.5586594604470972e-05
Step 387700 Loss: 0.2096, lr 2.5586594604470972e-05
Step 387800 Loss: 0.2077, lr 2.5586594604470972e-05
Step 387900 Loss: 0.2059, lr 2.5586594604470972e-05
Step 388000 Loss: 0.2040, lr 2.5586594604470972e-05
Step 388100 Loss: 0.2022, lr 2.5586594604470972e-05
Step 388200 Loss: 0.1999, lr 2.5586594604470972e-05
Step 388300 Loss: 0.1980, lr 2.5586594604470972e-05
Step 388400 Loss: 0.1959, lr 2.5586594604470972e-05
Step 388500 Loss: 0.1943, lr 2.5586594604470972e-05
Step 388600 Loss: 0.1926, lr 2.5586594604470972e-05
Step 388700 Loss: 0.1909, lr 2.5586594604470972e-05
Step 388800 Loss: 0.1894, lr 2.5586594604470972e-05
Step 388900 Loss: 0.1882, lr 2.5586594604470972e-05
Step 389000 Loss: 0.1870, lr 2.5586594604470972e-05
Step 389100 Loss: 0.1860, lr 2.5586594604470972e-05
Step 389200 Loss: 0.1849, lr 2.5586594604470972e-05
Step 389300 Loss: 0.1841, lr 2.5586594604470972e-05
Step 389400 Loss: 0.1831, lr 2.5586594604470972e-05
Step 389500 Loss: 0.1820, lr 2.5586594604470972e-05
Step 389600 Loss: 0.1809, lr 2.5586594604470972e-05
Step 389700 Loss: 0.1797, lr 2.5586594604470972e-05
Step 389800 Loss: 0.1787, lr 2.5586594604470972e-05
Step 389900 Loss: 0.1780, lr 2.5586594604470972e-05
Step 390000 Loss: 0.1776, lr 2.5586594604470972e-05
Step 390100 Loss: 0.1768, lr 2.5586594604470972e-05
Step 390200 Loss: 0.1763, lr 2.5586594604470972e-05
Step 390300 Loss: 0.1755, lr 2.5586594604470972e-05
Step 390400 Loss: 0.1748, lr 2.5586594604470972e-05
Step 390500 Loss: 0.1741, lr 2.5586594604470972e-05
Step 390600 Loss: 0.1737, lr 2.5586594604470972e-05
Step 390700 Loss: 0.1733, lr 2.5586594604470972e-05
Step 390800 Loss: 0.1727, lr 2.5586594604470972e-05
Step 390900 Loss: 0.1725, lr 2.5586594604470972e-05
Step 391000 Loss: 0.1723, lr 2.5586594604470972e-05
Step 391100 Loss: 0.1722, lr 2.5586594604470972e-05
Step 391200 Loss: 0.1720, lr 2.5586594604470972e-05
Step 391300 Loss: 0.1722, lr 2.5586594604470972e-05
Step 391400 Loss: 0.1726, lr 2.5586594604470972e-05
Step 391500 Loss: 0.1732, lr 2.5586594604470972e-05
Step 391600 Loss: 0.1737, lr 2.5586594604470972e-05
Step 391700 Loss: 0.1732, lr 2.5586594604470972e-05
Step 391800 Loss: 0.1729, lr 2.5586594604470972e-05
Train Epoch: [98/100] Loss: 0.1736,lr 0.000026
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Step 391900 Loss: 0.4970, lr 2.458694264367415e-05
Step 392000 Loss: 0.3971, lr 2.458694264367415e-05
Step 392100 Loss: 0.3563, lr 2.458694264367415e-05
Step 392200 Loss: 0.3301, lr 2.458694264367415e-05
Step 392300 Loss: 0.3165, lr 2.458694264367415e-05
Step 392400 Loss: 0.3043, lr 2.458694264367415e-05
Step 392500 Loss: 0.2912, lr 2.458694264367415e-05
Step 392600 Loss: 0.2833, lr 2.458694264367415e-05
Step 392700 Loss: 0.2755, lr 2.458694264367415e-05
Step 392800 Loss: 0.2683, lr 2.458694264367415e-05
Step 392900 Loss: 0.2623, lr 2.458694264367415e-05
Step 393000 Loss: 0.2571, lr 2.458694264367415e-05
Step 393100 Loss: 0.2525, lr 2.458694264367415e-05
Step 393200 Loss: 0.2470, lr 2.458694264367415e-05
Step 393300 Loss: 0.2431, lr 2.458694264367415e-05
Step 393400 Loss: 0.2403, lr 2.458694264367415e-05
Step 393500 Loss: 0.2374, lr 2.458694264367415e-05
Step 393600 Loss: 0.2340, lr 2.458694264367415e-05
Step 393700 Loss: 0.2310, lr 2.458694264367415e-05
Step 393800 Loss: 0.2275, lr 2.458694264367415e-05
Step 393900 Loss: 0.2236, lr 2.458694264367415e-05
Step 394000 Loss: 0.2209, lr 2.458694264367415e-05
Step 394100 Loss: 0.2182, lr 2.458694264367415e-05
Step 394200 Loss: 0.2154, lr 2.458694264367415e-05
Step 394300 Loss: 0.2128, lr 2.458694264367415e-05
Step 394400 Loss: 0.2106, lr 2.458694264367415e-05
Step 394500 Loss: 0.2082, lr 2.458694264367415e-05
Step 394600 Loss: 0.2063, lr 2.458694264367415e-05
Step 394700 Loss: 0.2044, lr 2.458694264367415e-05
Step 394800 Loss: 0.2027, lr 2.458694264367415e-05
Step 394900 Loss: 0.2006, lr 2.458694264367415e-05
Step 395000 Loss: 0.1984, lr 2.458694264367415e-05
Step 395100 Loss: 0.1966, lr 2.458694264367415e-05
Step 395200 Loss: 0.1945, lr 2.458694264367415e-05
Step 395300 Loss: 0.1928, lr 2.458694264367415e-05
Step 395400 Loss: 0.1911, lr 2.458694264367415e-05
Step 395500 Loss: 0.1897, lr 2.458694264367415e-05
Step 395600 Loss: 0.1884, lr 2.458694264367415e-05
Step 395700 Loss: 0.1869, lr 2.458694264367415e-05
Step 395800 Loss: 0.1859, lr 2.458694264367415e-05
Step 395900 Loss: 0.1849, lr 2.458694264367415e-05
Step 396000 Loss: 0.1839, lr 2.458694264367415e-05
Step 396100 Loss: 0.1828, lr 2.458694264367415e-05
Step 396200 Loss: 0.1819, lr 2.458694264367415e-05
Step 396300 Loss: 0.1809, lr 2.458694264367415e-05
Step 396400 Loss: 0.1797, lr 2.458694264367415e-05
Step 396500 Loss: 0.1785, lr 2.458694264367415e-05
Step 396600 Loss: 0.1778, lr 2.458694264367415e-05
Step 396700 Loss: 0.1772, lr 2.458694264367415e-05
Step 396800 Loss: 0.1765, lr 2.458694264367415e-05
Step 396900 Loss: 0.1758, lr 2.458694264367415e-05
Step 397000 Loss: 0.1753, lr 2.458694264367415e-05
Step 397100 Loss: 0.1745, lr 2.458694264367415e-05
Step 397200 Loss: 0.1738, lr 2.458694264367415e-05
Step 397300 Loss: 0.1732, lr 2.458694264367415e-05
Step 397400 Loss: 0.1728, lr 2.458694264367415e-05
Step 397500 Loss: 0.1724, lr 2.458694264367415e-05
Step 397600 Loss: 0.1720, lr 2.458694264367415e-05
Step 397700 Loss: 0.1719, lr 2.458694264367415e-05
Step 397800 Loss: 0.1717, lr 2.458694264367415e-05
Step 397900 Loss: 0.1717, lr 2.458694264367415e-05
Step 398000 Loss: 0.1716, lr 2.458694264367415e-05
Step 398100 Loss: 0.1721, lr 2.458694264367415e-05
Step 398200 Loss: 0.1724, lr 2.458694264367415e-05
Step 398300 Loss: 0.1731, lr 2.458694264367415e-05
Step 398400 Loss: 0.1731, lr 2.458694264367415e-05
Step 398500 Loss: 0.1726, lr 2.458694264367415e-05
Train Epoch: [99/100] Loss: 0.1726,lr 0.000025
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Step 398600 Loss: 0.9132, lr 2.3595322382253377e-05
Step 398700 Loss: 0.4527, lr 2.3595322382253377e-05
Step 398800 Loss: 0.3825, lr 2.3595322382253377e-05
Step 398900 Loss: 0.3468, lr 2.3595322382253377e-05
Step 399000 Loss: 0.3276, lr 2.3595322382253377e-05
Step 399100 Loss: 0.3136, lr 2.3595322382253377e-05
Step 399200 Loss: 0.2989, lr 2.3595322382253377e-05
Step 399300 Loss: 0.2877, lr 2.3595322382253377e-05
Step 399400 Loss: 0.2814, lr 2.3595322382253377e-05
Step 399500 Loss: 0.2729, lr 2.3595322382253377e-05
Step 399600 Loss: 0.2658, lr 2.3595322382253377e-05
Step 399700 Loss: 0.2605, lr 2.3595322382253377e-05
Step 399800 Loss: 0.2548, lr 2.3595322382253377e-05
Step 399900 Loss: 0.2497, lr 2.3595322382253377e-05
Step 400000 Loss: 0.2451, lr 2.3595322382253377e-05
Step 400100 Loss: 0.2421, lr 2.3595322382253377e-05
Step 400200 Loss: 0.2386, lr 2.3595322382253377e-05
Step 400300 Loss: 0.2358, lr 2.3595322382253377e-05
Step 400400 Loss: 0.2326, lr 2.3595322382253377e-05
Step 400500 Loss: 0.2296, lr 2.3595322382253377e-05
Step 400600 Loss: 0.2257, lr 2.3595322382253377e-05
Step 400700 Loss: 0.2221, lr 2.3595322382253377e-05
Step 400800 Loss: 0.2195, lr 2.3595322382253377e-05
Step 400900 Loss: 0.2167, lr 2.3595322382253377e-05
Step 401000 Loss: 0.2143, lr 2.3595322382253377e-05
Step 401100 Loss: 0.2115, lr 2.3595322382253377e-05
Step 401200 Loss: 0.2091, lr 2.3595322382253377e-05
Step 401300 Loss: 0.2072, lr 2.3595322382253377e-05
Step 401400 Loss: 0.2054, lr 2.3595322382253377e-05
Step 401500 Loss: 0.2036, lr 2.3595322382253377e-05
Step 401600 Loss: 0.2017, lr 2.3595322382253377e-05
Step 401700 Loss: 0.1995, lr 2.3595322382253377e-05
Step 401800 Loss: 0.1975, lr 2.3595322382253377e-05
Step 401900 Loss: 0.1955, lr 2.3595322382253377e-05
Step 402000 Loss: 0.1938, lr 2.3595322382253377e-05
Step 402100 Loss: 0.1919, lr 2.3595322382253377e-05
Step 402200 Loss: 0.1901, lr 2.3595322382253377e-05
Step 402300 Loss: 0.1886, lr 2.3595322382253377e-05
Step 402400 Loss: 0.1873, lr 2.3595322382253377e-05
Step 402500 Loss: 0.1860, lr 2.3595322382253377e-05
Step 402600 Loss: 0.1852, lr 2.3595322382253377e-05
Step 402700 Loss: 0.1840, lr 2.3595322382253377e-05
Step 402800 Loss: 0.1831, lr 2.3595322382253377e-05
Step 402900 Loss: 0.1821, lr 2.3595322382253377e-05
Step 403000 Loss: 0.1810, lr 2.3595322382253377e-05
Step 403100 Loss: 0.1800, lr 2.3595322382253377e-05
Step 403200 Loss: 0.1788, lr 2.3595322382253377e-05
Step 403300 Loss: 0.1779, lr 2.3595322382253377e-05
Step 403400 Loss: 0.1773, lr 2.3595322382253377e-05
Step 403500 Loss: 0.1767, lr 2.3595322382253377e-05
Step 403600 Loss: 0.1760, lr 2.3595322382253377e-05
Step 403700 Loss: 0.1754, lr 2.3595322382253377e-05
Step 403800 Loss: 0.1747, lr 2.3595322382253377e-05
Step 403900 Loss: 0.1741, lr 2.3595322382253377e-05
Step 404000 Loss: 0.1734, lr 2.3595322382253377e-05
Step 404100 Loss: 0.1728, lr 2.3595322382253377e-05
Step 404200 Loss: 0.1723, lr 2.3595322382253377e-05
Step 404300 Loss: 0.1719, lr 2.3595322382253377e-05
Step 404400 Loss: 0.1716, lr 2.3595322382253377e-05
Step 404500 Loss: 0.1714, lr 2.3595322382253377e-05
Step 404600 Loss: 0.1714, lr 2.3595322382253377e-05
Step 404700 Loss: 0.1712, lr 2.3595322382253377e-05
Step 404800 Loss: 0.1712, lr 2.3595322382253377e-05
Step 404900 Loss: 0.1716, lr 2.3595322382253377e-05
Step 405000 Loss: 0.1722, lr 2.3595322382253377e-05
Step 405100 Loss: 0.1726, lr 2.3595322382253377e-05
Step 405200 Loss: 0.1723, lr 2.3595322382253377e-05
Step 405300 Loss: 0.1720, lr 2.3595322382253377e-05
Train Epoch: [100/100] Loss: 0.1724,lr 0.000024
Model Saving at epoch 100
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 405400 Loss: 0.5615, lr 2.2612712429686892e-05
Step 405500 Loss: 0.4165, lr 2.2612712429686892e-05
Step 405600 Loss: 0.3671, lr 2.2612712429686892e-05
Step 405700 Loss: 0.3377, lr 2.2612712429686892e-05
Step 405800 Loss: 0.3222, lr 2.2612712429686892e-05
Step 405900 Loss: 0.3090, lr 2.2612712429686892e-05
Step 406000 Loss: 0.2944, lr 2.2612712429686892e-05
Step 406100 Loss: 0.2854, lr 2.2612712429686892e-05
Step 406200 Loss: 0.2780, lr 2.2612712429686892e-05
Step 406300 Loss: 0.2706, lr 2.2612712429686892e-05
Step 406400 Loss: 0.2642, lr 2.2612712429686892e-05
Step 406500 Loss: 0.2588, lr 2.2612712429686892e-05
Step 406600 Loss: 0.2538, lr 2.2612712429686892e-05
Step 406700 Loss: 0.2485, lr 2.2612712429686892e-05
Step 406800 Loss: 0.2439, lr 2.2612712429686892e-05
Step 406900 Loss: 0.2411, lr 2.2612712429686892e-05
Step 407000 Loss: 0.2378, lr 2.2612712429686892e-05
Step 407100 Loss: 0.2345, lr 2.2612712429686892e-05
Step 407200 Loss: 0.2313, lr 2.2612712429686892e-05
Step 407300 Loss: 0.2275, lr 2.2612712429686892e-05
Step 407400 Loss: 0.2238, lr 2.2612712429686892e-05
Step 407500 Loss: 0.2208, lr 2.2612712429686892e-05
Step 407600 Loss: 0.2181, lr 2.2612712429686892e-05
Step 407700 Loss: 0.2152, lr 2.2612712429686892e-05
Step 407800 Loss: 0.2126, lr 2.2612712429686892e-05
Step 407900 Loss: 0.2105, lr 2.2612712429686892e-05
Step 408000 Loss: 0.2079, lr 2.2612712429686892e-05
Step 408100 Loss: 0.2061, lr 2.2612712429686892e-05
Step 408200 Loss: 0.2040, lr 2.2612712429686892e-05
Step 408300 Loss: 0.2023, lr 2.2612712429686892e-05
Step 408400 Loss: 0.2004, lr 2.2612712429686892e-05
Step 408500 Loss: 0.1982, lr 2.2612712429686892e-05
Step 408600 Loss: 0.1965, lr 2.2612712429686892e-05
Step 408700 Loss: 0.1943, lr 2.2612712429686892e-05
Step 408800 Loss: 0.1926, lr 2.2612712429686892e-05
Step 408900 Loss: 0.1909, lr 2.2612712429686892e-05
Step 409000 Loss: 0.1893, lr 2.2612712429686892e-05
Step 409100 Loss: 0.1879, lr 2.2612712429686892e-05
Step 409200 Loss: 0.1865, lr 2.2612712429686892e-05
Step 409300 Loss: 0.1854, lr 2.2612712429686892e-05
Step 409400 Loss: 0.1844, lr 2.2612712429686892e-05
Step 409500 Loss: 0.1833, lr 2.2612712429686892e-05
Step 409600 Loss: 0.1825, lr 2.2612712429686892e-05
Step 409700 Loss: 0.1813, lr 2.2612712429686892e-05
Step 409800 Loss: 0.1804, lr 2.2612712429686892e-05
Step 409900 Loss: 0.1792, lr 2.2612712429686892e-05
Step 410000 Loss: 0.1782, lr 2.2612712429686892e-05
Step 410100 Loss: 0.1774, lr 2.2612712429686892e-05
Step 410200 Loss: 0.1768, lr 2.2612712429686892e-05
Step 410300 Loss: 0.1762, lr 2.2612712429686892e-05
Step 410400 Loss: 0.1753, lr 2.2612712429686892e-05
Step 410500 Loss: 0.1750, lr 2.2612712429686892e-05
Step 410600 Loss: 0.1742, lr 2.2612712429686892e-05
Step 410700 Loss: 0.1735, lr 2.2612712429686892e-05
Step 410800 Loss: 0.1730, lr 2.2612712429686892e-05
Step 410900 Loss: 0.1725, lr 2.2612712429686892e-05
Step 411000 Loss: 0.1720, lr 2.2612712429686892e-05
Step 411100 Loss: 0.1717, lr 2.2612712429686892e-05
Step 411200 Loss: 0.1715, lr 2.2612712429686892e-05
Step 411300 Loss: 0.1713, lr 2.2612712429686892e-05
Step 411400 Loss: 0.1713, lr 2.2612712429686892e-05
Step 411500 Loss: 0.1713, lr 2.2612712429686892e-05
Step 411600 Loss: 0.1717, lr 2.2612712429686892e-05
Step 411700 Loss: 0.1719, lr 2.2612712429686892e-05
Step 411800 Loss: 0.1726, lr 2.2612712429686892e-05
Step 411900 Loss: 0.1728, lr 2.2612712429686892e-05
Step 412000 Loss: 0.1723, lr 2.2612712429686892e-05
Train Epoch: [101/100] Loss: 0.1723,lr 0.000023
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6755
Step 412100 Loss: 1.3459, lr 2.1640082503365924e-05
Step 412200 Loss: 0.4694, lr 2.1640082503365924e-05
Step 412300 Loss: 0.3901, lr 2.1640082503365924e-05
Step 412400 Loss: 0.3524, lr 2.1640082503365924e-05
Step 412500 Loss: 0.3307, lr 2.1640082503365924e-05
Step 412600 Loss: 0.3164, lr 2.1640082503365924e-05
Step 412700 Loss: 0.3012, lr 2.1640082503365924e-05
Step 412800 Loss: 0.2901, lr 2.1640082503365924e-05
Step 412900 Loss: 0.2825, lr 2.1640082503365924e-05
Step 413000 Loss: 0.2741, lr 2.1640082503365924e-05
Step 413100 Loss: 0.2670, lr 2.1640082503365924e-05
Step 413200 Loss: 0.2615, lr 2.1640082503365924e-05
Step 413300 Loss: 0.2555, lr 2.1640082503365924e-05
Step 413400 Loss: 0.2508, lr 2.1640082503365924e-05
Step 413500 Loss: 0.2455, lr 2.1640082503365924e-05
Step 413600 Loss: 0.2425, lr 2.1640082503365924e-05
Step 413700 Loss: 0.2387, lr 2.1640082503365924e-05
Step 413800 Loss: 0.2360, lr 2.1640082503365924e-05
Step 413900 Loss: 0.2329, lr 2.1640082503365924e-05
Step 414000 Loss: 0.2300, lr 2.1640082503365924e-05
Step 414100 Loss: 0.2260, lr 2.1640082503365924e-05
Step 414200 Loss: 0.2225, lr 2.1640082503365924e-05
Step 414300 Loss: 0.2197, lr 2.1640082503365924e-05
Step 414400 Loss: 0.2170, lr 2.1640082503365924e-05
Step 414500 Loss: 0.2145, lr 2.1640082503365924e-05
Step 414600 Loss: 0.2117, lr 2.1640082503365924e-05
Step 414700 Loss: 0.2092, lr 2.1640082503365924e-05
Step 414800 Loss: 0.2071, lr 2.1640082503365924e-05
Step 414900 Loss: 0.2056, lr 2.1640082503365924e-05
Step 415000 Loss: 0.2036, lr 2.1640082503365924e-05
Step 415100 Loss: 0.2018, lr 2.1640082503365924e-05
Step 415200 Loss: 0.1995, lr 2.1640082503365924e-05
Step 415300 Loss: 0.1975, lr 2.1640082503365924e-05
Step 415400 Loss: 0.1956, lr 2.1640082503365924e-05
Step 415500 Loss: 0.1938, lr 2.1640082503365924e-05
Step 415600 Loss: 0.1920, lr 2.1640082503365924e-05
Step 415700 Loss: 0.1902, lr 2.1640082503365924e-05
Step 415800 Loss: 0.1887, lr 2.1640082503365924e-05
Step 415900 Loss: 0.1873, lr 2.1640082503365924e-05
Step 416000 Loss: 0.1859, lr 2.1640082503365924e-05
Step 416100 Loss: 0.1850, lr 2.1640082503365924e-05
Step 416200 Loss: 0.1836, lr 2.1640082503365924e-05
Step 416300 Loss: 0.1828, lr 2.1640082503365924e-05
Step 416400 Loss: 0.1818, lr 2.1640082503365924e-05
Step 416500 Loss: 0.1807, lr 2.1640082503365924e-05
Step 416600 Loss: 0.1797, lr 2.1640082503365924e-05
Step 416700 Loss: 0.1784, lr 2.1640082503365924e-05
Step 416800 Loss: 0.1775, lr 2.1640082503365924e-05
Step 416900 Loss: 0.1769, lr 2.1640082503365924e-05
Step 417000 Loss: 0.1762, lr 2.1640082503365924e-05
Step 417100 Loss: 0.1755, lr 2.1640082503365924e-05
Step 417200 Loss: 0.1748, lr 2.1640082503365924e-05
Step 417300 Loss: 0.1742, lr 2.1640082503365924e-05
Step 417400 Loss: 0.1734, lr 2.1640082503365924e-05
Step 417500 Loss: 0.1726, lr 2.1640082503365924e-05
Step 417600 Loss: 0.1721, lr 2.1640082503365924e-05
Step 417700 Loss: 0.1715, lr 2.1640082503365924e-05
Step 417800 Loss: 0.1712, lr 2.1640082503365924e-05
Step 417900 Loss: 0.1708, lr 2.1640082503365924e-05
Step 418000 Loss: 0.1706, lr 2.1640082503365924e-05
Step 418100 Loss: 0.1706, lr 2.1640082503365924e-05
Step 418200 Loss: 0.1703, lr 2.1640082503365924e-05
Step 418300 Loss: 0.1703, lr 2.1640082503365924e-05
Step 418400 Loss: 0.1706, lr 2.1640082503365924e-05
Step 418500 Loss: 0.1711, lr 2.1640082503365924e-05
Step 418600 Loss: 0.1718, lr 2.1640082503365924e-05
Step 418700 Loss: 0.1715, lr 2.1640082503365924e-05
Step 418800 Loss: 0.1709, lr 2.1640082503365924e-05
Train Epoch: [102/100] Loss: 0.1711,lr 0.000022
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Step 418900 Loss: 0.5892, lr 2.0678392471600752e-05
Step 419000 Loss: 0.4226, lr 2.0678392471600752e-05
Step 419100 Loss: 0.3683, lr 2.0678392471600752e-05
Step 419200 Loss: 0.3399, lr 2.0678392471600752e-05
Step 419300 Loss: 0.3215, lr 2.0678392471600752e-05
Step 419400 Loss: 0.3098, lr 2.0678392471600752e-05
Step 419500 Loss: 0.2952, lr 2.0678392471600752e-05
Step 419600 Loss: 0.2852, lr 2.0678392471600752e-05
Step 419700 Loss: 0.2780, lr 2.0678392471600752e-05
Step 419800 Loss: 0.2705, lr 2.0678392471600752e-05
Step 419900 Loss: 0.2638, lr 2.0678392471600752e-05
Step 420000 Loss: 0.2584, lr 2.0678392471600752e-05
Step 420100 Loss: 0.2530, lr 2.0678392471600752e-05
Step 420200 Loss: 0.2482, lr 2.0678392471600752e-05
Step 420300 Loss: 0.2436, lr 2.0678392471600752e-05
Step 420400 Loss: 0.2406, lr 2.0678392471600752e-05
Step 420500 Loss: 0.2375, lr 2.0678392471600752e-05
Step 420600 Loss: 0.2343, lr 2.0678392471600752e-05
Step 420700 Loss: 0.2313, lr 2.0678392471600752e-05
Step 420800 Loss: 0.2280, lr 2.0678392471600752e-05
Step 420900 Loss: 0.2241, lr 2.0678392471600752e-05
Step 421000 Loss: 0.2214, lr 2.0678392471600752e-05
Step 421100 Loss: 0.2184, lr 2.0678392471600752e-05
Step 421200 Loss: 0.2155, lr 2.0678392471600752e-05
Step 421300 Loss: 0.2128, lr 2.0678392471600752e-05
Step 421400 Loss: 0.2105, lr 2.0678392471600752e-05
Step 421500 Loss: 0.2082, lr 2.0678392471600752e-05
Step 421600 Loss: 0.2064, lr 2.0678392471600752e-05
Step 421700 Loss: 0.2042, lr 2.0678392471600752e-05
Step 421800 Loss: 0.2026, lr 2.0678392471600752e-05
Step 421900 Loss: 0.2006, lr 2.0678392471600752e-05
Step 422000 Loss: 0.1984, lr 2.0678392471600752e-05
Step 422100 Loss: 0.1963, lr 2.0678392471600752e-05
Step 422200 Loss: 0.1941, lr 2.0678392471600752e-05
Step 422300 Loss: 0.1924, lr 2.0678392471600752e-05
Step 422400 Loss: 0.1907, lr 2.0678392471600752e-05
Step 422500 Loss: 0.1889, lr 2.0678392471600752e-05
Step 422600 Loss: 0.1875, lr 2.0678392471600752e-05
Step 422700 Loss: 0.1861, lr 2.0678392471600752e-05
Step 422800 Loss: 0.1849, lr 2.0678392471600752e-05
Step 422900 Loss: 0.1838, lr 2.0678392471600752e-05
Step 423000 Loss: 0.1827, lr 2.0678392471600752e-05
Step 423100 Loss: 0.1818, lr 2.0678392471600752e-05
Step 423200 Loss: 0.1806, lr 2.0678392471600752e-05
Step 423300 Loss: 0.1796, lr 2.0678392471600752e-05
Step 423400 Loss: 0.1784, lr 2.0678392471600752e-05
Step 423500 Loss: 0.1773, lr 2.0678392471600752e-05
Step 423600 Loss: 0.1764, lr 2.0678392471600752e-05
Step 423700 Loss: 0.1757, lr 2.0678392471600752e-05
Step 423800 Loss: 0.1752, lr 2.0678392471600752e-05
Step 423900 Loss: 0.1743, lr 2.0678392471600752e-05
Step 424000 Loss: 0.1740, lr 2.0678392471600752e-05
Step 424100 Loss: 0.1731, lr 2.0678392471600752e-05
Step 424200 Loss: 0.1725, lr 2.0678392471600752e-05
Step 424300 Loss: 0.1718, lr 2.0678392471600752e-05
Step 424400 Loss: 0.1714, lr 2.0678392471600752e-05
Step 424500 Loss: 0.1709, lr 2.0678392471600752e-05
Step 424600 Loss: 0.1706, lr 2.0678392471600752e-05
Step 424700 Loss: 0.1702, lr 2.0678392471600752e-05
Step 424800 Loss: 0.1701, lr 2.0678392471600752e-05
Step 424900 Loss: 0.1700, lr 2.0678392471600752e-05
Step 425000 Loss: 0.1699, lr 2.0678392471600752e-05
Step 425100 Loss: 0.1702, lr 2.0678392471600752e-05
Step 425200 Loss: 0.1704, lr 2.0678392471600752e-05
Step 425300 Loss: 0.1710, lr 2.0678392471600752e-05
Step 425400 Loss: 0.1714, lr 2.0678392471600752e-05
Step 425500 Loss: 0.1709, lr 2.0678392471600752e-05
Step 425600 Loss: 0.1705, lr 2.0678392471600752e-05
Train Epoch: [103/100] Loss: 0.1713,lr 0.000021
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.48 s, total batches: 6756
Step 425700 Loss: 0.4787, lr 1.9728591406346443e-05
Step 425800 Loss: 0.3924, lr 1.9728591406346443e-05
Step 425900 Loss: 0.3528, lr 1.9728591406346443e-05
Step 426000 Loss: 0.3302, lr 1.9728591406346443e-05
Step 426100 Loss: 0.3167, lr 1.9728591406346443e-05
Step 426200 Loss: 0.3024, lr 1.9728591406346443e-05
Step 426300 Loss: 0.2905, lr 1.9728591406346443e-05
Step 426400 Loss: 0.2825, lr 1.9728591406346443e-05
Step 426500 Loss: 0.2743, lr 1.9728591406346443e-05
Step 426600 Loss: 0.2670, lr 1.9728591406346443e-05
Step 426700 Loss: 0.2613, lr 1.9728591406346443e-05
Step 426800 Loss: 0.2553, lr 1.9728591406346443e-05
Step 426900 Loss: 0.2504, lr 1.9728591406346443e-05
Step 427000 Loss: 0.2449, lr 1.9728591406346443e-05
Step 427100 Loss: 0.2416, lr 1.9728591406346443e-05
Step 427200 Loss: 0.2383, lr 1.9728591406346443e-05
Step 427300 Loss: 0.2353, lr 1.9728591406346443e-05
Step 427400 Loss: 0.2323, lr 1.9728591406346443e-05
Step 427500 Loss: 0.2290, lr 1.9728591406346443e-05
Step 427600 Loss: 0.2252, lr 1.9728591406346443e-05
Step 427700 Loss: 0.2216, lr 1.9728591406346443e-05
Step 427800 Loss: 0.2189, lr 1.9728591406346443e-05
Step 427900 Loss: 0.2163, lr 1.9728591406346443e-05
Step 428000 Loss: 0.2136, lr 1.9728591406346443e-05
Step 428100 Loss: 0.2109, lr 1.9728591406346443e-05
Step 428200 Loss: 0.2085, lr 1.9728591406346443e-05
Step 428300 Loss: 0.2063, lr 1.9728591406346443e-05
Step 428400 Loss: 0.2045, lr 1.9728591406346443e-05
Step 428500 Loss: 0.2024, lr 1.9728591406346443e-05
Step 428600 Loss: 0.2006, lr 1.9728591406346443e-05
Step 428700 Loss: 0.1985, lr 1.9728591406346443e-05
Step 428800 Loss: 0.1965, lr 1.9728591406346443e-05
Step 428900 Loss: 0.1944, lr 1.9728591406346443e-05
Step 429000 Loss: 0.1925, lr 1.9728591406346443e-05
Step 429100 Loss: 0.1908, lr 1.9728591406346443e-05
Step 429200 Loss: 0.1890, lr 1.9728591406346443e-05
Step 429300 Loss: 0.1875, lr 1.9728591406346443e-05
Step 429400 Loss: 0.1862, lr 1.9728591406346443e-05
Step 429500 Loss: 0.1847, lr 1.9728591406346443e-05
Step 429600 Loss: 0.1838, lr 1.9728591406346443e-05
Step 429700 Loss: 0.1826, lr 1.9728591406346443e-05
Step 429800 Loss: 0.1817, lr 1.9728591406346443e-05
Step 429900 Loss: 0.1807, lr 1.9728591406346443e-05
Step 430000 Loss: 0.1797, lr 1.9728591406346443e-05
Step 430100 Loss: 0.1787, lr 1.9728591406346443e-05
Step 430200 Loss: 0.1774, lr 1.9728591406346443e-05
Step 430300 Loss: 0.1764, lr 1.9728591406346443e-05
Step 430400 Loss: 0.1757, lr 1.9728591406346443e-05
Step 430500 Loss: 0.1751, lr 1.9728591406346443e-05
Step 430600 Loss: 0.1744, lr 1.9728591406346443e-05
Step 430700 Loss: 0.1737, lr 1.9728591406346443e-05
Step 430800 Loss: 0.1732, lr 1.9728591406346443e-05
Step 430900 Loss: 0.1724, lr 1.9728591406346443e-05
Step 431000 Loss: 0.1716, lr 1.9728591406346443e-05
Step 431100 Loss: 0.1711, lr 1.9728591406346443e-05
Step 431200 Loss: 0.1706, lr 1.9728591406346443e-05
Step 431300 Loss: 0.1703, lr 1.9728591406346443e-05
Step 431400 Loss: 0.1700, lr 1.9728591406346443e-05
Step 431500 Loss: 0.1697, lr 1.9728591406346443e-05
Step 431600 Loss: 0.1696, lr 1.9728591406346443e-05
Step 431700 Loss: 0.1694, lr 1.9728591406346443e-05
Step 431800 Loss: 0.1694, lr 1.9728591406346443e-05
Step 431900 Loss: 0.1697, lr 1.9728591406346443e-05
Step 432000 Loss: 0.1700, lr 1.9728591406346443e-05
Step 432100 Loss: 0.1706, lr 1.9728591406346443e-05
Step 432200 Loss: 0.1703, lr 1.9728591406346443e-05
Step 432300 Loss: 0.1697, lr 1.9728591406346443e-05
Train Epoch: [104/100] Loss: 0.1698,lr 0.000020
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6755
Step 432400 Loss: 0.6519, lr 1.879161664658357e-05
Step 432500 Loss: 0.4372, lr 1.879161664658357e-05
Step 432600 Loss: 0.3778, lr 1.879161664658357e-05
Step 432700 Loss: 0.3458, lr 1.879161664658357e-05
Step 432800 Loss: 0.3258, lr 1.879161664658357e-05
Step 432900 Loss: 0.3137, lr 1.879161664658357e-05
Step 433000 Loss: 0.2985, lr 1.879161664658357e-05
Step 433100 Loss: 0.2877, lr 1.879161664658357e-05
Step 433200 Loss: 0.2804, lr 1.879161664658357e-05
Step 433300 Loss: 0.2721, lr 1.879161664658357e-05
Step 433400 Loss: 0.2653, lr 1.879161664658357e-05
Step 433500 Loss: 0.2598, lr 1.879161664658357e-05
Step 433600 Loss: 0.2541, lr 1.879161664658357e-05
Step 433700 Loss: 0.2493, lr 1.879161664658357e-05
Step 433800 Loss: 0.2441, lr 1.879161664658357e-05
Step 433900 Loss: 0.2409, lr 1.879161664658357e-05
Step 434000 Loss: 0.2376, lr 1.879161664658357e-05
Step 434100 Loss: 0.2344, lr 1.879161664658357e-05
Step 434200 Loss: 0.2314, lr 1.879161664658357e-05
Step 434300 Loss: 0.2281, lr 1.879161664658357e-05
Step 434400 Loss: 0.2245, lr 1.879161664658357e-05
Step 434500 Loss: 0.2214, lr 1.879161664658357e-05
Step 434600 Loss: 0.2185, lr 1.879161664658357e-05
Step 434700 Loss: 0.2155, lr 1.879161664658357e-05
Step 434800 Loss: 0.2132, lr 1.879161664658357e-05
Step 434900 Loss: 0.2108, lr 1.879161664658357e-05
Step 435000 Loss: 0.2080, lr 1.879161664658357e-05
Step 435100 Loss: 0.2063, lr 1.879161664658357e-05
Step 435200 Loss: 0.2043, lr 1.879161664658357e-05
Step 435300 Loss: 0.2024, lr 1.879161664658357e-05
Step 435400 Loss: 0.2006, lr 1.879161664658357e-05
Step 435500 Loss: 0.1982, lr 1.879161664658357e-05
Step 435600 Loss: 0.1962, lr 1.879161664658357e-05
Step 435700 Loss: 0.1942, lr 1.879161664658357e-05
Step 435800 Loss: 0.1924, lr 1.879161664658357e-05
Step 435900 Loss: 0.1906, lr 1.879161664658357e-05
Step 436000 Loss: 0.1890, lr 1.879161664658357e-05
Step 436100 Loss: 0.1874, lr 1.879161664658357e-05
Step 436200 Loss: 0.1861, lr 1.879161664658357e-05
Step 436300 Loss: 0.1848, lr 1.879161664658357e-05
Step 436400 Loss: 0.1839, lr 1.879161664658357e-05
Step 436500 Loss: 0.1827, lr 1.879161664658357e-05
Step 436600 Loss: 0.1818, lr 1.879161664658357e-05
Step 436700 Loss: 0.1809, lr 1.879161664658357e-05
Step 436800 Loss: 0.1796, lr 1.879161664658357e-05
Step 436900 Loss: 0.1785, lr 1.879161664658357e-05
Step 437000 Loss: 0.1773, lr 1.879161664658357e-05
Step 437100 Loss: 0.1764, lr 1.879161664658357e-05
Step 437200 Loss: 0.1757, lr 1.879161664658357e-05
Step 437300 Loss: 0.1752, lr 1.879161664658357e-05
Step 437400 Loss: 0.1744, lr 1.879161664658357e-05
Step 437500 Loss: 0.1739, lr 1.879161664658357e-05
Step 437600 Loss: 0.1731, lr 1.879161664658357e-05
Step 437700 Loss: 0.1723, lr 1.879161664658357e-05
Step 437800 Loss: 0.1716, lr 1.879161664658357e-05
Step 437900 Loss: 0.1710, lr 1.879161664658357e-05
Step 438000 Loss: 0.1705, lr 1.879161664658357e-05
Step 438100 Loss: 0.1703, lr 1.879161664658357e-05
Step 438200 Loss: 0.1699, lr 1.879161664658357e-05
Step 438300 Loss: 0.1697, lr 1.879161664658357e-05
Step 438400 Loss: 0.1696, lr 1.879161664658357e-05
Step 438500 Loss: 0.1694, lr 1.879161664658357e-05
Step 438600 Loss: 0.1697, lr 1.879161664658357e-05
Step 438700 Loss: 0.1700, lr 1.879161664658357e-05
Step 438800 Loss: 0.1707, lr 1.879161664658357e-05
Step 438900 Loss: 0.1711, lr 1.879161664658357e-05
Step 439000 Loss: 0.1706, lr 1.879161664658357e-05
Step 439100 Loss: 0.1703, lr 1.879161664658357e-05
Train Epoch: [105/100] Loss: 0.1704,lr 0.000019
Model Saving at epoch 105
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 439200 Loss: 0.5346, lr 1.7868392873277668e-05
Step 439300 Loss: 0.4181, lr 1.7868392873277668e-05
Step 439400 Loss: 0.3698, lr 1.7868392873277668e-05
Step 439500 Loss: 0.3427, lr 1.7868392873277668e-05
Step 439600 Loss: 0.3269, lr 1.7868392873277668e-05
Step 439700 Loss: 0.3112, lr 1.7868392873277668e-05
Step 439800 Loss: 0.2975, lr 1.7868392873277668e-05
Step 439900 Loss: 0.2885, lr 1.7868392873277668e-05
Step 440000 Loss: 0.2797, lr 1.7868392873277668e-05
Step 440100 Loss: 0.2722, lr 1.7868392873277668e-05
Step 440200 Loss: 0.2658, lr 1.7868392873277668e-05
Step 440300 Loss: 0.2600, lr 1.7868392873277668e-05
Step 440400 Loss: 0.2548, lr 1.7868392873277668e-05
Step 440500 Loss: 0.2488, lr 1.7868392873277668e-05
Step 440600 Loss: 0.2450, lr 1.7868392873277668e-05
Step 440700 Loss: 0.2416, lr 1.7868392873277668e-05
Step 440800 Loss: 0.2385, lr 1.7868392873277668e-05
Step 440900 Loss: 0.2353, lr 1.7868392873277668e-05
Step 441000 Loss: 0.2321, lr 1.7868392873277668e-05
Step 441100 Loss: 0.2283, lr 1.7868392873277668e-05
Step 441200 Loss: 0.2245, lr 1.7868392873277668e-05
Step 441300 Loss: 0.2216, lr 1.7868392873277668e-05
Step 441400 Loss: 0.2187, lr 1.7868392873277668e-05
Step 441500 Loss: 0.2159, lr 1.7868392873277668e-05
Step 441600 Loss: 0.2130, lr 1.7868392873277668e-05
Step 441700 Loss: 0.2105, lr 1.7868392873277668e-05
Step 441800 Loss: 0.2083, lr 1.7868392873277668e-05
Step 441900 Loss: 0.2063, lr 1.7868392873277668e-05
Step 442000 Loss: 0.2042, lr 1.7868392873277668e-05
Step 442100 Loss: 0.2024, lr 1.7868392873277668e-05
Step 442200 Loss: 0.2002, lr 1.7868392873277668e-05
Step 442300 Loss: 0.1981, lr 1.7868392873277668e-05
Step 442400 Loss: 0.1962, lr 1.7868392873277668e-05
Step 442500 Loss: 0.1941, lr 1.7868392873277668e-05
Step 442600 Loss: 0.1923, lr 1.7868392873277668e-05
Step 442700 Loss: 0.1905, lr 1.7868392873277668e-05
Step 442800 Loss: 0.1890, lr 1.7868392873277668e-05
Step 442900 Loss: 0.1876, lr 1.7868392873277668e-05
Step 443000 Loss: 0.1861, lr 1.7868392873277668e-05
Step 443100 Loss: 0.1850, lr 1.7868392873277668e-05
Step 443200 Loss: 0.1839, lr 1.7868392873277668e-05
Step 443300 Loss: 0.1828, lr 1.7868392873277668e-05
Step 443400 Loss: 0.1817, lr 1.7868392873277668e-05
Step 443500 Loss: 0.1808, lr 1.7868392873277668e-05
Step 443600 Loss: 0.1797, lr 1.7868392873277668e-05
Step 443700 Loss: 0.1784, lr 1.7868392873277668e-05
Step 443800 Loss: 0.1772, lr 1.7868392873277668e-05
Step 443900 Loss: 0.1765, lr 1.7868392873277668e-05
Step 444000 Loss: 0.1759, lr 1.7868392873277668e-05
Step 444100 Loss: 0.1753, lr 1.7868392873277668e-05
Step 444200 Loss: 0.1745, lr 1.7868392873277668e-05
Step 444300 Loss: 0.1739, lr 1.7868392873277668e-05
Step 444400 Loss: 0.1731, lr 1.7868392873277668e-05
Step 444500 Loss: 0.1724, lr 1.7868392873277668e-05
Step 444600 Loss: 0.1720, lr 1.7868392873277668e-05
Step 444700 Loss: 0.1715, lr 1.7868392873277668e-05
Step 444800 Loss: 0.1710, lr 1.7868392873277668e-05
Step 444900 Loss: 0.1706, lr 1.7868392873277668e-05
Step 445000 Loss: 0.1703, lr 1.7868392873277668e-05
Step 445100 Loss: 0.1700, lr 1.7868392873277668e-05
Step 445200 Loss: 0.1699, lr 1.7868392873277668e-05
Step 445300 Loss: 0.1698, lr 1.7868392873277668e-05
Step 445400 Loss: 0.1702, lr 1.7868392873277668e-05
Step 445500 Loss: 0.1704, lr 1.7868392873277668e-05
Step 445600 Loss: 0.1710, lr 1.7868392873277668e-05
Step 445700 Loss: 0.1710, lr 1.7868392873277668e-05
Step 445800 Loss: 0.1705, lr 1.7868392873277668e-05
Train Epoch: [106/100] Loss: 0.1707,lr 0.000018
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6755
Step 445900 Loss: 0.7429, lr 1.6959831196830827e-05
Step 446000 Loss: 0.4462, lr 1.6959831196830827e-05
Step 446100 Loss: 0.3798, lr 1.6959831196830827e-05
Step 446200 Loss: 0.3466, lr 1.6959831196830827e-05
Step 446300 Loss: 0.3275, lr 1.6959831196830827e-05
Step 446400 Loss: 0.3149, lr 1.6959831196830827e-05
Step 446500 Loss: 0.2991, lr 1.6959831196830827e-05
Step 446600 Loss: 0.2878, lr 1.6959831196830827e-05
Step 446700 Loss: 0.2811, lr 1.6959831196830827e-05
Step 446800 Loss: 0.2727, lr 1.6959831196830827e-05
Step 446900 Loss: 0.2658, lr 1.6959831196830827e-05
Step 447000 Loss: 0.2603, lr 1.6959831196830827e-05
Step 447100 Loss: 0.2544, lr 1.6959831196830827e-05
Step 447200 Loss: 0.2496, lr 1.6959831196830827e-05
Step 447300 Loss: 0.2447, lr 1.6959831196830827e-05
Step 447400 Loss: 0.2414, lr 1.6959831196830827e-05
Step 447500 Loss: 0.2382, lr 1.6959831196830827e-05
Step 447600 Loss: 0.2350, lr 1.6959831196830827e-05
Step 447700 Loss: 0.2321, lr 1.6959831196830827e-05
Step 447800 Loss: 0.2286, lr 1.6959831196830827e-05
Step 447900 Loss: 0.2247, lr 1.6959831196830827e-05
Step 448000 Loss: 0.2214, lr 1.6959831196830827e-05
Step 448100 Loss: 0.2186, lr 1.6959831196830827e-05
Step 448200 Loss: 0.2156, lr 1.6959831196830827e-05
Step 448300 Loss: 0.2133, lr 1.6959831196830827e-05
Step 448400 Loss: 0.2105, lr 1.6959831196830827e-05
Step 448500 Loss: 0.2080, lr 1.6959831196830827e-05
Step 448600 Loss: 0.2063, lr 1.6959831196830827e-05
Step 448700 Loss: 0.2042, lr 1.6959831196830827e-05
Step 448800 Loss: 0.2021, lr 1.6959831196830827e-05
Step 448900 Loss: 0.2003, lr 1.6959831196830827e-05
Step 449000 Loss: 0.1980, lr 1.6959831196830827e-05
Step 449100 Loss: 0.1960, lr 1.6959831196830827e-05
Step 449200 Loss: 0.1939, lr 1.6959831196830827e-05
Step 449300 Loss: 0.1922, lr 1.6959831196830827e-05
Step 449400 Loss: 0.1905, lr 1.6959831196830827e-05
Step 449500 Loss: 0.1889, lr 1.6959831196830827e-05
Step 449600 Loss: 0.1874, lr 1.6959831196830827e-05
Step 449700 Loss: 0.1859, lr 1.6959831196830827e-05
Step 449800 Loss: 0.1847, lr 1.6959831196830827e-05
Step 449900 Loss: 0.1837, lr 1.6959831196830827e-05
Step 450000 Loss: 0.1824, lr 1.6959831196830827e-05
Step 450100 Loss: 0.1816, lr 1.6959831196830827e-05
Step 450200 Loss: 0.1806, lr 1.6959831196830827e-05
Step 450300 Loss: 0.1795, lr 1.6959831196830827e-05
Step 450400 Loss: 0.1783, lr 1.6959831196830827e-05
Step 450500 Loss: 0.1773, lr 1.6959831196830827e-05
Step 450600 Loss: 0.1763, lr 1.6959831196830827e-05
Step 450700 Loss: 0.1756, lr 1.6959831196830827e-05
Step 450800 Loss: 0.1751, lr 1.6959831196830827e-05
Step 450900 Loss: 0.1743, lr 1.6959831196830827e-05
Step 451000 Loss: 0.1737, lr 1.6959831196830827e-05
Step 451100 Loss: 0.1729, lr 1.6959831196830827e-05
Step 451200 Loss: 0.1723, lr 1.6959831196830827e-05
Step 451300 Loss: 0.1716, lr 1.6959831196830827e-05
Step 451400 Loss: 0.1710, lr 1.6959831196830827e-05
Step 451500 Loss: 0.1705, lr 1.6959831196830827e-05
Step 451600 Loss: 0.1700, lr 1.6959831196830827e-05
Step 451700 Loss: 0.1697, lr 1.6959831196830827e-05
Step 451800 Loss: 0.1695, lr 1.6959831196830827e-05
Step 451900 Loss: 0.1694, lr 1.6959831196830827e-05
Step 452000 Loss: 0.1692, lr 1.6959831196830827e-05
Step 452100 Loss: 0.1692, lr 1.6959831196830827e-05
Step 452200 Loss: 0.1696, lr 1.6959831196830827e-05
Step 452300 Loss: 0.1701, lr 1.6959831196830827e-05
Step 452400 Loss: 0.1706, lr 1.6959831196830827e-05
Step 452500 Loss: 0.1702, lr 1.6959831196830827e-05
Step 452600 Loss: 0.1696, lr 1.6959831196830827e-05
Train Epoch: [107/100] Loss: 0.1699,lr 0.000017
Calling G2SDataset.batch()
Done, time:  2.32 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Step 452700 Loss: 0.5017, lr 1.6066828257925737e-05
Step 452800 Loss: 0.4012, lr 1.6066828257925737e-05
Step 452900 Loss: 0.3576, lr 1.6066828257925737e-05
Step 453000 Loss: 0.3318, lr 1.6066828257925737e-05
Step 453100 Loss: 0.3178, lr 1.6066828257925737e-05
Step 453200 Loss: 0.3044, lr 1.6066828257925737e-05
Step 453300 Loss: 0.2904, lr 1.6066828257925737e-05
Step 453400 Loss: 0.2824, lr 1.6066828257925737e-05
Step 453500 Loss: 0.2748, lr 1.6066828257925737e-05
Step 453600 Loss: 0.2677, lr 1.6066828257925737e-05
Step 453700 Loss: 0.2614, lr 1.6066828257925737e-05
Step 453800 Loss: 0.2559, lr 1.6066828257925737e-05
Step 453900 Loss: 0.2510, lr 1.6066828257925737e-05
Step 454000 Loss: 0.2457, lr 1.6066828257925737e-05
Step 454100 Loss: 0.2415, lr 1.6066828257925737e-05
Step 454200 Loss: 0.2386, lr 1.6066828257925737e-05
Step 454300 Loss: 0.2355, lr 1.6066828257925737e-05
Step 454400 Loss: 0.2324, lr 1.6066828257925737e-05
Step 454500 Loss: 0.2295, lr 1.6066828257925737e-05
Step 454600 Loss: 0.2260, lr 1.6066828257925737e-05
Step 454700 Loss: 0.2225, lr 1.6066828257925737e-05
Step 454800 Loss: 0.2196, lr 1.6066828257925737e-05
Step 454900 Loss: 0.2170, lr 1.6066828257925737e-05
Step 455000 Loss: 0.2141, lr 1.6066828257925737e-05
Step 455100 Loss: 0.2113, lr 1.6066828257925737e-05
Step 455200 Loss: 0.2090, lr 1.6066828257925737e-05
Step 455300 Loss: 0.2065, lr 1.6066828257925737e-05
Step 455400 Loss: 0.2047, lr 1.6066828257925737e-05
Step 455500 Loss: 0.2026, lr 1.6066828257925737e-05
Step 455600 Loss: 0.2008, lr 1.6066828257925737e-05
Step 455700 Loss: 0.1987, lr 1.6066828257925737e-05
Step 455800 Loss: 0.1966, lr 1.6066828257925737e-05
Step 455900 Loss: 0.1949, lr 1.6066828257925737e-05
Step 456000 Loss: 0.1927, lr 1.6066828257925737e-05
Step 456100 Loss: 0.1910, lr 1.6066828257925737e-05
Step 456200 Loss: 0.1891, lr 1.6066828257925737e-05
Step 456300 Loss: 0.1877, lr 1.6066828257925737e-05
Step 456400 Loss: 0.1862, lr 1.6066828257925737e-05
Step 456500 Loss: 0.1847, lr 1.6066828257925737e-05
Step 456600 Loss: 0.1836, lr 1.6066828257925737e-05
Step 456700 Loss: 0.1826, lr 1.6066828257925737e-05
Step 456800 Loss: 0.1816, lr 1.6066828257925737e-05
Step 456900 Loss: 0.1805, lr 1.6066828257925737e-05
Step 457000 Loss: 0.1794, lr 1.6066828257925737e-05
Step 457100 Loss: 0.1784, lr 1.6066828257925737e-05
Step 457200 Loss: 0.1771, lr 1.6066828257925737e-05
Step 457300 Loss: 0.1760, lr 1.6066828257925737e-05
Step 457400 Loss: 0.1751, lr 1.6066828257925737e-05
Step 457500 Loss: 0.1745, lr 1.6066828257925737e-05
Step 457600 Loss: 0.1739, lr 1.6066828257925737e-05
Step 457700 Loss: 0.1731, lr 1.6066828257925737e-05
Step 457800 Loss: 0.1726, lr 1.6066828257925737e-05
Step 457900 Loss: 0.1717, lr 1.6066828257925737e-05
Step 458000 Loss: 0.1711, lr 1.6066828257925737e-05
Step 458100 Loss: 0.1705, lr 1.6066828257925737e-05
Step 458200 Loss: 0.1700, lr 1.6066828257925737e-05
Step 458300 Loss: 0.1694, lr 1.6066828257925737e-05
Step 458400 Loss: 0.1690, lr 1.6066828257925737e-05
Step 458500 Loss: 0.1687, lr 1.6066828257925737e-05
Step 458600 Loss: 0.1684, lr 1.6066828257925737e-05
Step 458700 Loss: 0.1682, lr 1.6066828257925737e-05
Step 458800 Loss: 0.1682, lr 1.6066828257925737e-05
Step 458900 Loss: 0.1686, lr 1.6066828257925737e-05
Step 459000 Loss: 0.1689, lr 1.6066828257925737e-05
Step 459100 Loss: 0.1695, lr 1.6066828257925737e-05
Step 459200 Loss: 0.1695, lr 1.6066828257925737e-05
Step 459300 Loss: 0.1689, lr 1.6066828257925737e-05
Train Epoch: [108/100] Loss: 0.1692,lr 0.000016
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Step 459400 Loss: 0.9219, lr 1.5190265342649522e-05
Step 459500 Loss: 0.4644, lr 1.5190265342649522e-05
Step 459600 Loss: 0.3902, lr 1.5190265342649522e-05
Step 459700 Loss: 0.3532, lr 1.5190265342649522e-05
Step 459800 Loss: 0.3331, lr 1.5190265342649522e-05
Step 459900 Loss: 0.3181, lr 1.5190265342649522e-05
Step 460000 Loss: 0.3024, lr 1.5190265342649522e-05
Step 460100 Loss: 0.2906, lr 1.5190265342649522e-05
Step 460200 Loss: 0.2836, lr 1.5190265342649522e-05
Step 460300 Loss: 0.2748, lr 1.5190265342649522e-05
Step 460400 Loss: 0.2676, lr 1.5190265342649522e-05
Step 460500 Loss: 0.2622, lr 1.5190265342649522e-05
Step 460600 Loss: 0.2563, lr 1.5190265342649522e-05
Step 460700 Loss: 0.2509, lr 1.5190265342649522e-05
Step 460800 Loss: 0.2459, lr 1.5190265342649522e-05
Step 460900 Loss: 0.2427, lr 1.5190265342649522e-05
Step 461000 Loss: 0.2391, lr 1.5190265342649522e-05
Step 461100 Loss: 0.2361, lr 1.5190265342649522e-05
Step 461200 Loss: 0.2328, lr 1.5190265342649522e-05
Step 461300 Loss: 0.2299, lr 1.5190265342649522e-05
Step 461400 Loss: 0.2263, lr 1.5190265342649522e-05
Step 461500 Loss: 0.2228, lr 1.5190265342649522e-05
Step 461600 Loss: 0.2202, lr 1.5190265342649522e-05
Step 461700 Loss: 0.2171, lr 1.5190265342649522e-05
Step 461800 Loss: 0.2146, lr 1.5190265342649522e-05
Step 461900 Loss: 0.2119, lr 1.5190265342649522e-05
Step 462000 Loss: 0.2093, lr 1.5190265342649522e-05
Step 462100 Loss: 0.2072, lr 1.5190265342649522e-05
Step 462200 Loss: 0.2053, lr 1.5190265342649522e-05
Step 462300 Loss: 0.2032, lr 1.5190265342649522e-05
Step 462400 Loss: 0.2013, lr 1.5190265342649522e-05
Step 462500 Loss: 0.1990, lr 1.5190265342649522e-05
Step 462600 Loss: 0.1970, lr 1.5190265342649522e-05
Step 462700 Loss: 0.1949, lr 1.5190265342649522e-05
Step 462800 Loss: 0.1932, lr 1.5190265342649522e-05
Step 462900 Loss: 0.1914, lr 1.5190265342649522e-05
Step 463000 Loss: 0.1897, lr 1.5190265342649522e-05
Step 463100 Loss: 0.1881, lr 1.5190265342649522e-05
Step 463200 Loss: 0.1866, lr 1.5190265342649522e-05
Step 463300 Loss: 0.1854, lr 1.5190265342649522e-05
Step 463400 Loss: 0.1844, lr 1.5190265342649522e-05
Step 463500 Loss: 0.1831, lr 1.5190265342649522e-05
Step 463600 Loss: 0.1822, lr 1.5190265342649522e-05
Step 463700 Loss: 0.1811, lr 1.5190265342649522e-05
Step 463800 Loss: 0.1801, lr 1.5190265342649522e-05
Step 463900 Loss: 0.1789, lr 1.5190265342649522e-05
Step 464000 Loss: 0.1777, lr 1.5190265342649522e-05
Step 464100 Loss: 0.1768, lr 1.5190265342649522e-05
Step 464200 Loss: 0.1761, lr 1.5190265342649522e-05
Step 464300 Loss: 0.1755, lr 1.5190265342649522e-05
Step 464400 Loss: 0.1747, lr 1.5190265342649522e-05
Step 464500 Loss: 0.1741, lr 1.5190265342649522e-05
Step 464600 Loss: 0.1734, lr 1.5190265342649522e-05
Step 464700 Loss: 0.1726, lr 1.5190265342649522e-05
Step 464800 Loss: 0.1718, lr 1.5190265342649522e-05
Step 464900 Loss: 0.1712, lr 1.5190265342649522e-05
Step 465000 Loss: 0.1706, lr 1.5190265342649522e-05
Step 465100 Loss: 0.1700, lr 1.5190265342649522e-05
Step 465200 Loss: 0.1697, lr 1.5190265342649522e-05
Step 465300 Loss: 0.1694, lr 1.5190265342649522e-05
Step 465400 Loss: 0.1693, lr 1.5190265342649522e-05
Step 465500 Loss: 0.1690, lr 1.5190265342649522e-05
Step 465600 Loss: 0.1691, lr 1.5190265342649522e-05
Step 465700 Loss: 0.1694, lr 1.5190265342649522e-05
Step 465800 Loss: 0.1698, lr 1.5190265342649522e-05
Step 465900 Loss: 0.1703, lr 1.5190265342649522e-05
Step 466000 Loss: 0.1699, lr 1.5190265342649522e-05
Step 466100 Loss: 0.1694, lr 1.5190265342649522e-05
Train Epoch: [109/100] Loss: 0.1699,lr 0.000015
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6755
Step 466200 Loss: 0.5930, lr 1.4331007512770862e-05
Step 466300 Loss: 0.4386, lr 1.4331007512770862e-05
Step 466400 Loss: 0.3848, lr 1.4331007512770862e-05
Step 466500 Loss: 0.3520, lr 1.4331007512770862e-05
Step 466600 Loss: 0.3330, lr 1.4331007512770862e-05
Step 466700 Loss: 0.3191, lr 1.4331007512770862e-05
Step 466800 Loss: 0.3024, lr 1.4331007512770862e-05
Step 466900 Loss: 0.2918, lr 1.4331007512770862e-05
Step 467000 Loss: 0.2834, lr 1.4331007512770862e-05
Step 467100 Loss: 0.2756, lr 1.4331007512770862e-05
Step 467200 Loss: 0.2683, lr 1.4331007512770862e-05
Step 467300 Loss: 0.2629, lr 1.4331007512770862e-05
Step 467400 Loss: 0.2570, lr 1.4331007512770862e-05
Step 467500 Loss: 0.2518, lr 1.4331007512770862e-05
Step 467600 Loss: 0.2470, lr 1.4331007512770862e-05
Step 467700 Loss: 0.2436, lr 1.4331007512770862e-05
Step 467800 Loss: 0.2399, lr 1.4331007512770862e-05
Step 467900 Loss: 0.2366, lr 1.4331007512770862e-05
Step 468000 Loss: 0.2334, lr 1.4331007512770862e-05
Step 468100 Loss: 0.2298, lr 1.4331007512770862e-05
Step 468200 Loss: 0.2264, lr 1.4331007512770862e-05
Step 468300 Loss: 0.2239, lr 1.4331007512770862e-05
Step 468400 Loss: 0.2213, lr 1.4331007512770862e-05
Step 468500 Loss: 0.2187, lr 1.4331007512770862e-05
Step 468600 Loss: 0.2158, lr 1.4331007512770862e-05
Step 468700 Loss: 0.2134, lr 1.4331007512770862e-05
Step 468800 Loss: 0.2107, lr 1.4331007512770862e-05
Step 468900 Loss: 0.2087, lr 1.4331007512770862e-05
Step 469000 Loss: 0.2065, lr 1.4331007512770862e-05
Step 469100 Loss: 0.2044, lr 1.4331007512770862e-05
Step 469200 Loss: 0.2023, lr 1.4331007512770862e-05
Step 469300 Loss: 0.2000, lr 1.4331007512770862e-05
Step 469400 Loss: 0.1980, lr 1.4331007512770862e-05
Step 469500 Loss: 0.1957, lr 1.4331007512770862e-05
Step 469600 Loss: 0.1939, lr 1.4331007512770862e-05
Step 469700 Loss: 0.1921, lr 1.4331007512770862e-05
Step 469800 Loss: 0.1905, lr 1.4331007512770862e-05
Step 469900 Loss: 0.1891, lr 1.4331007512770862e-05
Step 470000 Loss: 0.1876, lr 1.4331007512770862e-05
Step 470100 Loss: 0.1863, lr 1.4331007512770862e-05
Step 470200 Loss: 0.1853, lr 1.4331007512770862e-05
Step 470300 Loss: 0.1841, lr 1.4331007512770862e-05
Step 470400 Loss: 0.1831, lr 1.4331007512770862e-05
Step 470500 Loss: 0.1818, lr 1.4331007512770862e-05
Step 470600 Loss: 0.1809, lr 1.4331007512770862e-05
Step 470700 Loss: 0.1794, lr 1.4331007512770862e-05
Step 470800 Loss: 0.1783, lr 1.4331007512770862e-05
Step 470900 Loss: 0.1775, lr 1.4331007512770862e-05
Step 471000 Loss: 0.1769, lr 1.4331007512770862e-05
Step 471100 Loss: 0.1762, lr 1.4331007512770862e-05
Step 471200 Loss: 0.1754, lr 1.4331007512770862e-05
Step 471300 Loss: 0.1750, lr 1.4331007512770862e-05
Step 471400 Loss: 0.1740, lr 1.4331007512770862e-05
Step 471500 Loss: 0.1733, lr 1.4331007512770862e-05
Step 471600 Loss: 0.1728, lr 1.4331007512770862e-05
Step 471700 Loss: 0.1722, lr 1.4331007512770862e-05
Step 471800 Loss: 0.1716, lr 1.4331007512770862e-05
Step 471900 Loss: 0.1711, lr 1.4331007512770862e-05
Step 472000 Loss: 0.1709, lr 1.4331007512770862e-05
Step 472100 Loss: 0.1705, lr 1.4331007512770862e-05
Step 472200 Loss: 0.1703, lr 1.4331007512770862e-05
Step 472300 Loss: 0.1701, lr 1.4331007512770862e-05
Step 472400 Loss: 0.1703, lr 1.4331007512770862e-05
Step 472500 Loss: 0.1705, lr 1.4331007512770862e-05
Step 472600 Loss: 0.1710, lr 1.4331007512770862e-05
Step 472700 Loss: 0.1711, lr 1.4331007512770862e-05
Step 472800 Loss: 0.1706, lr 1.4331007512770862e-05
Train Epoch: [110/100] Loss: 0.1706,lr 0.000014
Model Saving at epoch 110
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.80 s, total batches: 6756
Step 472900 Loss: 1.1813, lr 1.3489902752028382e-05
Step 473000 Loss: 0.4832, lr 1.3489902752028382e-05
Step 473100 Loss: 0.4085, lr 1.3489902752028382e-05
Step 473200 Loss: 0.3693, lr 1.3489902752028382e-05
Step 473300 Loss: 0.3463, lr 1.3489902752028382e-05
Step 473400 Loss: 0.3294, lr 1.3489902752028382e-05
Step 473500 Loss: 0.3128, lr 1.3489902752028382e-05
Step 473600 Loss: 0.3008, lr 1.3489902752028382e-05
Step 473700 Loss: 0.2924, lr 1.3489902752028382e-05
Step 473800 Loss: 0.2825, lr 1.3489902752028382e-05
Step 473900 Loss: 0.2746, lr 1.3489902752028382e-05
Step 474000 Loss: 0.2686, lr 1.3489902752028382e-05
Step 474100 Loss: 0.2620, lr 1.3489902752028382e-05
Step 474200 Loss: 0.2567, lr 1.3489902752028382e-05
Step 474300 Loss: 0.2510, lr 1.3489902752028382e-05
Step 474400 Loss: 0.2476, lr 1.3489902752028382e-05
Step 474500 Loss: 0.2435, lr 1.3489902752028382e-05
Step 474600 Loss: 0.2400, lr 1.3489902752028382e-05
Step 474700 Loss: 0.2366, lr 1.3489902752028382e-05
Step 474800 Loss: 0.2336, lr 1.3489902752028382e-05
Step 474900 Loss: 0.2299, lr 1.3489902752028382e-05
Step 475000 Loss: 0.2262, lr 1.3489902752028382e-05
Step 475100 Loss: 0.2235, lr 1.3489902752028382e-05
Step 475200 Loss: 0.2205, lr 1.3489902752028382e-05
Step 475300 Loss: 0.2176, lr 1.3489902752028382e-05
Step 475400 Loss: 0.2148, lr 1.3489902752028382e-05
Step 475500 Loss: 0.2120, lr 1.3489902752028382e-05
Step 475600 Loss: 0.2098, lr 1.3489902752028382e-05
Step 475700 Loss: 0.2079, lr 1.3489902752028382e-05
Step 475800 Loss: 0.2057, lr 1.3489902752028382e-05
Step 475900 Loss: 0.2037, lr 1.3489902752028382e-05
Step 476000 Loss: 0.2014, lr 1.3489902752028382e-05
Step 476100 Loss: 0.1993, lr 1.3489902752028382e-05
Step 476200 Loss: 0.1971, lr 1.3489902752028382e-05
Step 476300 Loss: 0.1952, lr 1.3489902752028382e-05
Step 476400 Loss: 0.1934, lr 1.3489902752028382e-05
Step 476500 Loss: 0.1915, lr 1.3489902752028382e-05
Step 476600 Loss: 0.1899, lr 1.3489902752028382e-05
Step 476700 Loss: 0.1885, lr 1.3489902752028382e-05
Step 476800 Loss: 0.1871, lr 1.3489902752028382e-05
Step 476900 Loss: 0.1860, lr 1.3489902752028382e-05
Step 477000 Loss: 0.1846, lr 1.3489902752028382e-05
Step 477100 Loss: 0.1836, lr 1.3489902752028382e-05
Step 477200 Loss: 0.1826, lr 1.3489902752028382e-05
Step 477300 Loss: 0.1813, lr 1.3489902752028382e-05
Step 477400 Loss: 0.1803, lr 1.3489902752028382e-05
Step 477500 Loss: 0.1789, lr 1.3489902752028382e-05
Step 477600 Loss: 0.1780, lr 1.3489902752028382e-05
Step 477700 Loss: 0.1772, lr 1.3489902752028382e-05
Step 477800 Loss: 0.1765, lr 1.3489902752028382e-05
Step 477900 Loss: 0.1758, lr 1.3489902752028382e-05
Step 478000 Loss: 0.1752, lr 1.3489902752028382e-05
Step 478100 Loss: 0.1745, lr 1.3489902752028382e-05
Step 478200 Loss: 0.1737, lr 1.3489902752028382e-05
Step 478300 Loss: 0.1728, lr 1.3489902752028382e-05
Step 478400 Loss: 0.1722, lr 1.3489902752028382e-05
Step 478500 Loss: 0.1715, lr 1.3489902752028382e-05
Step 478600 Loss: 0.1711, lr 1.3489902752028382e-05
Step 478700 Loss: 0.1708, lr 1.3489902752028382e-05
Step 478800 Loss: 0.1704, lr 1.3489902752028382e-05
Step 478900 Loss: 0.1702, lr 1.3489902752028382e-05
Step 479000 Loss: 0.1699, lr 1.3489902752028382e-05
Step 479100 Loss: 0.1699, lr 1.3489902752028382e-05
Step 479200 Loss: 0.1701, lr 1.3489902752028382e-05
Step 479300 Loss: 0.1706, lr 1.3489902752028382e-05
Step 479400 Loss: 0.1712, lr 1.3489902752028382e-05
Step 479500 Loss: 0.1707, lr 1.3489902752028382e-05
Step 479600 Loss: 0.1702, lr 1.3489902752028382e-05
Train Epoch: [111/100] Loss: 0.1704,lr 0.000013
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Step 479700 Loss: 0.6447, lr 1.2667781129273109e-05
Step 479800 Loss: 0.4446, lr 1.2667781129273109e-05
Step 479900 Loss: 0.3858, lr 1.2667781129273109e-05
Step 480000 Loss: 0.3551, lr 1.2667781129273109e-05
Step 480100 Loss: 0.3365, lr 1.2667781129273109e-05
Step 480200 Loss: 0.3226, lr 1.2667781129273109e-05
Step 480300 Loss: 0.3064, lr 1.2667781129273109e-05
Step 480400 Loss: 0.2954, lr 1.2667781129273109e-05
Step 480500 Loss: 0.2875, lr 1.2667781129273109e-05
Step 480600 Loss: 0.2789, lr 1.2667781129273109e-05
Step 480700 Loss: 0.2718, lr 1.2667781129273109e-05
Step 480800 Loss: 0.2659, lr 1.2667781129273109e-05
Step 480900 Loss: 0.2603, lr 1.2667781129273109e-05
Step 481000 Loss: 0.2549, lr 1.2667781129273109e-05
Step 481100 Loss: 0.2495, lr 1.2667781129273109e-05
Step 481200 Loss: 0.2462, lr 1.2667781129273109e-05
Step 481300 Loss: 0.2425, lr 1.2667781129273109e-05
Step 481400 Loss: 0.2390, lr 1.2667781129273109e-05
Step 481500 Loss: 0.2355, lr 1.2667781129273109e-05
Step 481600 Loss: 0.2320, lr 1.2667781129273109e-05
Step 481700 Loss: 0.2280, lr 1.2667781129273109e-05
Step 481800 Loss: 0.2250, lr 1.2667781129273109e-05
Step 481900 Loss: 0.2219, lr 1.2667781129273109e-05
Step 482000 Loss: 0.2189, lr 1.2667781129273109e-05
Step 482100 Loss: 0.2162, lr 1.2667781129273109e-05
Step 482200 Loss: 0.2135, lr 1.2667781129273109e-05
Step 482300 Loss: 0.2109, lr 1.2667781129273109e-05
Step 482400 Loss: 0.2090, lr 1.2667781129273109e-05
Step 482500 Loss: 0.2068, lr 1.2667781129273109e-05
Step 482600 Loss: 0.2048, lr 1.2667781129273109e-05
Step 482700 Loss: 0.2026, lr 1.2667781129273109e-05
Step 482800 Loss: 0.2003, lr 1.2667781129273109e-05
Step 482900 Loss: 0.1982, lr 1.2667781129273109e-05
Step 483000 Loss: 0.1959, lr 1.2667781129273109e-05
Step 483100 Loss: 0.1940, lr 1.2667781129273109e-05
Step 483200 Loss: 0.1923, lr 1.2667781129273109e-05
Step 483300 Loss: 0.1906, lr 1.2667781129273109e-05
Step 483400 Loss: 0.1889, lr 1.2667781129273109e-05
Step 483500 Loss: 0.1875, lr 1.2667781129273109e-05
Step 483600 Loss: 0.1861, lr 1.2667781129273109e-05
Step 483700 Loss: 0.1850, lr 1.2667781129273109e-05
Step 483800 Loss: 0.1839, lr 1.2667781129273109e-05
Step 483900 Loss: 0.1828, lr 1.2667781129273109e-05
Step 484000 Loss: 0.1817, lr 1.2667781129273109e-05
Step 484100 Loss: 0.1806, lr 1.2667781129273109e-05
Step 484200 Loss: 0.1793, lr 1.2667781129273109e-05
Step 484300 Loss: 0.1781, lr 1.2667781129273109e-05
Step 484400 Loss: 0.1771, lr 1.2667781129273109e-05
Step 484500 Loss: 0.1765, lr 1.2667781129273109e-05
Step 484600 Loss: 0.1758, lr 1.2667781129273109e-05
Step 484700 Loss: 0.1749, lr 1.2667781129273109e-05
Step 484800 Loss: 0.1744, lr 1.2667781129273109e-05
Step 484900 Loss: 0.1736, lr 1.2667781129273109e-05
Step 485000 Loss: 0.1728, lr 1.2667781129273109e-05
Step 485100 Loss: 0.1721, lr 1.2667781129273109e-05
Step 485200 Loss: 0.1715, lr 1.2667781129273109e-05
Step 485300 Loss: 0.1709, lr 1.2667781129273109e-05
Step 485400 Loss: 0.1706, lr 1.2667781129273109e-05
Step 485500 Loss: 0.1702, lr 1.2667781129273109e-05
Step 485600 Loss: 0.1699, lr 1.2667781129273109e-05
Step 485700 Loss: 0.1697, lr 1.2667781129273109e-05
Step 485800 Loss: 0.1694, lr 1.2667781129273109e-05
Step 485900 Loss: 0.1696, lr 1.2667781129273109e-05
Step 486000 Loss: 0.1699, lr 1.2667781129273109e-05
Step 486100 Loss: 0.1705, lr 1.2667781129273109e-05
Step 486200 Loss: 0.1707, lr 1.2667781129273109e-05
Step 486300 Loss: 0.1701, lr 1.2667781129273109e-05
Step 486400 Loss: 0.1698, lr 1.2667781129273109e-05
Train Epoch: [112/100] Loss: 0.1703,lr 0.000013
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6755
Step 486500 Loss: 0.4743, lr 1.186545397929078e-05
Step 486600 Loss: 0.3938, lr 1.186545397929078e-05
Step 486700 Loss: 0.3559, lr 1.186545397929078e-05
Step 486800 Loss: 0.3343, lr 1.186545397929078e-05
Step 486900 Loss: 0.3203, lr 1.186545397929078e-05
Step 487000 Loss: 0.3055, lr 1.186545397929078e-05
Step 487100 Loss: 0.2935, lr 1.186545397929078e-05
Step 487200 Loss: 0.2850, lr 1.186545397929078e-05
Step 487300 Loss: 0.2764, lr 1.186545397929078e-05
Step 487400 Loss: 0.2692, lr 1.186545397929078e-05
Step 487500 Loss: 0.2633, lr 1.186545397929078e-05
Step 487600 Loss: 0.2572, lr 1.186545397929078e-05
Step 487700 Loss: 0.2523, lr 1.186545397929078e-05
Step 487800 Loss: 0.2468, lr 1.186545397929078e-05
Step 487900 Loss: 0.2431, lr 1.186545397929078e-05
Step 488000 Loss: 0.2397, lr 1.186545397929078e-05
Step 488100 Loss: 0.2366, lr 1.186545397929078e-05
Step 488200 Loss: 0.2334, lr 1.186545397929078e-05
Step 488300 Loss: 0.2302, lr 1.186545397929078e-05
Step 488400 Loss: 0.2262, lr 1.186545397929078e-05
Step 488500 Loss: 0.2227, lr 1.186545397929078e-05
Step 488600 Loss: 0.2200, lr 1.186545397929078e-05
Step 488700 Loss: 0.2174, lr 1.186545397929078e-05
Step 488800 Loss: 0.2147, lr 1.186545397929078e-05
Step 488900 Loss: 0.2120, lr 1.186545397929078e-05
Step 489000 Loss: 0.2095, lr 1.186545397929078e-05
Step 489100 Loss: 0.2073, lr 1.186545397929078e-05
Step 489200 Loss: 0.2054, lr 1.186545397929078e-05
Step 489300 Loss: 0.2035, lr 1.186545397929078e-05
Step 489400 Loss: 0.2014, lr 1.186545397929078e-05
Step 489500 Loss: 0.1993, lr 1.186545397929078e-05
Step 489600 Loss: 0.1971, lr 1.186545397929078e-05
Step 489700 Loss: 0.1950, lr 1.186545397929078e-05
Step 489800 Loss: 0.1930, lr 1.186545397929078e-05
Step 489900 Loss: 0.1912, lr 1.186545397929078e-05
Step 490000 Loss: 0.1893, lr 1.186545397929078e-05
Step 490100 Loss: 0.1878, lr 1.186545397929078e-05
Step 490200 Loss: 0.1863, lr 1.186545397929078e-05
Step 490300 Loss: 0.1849, lr 1.186545397929078e-05
Step 490400 Loss: 0.1838, lr 1.186545397929078e-05
Step 490500 Loss: 0.1825, lr 1.186545397929078e-05
Step 490600 Loss: 0.1815, lr 1.186545397929078e-05
Step 490700 Loss: 0.1805, lr 1.186545397929078e-05
Step 490800 Loss: 0.1794, lr 1.186545397929078e-05
Step 490900 Loss: 0.1782, lr 1.186545397929078e-05
Step 491000 Loss: 0.1769, lr 1.186545397929078e-05
Step 491100 Loss: 0.1759, lr 1.186545397929078e-05
Step 491200 Loss: 0.1752, lr 1.186545397929078e-05
Step 491300 Loss: 0.1745, lr 1.186545397929078e-05
Step 491400 Loss: 0.1738, lr 1.186545397929078e-05
Step 491500 Loss: 0.1731, lr 1.186545397929078e-05
Step 491600 Loss: 0.1724, lr 1.186545397929078e-05
Step 491700 Loss: 0.1717, lr 1.186545397929078e-05
Step 491800 Loss: 0.1709, lr 1.186545397929078e-05
Step 491900 Loss: 0.1704, lr 1.186545397929078e-05
Step 492000 Loss: 0.1697, lr 1.186545397929078e-05
Step 492100 Loss: 0.1694, lr 1.186545397929078e-05
Step 492200 Loss: 0.1689, lr 1.186545397929078e-05
Step 492300 Loss: 0.1686, lr 1.186545397929078e-05
Step 492400 Loss: 0.1684, lr 1.186545397929078e-05
Step 492500 Loss: 0.1681, lr 1.186545397929078e-05
Step 492600 Loss: 0.1681, lr 1.186545397929078e-05
Step 492700 Loss: 0.1684, lr 1.186545397929078e-05
Step 492800 Loss: 0.1688, lr 1.186545397929078e-05
Step 492900 Loss: 0.1694, lr 1.186545397929078e-05
Step 493000 Loss: 0.1691, lr 1.186545397929078e-05
Step 493100 Loss: 0.1685, lr 1.186545397929078e-05
Train Epoch: [113/100] Loss: 0.1686,lr 0.000012
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.13 s, total batches: 6755
Step 493200 Loss: 0.6799, lr 1.1083713102112294e-05
Step 493300 Loss: 0.4423, lr 1.1083713102112294e-05
Step 493400 Loss: 0.3803, lr 1.1083713102112294e-05
Step 493500 Loss: 0.3480, lr 1.1083713102112294e-05
Step 493600 Loss: 0.3271, lr 1.1083713102112294e-05
Step 493700 Loss: 0.3153, lr 1.1083713102112294e-05
Step 493800 Loss: 0.3000, lr 1.1083713102112294e-05
Step 493900 Loss: 0.2888, lr 1.1083713102112294e-05
Step 494000 Loss: 0.2817, lr 1.1083713102112294e-05
Step 494100 Loss: 0.2737, lr 1.1083713102112294e-05
Step 494200 Loss: 0.2664, lr 1.1083713102112294e-05
Step 494300 Loss: 0.2607, lr 1.1083713102112294e-05
Step 494400 Loss: 0.2552, lr 1.1083713102112294e-05
Step 494500 Loss: 0.2502, lr 1.1083713102112294e-05
Step 494600 Loss: 0.2451, lr 1.1083713102112294e-05
Step 494700 Loss: 0.2420, lr 1.1083713102112294e-05
Step 494800 Loss: 0.2389, lr 1.1083713102112294e-05
Step 494900 Loss: 0.2353, lr 1.1083713102112294e-05
Step 495000 Loss: 0.2323, lr 1.1083713102112294e-05
Step 495100 Loss: 0.2290, lr 1.1083713102112294e-05
Step 495200 Loss: 0.2249, lr 1.1083713102112294e-05
Step 495300 Loss: 0.2221, lr 1.1083713102112294e-05
Step 495400 Loss: 0.2193, lr 1.1083713102112294e-05
Step 495500 Loss: 0.2163, lr 1.1083713102112294e-05
Step 495600 Loss: 0.2137, lr 1.1083713102112294e-05
Step 495700 Loss: 0.2111, lr 1.1083713102112294e-05
Step 495800 Loss: 0.2084, lr 1.1083713102112294e-05
Step 495900 Loss: 0.2068, lr 1.1083713102112294e-05
Step 496000 Loss: 0.2045, lr 1.1083713102112294e-05
Step 496100 Loss: 0.2026, lr 1.1083713102112294e-05
Step 496200 Loss: 0.2007, lr 1.1083713102112294e-05
Step 496300 Loss: 0.1984, lr 1.1083713102112294e-05
Step 496400 Loss: 0.1962, lr 1.1083713102112294e-05
Step 496500 Loss: 0.1941, lr 1.1083713102112294e-05
Step 496600 Loss: 0.1923, lr 1.1083713102112294e-05
Step 496700 Loss: 0.1906, lr 1.1083713102112294e-05
Step 496800 Loss: 0.1889, lr 1.1083713102112294e-05
Step 496900 Loss: 0.1872, lr 1.1083713102112294e-05
Step 497000 Loss: 0.1858, lr 1.1083713102112294e-05
Step 497100 Loss: 0.1846, lr 1.1083713102112294e-05
Step 497200 Loss: 0.1835, lr 1.1083713102112294e-05
Step 497300 Loss: 0.1822, lr 1.1083713102112294e-05
Step 497400 Loss: 0.1814, lr 1.1083713102112294e-05
Step 497500 Loss: 0.1802, lr 1.1083713102112294e-05
Step 497600 Loss: 0.1792, lr 1.1083713102112294e-05
Step 497700 Loss: 0.1779, lr 1.1083713102112294e-05
Step 497800 Loss: 0.1768, lr 1.1083713102112294e-05
Step 497900 Loss: 0.1757, lr 1.1083713102112294e-05
Step 498000 Loss: 0.1751, lr 1.1083713102112294e-05
Step 498100 Loss: 0.1744, lr 1.1083713102112294e-05
Step 498200 Loss: 0.1736, lr 1.1083713102112294e-05
Step 498300 Loss: 0.1731, lr 1.1083713102112294e-05
Step 498400 Loss: 0.1722, lr 1.1083713102112294e-05
Step 498500 Loss: 0.1715, lr 1.1083713102112294e-05
Step 498600 Loss: 0.1707, lr 1.1083713102112294e-05
Step 498700 Loss: 0.1703, lr 1.1083713102112294e-05
Step 498800 Loss: 0.1696, lr 1.1083713102112294e-05
Step 498900 Loss: 0.1693, lr 1.1083713102112294e-05
Step 499000 Loss: 0.1690, lr 1.1083713102112294e-05
Step 499100 Loss: 0.1687, lr 1.1083713102112294e-05
Step 499200 Loss: 0.1686, lr 1.1083713102112294e-05
Step 499300 Loss: 0.1683, lr 1.1083713102112294e-05
Step 499400 Loss: 0.1684, lr 1.1083713102112294e-05
Step 499500 Loss: 0.1687, lr 1.1083713102112294e-05
Step 499600 Loss: 0.1694, lr 1.1083713102112294e-05
Step 499700 Loss: 0.1697, lr 1.1083713102112294e-05
Step 499800 Loss: 0.1691, lr 1.1083713102112294e-05
Step 499900 Loss: 0.1687, lr 1.1083713102112294e-05
Train Epoch: [114/100] Loss: 0.1689,lr 0.000011
Calling G2SDataset.batch()
Done, time:  2.22 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.45 s, total batches: 6756
Step 500000 Loss: 0.5366, lr 1.0323329981602685e-05
Step 500100 Loss: 0.4253, lr 1.0323329981602685e-05
Step 500200 Loss: 0.3753, lr 1.0323329981602685e-05
Step 500300 Loss: 0.3460, lr 1.0323329981602685e-05
Step 500400 Loss: 0.3294, lr 1.0323329981602685e-05
Step 500500 Loss: 0.3138, lr 1.0323329981602685e-05
Step 500600 Loss: 0.2989, lr 1.0323329981602685e-05
Step 500700 Loss: 0.2902, lr 1.0323329981602685e-05
Step 500800 Loss: 0.2810, lr 1.0323329981602685e-05
Step 500900 Loss: 0.2735, lr 1.0323329981602685e-05
Step 501000 Loss: 0.2667, lr 1.0323329981602685e-05
Step 501100 Loss: 0.2607, lr 1.0323329981602685e-05
Step 501200 Loss: 0.2554, lr 1.0323329981602685e-05
Step 501300 Loss: 0.2496, lr 1.0323329981602685e-05
Step 501400 Loss: 0.2456, lr 1.0323329981602685e-05
Step 501500 Loss: 0.2422, lr 1.0323329981602685e-05
Step 501600 Loss: 0.2390, lr 1.0323329981602685e-05
Step 501700 Loss: 0.2356, lr 1.0323329981602685e-05
Step 501800 Loss: 0.2324, lr 1.0323329981602685e-05
Step 501900 Loss: 0.2285, lr 1.0323329981602685e-05
Step 502000 Loss: 0.2250, lr 1.0323329981602685e-05
Step 502100 Loss: 0.2221, lr 1.0323329981602685e-05
Step 502200 Loss: 0.2195, lr 1.0323329981602685e-05
Step 502300 Loss: 0.2167, lr 1.0323329981602685e-05
Step 502400 Loss: 0.2137, lr 1.0323329981602685e-05
Step 502500 Loss: 0.2112, lr 1.0323329981602685e-05
Step 502600 Loss: 0.2089, lr 1.0323329981602685e-05
Step 502700 Loss: 0.2068, lr 1.0323329981602685e-05
Step 502800 Loss: 0.2047, lr 1.0323329981602685e-05
Step 502900 Loss: 0.2028, lr 1.0323329981602685e-05
Step 503000 Loss: 0.2006, lr 1.0323329981602685e-05
Step 503100 Loss: 0.1983, lr 1.0323329981602685e-05
Step 503200 Loss: 0.1964, lr 1.0323329981602685e-05
Step 503300 Loss: 0.1943, lr 1.0323329981602685e-05
Step 503400 Loss: 0.1925, lr 1.0323329981602685e-05
Step 503500 Loss: 0.1905, lr 1.0323329981602685e-05
Step 503600 Loss: 0.1888, lr 1.0323329981602685e-05
Step 503700 Loss: 0.1874, lr 1.0323329981602685e-05
Step 503800 Loss: 0.1859, lr 1.0323329981602685e-05
Step 503900 Loss: 0.1848, lr 1.0323329981602685e-05
Step 504000 Loss: 0.1836, lr 1.0323329981602685e-05
Step 504100 Loss: 0.1825, lr 1.0323329981602685e-05
Step 504200 Loss: 0.1814, lr 1.0323329981602685e-05
Step 504300 Loss: 0.1803, lr 1.0323329981602685e-05
Step 504400 Loss: 0.1792, lr 1.0323329981602685e-05
Step 504500 Loss: 0.1780, lr 1.0323329981602685e-05
Step 504600 Loss: 0.1768, lr 1.0323329981602685e-05
Step 504700 Loss: 0.1761, lr 1.0323329981602685e-05
Step 504800 Loss: 0.1755, lr 1.0323329981602685e-05
Step 504900 Loss: 0.1747, lr 1.0323329981602685e-05
Step 505000 Loss: 0.1739, lr 1.0323329981602685e-05
Step 505100 Loss: 0.1734, lr 1.0323329981602685e-05
Step 505200 Loss: 0.1726, lr 1.0323329981602685e-05
Step 505300 Loss: 0.1718, lr 1.0323329981602685e-05
Step 505400 Loss: 0.1712, lr 1.0323329981602685e-05
Step 505500 Loss: 0.1706, lr 1.0323329981602685e-05
Step 505600 Loss: 0.1701, lr 1.0323329981602685e-05
Step 505700 Loss: 0.1697, lr 1.0323329981602685e-05
Step 505800 Loss: 0.1693, lr 1.0323329981602685e-05
Step 505900 Loss: 0.1691, lr 1.0323329981602685e-05
Step 506000 Loss: 0.1687, lr 1.0323329981602685e-05
Step 506100 Loss: 0.1687, lr 1.0323329981602685e-05
Step 506200 Loss: 0.1690, lr 1.0323329981602685e-05
Step 506300 Loss: 0.1693, lr 1.0323329981602685e-05
Step 506400 Loss: 0.1697, lr 1.0323329981602685e-05
Step 506500 Loss: 0.1695, lr 1.0323329981602685e-05
Step 506600 Loss: 0.1690, lr 1.0323329981602685e-05
Train Epoch: [115/100] Loss: 0.1690,lr 0.000010
Model Saving at epoch 115
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6755
Step 506700 Loss: 0.7941, lr 9.585055024099757e-06
Step 506800 Loss: 0.4744, lr 9.585055024099757e-06
Step 506900 Loss: 0.4017, lr 9.585055024099757e-06
Step 507000 Loss: 0.3610, lr 9.585055024099757e-06
Step 507100 Loss: 0.3400, lr 9.585055024099757e-06
Step 507200 Loss: 0.3250, lr 9.585055024099757e-06
Step 507300 Loss: 0.3084, lr 9.585055024099757e-06
Step 507400 Loss: 0.2962, lr 9.585055024099757e-06
Step 507500 Loss: 0.2882, lr 9.585055024099757e-06
Step 507600 Loss: 0.2788, lr 9.585055024099757e-06
Step 507700 Loss: 0.2718, lr 9.585055024099757e-06
Step 507800 Loss: 0.2659, lr 9.585055024099757e-06
Step 507900 Loss: 0.2594, lr 9.585055024099757e-06
Step 508000 Loss: 0.2542, lr 9.585055024099757e-06
Step 508100 Loss: 0.2490, lr 9.585055024099757e-06
Step 508200 Loss: 0.2456, lr 9.585055024099757e-06
Step 508300 Loss: 0.2419, lr 9.585055024099757e-06
Step 508400 Loss: 0.2388, lr 9.585055024099757e-06
Step 508500 Loss: 0.2355, lr 9.585055024099757e-06
Step 508600 Loss: 0.2320, lr 9.585055024099757e-06
Step 508700 Loss: 0.2283, lr 9.585055024099757e-06
Step 508800 Loss: 0.2250, lr 9.585055024099757e-06
Step 508900 Loss: 0.2221, lr 9.585055024099757e-06
Step 509000 Loss: 0.2191, lr 9.585055024099757e-06
Step 509100 Loss: 0.2164, lr 9.585055024099757e-06
Step 509200 Loss: 0.2136, lr 9.585055024099757e-06
Step 509300 Loss: 0.2109, lr 9.585055024099757e-06
Step 509400 Loss: 0.2089, lr 9.585055024099757e-06
Step 509500 Loss: 0.2069, lr 9.585055024099757e-06
Step 509600 Loss: 0.2048, lr 9.585055024099757e-06
Step 509700 Loss: 0.2028, lr 9.585055024099757e-06
Step 509800 Loss: 0.2004, lr 9.585055024099757e-06
Step 509900 Loss: 0.1982, lr 9.585055024099757e-06
Step 510000 Loss: 0.1961, lr 9.585055024099757e-06
Step 510100 Loss: 0.1942, lr 9.585055024099757e-06
Step 510200 Loss: 0.1922, lr 9.585055024099757e-06
Step 510300 Loss: 0.1906, lr 9.585055024099757e-06
Step 510400 Loss: 0.1889, lr 9.585055024099757e-06
Step 510500 Loss: 0.1875, lr 9.585055024099757e-06
Step 510600 Loss: 0.1862, lr 9.585055024099757e-06
Step 510700 Loss: 0.1851, lr 9.585055024099757e-06
Step 510800 Loss: 0.1839, lr 9.585055024099757e-06
Step 510900 Loss: 0.1828, lr 9.585055024099757e-06
Step 511000 Loss: 0.1817, lr 9.585055024099757e-06
Step 511100 Loss: 0.1805, lr 9.585055024099757e-06
Step 511200 Loss: 0.1793, lr 9.585055024099757e-06
Step 511300 Loss: 0.1781, lr 9.585055024099757e-06
Step 511400 Loss: 0.1770, lr 9.585055024099757e-06
Step 511500 Loss: 0.1764, lr 9.585055024099757e-06
Step 511600 Loss: 0.1758, lr 9.585055024099757e-06
Step 511700 Loss: 0.1750, lr 9.585055024099757e-06
Step 511800 Loss: 0.1746, lr 9.585055024099757e-06
Step 511900 Loss: 0.1737, lr 9.585055024099757e-06
Step 512000 Loss: 0.1730, lr 9.585055024099757e-06
Step 512100 Loss: 0.1722, lr 9.585055024099757e-06
Step 512200 Loss: 0.1715, lr 9.585055024099757e-06
Step 512300 Loss: 0.1710, lr 9.585055024099757e-06
Step 512400 Loss: 0.1704, lr 9.585055024099757e-06
Step 512500 Loss: 0.1701, lr 9.585055024099757e-06
Step 512600 Loss: 0.1699, lr 9.585055024099757e-06
Step 512700 Loss: 0.1698, lr 9.585055024099757e-06
Step 512800 Loss: 0.1695, lr 9.585055024099757e-06
Step 512900 Loss: 0.1696, lr 9.585055024099757e-06
Step 513000 Loss: 0.1699, lr 9.585055024099757e-06
Step 513100 Loss: 0.1705, lr 9.585055024099757e-06
Step 513200 Loss: 0.1708, lr 9.585055024099757e-06
Step 513300 Loss: 0.1702, lr 9.585055024099757e-06
Step 513400 Loss: 0.1698, lr 9.585055024099757e-06
Train Epoch: [116/100] Loss: 0.1699,lr 0.000010
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6754
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6754
Step 513500 Loss: 0.5655, lr 8.869616817853536e-06
Step 513600 Loss: 0.4442, lr 8.869616817853536e-06
Step 513700 Loss: 0.3889, lr 8.869616817853536e-06
Step 513800 Loss: 0.3565, lr 8.869616817853536e-06
Step 513900 Loss: 0.3376, lr 8.869616817853536e-06
Step 514000 Loss: 0.3218, lr 8.869616817853536e-06
Step 514100 Loss: 0.3055, lr 8.869616817853536e-06
Step 514200 Loss: 0.2951, lr 8.869616817853536e-06
Step 514300 Loss: 0.2860, lr 8.869616817853536e-06
Step 514400 Loss: 0.2777, lr 8.869616817853536e-06
Step 514500 Loss: 0.2705, lr 8.869616817853536e-06
Step 514600 Loss: 0.2643, lr 8.869616817853536e-06
Step 514700 Loss: 0.2588, lr 8.869616817853536e-06
Step 514800 Loss: 0.2530, lr 8.869616817853536e-06
Step 514900 Loss: 0.2484, lr 8.869616817853536e-06
Step 515000 Loss: 0.2449, lr 8.869616817853536e-06
Step 515100 Loss: 0.2413, lr 8.869616817853536e-06
Step 515200 Loss: 0.2379, lr 8.869616817853536e-06
Step 515300 Loss: 0.2345, lr 8.869616817853536e-06
Step 515400 Loss: 0.2307, lr 8.869616817853536e-06
Step 515500 Loss: 0.2267, lr 8.869616817853536e-06
Step 515600 Loss: 0.2238, lr 8.869616817853536e-06
Step 515700 Loss: 0.2209, lr 8.869616817853536e-06
Step 515800 Loss: 0.2181, lr 8.869616817853536e-06
Step 515900 Loss: 0.2151, lr 8.869616817853536e-06
Step 516000 Loss: 0.2126, lr 8.869616817853536e-06
Step 516100 Loss: 0.2101, lr 8.869616817853536e-06
Step 516200 Loss: 0.2081, lr 8.869616817853536e-06
Step 516300 Loss: 0.2059, lr 8.869616817853536e-06
Step 516400 Loss: 0.2038, lr 8.869616817853536e-06
Step 516500 Loss: 0.2018, lr 8.869616817853536e-06
Step 516600 Loss: 0.1994, lr 8.869616817853536e-06
Step 516700 Loss: 0.1976, lr 8.869616817853536e-06
Step 516800 Loss: 0.1953, lr 8.869616817853536e-06
Step 516900 Loss: 0.1935, lr 8.869616817853536e-06
Step 517000 Loss: 0.1917, lr 8.869616817853536e-06
Step 517100 Loss: 0.1901, lr 8.869616817853536e-06
Step 517200 Loss: 0.1886, lr 8.869616817853536e-06
Step 517300 Loss: 0.1870, lr 8.869616817853536e-06
Step 517400 Loss: 0.1859, lr 8.869616817853536e-06
Step 517500 Loss: 0.1848, lr 8.869616817853536e-06
Step 517600 Loss: 0.1836, lr 8.869616817853536e-06
Step 517700 Loss: 0.1825, lr 8.869616817853536e-06
Step 517800 Loss: 0.1814, lr 8.869616817853536e-06
Step 517900 Loss: 0.1803, lr 8.869616817853536e-06
Step 518000 Loss: 0.1789, lr 8.869616817853536e-06
Step 518100 Loss: 0.1777, lr 8.869616817853536e-06
Step 518200 Loss: 0.1769, lr 8.869616817853536e-06
Step 518300 Loss: 0.1763, lr 8.869616817853536e-06
Step 518400 Loss: 0.1755, lr 8.869616817853536e-06
Step 518500 Loss: 0.1748, lr 8.869616817853536e-06
Step 518600 Loss: 0.1743, lr 8.869616817853536e-06
Step 518700 Loss: 0.1733, lr 8.869616817853536e-06
Step 518800 Loss: 0.1726, lr 8.869616817853536e-06
Step 518900 Loss: 0.1719, lr 8.869616817853536e-06
Step 519000 Loss: 0.1713, lr 8.869616817853536e-06
Step 519100 Loss: 0.1707, lr 8.869616817853536e-06
Step 519200 Loss: 0.1703, lr 8.869616817853536e-06
Step 519300 Loss: 0.1700, lr 8.869616817853536e-06
Step 519400 Loss: 0.1697, lr 8.869616817853536e-06
Step 519500 Loss: 0.1696, lr 8.869616817853536e-06
Step 519600 Loss: 0.1693, lr 8.869616817853536e-06
Step 519700 Loss: 0.1697, lr 8.869616817853536e-06
Step 519800 Loss: 0.1699, lr 8.869616817853536e-06
Step 519900 Loss: 0.1704, lr 8.869616817853536e-06
Step 520000 Loss: 0.1703, lr 8.869616817853536e-06
Step 520100 Loss: 0.1697, lr 8.869616817853536e-06
Train Epoch: [117/100] Loss: 0.1697,lr 0.000009
Calling G2SDataset.batch()
Done, time:  2.33 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6756
Step 520200 Loss: 0.8681, lr 8.177721413997674e-06
Step 520300 Loss: 0.4851, lr 8.177721413997674e-06
Step 520400 Loss: 0.4132, lr 8.177721413997674e-06
Step 520500 Loss: 0.3707, lr 8.177721413997674e-06
Step 520600 Loss: 0.3484, lr 8.177721413997674e-06
Step 520700 Loss: 0.3318, lr 8.177721413997674e-06
Step 520800 Loss: 0.3140, lr 8.177721413997674e-06
Step 520900 Loss: 0.3010, lr 8.177721413997674e-06
Step 521000 Loss: 0.2930, lr 8.177721413997674e-06
Step 521100 Loss: 0.2833, lr 8.177721413997674e-06
Step 521200 Loss: 0.2755, lr 8.177721413997674e-06
Step 521300 Loss: 0.2692, lr 8.177721413997674e-06
Step 521400 Loss: 0.2626, lr 8.177721413997674e-06
Step 521500 Loss: 0.2568, lr 8.177721413997674e-06
Step 521600 Loss: 0.2515, lr 8.177721413997674e-06
Step 521700 Loss: 0.2478, lr 8.177721413997674e-06
Step 521800 Loss: 0.2438, lr 8.177721413997674e-06
Step 521900 Loss: 0.2406, lr 8.177721413997674e-06
Step 522000 Loss: 0.2373, lr 8.177721413997674e-06
Step 522100 Loss: 0.2340, lr 8.177721413997674e-06
Step 522200 Loss: 0.2303, lr 8.177721413997674e-06
Step 522300 Loss: 0.2266, lr 8.177721413997674e-06
Step 522400 Loss: 0.2237, lr 8.177721413997674e-06
Step 522500 Loss: 0.2207, lr 8.177721413997674e-06
Step 522600 Loss: 0.2182, lr 8.177721413997674e-06
Step 522700 Loss: 0.2152, lr 8.177721413997674e-06
Step 522800 Loss: 0.2126, lr 8.177721413997674e-06
Step 522900 Loss: 0.2106, lr 8.177721413997674e-06
Step 523000 Loss: 0.2083, lr 8.177721413997674e-06
Step 523100 Loss: 0.2060, lr 8.177721413997674e-06
Step 523200 Loss: 0.2039, lr 8.177721413997674e-06
Step 523300 Loss: 0.2017, lr 8.177721413997674e-06
Step 523400 Loss: 0.1996, lr 8.177721413997674e-06
Step 523500 Loss: 0.1974, lr 8.177721413997674e-06
Step 523600 Loss: 0.1956, lr 8.177721413997674e-06
Step 523700 Loss: 0.1938, lr 8.177721413997674e-06
Step 523800 Loss: 0.1921, lr 8.177721413997674e-06
Step 523900 Loss: 0.1904, lr 8.177721413997674e-06
Step 524000 Loss: 0.1889, lr 8.177721413997674e-06
Step 524100 Loss: 0.1876, lr 8.177721413997674e-06
Step 524200 Loss: 0.1864, lr 8.177721413997674e-06
Step 524300 Loss: 0.1851, lr 8.177721413997674e-06
Step 524400 Loss: 0.1841, lr 8.177721413997674e-06
Step 524500 Loss: 0.1830, lr 8.177721413997674e-06
Step 524600 Loss: 0.1817, lr 8.177721413997674e-06
Step 524700 Loss: 0.1805, lr 8.177721413997674e-06
Step 524800 Loss: 0.1792, lr 8.177721413997674e-06
Step 524900 Loss: 0.1782, lr 8.177721413997674e-06
Step 525000 Loss: 0.1774, lr 8.177721413997674e-06
Step 525100 Loss: 0.1769, lr 8.177721413997674e-06
Step 525200 Loss: 0.1760, lr 8.177721413997674e-06
Step 525300 Loss: 0.1753, lr 8.177721413997674e-06
Step 525400 Loss: 0.1745, lr 8.177721413997674e-06
Step 525500 Loss: 0.1737, lr 8.177721413997674e-06
Step 525600 Loss: 0.1729, lr 8.177721413997674e-06
Step 525700 Loss: 0.1724, lr 8.177721413997674e-06
Step 525800 Loss: 0.1718, lr 8.177721413997674e-06
Step 525900 Loss: 0.1711, lr 8.177721413997674e-06
Step 526000 Loss: 0.1708, lr 8.177721413997674e-06
Step 526100 Loss: 0.1704, lr 8.177721413997674e-06
Step 526200 Loss: 0.1705, lr 8.177721413997674e-06
Step 526300 Loss: 0.1701, lr 8.177721413997674e-06
Step 526400 Loss: 0.1700, lr 8.177721413997674e-06
Step 526500 Loss: 0.1705, lr 8.177721413997674e-06
Step 526600 Loss: 0.1710, lr 8.177721413997674e-06
Step 526700 Loss: 0.1715, lr 8.177721413997674e-06
Step 526800 Loss: 0.1710, lr 8.177721413997674e-06
Step 526900 Loss: 0.1706, lr 8.177721413997674e-06
Train Epoch: [118/100] Loss: 0.1707,lr 0.000008
Calling G2SDataset.batch()
Done, time:  2.11 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6756
Step 527000 Loss: 0.6148, lr 7.510051629762259e-06
Step 527100 Loss: 0.4613, lr 7.510051629762259e-06
Step 527200 Loss: 0.4014, lr 7.510051629762259e-06
Step 527300 Loss: 0.3670, lr 7.510051629762259e-06
Step 527400 Loss: 0.3460, lr 7.510051629762259e-06
Step 527500 Loss: 0.3289, lr 7.510051629762259e-06
Step 527600 Loss: 0.3117, lr 7.510051629762259e-06
Step 527700 Loss: 0.3003, lr 7.510051629762259e-06
Step 527800 Loss: 0.2911, lr 7.510051629762259e-06
Step 527900 Loss: 0.2822, lr 7.510051629762259e-06
Step 528000 Loss: 0.2747, lr 7.510051629762259e-06
Step 528100 Loss: 0.2685, lr 7.510051629762259e-06
Step 528200 Loss: 0.2628, lr 7.510051629762259e-06
Step 528300 Loss: 0.2566, lr 7.510051629762259e-06
Step 528400 Loss: 0.2516, lr 7.510051629762259e-06
Step 528500 Loss: 0.2481, lr 7.510051629762259e-06
Step 528600 Loss: 0.2445, lr 7.510051629762259e-06
Step 528700 Loss: 0.2408, lr 7.510051629762259e-06
Step 528800 Loss: 0.2376, lr 7.510051629762259e-06
Step 528900 Loss: 0.2336, lr 7.510051629762259e-06
Step 529000 Loss: 0.2297, lr 7.510051629762259e-06
Step 529100 Loss: 0.2267, lr 7.510051629762259e-06
Step 529200 Loss: 0.2238, lr 7.510051629762259e-06
Step 529300 Loss: 0.2206, lr 7.510051629762259e-06
Step 529400 Loss: 0.2178, lr 7.510051629762259e-06
Step 529500 Loss: 0.2152, lr 7.510051629762259e-06
Step 529600 Loss: 0.2125, lr 7.510051629762259e-06
Step 529700 Loss: 0.2105, lr 7.510051629762259e-06
Step 529800 Loss: 0.2082, lr 7.510051629762259e-06
Step 529900 Loss: 0.2063, lr 7.510051629762259e-06
Step 530000 Loss: 0.2041, lr 7.510051629762259e-06
Step 530100 Loss: 0.2017, lr 7.510051629762259e-06
Step 530200 Loss: 0.1998, lr 7.510051629762259e-06
Step 530300 Loss: 0.1974, lr 7.510051629762259e-06
Step 530400 Loss: 0.1955, lr 7.510051629762259e-06
Step 530500 Loss: 0.1936, lr 7.510051629762259e-06
Step 530600 Loss: 0.1921, lr 7.510051629762259e-06
Step 530700 Loss: 0.1904, lr 7.510051629762259e-06
Step 530800 Loss: 0.1888, lr 7.510051629762259e-06
Step 530900 Loss: 0.1875, lr 7.510051629762259e-06
Step 531000 Loss: 0.1863, lr 7.510051629762259e-06
Step 531100 Loss: 0.1851, lr 7.510051629762259e-06
Step 531200 Loss: 0.1841, lr 7.510051629762259e-06
Step 531300 Loss: 0.1828, lr 7.510051629762259e-06
Step 531400 Loss: 0.1817, lr 7.510051629762259e-06
Step 531500 Loss: 0.1803, lr 7.510051629762259e-06
Step 531600 Loss: 0.1792, lr 7.510051629762259e-06
Step 531700 Loss: 0.1783, lr 7.510051629762259e-06
Step 531800 Loss: 0.1777, lr 7.510051629762259e-06
Step 531900 Loss: 0.1769, lr 7.510051629762259e-06
Step 532000 Loss: 0.1760, lr 7.510051629762259e-06
Step 532100 Loss: 0.1755, lr 7.510051629762259e-06
Step 532200 Loss: 0.1745, lr 7.510051629762259e-06
Step 532300 Loss: 0.1738, lr 7.510051629762259e-06
Step 532400 Loss: 0.1731, lr 7.510051629762259e-06
Step 532500 Loss: 0.1724, lr 7.510051629762259e-06
Step 532600 Loss: 0.1718, lr 7.510051629762259e-06
Step 532700 Loss: 0.1714, lr 7.510051629762259e-06
Step 532800 Loss: 0.1710, lr 7.510051629762259e-06
Step 532900 Loss: 0.1707, lr 7.510051629762259e-06
Step 533000 Loss: 0.1705, lr 7.510051629762259e-06
Step 533100 Loss: 0.1703, lr 7.510051629762259e-06
Step 533200 Loss: 0.1707, lr 7.510051629762259e-06
Step 533300 Loss: 0.1709, lr 7.510051629762259e-06
Step 533400 Loss: 0.1717, lr 7.510051629762259e-06
Step 533500 Loss: 0.1719, lr 7.510051629762259e-06
Step 533600 Loss: 0.1714, lr 7.510051629762259e-06
Train Epoch: [119/100] Loss: 0.1719,lr 0.000008
Calling G2SDataset.batch()
Done, time:  2.25 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.92 s, total batches: 6755
Step 533700 Loss: 1.0797, lr 6.867266374615558e-06
Step 533800 Loss: 0.4930, lr 6.867266374615558e-06
Step 533900 Loss: 0.4154, lr 6.867266374615558e-06
Step 534000 Loss: 0.3747, lr 6.867266374615558e-06
Step 534100 Loss: 0.3520, lr 6.867266374615558e-06
Step 534200 Loss: 0.3347, lr 6.867266374615558e-06
Step 534300 Loss: 0.3169, lr 6.867266374615558e-06
Step 534400 Loss: 0.3044, lr 6.867266374615558e-06
Step 534500 Loss: 0.2950, lr 6.867266374615558e-06
Step 534600 Loss: 0.2860, lr 6.867266374615558e-06
Step 534700 Loss: 0.2777, lr 6.867266374615558e-06
Step 534800 Loss: 0.2711, lr 6.867266374615558e-06
Step 534900 Loss: 0.2644, lr 6.867266374615558e-06
Step 535000 Loss: 0.2590, lr 6.867266374615558e-06
Step 535100 Loss: 0.2530, lr 6.867266374615558e-06
Step 535200 Loss: 0.2494, lr 6.867266374615558e-06
Step 535300 Loss: 0.2452, lr 6.867266374615558e-06
Step 535400 Loss: 0.2421, lr 6.867266374615558e-06
Step 535500 Loss: 0.2386, lr 6.867266374615558e-06
Step 535600 Loss: 0.2353, lr 6.867266374615558e-06
Step 535700 Loss: 0.2316, lr 6.867266374615558e-06
Step 535800 Loss: 0.2279, lr 6.867266374615558e-06
Step 535900 Loss: 0.2251, lr 6.867266374615558e-06
Step 536000 Loss: 0.2223, lr 6.867266374615558e-06
Step 536100 Loss: 0.2196, lr 6.867266374615558e-06
Step 536200 Loss: 0.2166, lr 6.867266374615558e-06
Step 536300 Loss: 0.2142, lr 6.867266374615558e-06
Step 536400 Loss: 0.2118, lr 6.867266374615558e-06
Step 536500 Loss: 0.2098, lr 6.867266374615558e-06
Step 536600 Loss: 0.2076, lr 6.867266374615558e-06
Step 536700 Loss: 0.2056, lr 6.867266374615558e-06
Step 536800 Loss: 0.2032, lr 6.867266374615558e-06
Step 536900 Loss: 0.2010, lr 6.867266374615558e-06
Step 537000 Loss: 0.1988, lr 6.867266374615558e-06
Step 537100 Loss: 0.1969, lr 6.867266374615558e-06
Step 537200 Loss: 0.1951, lr 6.867266374615558e-06
Step 537300 Loss: 0.1932, lr 6.867266374615558e-06
Step 537400 Loss: 0.1915, lr 6.867266374615558e-06
Step 537500 Loss: 0.1901, lr 6.867266374615558e-06
Step 537600 Loss: 0.1886, lr 6.867266374615558e-06
Step 537700 Loss: 0.1875, lr 6.867266374615558e-06
Step 537800 Loss: 0.1860, lr 6.867266374615558e-06
Step 537900 Loss: 0.1850, lr 6.867266374615558e-06
Step 538000 Loss: 0.1840, lr 6.867266374615558e-06
Step 538100 Loss: 0.1827, lr 6.867266374615558e-06
Step 538200 Loss: 0.1816, lr 6.867266374615558e-06
Step 538300 Loss: 0.1802, lr 6.867266374615558e-06
Step 538400 Loss: 0.1791, lr 6.867266374615558e-06
Step 538500 Loss: 0.1783, lr 6.867266374615558e-06
Step 538600 Loss: 0.1776, lr 6.867266374615558e-06
Step 538700 Loss: 0.1769, lr 6.867266374615558e-06
Step 538800 Loss: 0.1762, lr 6.867266374615558e-06
Step 538900 Loss: 0.1754, lr 6.867266374615558e-06
Step 539000 Loss: 0.1746, lr 6.867266374615558e-06
Step 539100 Loss: 0.1737, lr 6.867266374615558e-06
Step 539200 Loss: 0.1733, lr 6.867266374615558e-06
Step 539300 Loss: 0.1726, lr 6.867266374615558e-06
Step 539400 Loss: 0.1722, lr 6.867266374615558e-06
Step 539500 Loss: 0.1718, lr 6.867266374615558e-06
Step 539600 Loss: 0.1714, lr 6.867266374615558e-06
Step 539700 Loss: 0.1713, lr 6.867266374615558e-06
Step 539800 Loss: 0.1710, lr 6.867266374615558e-06
Step 539900 Loss: 0.1710, lr 6.867266374615558e-06
Step 540000 Loss: 0.1712, lr 6.867266374615558e-06
Step 540100 Loss: 0.1716, lr 6.867266374615558e-06
Step 540200 Loss: 0.1721, lr 6.867266374615558e-06
Step 540300 Loss: 0.1719, lr 6.867266374615558e-06
Step 540400 Loss: 0.1713, lr 6.867266374615558e-06
Train Epoch: [120/100] Loss: 0.1714,lr 0.000007
Model Saving at epoch 120
Calling G2SDataset.batch()
Done, time:  2.29 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Step 540500 Loss: 0.6411, lr 6.250000000000015e-06
Step 540600 Loss: 0.4644, lr 6.250000000000015e-06
Step 540700 Loss: 0.4038, lr 6.250000000000015e-06
Step 540800 Loss: 0.3711, lr 6.250000000000015e-06
Step 540900 Loss: 0.3515, lr 6.250000000000015e-06
Step 541000 Loss: 0.3352, lr 6.250000000000015e-06
Step 541100 Loss: 0.3175, lr 6.250000000000015e-06
Step 541200 Loss: 0.3055, lr 6.250000000000015e-06
Step 541300 Loss: 0.2964, lr 6.250000000000015e-06
Step 541400 Loss: 0.2874, lr 6.250000000000015e-06
Step 541500 Loss: 0.2792, lr 6.250000000000015e-06
Step 541600 Loss: 0.2727, lr 6.250000000000015e-06
Step 541700 Loss: 0.2663, lr 6.250000000000015e-06
Step 541800 Loss: 0.2604, lr 6.250000000000015e-06
Step 541900 Loss: 0.2547, lr 6.250000000000015e-06
Step 542000 Loss: 0.2513, lr 6.250000000000015e-06
Step 542100 Loss: 0.2474, lr 6.250000000000015e-06
Step 542200 Loss: 0.2436, lr 6.250000000000015e-06
Step 542300 Loss: 0.2401, lr 6.250000000000015e-06
Step 542400 Loss: 0.2365, lr 6.250000000000015e-06
Step 542500 Loss: 0.2327, lr 6.250000000000015e-06
Step 542600 Loss: 0.2295, lr 6.250000000000015e-06
Step 542700 Loss: 0.2265, lr 6.250000000000015e-06
Step 542800 Loss: 0.2234, lr 6.250000000000015e-06
Step 542900 Loss: 0.2205, lr 6.250000000000015e-06
Step 543000 Loss: 0.2177, lr 6.250000000000015e-06
Step 543100 Loss: 0.2150, lr 6.250000000000015e-06
Step 543200 Loss: 0.2130, lr 6.250000000000015e-06
Step 543300 Loss: 0.2106, lr 6.250000000000015e-06
Step 543400 Loss: 0.2086, lr 6.250000000000015e-06
Step 543500 Loss: 0.2065, lr 6.250000000000015e-06
Step 543600 Loss: 0.2042, lr 6.250000000000015e-06
Step 543700 Loss: 0.2022, lr 6.250000000000015e-06
Step 543800 Loss: 0.1998, lr 6.250000000000015e-06
Step 543900 Loss: 0.1979, lr 6.250000000000015e-06
Step 544000 Loss: 0.1960, lr 6.250000000000015e-06
Step 544100 Loss: 0.1942, lr 6.250000000000015e-06
Step 544200 Loss: 0.1925, lr 6.250000000000015e-06
Step 544300 Loss: 0.1909, lr 6.250000000000015e-06
Step 544400 Loss: 0.1896, lr 6.250000000000015e-06
Step 544500 Loss: 0.1883, lr 6.250000000000015e-06
Step 544600 Loss: 0.1870, lr 6.250000000000015e-06
Step 544700 Loss: 0.1860, lr 6.250000000000015e-06
Step 544800 Loss: 0.1847, lr 6.250000000000015e-06
Step 544900 Loss: 0.1836, lr 6.250000000000015e-06
Step 545000 Loss: 0.1822, lr 6.250000000000015e-06
Step 545100 Loss: 0.1810, lr 6.250000000000015e-06
Step 545200 Loss: 0.1800, lr 6.250000000000015e-06
Step 545300 Loss: 0.1793, lr 6.250000000000015e-06
Step 545400 Loss: 0.1785, lr 6.250000000000015e-06
Step 545500 Loss: 0.1775, lr 6.250000000000015e-06
Step 545600 Loss: 0.1770, lr 6.250000000000015e-06
Step 545700 Loss: 0.1760, lr 6.250000000000015e-06
Step 545800 Loss: 0.1752, lr 6.250000000000015e-06
Step 545900 Loss: 0.1744, lr 6.250000000000015e-06
Step 546000 Loss: 0.1738, lr 6.250000000000015e-06
Step 546100 Loss: 0.1731, lr 6.250000000000015e-06
Step 546200 Loss: 0.1728, lr 6.250000000000015e-06
Step 546300 Loss: 0.1724, lr 6.250000000000015e-06
Step 546400 Loss: 0.1721, lr 6.250000000000015e-06
Step 546500 Loss: 0.1719, lr 6.250000000000015e-06
Step 546600 Loss: 0.1717, lr 6.250000000000015e-06
Step 546700 Loss: 0.1719, lr 6.250000000000015e-06
Step 546800 Loss: 0.1721, lr 6.250000000000015e-06
Step 546900 Loss: 0.1727, lr 6.250000000000015e-06
Step 547000 Loss: 0.1728, lr 6.250000000000015e-06
Step 547100 Loss: 0.1722, lr 6.250000000000015e-06
Step 547200 Loss: 0.1718, lr 6.250000000000015e-06
Train Epoch: [121/100] Loss: 0.1721,lr 0.000006
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.42 s, total batches: 6756
Step 547300 Loss: 0.5189, lr 5.65886167330389e-06
Step 547400 Loss: 0.4328, lr 5.65886167330389e-06
Step 547500 Loss: 0.3881, lr 5.65886167330389e-06
Step 547600 Loss: 0.3631, lr 5.65886167330389e-06
Step 547700 Loss: 0.3456, lr 5.65886167330389e-06
Step 547800 Loss: 0.3272, lr 5.65886167330389e-06
Step 547900 Loss: 0.3131, lr 5.65886167330389e-06
Step 548000 Loss: 0.3031, lr 5.65886167330389e-06
Step 548100 Loss: 0.2924, lr 5.65886167330389e-06
Step 548200 Loss: 0.2841, lr 5.65886167330389e-06
Step 548300 Loss: 0.2770, lr 5.65886167330389e-06
Step 548400 Loss: 0.2702, lr 5.65886167330389e-06
Step 548500 Loss: 0.2643, lr 5.65886167330389e-06
Step 548600 Loss: 0.2578, lr 5.65886167330389e-06
Step 548700 Loss: 0.2536, lr 5.65886167330389e-06
Step 548800 Loss: 0.2497, lr 5.65886167330389e-06
Step 548900 Loss: 0.2460, lr 5.65886167330389e-06
Step 549000 Loss: 0.2425, lr 5.65886167330389e-06
Step 549100 Loss: 0.2391, lr 5.65886167330389e-06
Step 549200 Loss: 0.2351, lr 5.65886167330389e-06
Step 549300 Loss: 0.2312, lr 5.65886167330389e-06
Step 549400 Loss: 0.2282, lr 5.65886167330389e-06
Step 549500 Loss: 0.2252, lr 5.65886167330389e-06
Step 549600 Loss: 0.2224, lr 5.65886167330389e-06
Step 549700 Loss: 0.2193, lr 5.65886167330389e-06
Step 549800 Loss: 0.2165, lr 5.65886167330389e-06
Step 549900 Loss: 0.2141, lr 5.65886167330389e-06
Step 550000 Loss: 0.2121, lr 5.65886167330389e-06
Step 550100 Loss: 0.2098, lr 5.65886167330389e-06
Step 550200 Loss: 0.2078, lr 5.65886167330389e-06
Step 550300 Loss: 0.2054, lr 5.65886167330389e-06
Step 550400 Loss: 0.2032, lr 5.65886167330389e-06
Step 550500 Loss: 0.2010, lr 5.65886167330389e-06
Step 550600 Loss: 0.1989, lr 5.65886167330389e-06
Step 550700 Loss: 0.1970, lr 5.65886167330389e-06
Step 550800 Loss: 0.1951, lr 5.65886167330389e-06
Step 550900 Loss: 0.1933, lr 5.65886167330389e-06
Step 551000 Loss: 0.1918, lr 5.65886167330389e-06
Step 551100 Loss: 0.1901, lr 5.65886167330389e-06
Step 551200 Loss: 0.1891, lr 5.65886167330389e-06
Step 551300 Loss: 0.1877, lr 5.65886167330389e-06
Step 551400 Loss: 0.1866, lr 5.65886167330389e-06
Step 551500 Loss: 0.1854, lr 5.65886167330389e-06
Step 551600 Loss: 0.1841, lr 5.65886167330389e-06
Step 551700 Loss: 0.1830, lr 5.65886167330389e-06
Step 551800 Loss: 0.1816, lr 5.65886167330389e-06
Step 551900 Loss: 0.1805, lr 5.65886167330389e-06
Step 552000 Loss: 0.1796, lr 5.65886167330389e-06
Step 552100 Loss: 0.1788, lr 5.65886167330389e-06
Step 552200 Loss: 0.1780, lr 5.65886167330389e-06
Step 552300 Loss: 0.1772, lr 5.65886167330389e-06
Step 552400 Loss: 0.1765, lr 5.65886167330389e-06
Step 552500 Loss: 0.1757, lr 5.65886167330389e-06
Step 552600 Loss: 0.1748, lr 5.65886167330389e-06
Step 552700 Loss: 0.1742, lr 5.65886167330389e-06
Step 552800 Loss: 0.1734, lr 5.65886167330389e-06
Step 552900 Loss: 0.1728, lr 5.65886167330389e-06
Step 553000 Loss: 0.1725, lr 5.65886167330389e-06
Step 553100 Loss: 0.1721, lr 5.65886167330389e-06
Step 553200 Loss: 0.1718, lr 5.65886167330389e-06
Step 553300 Loss: 0.1716, lr 5.65886167330389e-06
Step 553400 Loss: 0.1716, lr 5.65886167330389e-06
Step 553500 Loss: 0.1719, lr 5.65886167330389e-06
Step 553600 Loss: 0.1722, lr 5.65886167330389e-06
Step 553700 Loss: 0.1727, lr 5.65886167330389e-06
Step 553800 Loss: 0.1724, lr 5.65886167330389e-06
Step 553900 Loss: 0.1718, lr 5.65886167330389e-06
Train Epoch: [122/100] Loss: 0.1720,lr 0.000006
Calling G2SDataset.batch()
Done, time:  2.27 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.25 s, total batches: 6756
Step 554000 Loss: 0.7358, lr 5.094434776686645e-06
Step 554100 Loss: 0.4928, lr 5.094434776686645e-06
Step 554200 Loss: 0.4235, lr 5.094434776686645e-06
Step 554300 Loss: 0.3851, lr 5.094434776686645e-06
Step 554400 Loss: 0.3617, lr 5.094434776686645e-06
Step 554500 Loss: 0.3452, lr 5.094434776686645e-06
Step 554600 Loss: 0.3268, lr 5.094434776686645e-06
Step 554700 Loss: 0.3136, lr 5.094434776686645e-06
Step 554800 Loss: 0.3039, lr 5.094434776686645e-06
Step 554900 Loss: 0.2940, lr 5.094434776686645e-06
Step 555000 Loss: 0.2854, lr 5.094434776686645e-06
Step 555100 Loss: 0.2786, lr 5.094434776686645e-06
Step 555200 Loss: 0.2718, lr 5.094434776686645e-06
Step 555300 Loss: 0.2656, lr 5.094434776686645e-06
Step 555400 Loss: 0.2597, lr 5.094434776686645e-06
Step 555500 Loss: 0.2557, lr 5.094434776686645e-06
Step 555600 Loss: 0.2516, lr 5.094434776686645e-06
Step 555700 Loss: 0.2474, lr 5.094434776686645e-06
Step 555800 Loss: 0.2437, lr 5.094434776686645e-06
Step 555900 Loss: 0.2400, lr 5.094434776686645e-06
Step 556000 Loss: 0.2362, lr 5.094434776686645e-06
Step 556100 Loss: 0.2327, lr 5.094434776686645e-06
Step 556200 Loss: 0.2295, lr 5.094434776686645e-06
Step 556300 Loss: 0.2262, lr 5.094434776686645e-06
Step 556400 Loss: 0.2233, lr 5.094434776686645e-06
Step 556500 Loss: 0.2205, lr 5.094434776686645e-06
Step 556600 Loss: 0.2175, lr 5.094434776686645e-06
Step 556700 Loss: 0.2155, lr 5.094434776686645e-06
Step 556800 Loss: 0.2131, lr 5.094434776686645e-06
Step 556900 Loss: 0.2110, lr 5.094434776686645e-06
Step 557000 Loss: 0.2089, lr 5.094434776686645e-06
Step 557100 Loss: 0.2064, lr 5.094434776686645e-06
Step 557200 Loss: 0.2041, lr 5.094434776686645e-06
Step 557300 Loss: 0.2017, lr 5.094434776686645e-06
Step 557400 Loss: 0.1997, lr 5.094434776686645e-06
Step 557500 Loss: 0.1979, lr 5.094434776686645e-06
Step 557600 Loss: 0.1960, lr 5.094434776686645e-06
Step 557700 Loss: 0.1941, lr 5.094434776686645e-06
Step 557800 Loss: 0.1926, lr 5.094434776686645e-06
Step 557900 Loss: 0.1911, lr 5.094434776686645e-06
Step 558000 Loss: 0.1899, lr 5.094434776686645e-06
Step 558100 Loss: 0.1885, lr 5.094434776686645e-06
Step 558200 Loss: 0.1873, lr 5.094434776686645e-06
Step 558300 Loss: 0.1861, lr 5.094434776686645e-06
Step 558400 Loss: 0.1849, lr 5.094434776686645e-06
Step 558500 Loss: 0.1834, lr 5.094434776686645e-06
Step 558600 Loss: 0.1822, lr 5.094434776686645e-06
Step 558700 Loss: 0.1811, lr 5.094434776686645e-06
Step 558800 Loss: 0.1803, lr 5.094434776686645e-06
Step 558900 Loss: 0.1796, lr 5.094434776686645e-06
Step 559000 Loss: 0.1786, lr 5.094434776686645e-06
Step 559100 Loss: 0.1781, lr 5.094434776686645e-06
Step 559200 Loss: 0.1772, lr 5.094434776686645e-06
Step 559300 Loss: 0.1763, lr 5.094434776686645e-06
Step 559400 Loss: 0.1755, lr 5.094434776686645e-06
Step 559500 Loss: 0.1748, lr 5.094434776686645e-06
Step 559600 Loss: 0.1741, lr 5.094434776686645e-06
Step 559700 Loss: 0.1736, lr 5.094434776686645e-06
Step 559800 Loss: 0.1732, lr 5.094434776686645e-06
Step 559900 Loss: 0.1728, lr 5.094434776686645e-06
Step 560000 Loss: 0.1725, lr 5.094434776686645e-06
Step 560100 Loss: 0.1723, lr 5.094434776686645e-06
Step 560200 Loss: 0.1724, lr 5.094434776686645e-06
Step 560300 Loss: 0.1726, lr 5.094434776686645e-06
Step 560400 Loss: 0.1732, lr 5.094434776686645e-06
Step 560500 Loss: 0.1734, lr 5.094434776686645e-06
Step 560600 Loss: 0.1730, lr 5.094434776686645e-06
Step 560700 Loss: 0.1723, lr 5.094434776686645e-06
Train Epoch: [123/100] Loss: 0.1731,lr 0.000005
Calling G2SDataset.batch()
Done, time:  2.31 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Step 560800 Loss: 0.5448, lr 4.557276331351138e-06
Step 560900 Loss: 0.4462, lr 4.557276331351138e-06
Step 561000 Loss: 0.4000, lr 4.557276331351138e-06
Step 561100 Loss: 0.3722, lr 4.557276331351138e-06
Step 561200 Loss: 0.3551, lr 4.557276331351138e-06
Step 561300 Loss: 0.3388, lr 4.557276331351138e-06
Step 561400 Loss: 0.3233, lr 4.557276331351138e-06
Step 561500 Loss: 0.3136, lr 4.557276331351138e-06
Step 561600 Loss: 0.3037, lr 4.557276331351138e-06
Step 561700 Loss: 0.2950, lr 4.557276331351138e-06
Step 561800 Loss: 0.2874, lr 4.557276331351138e-06
Step 561900 Loss: 0.2802, lr 4.557276331351138e-06
Step 562000 Loss: 0.2741, lr 4.557276331351138e-06
Step 562100 Loss: 0.2672, lr 4.557276331351138e-06
Step 562200 Loss: 0.2623, lr 4.557276331351138e-06
Step 562300 Loss: 0.2582, lr 4.557276331351138e-06
Step 562400 Loss: 0.2542, lr 4.557276331351138e-06
Step 562500 Loss: 0.2501, lr 4.557276331351138e-06
Step 562600 Loss: 0.2463, lr 4.557276331351138e-06
Step 562700 Loss: 0.2420, lr 4.557276331351138e-06
Step 562800 Loss: 0.2375, lr 4.557276331351138e-06
Step 562900 Loss: 0.2343, lr 4.557276331351138e-06
Step 563000 Loss: 0.2312, lr 4.557276331351138e-06
Step 563100 Loss: 0.2280, lr 4.557276331351138e-06
Step 563200 Loss: 0.2247, lr 4.557276331351138e-06
Step 563300 Loss: 0.2219, lr 4.557276331351138e-06
Step 563400 Loss: 0.2193, lr 4.557276331351138e-06
Step 563500 Loss: 0.2170, lr 4.557276331351138e-06
Step 563600 Loss: 0.2145, lr 4.557276331351138e-06
Step 563700 Loss: 0.2125, lr 4.557276331351138e-06
Step 563800 Loss: 0.2100, lr 4.557276331351138e-06
Step 563900 Loss: 0.2077, lr 4.557276331351138e-06
Step 564000 Loss: 0.2056, lr 4.557276331351138e-06
Step 564100 Loss: 0.2033, lr 4.557276331351138e-06
Step 564200 Loss: 0.2012, lr 4.557276331351138e-06
Step 564300 Loss: 0.1992, lr 4.557276331351138e-06
Step 564400 Loss: 0.1975, lr 4.557276331351138e-06
Step 564500 Loss: 0.1959, lr 4.557276331351138e-06
Step 564600 Loss: 0.1940, lr 4.557276331351138e-06
Step 564700 Loss: 0.1928, lr 4.557276331351138e-06
Step 564800 Loss: 0.1915, lr 4.557276331351138e-06
Step 564900 Loss: 0.1902, lr 4.557276331351138e-06
Step 565000 Loss: 0.1889, lr 4.557276331351138e-06
Step 565100 Loss: 0.1877, lr 4.557276331351138e-06
Step 565200 Loss: 0.1865, lr 4.557276331351138e-06
Step 565300 Loss: 0.1850, lr 4.557276331351138e-06
Step 565400 Loss: 0.1837, lr 4.557276331351138e-06
Step 565500 Loss: 0.1828, lr 4.557276331351138e-06
Step 565600 Loss: 0.1819, lr 4.557276331351138e-06
Step 565700 Loss: 0.1811, lr 4.557276331351138e-06
Step 565800 Loss: 0.1802, lr 4.557276331351138e-06
Step 565900 Loss: 0.1796, lr 4.557276331351138e-06
Step 566000 Loss: 0.1786, lr 4.557276331351138e-06
Step 566100 Loss: 0.1777, lr 4.557276331351138e-06
Step 566200 Loss: 0.1769, lr 4.557276331351138e-06
Step 566300 Loss: 0.1762, lr 4.557276331351138e-06
Step 566400 Loss: 0.1755, lr 4.557276331351138e-06
Step 566500 Loss: 0.1750, lr 4.557276331351138e-06
Step 566600 Loss: 0.1746, lr 4.557276331351138e-06
Step 566700 Loss: 0.1742, lr 4.557276331351138e-06
Step 566800 Loss: 0.1739, lr 4.557276331351138e-06
Step 566900 Loss: 0.1737, lr 4.557276331351138e-06
Step 567000 Loss: 0.1739, lr 4.557276331351138e-06
Step 567100 Loss: 0.1743, lr 4.557276331351138e-06
Step 567200 Loss: 0.1748, lr 4.557276331351138e-06
Step 567300 Loss: 0.1747, lr 4.557276331351138e-06
Step 567400 Loss: 0.1742, lr 4.557276331351138e-06
Train Epoch: [124/100] Loss: 0.1743,lr 0.000005
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Step 567500 Loss: 0.7781, lr 4.047916447831018e-06
Step 567600 Loss: 0.4866, lr 4.047916447831018e-06
Step 567700 Loss: 0.4217, lr 4.047916447831018e-06
Step 567800 Loss: 0.3843, lr 4.047916447831018e-06
Step 567900 Loss: 0.3624, lr 4.047916447831018e-06
Step 568000 Loss: 0.3482, lr 4.047916447831018e-06
Step 568100 Loss: 0.3307, lr 4.047916447831018e-06
Step 568200 Loss: 0.3181, lr 4.047916447831018e-06
Step 568300 Loss: 0.3094, lr 4.047916447831018e-06
Step 568400 Loss: 0.2998, lr 4.047916447831018e-06
Step 568500 Loss: 0.2926, lr 4.047916447831018e-06
Step 568600 Loss: 0.2857, lr 4.047916447831018e-06
Step 568700 Loss: 0.2787, lr 4.047916447831018e-06
Step 568800 Loss: 0.2724, lr 4.047916447831018e-06
Step 568900 Loss: 0.2665, lr 4.047916447831018e-06
Step 569000 Loss: 0.2624, lr 4.047916447831018e-06
Step 569100 Loss: 0.2581, lr 4.047916447831018e-06
Step 569200 Loss: 0.2538, lr 4.047916447831018e-06
Step 569300 Loss: 0.2499, lr 4.047916447831018e-06
Step 569400 Loss: 0.2461, lr 4.047916447831018e-06
Step 569500 Loss: 0.2418, lr 4.047916447831018e-06
Step 569600 Loss: 0.2380, lr 4.047916447831018e-06
Step 569700 Loss: 0.2348, lr 4.047916447831018e-06
Step 569800 Loss: 0.2312, lr 4.047916447831018e-06
Step 569900 Loss: 0.2283, lr 4.047916447831018e-06
Step 570000 Loss: 0.2252, lr 4.047916447831018e-06
Step 570100 Loss: 0.2223, lr 4.047916447831018e-06
Step 570200 Loss: 0.2200, lr 4.047916447831018e-06
Step 570300 Loss: 0.2177, lr 4.047916447831018e-06
Step 570400 Loss: 0.2153, lr 4.047916447831018e-06
Step 570500 Loss: 0.2130, lr 4.047916447831018e-06
Step 570600 Loss: 0.2103, lr 4.047916447831018e-06
Step 570700 Loss: 0.2081, lr 4.047916447831018e-06
Step 570800 Loss: 0.2057, lr 4.047916447831018e-06
Step 570900 Loss: 0.2037, lr 4.047916447831018e-06
Step 571000 Loss: 0.2017, lr 4.047916447831018e-06
Step 571100 Loss: 0.1997, lr 4.047916447831018e-06
Step 571200 Loss: 0.1978, lr 4.047916447831018e-06
Step 571300 Loss: 0.1962, lr 4.047916447831018e-06
Step 571400 Loss: 0.1947, lr 4.047916447831018e-06
Step 571500 Loss: 0.1934, lr 4.047916447831018e-06
Step 571600 Loss: 0.1920, lr 4.047916447831018e-06
Step 571700 Loss: 0.1908, lr 4.047916447831018e-06
Step 571800 Loss: 0.1895, lr 4.047916447831018e-06
Step 571900 Loss: 0.1881, lr 4.047916447831018e-06
Step 572000 Loss: 0.1868, lr 4.047916447831018e-06
Step 572100 Loss: 0.1855, lr 4.047916447831018e-06
Step 572200 Loss: 0.1842, lr 4.047916447831018e-06
Step 572300 Loss: 0.1833, lr 4.047916447831018e-06
Step 572400 Loss: 0.1827, lr 4.047916447831018e-06
Step 572500 Loss: 0.1817, lr 4.047916447831018e-06
Step 572600 Loss: 0.1810, lr 4.047916447831018e-06
Step 572700 Loss: 0.1799, lr 4.047916447831018e-06
Step 572800 Loss: 0.1791, lr 4.047916447831018e-06
Step 572900 Loss: 0.1783, lr 4.047916447831018e-06
Step 573000 Loss: 0.1775, lr 4.047916447831018e-06
Step 573100 Loss: 0.1769, lr 4.047916447831018e-06
Step 573200 Loss: 0.1762, lr 4.047916447831018e-06
Step 573300 Loss: 0.1758, lr 4.047916447831018e-06
Step 573400 Loss: 0.1756, lr 4.047916447831018e-06
Step 573500 Loss: 0.1753, lr 4.047916447831018e-06
Step 573600 Loss: 0.1749, lr 4.047916447831018e-06
Step 573700 Loss: 0.1749, lr 4.047916447831018e-06
Step 573800 Loss: 0.1752, lr 4.047916447831018e-06
Step 573900 Loss: 0.1756, lr 4.047916447831018e-06
Step 574000 Loss: 0.1759, lr 4.047916447831018e-06
Step 574100 Loss: 0.1754, lr 4.047916447831018e-06
Step 574200 Loss: 0.1749, lr 4.047916447831018e-06
Train Epoch: [125/100] Loss: 0.1754,lr 0.000004
Model Saving at epoch 125
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 574300 Loss: 0.6167, lr 3.5668578028355405e-06
Step 574400 Loss: 0.4780, lr 3.5668578028355405e-06
Step 574500 Loss: 0.4201, lr 3.5668578028355405e-06
Step 574600 Loss: 0.3866, lr 3.5668578028355405e-06
Step 574700 Loss: 0.3668, lr 3.5668578028355405e-06
Step 574800 Loss: 0.3511, lr 3.5668578028355405e-06
Step 574900 Loss: 0.3338, lr 3.5668578028355405e-06
Step 575000 Loss: 0.3240, lr 3.5668578028355405e-06
Step 575100 Loss: 0.3143, lr 3.5668578028355405e-06
Step 575200 Loss: 0.3057, lr 3.5668578028355405e-06
Step 575300 Loss: 0.2985, lr 3.5668578028355405e-06
Step 575400 Loss: 0.2922, lr 3.5668578028355405e-06
Step 575500 Loss: 0.2859, lr 3.5668578028355405e-06
Step 575600 Loss: 0.2789, lr 3.5668578028355405e-06
Step 575700 Loss: 0.2736, lr 3.5668578028355405e-06
Step 575800 Loss: 0.2695, lr 3.5668578028355405e-06
Step 575900 Loss: 0.2652, lr 3.5668578028355405e-06
Step 576000 Loss: 0.2609, lr 3.5668578028355405e-06
Step 576100 Loss: 0.2569, lr 3.5668578028355405e-06
Step 576200 Loss: 0.2523, lr 3.5668578028355405e-06
Step 576300 Loss: 0.2477, lr 3.5668578028355405e-06
Step 576400 Loss: 0.2442, lr 3.5668578028355405e-06
Step 576500 Loss: 0.2406, lr 3.5668578028355405e-06
Step 576600 Loss: 0.2370, lr 3.5668578028355405e-06
Step 576700 Loss: 0.2334, lr 3.5668578028355405e-06
Step 576800 Loss: 0.2304, lr 3.5668578028355405e-06
Step 576900 Loss: 0.2274, lr 3.5668578028355405e-06
Step 577000 Loss: 0.2249, lr 3.5668578028355405e-06
Step 577100 Loss: 0.2222, lr 3.5668578028355405e-06
Step 577200 Loss: 0.2200, lr 3.5668578028355405e-06
Step 577300 Loss: 0.2172, lr 3.5668578028355405e-06
Step 577400 Loss: 0.2144, lr 3.5668578028355405e-06
Step 577500 Loss: 0.2121, lr 3.5668578028355405e-06
Step 577600 Loss: 0.2094, lr 3.5668578028355405e-06
Step 577700 Loss: 0.2073, lr 3.5668578028355405e-06
Step 577800 Loss: 0.2052, lr 3.5668578028355405e-06
Step 577900 Loss: 0.2033, lr 3.5668578028355405e-06
Step 578000 Loss: 0.2014, lr 3.5668578028355405e-06
Step 578100 Loss: 0.1995, lr 3.5668578028355405e-06
Step 578200 Loss: 0.1980, lr 3.5668578028355405e-06
Step 578300 Loss: 0.1966, lr 3.5668578028355405e-06
Step 578400 Loss: 0.1952, lr 3.5668578028355405e-06
Step 578500 Loss: 0.1938, lr 3.5668578028355405e-06
Step 578600 Loss: 0.1924, lr 3.5668578028355405e-06
Step 578700 Loss: 0.1911, lr 3.5668578028355405e-06
Step 578800 Loss: 0.1895, lr 3.5668578028355405e-06
Step 578900 Loss: 0.1882, lr 3.5668578028355405e-06
Step 579000 Loss: 0.1871, lr 3.5668578028355405e-06
Step 579100 Loss: 0.1863, lr 3.5668578028355405e-06
Step 579200 Loss: 0.1854, lr 3.5668578028355405e-06
Step 579300 Loss: 0.1843, lr 3.5668578028355405e-06
Step 579400 Loss: 0.1837, lr 3.5668578028355405e-06
Step 579500 Loss: 0.1826, lr 3.5668578028355405e-06
Step 579600 Loss: 0.1817, lr 3.5668578028355405e-06
Step 579700 Loss: 0.1809, lr 3.5668578028355405e-06
Step 579800 Loss: 0.1801, lr 3.5668578028355405e-06
Step 579900 Loss: 0.1794, lr 3.5668578028355405e-06
Step 580000 Loss: 0.1789, lr 3.5668578028355405e-06
Step 580100 Loss: 0.1785, lr 3.5668578028355405e-06
Step 580200 Loss: 0.1781, lr 3.5668578028355405e-06
Step 580300 Loss: 0.1777, lr 3.5668578028355405e-06
Step 580400 Loss: 0.1774, lr 3.5668578028355405e-06
Step 580500 Loss: 0.1777, lr 3.5668578028355405e-06
Step 580600 Loss: 0.1779, lr 3.5668578028355405e-06
Step 580700 Loss: 0.1783, lr 3.5668578028355405e-06
Step 580800 Loss: 0.1784, lr 3.5668578028355405e-06
Step 580900 Loss: 0.1777, lr 3.5668578028355405e-06
Train Epoch: [126/100] Loss: 0.1780,lr 0.000004
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 581000 Loss: 0.9186, lr 3.114575143168404e-06
Step 581100 Loss: 0.5098, lr 3.114575143168404e-06
Step 581200 Loss: 0.4329, lr 3.114575143168404e-06
Step 581300 Loss: 0.3924, lr 3.114575143168404e-06
Step 581400 Loss: 0.3705, lr 3.114575143168404e-06
Step 581500 Loss: 0.3551, lr 3.114575143168404e-06
Step 581600 Loss: 0.3378, lr 3.114575143168404e-06
Step 581700 Loss: 0.3252, lr 3.114575143168404e-06
Step 581800 Loss: 0.3173, lr 3.114575143168404e-06
Step 581900 Loss: 0.3075, lr 3.114575143168404e-06
Step 582000 Loss: 0.2999, lr 3.114575143168404e-06
Step 582100 Loss: 0.2941, lr 3.114575143168404e-06
Step 582200 Loss: 0.2877, lr 3.114575143168404e-06
Step 582300 Loss: 0.2816, lr 3.114575143168404e-06
Step 582400 Loss: 0.2760, lr 3.114575143168404e-06
Step 582500 Loss: 0.2720, lr 3.114575143168404e-06
Step 582600 Loss: 0.2677, lr 3.114575143168404e-06
Step 582700 Loss: 0.2642, lr 3.114575143168404e-06
Step 582800 Loss: 0.2601, lr 3.114575143168404e-06
Step 582900 Loss: 0.2565, lr 3.114575143168404e-06
Step 583000 Loss: 0.2523, lr 3.114575143168404e-06
Step 583100 Loss: 0.2481, lr 3.114575143168404e-06
Step 583200 Loss: 0.2455, lr 3.114575143168404e-06
Step 583300 Loss: 0.2424, lr 3.114575143168404e-06
Step 583400 Loss: 0.2396, lr 3.114575143168404e-06
Step 583500 Loss: 0.2362, lr 3.114575143168404e-06
Step 583600 Loss: 0.2333, lr 3.114575143168404e-06
Step 583700 Loss: 0.2307, lr 3.114575143168404e-06
Step 583800 Loss: 0.2283, lr 3.114575143168404e-06
Step 583900 Loss: 0.2258, lr 3.114575143168404e-06
Step 584000 Loss: 0.2232, lr 3.114575143168404e-06
Step 584100 Loss: 0.2203, lr 3.114575143168404e-06
Step 584200 Loss: 0.2177, lr 3.114575143168404e-06
Step 584300 Loss: 0.2152, lr 3.114575143168404e-06
Step 584400 Loss: 0.2129, lr 3.114575143168404e-06
Step 584500 Loss: 0.2108, lr 3.114575143168404e-06
Step 584600 Loss: 0.2085, lr 3.114575143168404e-06
Step 584700 Loss: 0.2065, lr 3.114575143168404e-06
Step 584800 Loss: 0.2047, lr 3.114575143168404e-06
Step 584900 Loss: 0.2029, lr 3.114575143168404e-06
Step 585000 Loss: 0.2015, lr 3.114575143168404e-06
Step 585100 Loss: 0.1998, lr 3.114575143168404e-06
Step 585200 Loss: 0.1984, lr 3.114575143168404e-06
Step 585300 Loss: 0.1970, lr 3.114575143168404e-06
Step 585400 Loss: 0.1955, lr 3.114575143168404e-06
Step 585500 Loss: 0.1941, lr 3.114575143168404e-06
Step 585600 Loss: 0.1926, lr 3.114575143168404e-06
Step 585700 Loss: 0.1913, lr 3.114575143168404e-06
Step 585800 Loss: 0.1903, lr 3.114575143168404e-06
Step 585900 Loss: 0.1894, lr 3.114575143168404e-06
Step 586000 Loss: 0.1884, lr 3.114575143168404e-06
Step 586100 Loss: 0.1875, lr 3.114575143168404e-06
Step 586200 Loss: 0.1865, lr 3.114575143168404e-06
Step 586300 Loss: 0.1855, lr 3.114575143168404e-06
Step 586400 Loss: 0.1846, lr 3.114575143168404e-06
Step 586500 Loss: 0.1838, lr 3.114575143168404e-06
Step 586600 Loss: 0.1831, lr 3.114575143168404e-06
Step 586700 Loss: 0.1824, lr 3.114575143168404e-06
Step 586800 Loss: 0.1819, lr 3.114575143168404e-06
Step 586900 Loss: 0.1814, lr 3.114575143168404e-06
Step 587000 Loss: 0.1811, lr 3.114575143168404e-06
Step 587100 Loss: 0.1806, lr 3.114575143168404e-06
Step 587200 Loss: 0.1804, lr 3.114575143168404e-06
Step 587300 Loss: 0.1806, lr 3.114575143168404e-06
Step 587400 Loss: 0.1810, lr 3.114575143168404e-06
Step 587500 Loss: 0.1815, lr 3.114575143168404e-06
Step 587600 Loss: 0.1810, lr 3.114575143168404e-06
Step 587700 Loss: 0.1803, lr 3.114575143168404e-06
Train Epoch: [127/100] Loss: 0.1807,lr 0.000003
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Step 587800 Loss: 0.6974, lr 2.6915148172099505e-06
Step 587900 Loss: 0.5101, lr 2.6915148172099505e-06
Step 588000 Loss: 0.4402, lr 2.6915148172099505e-06
Step 588100 Loss: 0.4009, lr 2.6915148172099505e-06
Step 588200 Loss: 0.3784, lr 2.6915148172099505e-06
Step 588300 Loss: 0.3618, lr 2.6915148172099505e-06
Step 588400 Loss: 0.3429, lr 2.6915148172099505e-06
Step 588500 Loss: 0.3314, lr 2.6915148172099505e-06
Step 588600 Loss: 0.3217, lr 2.6915148172099505e-06
Step 588700 Loss: 0.3131, lr 2.6915148172099505e-06
Step 588800 Loss: 0.3052, lr 2.6915148172099505e-06
Step 588900 Loss: 0.2989, lr 2.6915148172099505e-06
Step 589000 Loss: 0.2928, lr 2.6915148172099505e-06
Step 589100 Loss: 0.2860, lr 2.6915148172099505e-06
Step 589200 Loss: 0.2806, lr 2.6915148172099505e-06
Step 589300 Loss: 0.2765, lr 2.6915148172099505e-06
Step 589400 Loss: 0.2724, lr 2.6915148172099505e-06
Step 589500 Loss: 0.2680, lr 2.6915148172099505e-06
Step 589600 Loss: 0.2642, lr 2.6915148172099505e-06
Step 589700 Loss: 0.2601, lr 2.6915148172099505e-06
Step 589800 Loss: 0.2563, lr 2.6915148172099505e-06
Step 589900 Loss: 0.2527, lr 2.6915148172099505e-06
Step 590000 Loss: 0.2494, lr 2.6915148172099505e-06
Step 590100 Loss: 0.2460, lr 2.6915148172099505e-06
Step 590200 Loss: 0.2427, lr 2.6915148172099505e-06
Step 590300 Loss: 0.2398, lr 2.6915148172099505e-06
Step 590400 Loss: 0.2372, lr 2.6915148172099505e-06
Step 590500 Loss: 0.2349, lr 2.6915148172099505e-06
Step 590600 Loss: 0.2320, lr 2.6915148172099505e-06
Step 590700 Loss: 0.2295, lr 2.6915148172099505e-06
Step 590800 Loss: 0.2269, lr 2.6915148172099505e-06
Step 590900 Loss: 0.2241, lr 2.6915148172099505e-06
Step 591000 Loss: 0.2218, lr 2.6915148172099505e-06
Step 591100 Loss: 0.2189, lr 2.6915148172099505e-06
Step 591200 Loss: 0.2167, lr 2.6915148172099505e-06
Step 591300 Loss: 0.2145, lr 2.6915148172099505e-06
Step 591400 Loss: 0.2124, lr 2.6915148172099505e-06
Step 591500 Loss: 0.2104, lr 2.6915148172099505e-06
Step 591600 Loss: 0.2085, lr 2.6915148172099505e-06
Step 591700 Loss: 0.2068, lr 2.6915148172099505e-06
Step 591800 Loss: 0.2053, lr 2.6915148172099505e-06
Step 591900 Loss: 0.2036, lr 2.6915148172099505e-06
Step 592000 Loss: 0.2022, lr 2.6915148172099505e-06
Step 592100 Loss: 0.2005, lr 2.6915148172099505e-06
Step 592200 Loss: 0.1991, lr 2.6915148172099505e-06
Step 592300 Loss: 0.1974, lr 2.6915148172099505e-06
Step 592400 Loss: 0.1959, lr 2.6915148172099505e-06
Step 592500 Loss: 0.1947, lr 2.6915148172099505e-06
Step 592600 Loss: 0.1937, lr 2.6915148172099505e-06
Step 592700 Loss: 0.1928, lr 2.6915148172099505e-06
Step 592800 Loss: 0.1916, lr 2.6915148172099505e-06
Step 592900 Loss: 0.1908, lr 2.6915148172099505e-06
Step 593000 Loss: 0.1898, lr 2.6915148172099505e-06
Step 593100 Loss: 0.1889, lr 2.6915148172099505e-06
Step 593200 Loss: 0.1880, lr 2.6915148172099505e-06
Step 593300 Loss: 0.1872, lr 2.6915148172099505e-06
Step 593400 Loss: 0.1864, lr 2.6915148172099505e-06
Step 593500 Loss: 0.1858, lr 2.6915148172099505e-06
Step 593600 Loss: 0.1852, lr 2.6915148172099505e-06
Step 593700 Loss: 0.1848, lr 2.6915148172099505e-06
Step 593800 Loss: 0.1843, lr 2.6915148172099505e-06
Step 593900 Loss: 0.1840, lr 2.6915148172099505e-06
Step 594000 Loss: 0.1843, lr 2.6915148172099505e-06
Step 594100 Loss: 0.1843, lr 2.6915148172099505e-06
Step 594200 Loss: 0.1850, lr 2.6915148172099505e-06
Step 594300 Loss: 0.1849, lr 2.6915148172099505e-06
Step 594400 Loss: 0.1843, lr 2.6915148172099505e-06
Train Epoch: [128/100] Loss: 0.1840,lr 0.000003
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 594500 Loss: 1.1729, lr 2.298094334425203e-06
Step 594600 Loss: 0.5692, lr 2.298094334425203e-06
Step 594700 Loss: 0.4698, lr 2.298094334425203e-06
Step 594800 Loss: 0.4205, lr 2.298094334425203e-06
Step 594900 Loss: 0.3929, lr 2.298094334425203e-06
Step 595000 Loss: 0.3733, lr 2.298094334425203e-06
Step 595100 Loss: 0.3545, lr 2.298094334425203e-06
Step 595200 Loss: 0.3400, lr 2.298094334425203e-06
Step 595300 Loss: 0.3296, lr 2.298094334425203e-06
Step 595400 Loss: 0.3194, lr 2.298094334425203e-06
Step 595500 Loss: 0.3112, lr 2.298094334425203e-06
Step 595600 Loss: 0.3047, lr 2.298094334425203e-06
Step 595700 Loss: 0.2977, lr 2.298094334425203e-06
Step 595800 Loss: 0.2920, lr 2.298094334425203e-06
Step 595900 Loss: 0.2856, lr 2.298094334425203e-06
Step 596000 Loss: 0.2814, lr 2.298094334425203e-06
Step 596100 Loss: 0.2770, lr 2.298094334425203e-06
Step 596200 Loss: 0.2731, lr 2.298094334425203e-06
Step 596300 Loss: 0.2690, lr 2.298094334425203e-06
Step 596400 Loss: 0.2654, lr 2.298094334425203e-06
Step 596500 Loss: 0.2615, lr 2.298094334425203e-06
Step 596600 Loss: 0.2574, lr 2.298094334425203e-06
Step 596700 Loss: 0.2547, lr 2.298094334425203e-06
Step 596800 Loss: 0.2519, lr 2.298094334425203e-06
Step 596900 Loss: 0.2491, lr 2.298094334425203e-06
Step 597000 Loss: 0.2464, lr 2.298094334425203e-06
Step 597100 Loss: 0.2441, lr 2.298094334425203e-06
Step 597200 Loss: 0.2421, lr 2.298094334425203e-06
Step 597300 Loss: 0.2402, lr 2.298094334425203e-06
Step 597400 Loss: 0.2379, lr 2.298094334425203e-06
Step 597500 Loss: 0.2357, lr 2.298094334425203e-06
Step 597600 Loss: 0.2332, lr 2.298094334425203e-06
Step 597700 Loss: 0.2308, lr 2.298094334425203e-06
Step 597800 Loss: 0.2284, lr 2.298094334425203e-06
Step 597900 Loss: 0.2261, lr 2.298094334425203e-06
Step 598000 Loss: 0.2240, lr 2.298094334425203e-06
Step 598100 Loss: 0.2217, lr 2.298094334425203e-06
Step 598200 Loss: 0.2196, lr 2.298094334425203e-06
Step 598300 Loss: 0.2178, lr 2.298094334425203e-06
Step 598400 Loss: 0.2160, lr 2.298094334425203e-06
Step 598500 Loss: 0.2145, lr 2.298094334425203e-06
Step 598600 Loss: 0.2125, lr 2.298094334425203e-06
Step 598700 Loss: 0.2111, lr 2.298094334425203e-06
Step 598800 Loss: 0.2095, lr 2.298094334425203e-06
Step 598900 Loss: 0.2078, lr 2.298094334425203e-06
Step 599000 Loss: 0.2062, lr 2.298094334425203e-06
Step 599100 Loss: 0.2044, lr 2.298094334425203e-06
Step 599200 Loss: 0.2028, lr 2.298094334425203e-06
Step 599300 Loss: 0.2016, lr 2.298094334425203e-06
Step 599400 Loss: 0.2005, lr 2.298094334425203e-06
Step 599500 Loss: 0.1994, lr 2.298094334425203e-06
Step 599600 Loss: 0.1983, lr 2.298094334425203e-06
Step 599700 Loss: 0.1971, lr 2.298094334425203e-06
Step 599800 Loss: 0.1960, lr 2.298094334425203e-06
Step 599900 Loss: 0.1948, lr 2.298094334425203e-06
Step 600000 Loss: 0.1939, lr 2.298094334425203e-06
Step 600100 Loss: 0.1930, lr 2.298094334425203e-06
Step 600200 Loss: 0.1921, lr 2.298094334425203e-06
Step 600300 Loss: 0.1915, lr 2.298094334425203e-06
Step 600400 Loss: 0.1909, lr 2.298094334425203e-06
Step 600500 Loss: 0.1903, lr 2.298094334425203e-06
Step 600600 Loss: 0.1899, lr 2.298094334425203e-06
Step 600700 Loss: 0.1897, lr 2.298094334425203e-06
Step 600800 Loss: 0.1898, lr 2.298094334425203e-06
Step 600900 Loss: 0.1900, lr 2.298094334425203e-06
Step 601000 Loss: 0.1903, lr 2.298094334425203e-06
Step 601100 Loss: 0.1897, lr 2.298094334425203e-06
Step 601200 Loss: 0.1891, lr 2.298094334425203e-06
Train Epoch: [129/100] Loss: 0.1894,lr 0.000002
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 601300 Loss: 0.7475, lr 1.934701953332451e-06
Step 601400 Loss: 0.5475, lr 1.934701953332451e-06
Step 601500 Loss: 0.4703, lr 1.934701953332451e-06
Step 601600 Loss: 0.4278, lr 1.934701953332451e-06
Step 601700 Loss: 0.4000, lr 1.934701953332451e-06
Step 601800 Loss: 0.3819, lr 1.934701953332451e-06
Step 601900 Loss: 0.3614, lr 1.934701953332451e-06
Step 602000 Loss: 0.3482, lr 1.934701953332451e-06
Step 602100 Loss: 0.3375, lr 1.934701953332451e-06
Step 602200 Loss: 0.3276, lr 1.934701953332451e-06
Step 602300 Loss: 0.3190, lr 1.934701953332451e-06
Step 602400 Loss: 0.3116, lr 1.934701953332451e-06
Step 602500 Loss: 0.3048, lr 1.934701953332451e-06
Step 602600 Loss: 0.2981, lr 1.934701953332451e-06
Step 602700 Loss: 0.2920, lr 1.934701953332451e-06
Step 602800 Loss: 0.2878, lr 1.934701953332451e-06
Step 602900 Loss: 0.2832, lr 1.934701953332451e-06
Step 603000 Loss: 0.2787, lr 1.934701953332451e-06
Step 603100 Loss: 0.2743, lr 1.934701953332451e-06
Step 603200 Loss: 0.2704, lr 1.934701953332451e-06
Step 603300 Loss: 0.2661, lr 1.934701953332451e-06
Step 603400 Loss: 0.2626, lr 1.934701953332451e-06
Step 603500 Loss: 0.2596, lr 1.934701953332451e-06
Step 603600 Loss: 0.2563, lr 1.934701953332451e-06
Step 603700 Loss: 0.2535, lr 1.934701953332451e-06
Step 603800 Loss: 0.2509, lr 1.934701953332451e-06
Step 603900 Loss: 0.2484, lr 1.934701953332451e-06
Step 604000 Loss: 0.2465, lr 1.934701953332451e-06
Step 604100 Loss: 0.2441, lr 1.934701953332451e-06
Step 604200 Loss: 0.2418, lr 1.934701953332451e-06
Step 604300 Loss: 0.2397, lr 1.934701953332451e-06
Step 604400 Loss: 0.2372, lr 1.934701953332451e-06
Step 604500 Loss: 0.2350, lr 1.934701953332451e-06
Step 604600 Loss: 0.2326, lr 1.934701953332451e-06
Step 604700 Loss: 0.2304, lr 1.934701953332451e-06
Step 604800 Loss: 0.2285, lr 1.934701953332451e-06
Step 604900 Loss: 0.2266, lr 1.934701953332451e-06
Step 605000 Loss: 0.2246, lr 1.934701953332451e-06
Step 605100 Loss: 0.2231, lr 1.934701953332451e-06
Step 605200 Loss: 0.2216, lr 1.934701953332451e-06
Step 605300 Loss: 0.2200, lr 1.934701953332451e-06
Step 605400 Loss: 0.2185, lr 1.934701953332451e-06
Step 605500 Loss: 0.2171, lr 1.934701953332451e-06
Step 605600 Loss: 0.2155, lr 1.934701953332451e-06
Step 605700 Loss: 0.2139, lr 1.934701953332451e-06
Step 605800 Loss: 0.2120, lr 1.934701953332451e-06
Step 605900 Loss: 0.2104, lr 1.934701953332451e-06
Step 606000 Loss: 0.2089, lr 1.934701953332451e-06
Step 606100 Loss: 0.2076, lr 1.934701953332451e-06
Step 606200 Loss: 0.2065, lr 1.934701953332451e-06
Step 606300 Loss: 0.2052, lr 1.934701953332451e-06
Step 606400 Loss: 0.2041, lr 1.934701953332451e-06
Step 606500 Loss: 0.2028, lr 1.934701953332451e-06
Step 606600 Loss: 0.2017, lr 1.934701953332451e-06
Step 606700 Loss: 0.2005, lr 1.934701953332451e-06
Step 606800 Loss: 0.1995, lr 1.934701953332451e-06
Step 606900 Loss: 0.1984, lr 1.934701953332451e-06
Step 607000 Loss: 0.1977, lr 1.934701953332451e-06
Step 607100 Loss: 0.1969, lr 1.934701953332451e-06
Step 607200 Loss: 0.1964, lr 1.934701953332451e-06
Step 607300 Loss: 0.1960, lr 1.934701953332451e-06
Step 607400 Loss: 0.1955, lr 1.934701953332451e-06
Step 607500 Loss: 0.1955, lr 1.934701953332451e-06
Step 607600 Loss: 0.1955, lr 1.934701953332451e-06
Step 607700 Loss: 0.1957, lr 1.934701953332451e-06
Step 607800 Loss: 0.1956, lr 1.934701953332451e-06
Step 607900 Loss: 0.1948, lr 1.934701953332451e-06
Step 608000 Loss: 0.1942, lr 1.934701953332451e-06
Train Epoch: [130/100] Loss: 0.1947,lr 0.000002
Model Saving at epoch 130
Calling G2SDataset.batch()
Done, time:  2.31 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Step 608100 Loss: 0.6672, lr 1.601696298338937e-06
Step 608200 Loss: 0.5480, lr 1.601696298338937e-06
Step 608300 Loss: 0.4871, lr 1.601696298338937e-06
Step 608400 Loss: 0.4471, lr 1.601696298338937e-06
Step 608500 Loss: 0.4223, lr 1.601696298338937e-06
Step 608600 Loss: 0.3997, lr 1.601696298338937e-06
Step 608700 Loss: 0.3809, lr 1.601696298338937e-06
Step 608800 Loss: 0.3673, lr 1.601696298338937e-06
Step 608900 Loss: 0.3534, lr 1.601696298338937e-06
Step 609000 Loss: 0.3429, lr 1.601696298338937e-06
Step 609100 Loss: 0.3338, lr 1.601696298338937e-06
Step 609200 Loss: 0.3251, lr 1.601696298338937e-06
Step 609300 Loss: 0.3177, lr 1.601696298338937e-06
Step 609400 Loss: 0.3096, lr 1.601696298338937e-06
Step 609500 Loss: 0.3039, lr 1.601696298338937e-06
Step 609600 Loss: 0.2984, lr 1.601696298338937e-06
Step 609700 Loss: 0.2931, lr 1.601696298338937e-06
Step 609800 Loss: 0.2881, lr 1.601696298338937e-06
Step 609900 Loss: 0.2832, lr 1.601696298338937e-06
Step 610000 Loss: 0.2784, lr 1.601696298338937e-06
Step 610100 Loss: 0.2736, lr 1.601696298338937e-06
Step 610200 Loss: 0.2703, lr 1.601696298338937e-06
Step 610300 Loss: 0.2670, lr 1.601696298338937e-06
Step 610400 Loss: 0.2635, lr 1.601696298338937e-06
Step 610500 Loss: 0.2602, lr 1.601696298338937e-06
Step 610600 Loss: 0.2576, lr 1.601696298338937e-06
Step 610700 Loss: 0.2552, lr 1.601696298338937e-06
Step 610800 Loss: 0.2529, lr 1.601696298338937e-06
Step 610900 Loss: 0.2504, lr 1.601696298338937e-06
Step 611000 Loss: 0.2482, lr 1.601696298338937e-06
Step 611100 Loss: 0.2457, lr 1.601696298338937e-06
Step 611200 Loss: 0.2434, lr 1.601696298338937e-06
Step 611300 Loss: 0.2411, lr 1.601696298338937e-06
Step 611400 Loss: 0.2388, lr 1.601696298338937e-06
Step 611500 Loss: 0.2367, lr 1.601696298338937e-06
Step 611600 Loss: 0.2348, lr 1.601696298338937e-06
Step 611700 Loss: 0.2330, lr 1.601696298338937e-06
Step 611800 Loss: 0.2315, lr 1.601696298338937e-06
Step 611900 Loss: 0.2298, lr 1.601696298338937e-06
Step 612000 Loss: 0.2287, lr 1.601696298338937e-06
Step 612100 Loss: 0.2271, lr 1.601696298338937e-06
Step 612200 Loss: 0.2259, lr 1.601696298338937e-06
Step 612300 Loss: 0.2244, lr 1.601696298338937e-06
Step 612400 Loss: 0.2229, lr 1.601696298338937e-06
Step 612500 Loss: 0.2214, lr 1.601696298338937e-06
Step 612600 Loss: 0.2196, lr 1.601696298338937e-06
Step 612700 Loss: 0.2182, lr 1.601696298338937e-06
Step 612800 Loss: 0.2169, lr 1.601696298338937e-06
Step 612900 Loss: 0.2157, lr 1.601696298338937e-06
Step 613000 Loss: 0.2146, lr 1.601696298338937e-06
Step 613100 Loss: 0.2134, lr 1.601696298338937e-06
Step 613200 Loss: 0.2123, lr 1.601696298338937e-06
Step 613300 Loss: 0.2109, lr 1.601696298338937e-06
Step 613400 Loss: 0.2095, lr 1.601696298338937e-06
Step 613500 Loss: 0.2085, lr 1.601696298338937e-06
Step 613600 Loss: 0.2073, lr 1.601696298338937e-06
Step 613700 Loss: 0.2063, lr 1.601696298338937e-06
Step 613800 Loss: 0.2055, lr 1.601696298338937e-06
Step 613900 Loss: 0.2047, lr 1.601696298338937e-06
Step 614000 Loss: 0.2041, lr 1.601696298338937e-06
Step 614100 Loss: 0.2034, lr 1.601696298338937e-06
Step 614200 Loss: 0.2029, lr 1.601696298338937e-06
Step 614300 Loss: 0.2028, lr 1.601696298338937e-06
Step 614400 Loss: 0.2029, lr 1.601696298338937e-06
Step 614500 Loss: 0.2030, lr 1.601696298338937e-06
Step 614600 Loss: 0.2026, lr 1.601696298338937e-06
Step 614700 Loss: 0.2017, lr 1.601696298338937e-06
Train Epoch: [131/100] Loss: 0.2022,lr 0.000002
Calling G2SDataset.batch()
Done, time:  2.34 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 614800 Loss: 0.8296, lr 1.2994060058218793e-06
Step 614900 Loss: 0.6205, lr 1.2994060058218793e-06
Step 615000 Loss: 0.5420, lr 1.2994060058218793e-06
Step 615100 Loss: 0.4923, lr 1.2994060058218793e-06
Step 615200 Loss: 0.4588, lr 1.2994060058218793e-06
Step 615300 Loss: 0.4358, lr 1.2994060058218793e-06
Step 615400 Loss: 0.4128, lr 1.2994060058218793e-06
Step 615500 Loss: 0.3952, lr 1.2994060058218793e-06
Step 615600 Loss: 0.3805, lr 1.2994060058218793e-06
Step 615700 Loss: 0.3668, lr 1.2994060058218793e-06
Step 615800 Loss: 0.3555, lr 1.2994060058218793e-06
Step 615900 Loss: 0.3457, lr 1.2994060058218793e-06
Step 616000 Loss: 0.3367, lr 1.2994060058218793e-06
Step 616100 Loss: 0.3282, lr 1.2994060058218793e-06
Step 616200 Loss: 0.3207, lr 1.2994060058218793e-06
Step 616300 Loss: 0.3146, lr 1.2994060058218793e-06
Step 616400 Loss: 0.3086, lr 1.2994060058218793e-06
Step 616500 Loss: 0.3029, lr 1.2994060058218793e-06
Step 616600 Loss: 0.2974, lr 1.2994060058218793e-06
Step 616700 Loss: 0.2922, lr 1.2994060058218793e-06
Step 616800 Loss: 0.2869, lr 1.2994060058218793e-06
Step 616900 Loss: 0.2822, lr 1.2994060058218793e-06
Step 617000 Loss: 0.2782, lr 1.2994060058218793e-06
Step 617100 Loss: 0.2742, lr 1.2994060058218793e-06
Step 617200 Loss: 0.2708, lr 1.2994060058218793e-06
Step 617300 Loss: 0.2672, lr 1.2994060058218793e-06
Step 617400 Loss: 0.2642, lr 1.2994060058218793e-06
Step 617500 Loss: 0.2618, lr 1.2994060058218793e-06
Step 617600 Loss: 0.2591, lr 1.2994060058218793e-06
Step 617700 Loss: 0.2565, lr 1.2994060058218793e-06
Step 617800 Loss: 0.2540, lr 1.2994060058218793e-06
Step 617900 Loss: 0.2515, lr 1.2994060058218793e-06
Step 618000 Loss: 0.2489, lr 1.2994060058218793e-06
Step 618100 Loss: 0.2463, lr 1.2994060058218793e-06
Step 618200 Loss: 0.2440, lr 1.2994060058218793e-06
Step 618300 Loss: 0.2420, lr 1.2994060058218793e-06
Step 618400 Loss: 0.2401, lr 1.2994060058218793e-06
Step 618500 Loss: 0.2382, lr 1.2994060058218793e-06
Step 618600 Loss: 0.2368, lr 1.2994060058218793e-06
Step 618700 Loss: 0.2354, lr 1.2994060058218793e-06
Step 618800 Loss: 0.2342, lr 1.2994060058218793e-06
Step 618900 Loss: 0.2327, lr 1.2994060058218793e-06
Step 619000 Loss: 0.2315, lr 1.2994060058218793e-06
Step 619100 Loss: 0.2302, lr 1.2994060058218793e-06
Step 619200 Loss: 0.2287, lr 1.2994060058218793e-06
Step 619300 Loss: 0.2272, lr 1.2994060058218793e-06
Step 619400 Loss: 0.2258, lr 1.2994060058218793e-06
Step 619500 Loss: 0.2245, lr 1.2994060058218793e-06
Step 619600 Loss: 0.2234, lr 1.2994060058218793e-06
Step 619700 Loss: 0.2229, lr 1.2994060058218793e-06
Step 619800 Loss: 0.2219, lr 1.2994060058218793e-06
Step 619900 Loss: 0.2214, lr 1.2994060058218793e-06
Step 620000 Loss: 0.2203, lr 1.2994060058218793e-06
Step 620100 Loss: 0.2195, lr 1.2994060058218793e-06
Step 620200 Loss: 0.2185, lr 1.2994060058218793e-06
Step 620300 Loss: 0.2174, lr 1.2994060058218793e-06
Step 620400 Loss: 0.2160, lr 1.2994060058218793e-06
Step 620500 Loss: 0.2149, lr 1.2994060058218793e-06
Step 620600 Loss: 0.2139, lr 1.2994060058218793e-06
Step 620700 Loss: 0.2131, lr 1.2994060058218793e-06
Step 620800 Loss: 0.2125, lr 1.2994060058218793e-06
Step 620900 Loss: 0.2117, lr 1.2994060058218793e-06
Step 621000 Loss: 0.2114, lr 1.2994060058218793e-06
Step 621100 Loss: 0.2114, lr 1.2994060058218793e-06
Step 621200 Loss: 0.2115, lr 1.2994060058218793e-06
Step 621300 Loss: 0.2114, lr 1.2994060058218793e-06
Step 621400 Loss: 0.2105, lr 1.2994060058218793e-06
Step 621500 Loss: 0.2098, lr 1.2994060058218793e-06
Train Epoch: [132/100] Loss: 0.2100,lr 0.000001
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Step 621600 Loss: 0.7109, lr 1.02812939980404e-06
Step 621700 Loss: 0.5998, lr 1.02812939980404e-06
Step 621800 Loss: 0.5430, lr 1.02812939980404e-06
Step 621900 Loss: 0.5020, lr 1.02812939980404e-06
Step 622000 Loss: 0.4746, lr 1.02812939980404e-06
Step 622100 Loss: 0.4515, lr 1.02812939980404e-06
Step 622200 Loss: 0.4299, lr 1.02812939980404e-06
Step 622300 Loss: 0.4138, lr 1.02812939980404e-06
Step 622400 Loss: 0.3986, lr 1.02812939980404e-06
Step 622500 Loss: 0.3852, lr 1.02812939980404e-06
Step 622600 Loss: 0.3739, lr 1.02812939980404e-06
Step 622700 Loss: 0.3639, lr 1.02812939980404e-06
Step 622800 Loss: 0.3549, lr 1.02812939980404e-06
Step 622900 Loss: 0.3454, lr 1.02812939980404e-06
Step 623000 Loss: 0.3379, lr 1.02812939980404e-06
Step 623100 Loss: 0.3316, lr 1.02812939980404e-06
Step 623200 Loss: 0.3253, lr 1.02812939980404e-06
Step 623300 Loss: 0.3188, lr 1.02812939980404e-06
Step 623400 Loss: 0.3129, lr 1.02812939980404e-06
Step 623500 Loss: 0.3068, lr 1.02812939980404e-06
Step 623600 Loss: 0.3008, lr 1.02812939980404e-06
Step 623700 Loss: 0.2960, lr 1.02812939980404e-06
Step 623800 Loss: 0.2916, lr 1.02812939980404e-06
Step 623900 Loss: 0.2872, lr 1.02812939980404e-06
Step 624000 Loss: 0.2827, lr 1.02812939980404e-06
Step 624100 Loss: 0.2789, lr 1.02812939980404e-06
Step 624200 Loss: 0.2754, lr 1.02812939980404e-06
Step 624300 Loss: 0.2722, lr 1.02812939980404e-06
Step 624400 Loss: 0.2690, lr 1.02812939980404e-06
Step 624500 Loss: 0.2660, lr 1.02812939980404e-06
Step 624600 Loss: 0.2629, lr 1.02812939980404e-06
Step 624700 Loss: 0.2599, lr 1.02812939980404e-06
Step 624800 Loss: 0.2572, lr 1.02812939980404e-06
Step 624900 Loss: 0.2542, lr 1.02812939980404e-06
Step 625000 Loss: 0.2516, lr 1.02812939980404e-06
Step 625100 Loss: 0.2493, lr 1.02812939980404e-06
Step 625200 Loss: 0.2472, lr 1.02812939980404e-06
Step 625300 Loss: 0.2453, lr 1.02812939980404e-06
Step 625400 Loss: 0.2435, lr 1.02812939980404e-06
Step 625500 Loss: 0.2419, lr 1.02812939980404e-06
Step 625600 Loss: 0.2403, lr 1.02812939980404e-06
Step 625700 Loss: 0.2390, lr 1.02812939980404e-06
Step 625800 Loss: 0.2374, lr 1.02812939980404e-06
Step 625900 Loss: 0.2359, lr 1.02812939980404e-06
Step 626000 Loss: 0.2345, lr 1.02812939980404e-06
Step 626100 Loss: 0.2329, lr 1.02812939980404e-06
Step 626200 Loss: 0.2314, lr 1.02812939980404e-06
Step 626300 Loss: 0.2302, lr 1.02812939980404e-06
Step 626400 Loss: 0.2295, lr 1.02812939980404e-06
Step 626500 Loss: 0.2289, lr 1.02812939980404e-06
Step 626600 Loss: 0.2282, lr 1.02812939980404e-06
Step 626700 Loss: 0.2278, lr 1.02812939980404e-06
Step 626800 Loss: 0.2271, lr 1.02812939980404e-06
Step 626900 Loss: 0.2266, lr 1.02812939980404e-06
Step 627000 Loss: 0.2262, lr 1.02812939980404e-06
Step 627100 Loss: 0.2256, lr 1.02812939980404e-06
Step 627200 Loss: 0.2248, lr 1.02812939980404e-06
Step 627300 Loss: 0.2241, lr 1.02812939980404e-06
Step 627400 Loss: 0.2234, lr 1.02812939980404e-06
Step 627500 Loss: 0.2224, lr 1.02812939980404e-06
Step 627600 Loss: 0.2216, lr 1.02812939980404e-06
Step 627700 Loss: 0.2209, lr 1.02812939980404e-06
Step 627800 Loss: 0.2207, lr 1.02812939980404e-06
Step 627900 Loss: 0.2207, lr 1.02812939980404e-06
Step 628000 Loss: 0.2207, lr 1.02812939980404e-06
Step 628100 Loss: 0.2203, lr 1.02812939980404e-06
Step 628200 Loss: 0.2192, lr 1.02812939980404e-06
Train Epoch: [133/100] Loss: 0.2190,lr 0.000001
Calling G2SDataset.batch()
Done, time:  2.42 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Step 628300 Loss: 0.8996, lr 7.881341975439346e-07
Step 628400 Loss: 0.6722, lr 7.881341975439346e-07
Step 628500 Loss: 0.5956, lr 7.881341975439346e-07
Step 628600 Loss: 0.5490, lr 7.881341975439346e-07
Step 628700 Loss: 0.5152, lr 7.881341975439346e-07
Step 628800 Loss: 0.4909, lr 7.881341975439346e-07
Step 628900 Loss: 0.4667, lr 7.881341975439346e-07
Step 629000 Loss: 0.4486, lr 7.881341975439346e-07
Step 629100 Loss: 0.4339, lr 7.881341975439346e-07
Step 629200 Loss: 0.4186, lr 7.881341975439346e-07
Step 629300 Loss: 0.4060, lr 7.881341975439346e-07
Step 629400 Loss: 0.3954, lr 7.881341975439346e-07
Step 629500 Loss: 0.3847, lr 7.881341975439346e-07
Step 629600 Loss: 0.3754, lr 7.881341975439346e-07
Step 629700 Loss: 0.3668, lr 7.881341975439346e-07
Step 629800 Loss: 0.3595, lr 7.881341975439346e-07
Step 629900 Loss: 0.3522, lr 7.881341975439346e-07
Step 630000 Loss: 0.3455, lr 7.881341975439346e-07
Step 630100 Loss: 0.3389, lr 7.881341975439346e-07
Step 630200 Loss: 0.3326, lr 7.881341975439346e-07
Step 630300 Loss: 0.3260, lr 7.881341975439346e-07
Step 630400 Loss: 0.3197, lr 7.881341975439346e-07
Step 630500 Loss: 0.3147, lr 7.881341975439346e-07
Step 630600 Loss: 0.3092, lr 7.881341975439346e-07
Step 630700 Loss: 0.3046, lr 7.881341975439346e-07
Step 630800 Loss: 0.2998, lr 7.881341975439346e-07
Step 630900 Loss: 0.2953, lr 7.881341975439346e-07
Step 631000 Loss: 0.2914, lr 7.881341975439346e-07
Step 631100 Loss: 0.2878, lr 7.881341975439346e-07
Step 631200 Loss: 0.2840, lr 7.881341975439346e-07
Step 631300 Loss: 0.2806, lr 7.881341975439346e-07
Step 631400 Loss: 0.2770, lr 7.881341975439346e-07
Step 631500 Loss: 0.2737, lr 7.881341975439346e-07
Step 631600 Loss: 0.2703, lr 7.881341975439346e-07
Step 631700 Loss: 0.2674, lr 7.881341975439346e-07
Step 631800 Loss: 0.2645, lr 7.881341975439346e-07
Step 631900 Loss: 0.2618, lr 7.881341975439346e-07
Step 632000 Loss: 0.2592, lr 7.881341975439346e-07
Step 632100 Loss: 0.2570, lr 7.881341975439346e-07
Step 632200 Loss: 0.2550, lr 7.881341975439346e-07
Step 632300 Loss: 0.2533, lr 7.881341975439346e-07
Step 632400 Loss: 0.2511, lr 7.881341975439346e-07
Step 632500 Loss: 0.2495, lr 7.881341975439346e-07
Step 632600 Loss: 0.2478, lr 7.881341975439346e-07
Step 632700 Loss: 0.2458, lr 7.881341975439346e-07
Step 632800 Loss: 0.2441, lr 7.881341975439346e-07
Step 632900 Loss: 0.2424, lr 7.881341975439346e-07
Step 633000 Loss: 0.2408, lr 7.881341975439346e-07
Step 633100 Loss: 0.2397, lr 7.881341975439346e-07
Step 633200 Loss: 0.2387, lr 7.881341975439346e-07
Step 633300 Loss: 0.2379, lr 7.881341975439346e-07
Step 633400 Loss: 0.2374, lr 7.881341975439346e-07
Step 633500 Loss: 0.2366, lr 7.881341975439346e-07
Step 633600 Loss: 0.2361, lr 7.881341975439346e-07
Step 633700 Loss: 0.2355, lr 7.881341975439346e-07
Step 633800 Loss: 0.2353, lr 7.881341975439346e-07
Step 633900 Loss: 0.2348, lr 7.881341975439346e-07
Step 634000 Loss: 0.2347, lr 7.881341975439346e-07
Step 634100 Loss: 0.2344, lr 7.881341975439346e-07
Step 634200 Loss: 0.2345, lr 7.881341975439346e-07
Step 634300 Loss: 0.2345, lr 7.881341975439346e-07
Step 634400 Loss: 0.2343, lr 7.881341975439346e-07
Step 634500 Loss: 0.2343, lr 7.881341975439346e-07
Step 634600 Loss: 0.2344, lr 7.881341975439346e-07
Step 634700 Loss: 0.2345, lr 7.881341975439346e-07
Step 634800 Loss: 0.2346, lr 7.881341975439346e-07
Step 634900 Loss: 0.2335, lr 7.881341975439346e-07
Step 635000 Loss: 0.2325, lr 7.881341975439346e-07
Train Epoch: [134/100] Loss: 0.2326,lr 0.000001
Calling G2SDataset.batch()
Done, time:  2.35 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.80 s, total batches: 6756
Step 635100 Loss: 0.7632, lr 5.796572453312246e-07
Step 635200 Loss: 0.6592, lr 5.796572453312246e-07
Step 635300 Loss: 0.6036, lr 5.796572453312246e-07
Step 635400 Loss: 0.5629, lr 5.796572453312246e-07
Step 635500 Loss: 0.5345, lr 5.796572453312246e-07
Step 635600 Loss: 0.5103, lr 5.796572453312246e-07
Step 635700 Loss: 0.4874, lr 5.796572453312246e-07
Step 635800 Loss: 0.4708, lr 5.796572453312246e-07
Step 635900 Loss: 0.4559, lr 5.796572453312246e-07
Step 636000 Loss: 0.4421, lr 5.796572453312246e-07
Step 636100 Loss: 0.4310, lr 5.796572453312246e-07
Step 636200 Loss: 0.4206, lr 5.796572453312246e-07
Step 636300 Loss: 0.4117, lr 5.796572453312246e-07
Step 636400 Loss: 0.4012, lr 5.796572453312246e-07
Step 636500 Loss: 0.3930, lr 5.796572453312246e-07
Step 636600 Loss: 0.3860, lr 5.796572453312246e-07
Step 636700 Loss: 0.3788, lr 5.796572453312246e-07
Step 636800 Loss: 0.3719, lr 5.796572453312246e-07
Step 636900 Loss: 0.3655, lr 5.796572453312246e-07
Step 637000 Loss: 0.3585, lr 5.796572453312246e-07
Step 637100 Loss: 0.3518, lr 5.796572453312246e-07
Step 637200 Loss: 0.3458, lr 5.796572453312246e-07
Step 637300 Loss: 0.3402, lr 5.796572453312246e-07
Step 637400 Loss: 0.3344, lr 5.796572453312246e-07
Step 637500 Loss: 0.3290, lr 5.796572453312246e-07
Step 637600 Loss: 0.3237, lr 5.796572453312246e-07
Step 637700 Loss: 0.3186, lr 5.796572453312246e-07
Step 637800 Loss: 0.3144, lr 5.796572453312246e-07
Step 637900 Loss: 0.3098, lr 5.796572453312246e-07
Step 638000 Loss: 0.3058, lr 5.796572453312246e-07
Step 638100 Loss: 0.3018, lr 5.796572453312246e-07
Step 638200 Loss: 0.2975, lr 5.796572453312246e-07
Step 638300 Loss: 0.2939, lr 5.796572453312246e-07
Step 638400 Loss: 0.2899, lr 5.796572453312246e-07
Step 638500 Loss: 0.2864, lr 5.796572453312246e-07
Step 638600 Loss: 0.2830, lr 5.796572453312246e-07
Step 638700 Loss: 0.2799, lr 5.796572453312246e-07
Step 638800 Loss: 0.2769, lr 5.796572453312246e-07
Step 638900 Loss: 0.2742, lr 5.796572453312246e-07
Step 639000 Loss: 0.2716, lr 5.796572453312246e-07
Step 639100 Loss: 0.2694, lr 5.796572453312246e-07
Step 639200 Loss: 0.2671, lr 5.796572453312246e-07
Step 639300 Loss: 0.2649, lr 5.796572453312246e-07
Step 639400 Loss: 0.2626, lr 5.796572453312246e-07
Step 639500 Loss: 0.2607, lr 5.796572453312246e-07
Step 639600 Loss: 0.2584, lr 5.796572453312246e-07
Step 639700 Loss: 0.2562, lr 5.796572453312246e-07
Step 639800 Loss: 0.2544, lr 5.796572453312246e-07
Step 639900 Loss: 0.2530, lr 5.796572453312246e-07
Step 640000 Loss: 0.2519, lr 5.796572453312246e-07
Step 640100 Loss: 0.2507, lr 5.796572453312246e-07
Step 640200 Loss: 0.2500, lr 5.796572453312246e-07
Step 640300 Loss: 0.2490, lr 5.796572453312246e-07
Step 640400 Loss: 0.2482, lr 5.796572453312246e-07
Step 640500 Loss: 0.2477, lr 5.796572453312246e-07
Step 640600 Loss: 0.2471, lr 5.796572453312246e-07
Step 640700 Loss: 0.2465, lr 5.796572453312246e-07
Step 640800 Loss: 0.2461, lr 5.796572453312246e-07
Step 640900 Loss: 0.2461, lr 5.796572453312246e-07
Step 641000 Loss: 0.2462, lr 5.796572453312246e-07
Step 641100 Loss: 0.2466, lr 5.796572453312246e-07
Step 641200 Loss: 0.2470, lr 5.796572453312246e-07
Step 641300 Loss: 0.2478, lr 5.796572453312246e-07
Step 641400 Loss: 0.2485, lr 5.796572453312246e-07
Step 641500 Loss: 0.2494, lr 5.796572453312246e-07
Step 641600 Loss: 0.2497, lr 5.796572453312246e-07
Step 641700 Loss: 0.2493, lr 5.796572453312246e-07
Train Epoch: [135/100] Loss: 0.2496,lr 0.000001
Model Saving at epoch 135
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6755
Step 641800 Loss: 1.1150, lr 4.0290428474802477e-07
Step 641900 Loss: 0.7055, lr 4.0290428474802477e-07
Step 642000 Loss: 0.6188, lr 4.0290428474802477e-07
Step 642100 Loss: 0.5710, lr 4.0290428474802477e-07
Step 642200 Loss: 0.5384, lr 4.0290428474802477e-07
Step 642300 Loss: 0.5141, lr 4.0290428474802477e-07
Step 642400 Loss: 0.4905, lr 4.0290428474802477e-07
Step 642500 Loss: 0.4726, lr 4.0290428474802477e-07
Step 642600 Loss: 0.4583, lr 4.0290428474802477e-07
Step 642700 Loss: 0.4434, lr 4.0290428474802477e-07
Step 642800 Loss: 0.4311, lr 4.0290428474802477e-07
Step 642900 Loss: 0.4212, lr 4.0290428474802477e-07
Step 643000 Loss: 0.4107, lr 4.0290428474802477e-07
Step 643100 Loss: 0.4023, lr 4.0290428474802477e-07
Step 643200 Loss: 0.3934, lr 4.0290428474802477e-07
Step 643300 Loss: 0.3862, lr 4.0290428474802477e-07
Step 643400 Loss: 0.3791, lr 4.0290428474802477e-07
Step 643500 Loss: 0.3726, lr 4.0290428474802477e-07
Step 643600 Loss: 0.3664, lr 4.0290428474802477e-07
Step 643700 Loss: 0.3602, lr 4.0290428474802477e-07
Step 643800 Loss: 0.3535, lr 4.0290428474802477e-07
Step 643900 Loss: 0.3472, lr 4.0290428474802477e-07
Step 644000 Loss: 0.3420, lr 4.0290428474802477e-07
Step 644100 Loss: 0.3367, lr 4.0290428474802477e-07
Step 644200 Loss: 0.3317, lr 4.0290428474802477e-07
Step 644300 Loss: 0.3265, lr 4.0290428474802477e-07
Step 644400 Loss: 0.3218, lr 4.0290428474802477e-07
Step 644500 Loss: 0.3173, lr 4.0290428474802477e-07
Step 644600 Loss: 0.3133, lr 4.0290428474802477e-07
Step 644700 Loss: 0.3090, lr 4.0290428474802477e-07
Step 644800 Loss: 0.3050, lr 4.0290428474802477e-07
Step 644900 Loss: 0.3009, lr 4.0290428474802477e-07
Step 645000 Loss: 0.2971, lr 4.0290428474802477e-07
Step 645100 Loss: 0.2933, lr 4.0290428474802477e-07
Step 645200 Loss: 0.2898, lr 4.0290428474802477e-07
Step 645300 Loss: 0.2862, lr 4.0290428474802477e-07
Step 645400 Loss: 0.2828, lr 4.0290428474802477e-07
Step 645500 Loss: 0.2797, lr 4.0290428474802477e-07
Step 645600 Loss: 0.2769, lr 4.0290428474802477e-07
Step 645700 Loss: 0.2743, lr 4.0290428474802477e-07
Step 645800 Loss: 0.2720, lr 4.0290428474802477e-07
Step 645900 Loss: 0.2694, lr 4.0290428474802477e-07
Step 646000 Loss: 0.2672, lr 4.0290428474802477e-07
Step 646100 Loss: 0.2651, lr 4.0290428474802477e-07
Step 646200 Loss: 0.2627, lr 4.0290428474802477e-07
Step 646300 Loss: 0.2605, lr 4.0290428474802477e-07
Step 646400 Loss: 0.2582, lr 4.0290428474802477e-07
Step 646500 Loss: 0.2562, lr 4.0290428474802477e-07
Step 646600 Loss: 0.2546, lr 4.0290428474802477e-07
Step 646700 Loss: 0.2531, lr 4.0290428474802477e-07
Step 646800 Loss: 0.2519, lr 4.0290428474802477e-07
Step 646900 Loss: 0.2507, lr 4.0290428474802477e-07
Step 647000 Loss: 0.2497, lr 4.0290428474802477e-07
Step 647100 Loss: 0.2489, lr 4.0290428474802477e-07
Step 647200 Loss: 0.2479, lr 4.0290428474802477e-07
Step 647300 Loss: 0.2473, lr 4.0290428474802477e-07
Step 647400 Loss: 0.2465, lr 4.0290428474802477e-07
Step 647500 Loss: 0.2461, lr 4.0290428474802477e-07
Step 647600 Loss: 0.2457, lr 4.0290428474802477e-07
Step 647700 Loss: 0.2458, lr 4.0290428474802477e-07
Step 647800 Loss: 0.2459, lr 4.0290428474802477e-07
Step 647900 Loss: 0.2463, lr 4.0290428474802477e-07
Step 648000 Loss: 0.2470, lr 4.0290428474802477e-07
Step 648100 Loss: 0.2480, lr 4.0290428474802477e-07
Step 648200 Loss: 0.2494, lr 4.0290428474802477e-07
Step 648300 Loss: 0.2504, lr 4.0290428474802477e-07
Step 648400 Loss: 0.2503, lr 4.0290428474802477e-07
Step 648500 Loss: 0.2502, lr 4.0290428474802477e-07
Train Epoch: [136/100] Loss: 0.2506,lr 0.000000
Calling G2SDataset.batch()
Done, time:  2.17 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Step 648600 Loss: 0.7909, lr 2.580497496267766e-07
Step 648700 Loss: 0.6603, lr 2.580497496267766e-07
Step 648800 Loss: 0.6015, lr 2.580497496267766e-07
Step 648900 Loss: 0.5591, lr 2.580497496267766e-07
Step 649000 Loss: 0.5320, lr 2.580497496267766e-07
Step 649100 Loss: 0.5128, lr 2.580497496267766e-07
Step 649200 Loss: 0.4891, lr 2.580497496267766e-07
Step 649300 Loss: 0.4720, lr 2.580497496267766e-07
Step 649400 Loss: 0.4564, lr 2.580497496267766e-07
Step 649500 Loss: 0.4421, lr 2.580497496267766e-07
Step 649600 Loss: 0.4298, lr 2.580497496267766e-07
Step 649700 Loss: 0.4191, lr 2.580497496267766e-07
Step 649800 Loss: 0.4096, lr 2.580497496267766e-07
Step 649900 Loss: 0.3994, lr 2.580497496267766e-07
Step 650000 Loss: 0.3911, lr 2.580497496267766e-07
Step 650100 Loss: 0.3836, lr 2.580497496267766e-07
Step 650200 Loss: 0.3764, lr 2.580497496267766e-07
Step 650300 Loss: 0.3696, lr 2.580497496267766e-07
Step 650400 Loss: 0.3631, lr 2.580497496267766e-07
Step 650500 Loss: 0.3561, lr 2.580497496267766e-07
Step 650600 Loss: 0.3496, lr 2.580497496267766e-07
Step 650700 Loss: 0.3435, lr 2.580497496267766e-07
Step 650800 Loss: 0.3381, lr 2.580497496267766e-07
Step 650900 Loss: 0.3325, lr 2.580497496267766e-07
Step 651000 Loss: 0.3275, lr 2.580497496267766e-07
Step 651100 Loss: 0.3227, lr 2.580497496267766e-07
Step 651200 Loss: 0.3179, lr 2.580497496267766e-07
Step 651300 Loss: 0.3136, lr 2.580497496267766e-07
Step 651400 Loss: 0.3094, lr 2.580497496267766e-07
Step 651500 Loss: 0.3054, lr 2.580497496267766e-07
Step 651600 Loss: 0.3017, lr 2.580497496267766e-07
Step 651700 Loss: 0.2977, lr 2.580497496267766e-07
Step 651800 Loss: 0.2940, lr 2.580497496267766e-07
Step 651900 Loss: 0.2902, lr 2.580497496267766e-07
Step 652000 Loss: 0.2867, lr 2.580497496267766e-07
Step 652100 Loss: 0.2833, lr 2.580497496267766e-07
Step 652200 Loss: 0.2802, lr 2.580497496267766e-07
Step 652300 Loss: 0.2771, lr 2.580497496267766e-07
Step 652400 Loss: 0.2744, lr 2.580497496267766e-07
Step 652500 Loss: 0.2718, lr 2.580497496267766e-07
Step 652600 Loss: 0.2695, lr 2.580497496267766e-07
Step 652700 Loss: 0.2670, lr 2.580497496267766e-07
Step 652800 Loss: 0.2648, lr 2.580497496267766e-07
Step 652900 Loss: 0.2625, lr 2.580497496267766e-07
Step 653000 Loss: 0.2604, lr 2.580497496267766e-07
Step 653100 Loss: 0.2580, lr 2.580497496267766e-07
Step 653200 Loss: 0.2558, lr 2.580497496267766e-07
Step 653300 Loss: 0.2539, lr 2.580497496267766e-07
Step 653400 Loss: 0.2521, lr 2.580497496267766e-07
Step 653500 Loss: 0.2510, lr 2.580497496267766e-07
Step 653600 Loss: 0.2496, lr 2.580497496267766e-07
Step 653700 Loss: 0.2487, lr 2.580497496267766e-07
Step 653800 Loss: 0.2475, lr 2.580497496267766e-07
Step 653900 Loss: 0.2466, lr 2.580497496267766e-07
Step 654000 Loss: 0.2458, lr 2.580497496267766e-07
Step 654100 Loss: 0.2451, lr 2.580497496267766e-07
Step 654200 Loss: 0.2443, lr 2.580497496267766e-07
Step 654300 Loss: 0.2440, lr 2.580497496267766e-07
Step 654400 Loss: 0.2439, lr 2.580497496267766e-07
Step 654500 Loss: 0.2441, lr 2.580497496267766e-07
Step 654600 Loss: 0.2443, lr 2.580497496267766e-07
Step 654700 Loss: 0.2449, lr 2.580497496267766e-07
Step 654800 Loss: 0.2461, lr 2.580497496267766e-07
Step 654900 Loss: 0.2471, lr 2.580497496267766e-07
Step 655000 Loss: 0.2486, lr 2.580497496267766e-07
Step 655100 Loss: 0.2496, lr 2.580497496267766e-07
Step 655200 Loss: 0.2498, lr 2.580497496267766e-07
Step 655300 Loss: 0.2505, lr 2.580497496267766e-07
Train Epoch: [137/100] Loss: 0.2511,lr 0.000000
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Step 655400 Loss: 0.6979, lr 1.4523659390514658e-07
Step 655500 Loss: 0.6118, lr 1.4523659390514658e-07
Step 655600 Loss: 0.5621, lr 1.4523659390514658e-07
Step 655700 Loss: 0.5312, lr 1.4523659390514658e-07
Step 655800 Loss: 0.5091, lr 1.4523659390514658e-07
Step 655900 Loss: 0.4876, lr 1.4523659390514658e-07
Step 656000 Loss: 0.4712, lr 1.4523659390514658e-07
Step 656100 Loss: 0.4546, lr 1.4523659390514658e-07
Step 656200 Loss: 0.4391, lr 1.4523659390514658e-07
Step 656300 Loss: 0.4253, lr 1.4523659390514658e-07
Step 656400 Loss: 0.4143, lr 1.4523659390514658e-07
Step 656500 Loss: 0.4034, lr 1.4523659390514658e-07
Step 656600 Loss: 0.3946, lr 1.4523659390514658e-07
Step 656700 Loss: 0.3848, lr 1.4523659390514658e-07
Step 656800 Loss: 0.3772, lr 1.4523659390514658e-07
Step 656900 Loss: 0.3700, lr 1.4523659390514658e-07
Step 657000 Loss: 0.3633, lr 1.4523659390514658e-07
Step 657100 Loss: 0.3568, lr 1.4523659390514658e-07
Step 657200 Loss: 0.3503, lr 1.4523659390514658e-07
Step 657300 Loss: 0.3433, lr 1.4523659390514658e-07
Step 657400 Loss: 0.3368, lr 1.4523659390514658e-07
Step 657500 Loss: 0.3316, lr 1.4523659390514658e-07
Step 657600 Loss: 0.3263, lr 1.4523659390514658e-07
Step 657700 Loss: 0.3212, lr 1.4523659390514658e-07
Step 657800 Loss: 0.3159, lr 1.4523659390514658e-07
Step 657900 Loss: 0.3114, lr 1.4523659390514658e-07
Step 658000 Loss: 0.3071, lr 1.4523659390514658e-07
Step 658100 Loss: 0.3033, lr 1.4523659390514658e-07
Step 658200 Loss: 0.2990, lr 1.4523659390514658e-07
Step 658300 Loss: 0.2954, lr 1.4523659390514658e-07
Step 658400 Loss: 0.2916, lr 1.4523659390514658e-07
Step 658500 Loss: 0.2879, lr 1.4523659390514658e-07
Step 658600 Loss: 0.2843, lr 1.4523659390514658e-07
Step 658700 Loss: 0.2809, lr 1.4523659390514658e-07
Step 658800 Loss: 0.2775, lr 1.4523659390514658e-07
Step 658900 Loss: 0.2743, lr 1.4523659390514658e-07
Step 659000 Loss: 0.2712, lr 1.4523659390514658e-07
Step 659100 Loss: 0.2685, lr 1.4523659390514658e-07
Step 659200 Loss: 0.2658, lr 1.4523659390514658e-07
Step 659300 Loss: 0.2636, lr 1.4523659390514658e-07
Step 659400 Loss: 0.2610, lr 1.4523659390514658e-07
Step 659500 Loss: 0.2590, lr 1.4523659390514658e-07
Step 659600 Loss: 0.2568, lr 1.4523659390514658e-07
Step 659700 Loss: 0.2545, lr 1.4523659390514658e-07
Step 659800 Loss: 0.2525, lr 1.4523659390514658e-07
Step 659900 Loss: 0.2502, lr 1.4523659390514658e-07
Step 660000 Loss: 0.2482, lr 1.4523659390514658e-07
Step 660100 Loss: 0.2463, lr 1.4523659390514658e-07
Step 660200 Loss: 0.2449, lr 1.4523659390514658e-07
Step 660300 Loss: 0.2437, lr 1.4523659390514658e-07
Step 660400 Loss: 0.2425, lr 1.4523659390514658e-07
Step 660500 Loss: 0.2415, lr 1.4523659390514658e-07
Step 660600 Loss: 0.2407, lr 1.4523659390514658e-07
Step 660700 Loss: 0.2396, lr 1.4523659390514658e-07
Step 660800 Loss: 0.2391, lr 1.4523659390514658e-07
Step 660900 Loss: 0.2383, lr 1.4523659390514658e-07
Step 661000 Loss: 0.2380, lr 1.4523659390514658e-07
Step 661100 Loss: 0.2377, lr 1.4523659390514658e-07
Step 661200 Loss: 0.2377, lr 1.4523659390514658e-07
Step 661300 Loss: 0.2379, lr 1.4523659390514658e-07
Step 661400 Loss: 0.2385, lr 1.4523659390514658e-07
Step 661500 Loss: 0.2393, lr 1.4523659390514658e-07
Step 661600 Loss: 0.2405, lr 1.4523659390514658e-07
Step 661700 Loss: 0.2423, lr 1.4523659390514658e-07
Step 661800 Loss: 0.2437, lr 1.4523659390514658e-07
Step 661900 Loss: 0.2443, lr 1.4523659390514658e-07
Step 662000 Loss: 0.2448, lr 1.4523659390514658e-07
Train Epoch: [138/100] Loss: 0.2459,lr 0.000000
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 662100 Loss: 0.8179, lr 6.457615054773745e-08
Step 662200 Loss: 0.6475, lr 6.457615054773745e-08
Step 662300 Loss: 0.5790, lr 6.457615054773745e-08
Step 662400 Loss: 0.5391, lr 6.457615054773745e-08
Step 662500 Loss: 0.5110, lr 6.457615054773745e-08
Step 662600 Loss: 0.4933, lr 6.457615054773745e-08
Step 662700 Loss: 0.4725, lr 6.457615054773745e-08
Step 662800 Loss: 0.4554, lr 6.457615054773745e-08
Step 662900 Loss: 0.4402, lr 6.457615054773745e-08
Step 663000 Loss: 0.4258, lr 6.457615054773745e-08
Step 663100 Loss: 0.4139, lr 6.457615054773745e-08
Step 663200 Loss: 0.4036, lr 6.457615054773745e-08
Step 663300 Loss: 0.3937, lr 6.457615054773745e-08
Step 663400 Loss: 0.3842, lr 6.457615054773745e-08
Step 663500 Loss: 0.3759, lr 6.457615054773745e-08
Step 663600 Loss: 0.3690, lr 6.457615054773745e-08
Step 663700 Loss: 0.3624, lr 6.457615054773745e-08
Step 663800 Loss: 0.3559, lr 6.457615054773745e-08
Step 663900 Loss: 0.3493, lr 6.457615054773745e-08
Step 664000 Loss: 0.3430, lr 6.457615054773745e-08
Step 664100 Loss: 0.3365, lr 6.457615054773745e-08
Step 664200 Loss: 0.3309, lr 6.457615054773745e-08
Step 664300 Loss: 0.3258, lr 6.457615054773745e-08
Step 664400 Loss: 0.3205, lr 6.457615054773745e-08
Step 664500 Loss: 0.3157, lr 6.457615054773745e-08
Step 664600 Loss: 0.3109, lr 6.457615054773745e-08
Step 664700 Loss: 0.3064, lr 6.457615054773745e-08
Step 664800 Loss: 0.3027, lr 6.457615054773745e-08
Step 664900 Loss: 0.2986, lr 6.457615054773745e-08
Step 665000 Loss: 0.2948, lr 6.457615054773745e-08
Step 665100 Loss: 0.2914, lr 6.457615054773745e-08
Step 665200 Loss: 0.2876, lr 6.457615054773745e-08
Step 665300 Loss: 0.2842, lr 6.457615054773745e-08
Step 665400 Loss: 0.2806, lr 6.457615054773745e-08
Step 665500 Loss: 0.2772, lr 6.457615054773745e-08
Step 665600 Loss: 0.2741, lr 6.457615054773745e-08
Step 665700 Loss: 0.2710, lr 6.457615054773745e-08
Step 665800 Loss: 0.2679, lr 6.457615054773745e-08
Step 665900 Loss: 0.2655, lr 6.457615054773745e-08
Step 666000 Loss: 0.2629, lr 6.457615054773745e-08
Step 666100 Loss: 0.2605, lr 6.457615054773745e-08
Step 666200 Loss: 0.2583, lr 6.457615054773745e-08
Step 666300 Loss: 0.2561, lr 6.457615054773745e-08
Step 666400 Loss: 0.2540, lr 6.457615054773745e-08
Step 666500 Loss: 0.2519, lr 6.457615054773745e-08
Step 666600 Loss: 0.2497, lr 6.457615054773745e-08
Step 666700 Loss: 0.2476, lr 6.457615054773745e-08
Step 666800 Loss: 0.2455, lr 6.457615054773745e-08
Step 666900 Loss: 0.2438, lr 6.457615054773745e-08
Step 667000 Loss: 0.2426, lr 6.457615054773745e-08
Step 667100 Loss: 0.2412, lr 6.457615054773745e-08
Step 667200 Loss: 0.2402, lr 6.457615054773745e-08
Step 667300 Loss: 0.2391, lr 6.457615054773745e-08
Step 667400 Loss: 0.2382, lr 6.457615054773745e-08
Step 667500 Loss: 0.2373, lr 6.457615054773745e-08
Step 667600 Loss: 0.2366, lr 6.457615054773745e-08
Step 667700 Loss: 0.2360, lr 6.457615054773745e-08
Step 667800 Loss: 0.2356, lr 6.457615054773745e-08
Step 667900 Loss: 0.2354, lr 6.457615054773745e-08
Step 668000 Loss: 0.2355, lr 6.457615054773745e-08
Step 668100 Loss: 0.2359, lr 6.457615054773745e-08
Step 668200 Loss: 0.2366, lr 6.457615054773745e-08
Step 668300 Loss: 0.2377, lr 6.457615054773745e-08
Step 668400 Loss: 0.2390, lr 6.457615054773745e-08
Step 668500 Loss: 0.2410, lr 6.457615054773745e-08
Step 668600 Loss: 0.2422, lr 6.457615054773745e-08
Step 668700 Loss: 0.2424, lr 6.457615054773745e-08
Step 668800 Loss: 0.2434, lr 6.457615054773745e-08
Train Epoch: [139/100] Loss: 0.2442,lr 0.000000
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 668900 Loss: 0.6990, lr 1.6148021673887392e-08
Step 669000 Loss: 0.6070, lr 1.6148021673887392e-08
Step 669100 Loss: 0.5566, lr 1.6148021673887392e-08
Step 669200 Loss: 0.5219, lr 1.6148021673887392e-08
Step 669300 Loss: 0.4990, lr 1.6148021673887392e-08
Step 669400 Loss: 0.4802, lr 1.6148021673887392e-08
Step 669500 Loss: 0.4618, lr 1.6148021673887392e-08
Step 669600 Loss: 0.4461, lr 1.6148021673887392e-08
Step 669700 Loss: 0.4314, lr 1.6148021673887392e-08
Step 669800 Loss: 0.4181, lr 1.6148021673887392e-08
Step 669900 Loss: 0.4069, lr 1.6148021673887392e-08
Step 670000 Loss: 0.3965, lr 1.6148021673887392e-08
Step 670100 Loss: 0.3875, lr 1.6148021673887392e-08
Step 670200 Loss: 0.3774, lr 1.6148021673887392e-08
Step 670300 Loss: 0.3703, lr 1.6148021673887392e-08
Step 670400 Loss: 0.3633, lr 1.6148021673887392e-08
Step 670500 Loss: 0.3573, lr 1.6148021673887392e-08
Step 670600 Loss: 0.3511, lr 1.6148021673887392e-08
Step 670700 Loss: 0.3447, lr 1.6148021673887392e-08
Step 670800 Loss: 0.3382, lr 1.6148021673887392e-08
Step 670900 Loss: 0.3320, lr 1.6148021673887392e-08
Step 671000 Loss: 0.3269, lr 1.6148021673887392e-08
Step 671100 Loss: 0.3219, lr 1.6148021673887392e-08
Step 671200 Loss: 0.3170, lr 1.6148021673887392e-08
Step 671300 Loss: 0.3121, lr 1.6148021673887392e-08
Step 671400 Loss: 0.3077, lr 1.6148021673887392e-08
Step 671500 Loss: 0.3036, lr 1.6148021673887392e-08
Step 671600 Loss: 0.2996, lr 1.6148021673887392e-08
Step 671700 Loss: 0.2958, lr 1.6148021673887392e-08
Step 671800 Loss: 0.2923, lr 1.6148021673887392e-08
Step 671900 Loss: 0.2887, lr 1.6148021673887392e-08
Step 672000 Loss: 0.2852, lr 1.6148021673887392e-08
Step 672100 Loss: 0.2819, lr 1.6148021673887392e-08
Step 672200 Loss: 0.2784, lr 1.6148021673887392e-08
Step 672300 Loss: 0.2750, lr 1.6148021673887392e-08
Step 672400 Loss: 0.2720, lr 1.6148021673887392e-08
Step 672500 Loss: 0.2692, lr 1.6148021673887392e-08
Step 672600 Loss: 0.2665, lr 1.6148021673887392e-08
Step 672700 Loss: 0.2637, lr 1.6148021673887392e-08
Step 672800 Loss: 0.2616, lr 1.6148021673887392e-08
Step 672900 Loss: 0.2590, lr 1.6148021673887392e-08
Step 673000 Loss: 0.2571, lr 1.6148021673887392e-08
Step 673100 Loss: 0.2548, lr 1.6148021673887392e-08
Step 673200 Loss: 0.2529, lr 1.6148021673887392e-08
Step 673300 Loss: 0.2508, lr 1.6148021673887392e-08
Step 673400 Loss: 0.2486, lr 1.6148021673887392e-08
Step 673500 Loss: 0.2465, lr 1.6148021673887392e-08
Step 673600 Loss: 0.2447, lr 1.6148021673887392e-08
Step 673700 Loss: 0.2431, lr 1.6148021673887392e-08
Step 673800 Loss: 0.2417, lr 1.6148021673887392e-08
Step 673900 Loss: 0.2404, lr 1.6148021673887392e-08
Step 674000 Loss: 0.2394, lr 1.6148021673887392e-08
Step 674100 Loss: 0.2384, lr 1.6148021673887392e-08
Step 674200 Loss: 0.2374, lr 1.6148021673887392e-08
Step 674300 Loss: 0.2366, lr 1.6148021673887392e-08
Step 674400 Loss: 0.2359, lr 1.6148021673887392e-08
Step 674500 Loss: 0.2353, lr 1.6148021673887392e-08
Step 674600 Loss: 0.2349, lr 1.6148021673887392e-08
Step 674700 Loss: 0.2349, lr 1.6148021673887392e-08
Step 674800 Loss: 0.2350, lr 1.6148021673887392e-08
Step 674900 Loss: 0.2354, lr 1.6148021673887392e-08
Step 675000 Loss: 0.2362, lr 1.6148021673887392e-08
Step 675100 Loss: 0.2379, lr 1.6148021673887392e-08
Step 675200 Loss: 0.2394, lr 1.6148021673887392e-08
Step 675300 Loss: 0.2410, lr 1.6148021673887392e-08
Step 675400 Loss: 0.2416, lr 1.6148021673887392e-08
Step 675500 Loss: 0.2421, lr 1.6148021673887392e-08
Train Epoch: [140/100] Loss: 0.2439,lr 0.000000
Model Saving at epoch 140
