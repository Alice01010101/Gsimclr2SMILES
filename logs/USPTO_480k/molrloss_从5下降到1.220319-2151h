Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1/molr_loss_epoch5.pt*
**** embed_size = *256*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *256*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *256*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *100*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *256*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *False*
**** epoch = *200*
**** max_steps = *300000*
**** warmup_steps = *8000*
**** lr = *4.0*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *8192*
**** valid_batch_size = *8192*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *8192*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loaded pretrained state_dict from ./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1/molr_loss_epoch5.pt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=361, out_features=256, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=370, out_features=256, bias=True)
        (W_r): Linear(in_features=114, out_features=256, bias=False)
        (U_r): Linear(in_features=256, out_features=256, bias=True)
        (W_h): Linear(in_features=370, out_features=256, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=256, bias=True)
        (attn_W_k): Linear(in_features=256, out_features=256, bias=True)
        (attn_W_v): Linear(in_features=256, out_features=256, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=256, bias=True)
      (attn_W_k): Linear(in_features=256, out_features=256, bias=True)
      (attn_W_v): Linear(in_features=256, out_features=256, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (g): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=128, bias=True)
  )
)
Number of parameters = 1813648
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  1.48 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.49 s, total batches: 3396
Step 100 Loss: 2.3582
Step 200 Loss: 2.4131
Step 300 Loss: 2.4327
Step 400 Loss: 2.4459
Step 500 Loss: 2.4481
Step 600 Loss: 2.4494
Step 700 Loss: 2.4509
Step 800 Loss: 2.4529
Step 900 Loss: 2.4499
Step 1000 Loss: 2.4505
Step 1100 Loss: 2.4509
Step 1200 Loss: 2.4550
Step 1300 Loss: 2.4598
Step 1400 Loss: 2.4664
Step 1500 Loss: 2.4729
Step 1600 Loss: 2.4791
Step 1700 Loss: 2.4872
Step 1800 Loss: 2.4954
Step 1900 Loss: 2.5047
Step 2000 Loss: 2.5157
Step 2100 Loss: 2.5279
Step 2200 Loss: 2.5398
Step 2300 Loss: 2.5534
Step 2400 Loss: 2.5688
Step 2500 Loss: 2.5850
Step 2600 Loss: 2.6028
Step 2700 Loss: 2.6215
Step 2800 Loss: 2.6417
Step 2900 Loss: 2.6629
Step 3000 Loss: 2.6845
Step 3100 Loss: 2.7067
Step 3200 Loss: 2.7371
Step 3300 Loss: 2.7791
Train Epoch: [7/200] Loss: 2.8099
Calling G2SDataset.batch()
Done, time:  1.47 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.51 s, total batches: 3396
Step 3400 Loss: 2.0951
Step 3500 Loss: 2.2496
Step 3600 Loss: 2.3031
Step 3700 Loss: 2.3286
Step 3800 Loss: 2.3492
Step 3900 Loss: 2.3564
Step 4000 Loss: 2.3625
Step 4100 Loss: 2.3705
Step 4200 Loss: 2.3775
Step 4300 Loss: 2.3792
Step 4400 Loss: 2.3832
Step 4500 Loss: 2.3876
Step 4600 Loss: 2.3941
Step 4700 Loss: 2.4010
Step 4800 Loss: 2.4092
Step 4900 Loss: 2.4180
Step 5000 Loss: 2.4263
Step 5100 Loss: 2.4354
Step 5200 Loss: 2.4450
Step 5300 Loss: 2.4550
Step 5400 Loss: 2.4667
Step 5500 Loss: 2.4799
Step 5600 Loss: 2.4930
Step 5700 Loss: 2.5074
Step 5800 Loss: 2.5228
Step 5900 Loss: 2.5393
Step 6000 Loss: 2.5570
Step 6100 Loss: 2.5756
Step 6200 Loss: 2.5959
Step 6300 Loss: 2.6170
Step 6400 Loss: 2.6388
Step 6500 Loss: 2.6606
Step 6600 Loss: 2.6882
Step 6700 Loss: 2.7231
Train Epoch: [8/200] Loss: 2.7497
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 3396
Step 6800 Loss: 2.1223
Step 6900 Loss: 2.2298
Step 7000 Loss: 2.2754
Step 7100 Loss: 2.2988
Step 7200 Loss: 2.3161
Step 7300 Loss: 2.3241
Step 7400 Loss: 2.3286
Step 7500 Loss: 2.3347
Step 7600 Loss: 2.3400
Step 7700 Loss: 2.3404
Step 7800 Loss: 2.3441
Step 7900 Loss: 2.3481
Step 8000 Loss: 2.3539
Step 8100 Loss: 2.3609
Step 8200 Loss: 2.3684
Step 8300 Loss: 2.3774
Step 8400 Loss: 2.3853
Step 8500 Loss: 2.3944
Step 8600 Loss: 2.4044
Step 8700 Loss: 2.4143
Step 8800 Loss: 2.4259
Step 8900 Loss: 2.4390
Step 9000 Loss: 2.4520
Step 9100 Loss: 2.4658
Step 9200 Loss: 2.4809
Step 9300 Loss: 2.4971
Step 9400 Loss: 2.5145
Step 9500 Loss: 2.5329
Step 9600 Loss: 2.5531
Step 9700 Loss: 2.5738
Step 9800 Loss: 2.5956
Step 9900 Loss: 2.6168
Step 10000 Loss: 2.6419
Step 10100 Loss: 2.6717
Train Epoch: [9/200] Loss: 2.6954
Calling G2SDataset.batch()
Done, time:  1.48 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.49 s, total batches: 3396
Step 10200 Loss: 2.1205
Step 10300 Loss: 2.2074
Step 10400 Loss: 2.2439
Step 10500 Loss: 2.2633
Step 10600 Loss: 2.2776
Step 10700 Loss: 2.2838
Step 10800 Loss: 2.2856
Step 10900 Loss: 2.2894
Step 11000 Loss: 2.2938
Step 11100 Loss: 2.2927
Step 11200 Loss: 2.2955
Step 11300 Loss: 2.2993
Step 11400 Loss: 2.3041
Step 11500 Loss: 2.3103
Step 11600 Loss: 2.3179
Step 11700 Loss: 2.3263
Step 11800 Loss: 2.3347
Step 11900 Loss: 2.3436
Step 12000 Loss: 2.3534
Step 12100 Loss: 2.3633
Step 12200 Loss: 2.3747
Step 12300 Loss: 2.3870
Step 12400 Loss: 2.3997
Step 12500 Loss: 2.4133
Step 12600 Loss: 2.4281
Step 12700 Loss: 2.4434
Step 12800 Loss: 2.4605
Step 12900 Loss: 2.4788
Step 13000 Loss: 2.4993
Step 13100 Loss: 2.5200
Step 13200 Loss: 2.5421
Step 13300 Loss: 2.5629
Step 13400 Loss: 2.5859
Step 13500 Loss: 2.6132
Train Epoch: [10/200] Loss: 2.6356
Model Saving at epoch 10
Calling G2SDataset.batch()
Done, time:  1.45 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.49 s, total batches: 3396
Step 13600 Loss: 2.1171
Step 13700 Loss: 2.1685
Step 13800 Loss: 2.1971
Step 13900 Loss: 2.2123
Step 14000 Loss: 2.2224
Step 14100 Loss: 2.2252
Step 14200 Loss: 2.2258
Step 14300 Loss: 2.2281
Step 14400 Loss: 2.2326
Step 14500 Loss: 2.2311
Step 14600 Loss: 2.2337
Step 14700 Loss: 2.2381
Step 14800 Loss: 2.2427
Step 14900 Loss: 2.2492
Step 15000 Loss: 2.2565
Step 15100 Loss: 2.2643
Step 15200 Loss: 2.2721
Step 15300 Loss: 2.2806
Step 15400 Loss: 2.2899
Step 15500 Loss: 2.2988
Step 15600 Loss: 2.3098
Step 15700 Loss: 2.3217
Step 15800 Loss: 2.3338
Step 15900 Loss: 2.3468
Step 16000 Loss: 2.3609
Step 16100 Loss: 2.3759
Step 16200 Loss: 2.3927
Step 16300 Loss: 2.4108
Step 16400 Loss: 2.4307
Step 16500 Loss: 2.4518
Step 16600 Loss: 2.4744
Step 16700 Loss: 2.4952
Step 16800 Loss: 2.5181
Step 16900 Loss: 2.5455
Train Epoch: [11/200] Loss: 2.5686
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  2.10 s, total batches: 3396
Step 17000 Loss: 2.0664
Step 17100 Loss: 2.1196
Step 17200 Loss: 2.1427
Step 17300 Loss: 2.1532
Step 17400 Loss: 2.1598
Step 17500 Loss: 2.1614
Step 17600 Loss: 2.1605
Step 17700 Loss: 2.1617
Step 17800 Loss: 2.1645
Step 17900 Loss: 2.1632
Step 18000 Loss: 2.1647
Step 18100 Loss: 2.1684
Step 18200 Loss: 2.1726
Step 18300 Loss: 2.1792
Step 18400 Loss: 2.1859
Step 18500 Loss: 2.1930
Step 18600 Loss: 2.2000
Step 18700 Loss: 2.2077
Step 18800 Loss: 2.2167
Step 18900 Loss: 2.2249
Step 19000 Loss: 2.2350
Step 19100 Loss: 2.2454
Step 19200 Loss: 2.2574
Step 19300 Loss: 2.2695
Step 19400 Loss: 2.2826
Step 19500 Loss: 2.2972
Step 19600 Loss: 2.3135
Step 19700 Loss: 2.3318
Step 19800 Loss: 2.3522
Step 19900 Loss: 2.3745
Step 20000 Loss: 2.3981
Step 20100 Loss: 2.4196
Step 20200 Loss: 2.4436
Step 20300 Loss: 2.4733
Train Epoch: [12/200] Loss: 2.4983
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 3396
Step 20400 Loss: 2.0201
Step 20500 Loss: 2.0583
Step 20600 Loss: 2.0786
Step 20700 Loss: 2.0874
Step 20800 Loss: 2.0925
Step 20900 Loss: 2.0945
Step 21000 Loss: 2.0915
Step 21100 Loss: 2.0924
Step 21200 Loss: 2.0945
Step 21300 Loss: 2.0930
Step 21400 Loss: 2.0934
Step 21500 Loss: 2.0969
Step 21600 Loss: 2.1002
Step 21700 Loss: 2.1062
Step 21800 Loss: 2.1120
Step 21900 Loss: 2.1187
Step 22000 Loss: 2.1246
Step 22100 Loss: 2.1313
Step 22200 Loss: 2.1394
Step 22300 Loss: 2.1473
Step 22400 Loss: 2.1569
Step 22500 Loss: 2.1668
Step 22600 Loss: 2.1782
Step 22700 Loss: 2.1895
Step 22800 Loss: 2.2021
Step 22900 Loss: 2.2162
Step 23000 Loss: 2.2326
Step 23100 Loss: 2.2512
Step 23200 Loss: 2.2723
Step 23300 Loss: 2.2958
Step 23400 Loss: 2.3211
Step 23500 Loss: 2.3453
Step 23600 Loss: 2.3716
Step 23700 Loss: 2.4056
Train Epoch: [13/200] Loss: 2.4338
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 3396
Step 23800 Loss: 1.9766
Step 23900 Loss: 2.0132
Step 24000 Loss: 2.0326
Step 24100 Loss: 2.0380
Step 24200 Loss: 2.0419
Step 24300 Loss: 2.0400
Step 24400 Loss: 2.0349
Step 24500 Loss: 2.0330
Step 24600 Loss: 2.0344
Step 24700 Loss: 2.0319
Step 24800 Loss: 2.0310
Step 24900 Loss: 2.0332
Step 25000 Loss: 2.0358
Step 25100 Loss: 2.0411
Step 25200 Loss: 2.0454
Step 25300 Loss: 2.0506
Step 25400 Loss: 2.0553
Step 25500 Loss: 2.0618
Step 25600 Loss: 2.0694
Step 25700 Loss: 2.0769
Step 25800 Loss: 2.0865
Step 25900 Loss: 2.0953
Step 26000 Loss: 2.1064
Step 26100 Loss: 2.1174
Step 26200 Loss: 2.1299
Step 26300 Loss: 2.1445
Step 26400 Loss: 2.1619
Step 26500 Loss: 2.1808
Step 26600 Loss: 2.2034
Step 26700 Loss: 2.2286
Step 26800 Loss: 2.2563
Step 26900 Loss: 2.2825
Step 27000 Loss: 2.3109
Step 27100 Loss: 2.3511
Train Epoch: [14/200] Loss: 2.3819
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 3396
Step 27200 Loss: 1.9734
Step 27300 Loss: 1.9921
Step 27400 Loss: 2.0033
Step 27500 Loss: 2.0045
Step 27600 Loss: 2.0059
Step 27700 Loss: 2.0036
Step 27800 Loss: 1.9959
Step 27900 Loss: 1.9915
Step 28000 Loss: 1.9904
Step 28100 Loss: 1.9876
Step 28200 Loss: 1.9848
Step 28300 Loss: 1.9863
Step 28400 Loss: 1.9882
Step 28500 Loss: 1.9932
Step 28600 Loss: 1.9967
Step 28700 Loss: 2.0016
Step 28800 Loss: 2.0055
Step 28900 Loss: 2.0114
Step 29000 Loss: 2.0184
Step 29100 Loss: 2.0257
Step 29200 Loss: 2.0348
Step 29300 Loss: 2.0434
Step 29400 Loss: 2.0547
Step 29500 Loss: 2.0654
Step 29600 Loss: 2.0782
Step 29700 Loss: 2.0929
Step 29800 Loss: 2.1102
Step 29900 Loss: 2.1298
Step 30000 Loss: 2.1530
Step 30100 Loss: 2.1801
Step 30200 Loss: 2.2096
Step 30300 Loss: 2.2380
Step 30400 Loss: 2.2684
Step 30500 Loss: 2.3115
Train Epoch: [15/200] Loss: 2.3430
Model Saving at epoch 15
Calling G2SDataset.batch()
Done, time:  1.80 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 3396
Step 30600 Loss: 1.9741
Step 30700 Loss: 1.9941
Step 30800 Loss: 1.9986
Step 30900 Loss: 1.9940
Step 31000 Loss: 1.9901
Step 31100 Loss: 1.9835
Step 31200 Loss: 1.9740
Step 31300 Loss: 1.9703
Step 31400 Loss: 1.9657
Step 31500 Loss: 1.9612
Step 31600 Loss: 1.9570
Step 31700 Loss: 1.9560
Step 31800 Loss: 1.9558
Step 31900 Loss: 1.9594
Step 32000 Loss: 1.9625
Step 32100 Loss: 1.9669
Step 32200 Loss: 1.9704
Step 32300 Loss: 1.9753
Step 32400 Loss: 1.9813
Step 32500 Loss: 1.9883
Step 32600 Loss: 1.9967
Step 32700 Loss: 2.0054
Step 32800 Loss: 2.0161
Step 32900 Loss: 2.0268
Step 33000 Loss: 2.0400
Step 33100 Loss: 2.0547
Step 33200 Loss: 2.0725
Step 33300 Loss: 2.0928
Step 33400 Loss: 2.1165
Step 33500 Loss: 2.1452
Step 33600 Loss: 2.1759
Step 33700 Loss: 2.2060
Step 33800 Loss: 2.2379
Step 33900 Loss: 2.2848
Train Epoch: [16/200] Loss: 2.3161
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 3396
Step 34000 Loss: 1.9790
Step 34100 Loss: 1.9972
Step 34200 Loss: 1.9941
Step 34300 Loss: 1.9857
Step 34400 Loss: 1.9797
Step 34500 Loss: 1.9701
Step 34600 Loss: 1.9596
Step 34700 Loss: 1.9529
Step 34800 Loss: 1.9479
Step 34900 Loss: 1.9412
Step 35000 Loss: 1.9356
Step 35100 Loss: 1.9335
Step 35200 Loss: 1.9326
Step 35300 Loss: 1.9360
Step 35400 Loss: 1.9381
Step 35500 Loss: 1.9414
Step 35600 Loss: 1.9446
Step 35700 Loss: 1.9486
Step 35800 Loss: 1.9541
Step 35900 Loss: 1.9612
Step 36000 Loss: 1.9698
Step 36100 Loss: 1.9782
Step 36200 Loss: 1.9890
Step 36300 Loss: 1.9993
Step 36400 Loss: 2.0123
Step 36500 Loss: 2.0277
Step 36600 Loss: 2.0457
Step 36700 Loss: 2.0664
Step 36800 Loss: 2.0911
Step 36900 Loss: 2.1203
Step 37000 Loss: 2.1520
Step 37100 Loss: 2.1830
Step 37200 Loss: 2.2158
Step 37300 Loss: 2.2653
Train Epoch: [17/200] Loss: 2.2943
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 3396
Step 37400 Loss: 1.9940
Step 37500 Loss: 2.0005
Step 37600 Loss: 1.9938
Step 37700 Loss: 1.9830
Step 37800 Loss: 1.9725
Step 37900 Loss: 1.9612
Step 38000 Loss: 1.9485
Step 38100 Loss: 1.9386
Step 38200 Loss: 1.9311
Step 38300 Loss: 1.9252
Step 38400 Loss: 1.9175
Step 38500 Loss: 1.9161
Step 38600 Loss: 1.9155
Step 38700 Loss: 1.9197
Step 38800 Loss: 1.9213
Step 38900 Loss: 1.9243
Step 39000 Loss: 1.9266
Step 39100 Loss: 1.9314
Step 39200 Loss: 1.9375
Step 39300 Loss: 1.9440
Step 39400 Loss: 1.9518
Step 39500 Loss: 1.9594
Step 39600 Loss: 1.9701
Step 39700 Loss: 1.9801
Step 39800 Loss: 1.9929
Step 39900 Loss: 2.0079
Step 40000 Loss: 2.0265
Step 40100 Loss: 2.0472
Step 40200 Loss: 2.0718
Step 40300 Loss: 2.1021
Step 40400 Loss: 2.1347
Step 40500 Loss: 2.1668
Step 40600 Loss: 2.2010
Step 40700 Loss: 2.2504
Train Epoch: [18/200] Loss: 2.2786
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 3396
Step 40800 Loss: 2.0224
Step 40900 Loss: 2.0186
Step 41000 Loss: 2.0056
Step 41100 Loss: 1.9902
Step 41200 Loss: 1.9739
Step 41300 Loss: 1.9582
Step 41400 Loss: 1.9450
Step 41500 Loss: 1.9343
Step 41600 Loss: 1.9246
Step 41700 Loss: 1.9161
Step 41800 Loss: 1.9083
Step 41900 Loss: 1.9064
Step 42000 Loss: 1.9040
Step 42100 Loss: 1.9071
Step 42200 Loss: 1.9076
Step 42300 Loss: 1.9093
Step 42400 Loss: 1.9113
Step 42500 Loss: 1.9157
Step 42600 Loss: 1.9213
Step 42700 Loss: 1.9277
Step 42800 Loss: 1.9354
Step 42900 Loss: 1.9439
Step 43000 Loss: 1.9546
Step 43100 Loss: 1.9646
Step 43200 Loss: 1.9776
Step 43300 Loss: 1.9931
Step 43400 Loss: 2.0121
Step 43500 Loss: 2.0334
Step 43600 Loss: 2.0585
Step 43700 Loss: 2.0891
Step 43800 Loss: 2.1218
Step 43900 Loss: 2.1545
Step 44000 Loss: 2.1896
Step 44100 Loss: 2.2419
Train Epoch: [19/200] Loss: 2.2669
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.51 s, total batches: 3396
Step 44200 Loss: 2.0551
Step 44300 Loss: 2.0420
Step 44400 Loss: 2.0214
Step 44500 Loss: 1.9997
Step 44600 Loss: 1.9826
Step 44700 Loss: 1.9664
Step 44800 Loss: 1.9495
Step 44900 Loss: 1.9352
Step 45000 Loss: 1.9265
Step 45100 Loss: 1.9173
Step 45200 Loss: 1.9084
Step 45300 Loss: 1.9045
Step 45400 Loss: 1.9011
Step 45500 Loss: 1.9031
Step 45600 Loss: 1.9042
Step 45700 Loss: 1.9057
Step 45800 Loss: 1.9075
Step 45900 Loss: 1.9116
Step 46000 Loss: 1.9160
Step 46100 Loss: 1.9222
Step 46200 Loss: 1.9291
Step 46300 Loss: 1.9366
Step 46400 Loss: 1.9456
Step 46500 Loss: 1.9555
Step 46600 Loss: 1.9680
Step 46700 Loss: 1.9823
Step 46800 Loss: 2.0013
Step 46900 Loss: 2.0224
Step 47000 Loss: 2.0482
Step 47100 Loss: 2.0796
Step 47200 Loss: 2.1128
Step 47300 Loss: 2.1459
Step 47400 Loss: 2.1812
Step 47500 Loss: 2.2370
Train Epoch: [20/200] Loss: 2.2596
Model Saving at epoch 20
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 3396
Step 47600 Loss: 2.0848
Step 47700 Loss: 2.0623
Step 47800 Loss: 2.0412
Step 47900 Loss: 2.0205
Step 48000 Loss: 1.9982
Step 48100 Loss: 1.9765
Step 48200 Loss: 1.9578
Step 48300 Loss: 1.9437
Step 48400 Loss: 1.9316
Step 48500 Loss: 1.9199
Step 48600 Loss: 1.9094
Step 48700 Loss: 1.9043
Step 48800 Loss: 1.9012
Step 48900 Loss: 1.9025
Step 49000 Loss: 1.9017
Step 49100 Loss: 1.9028
Step 49200 Loss: 1.9042
Step 49300 Loss: 1.9081
Step 49400 Loss: 1.9117
Step 49500 Loss: 1.9168
Step 49600 Loss: 1.9232
Step 49700 Loss: 1.9304
Step 49800 Loss: 1.9392
Step 49900 Loss: 1.9481
Step 50000 Loss: 1.9604
Step 50100 Loss: 1.9756
Step 50200 Loss: 1.9944
Step 50300 Loss: 2.0158
Step 50400 Loss: 2.0416
Step 50500 Loss: 2.0737
Step 50600 Loss: 2.1071
Step 50700 Loss: 2.1402
Step 50800 Loss: 2.1762
Step 50900 Loss: 2.2319
Train Epoch: [21/200] Loss: nan
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 3396
Step 51000 Loss: 2.0986
Step 51100 Loss: 2.0675
Step 51200 Loss: 2.0444
Step 51300 Loss: 2.0192
Step 51400 Loss: 1.9948
Step 51500 Loss: 1.9725
Step 51600 Loss: 1.9547
Step 51700 Loss: 1.9397
Step 51800 Loss: 1.9268
Step 51900 Loss: 1.9154
Step 52000 Loss: 1.9051
Step 52100 Loss: 1.9007
Step 52200 Loss: 1.8949
Step 52300 Loss: 1.8962
Step 52400 Loss: 1.8954
Step 52500 Loss: 1.8952
Step 52600 Loss: 1.8950
Step 52700 Loss: 1.8988
Step 52800 Loss: 1.9025
Step 52900 Loss: 1.9081
Step 53000 Loss: 1.9157
Step 53100 Loss: 1.9230
Step 53200 Loss: 1.9323
Step 53300 Loss: 1.9415
Step 53400 Loss: 1.9532
Step 53500 Loss: 1.9687
Step 53600 Loss: 1.9872
Step 53700 Loss: 2.0086
Step 53800 Loss: 2.0349
Step 53900 Loss: 2.0668
Step 54000 Loss: 2.1011
Step 54100 Loss: 2.1344
Step 54200 Loss: 2.1710
Step 54300 Loss: 2.2265
Train Epoch: [22/200] Loss: 2.2442
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 3396
Step 54400 Loss: 2.1111
Step 54500 Loss: 2.0748
Step 54600 Loss: 2.0494
Step 54700 Loss: 2.0271
Step 54800 Loss: 2.0046
Step 54900 Loss: 1.9803
Step 55000 Loss: 1.9602
Step 55100 Loss: 1.9439
Step 55200 Loss: 1.9302
Step 55300 Loss: 1.9176
Step 55400 Loss: 1.9060
Step 55500 Loss: 1.8998
Step 55600 Loss: 1.8947
Step 55700 Loss: 1.8950
Step 55800 Loss: 1.8930
Step 55900 Loss: 1.8924
Step 56000 Loss: 1.8918
Step 56100 Loss: 1.8954
Step 56200 Loss: 1.8985
Step 56300 Loss: 1.9033
Step 56400 Loss: 1.9098
Step 56500 Loss: 1.9168
Step 56600 Loss: 1.9264
Step 56700 Loss: 1.9358
Step 56800 Loss: 1.9476
Step 56900 Loss: 1.9637
Step 57000 Loss: 1.9821
Step 57100 Loss: 2.0037
Step 57200 Loss: 2.0301
Step 57300 Loss: 2.0624
Step 57400 Loss: 2.0961
Step 57500 Loss: 2.1300
Step 57600 Loss: 2.1673
Step 57700 Loss: 2.2220
Train Epoch: [23/200] Loss: 2.2373
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 3396
Step 57800 Loss: 2.1362
Step 57900 Loss: 2.0909
Step 58000 Loss: 2.0611
Step 58100 Loss: 2.0330
Step 58200 Loss: 2.0103
Step 58300 Loss: 1.9824
Step 58400 Loss: 1.9607
Step 58500 Loss: 1.9427
Step 58600 Loss: 1.9276
Step 58700 Loss: 1.9142
Step 58800 Loss: 1.9026
Step 58900 Loss: 1.8962
Step 59000 Loss: 1.8920
Step 59100 Loss: 1.8940
Step 59200 Loss: 1.8924
Step 59300 Loss: 1.8921
Step 59400 Loss: 1.8912
Step 59500 Loss: 1.8955
Step 59600 Loss: 1.8986
Step 59700 Loss: 1.9045
Step 59800 Loss: 1.9105
Step 59900 Loss: 1.9165
Step 60000 Loss: 1.9256
Step 60100 Loss: 1.9343
Step 60200 Loss: 1.9466
Step 60300 Loss: 1.9622
Step 60400 Loss: 1.9804
Step 60500 Loss: 2.0024
Step 60600 Loss: 2.0285
Step 60700 Loss: 2.0612
Step 60800 Loss: 2.0951
Step 60900 Loss: 2.1287
Step 61000 Loss: 2.1668
Step 61100 Loss: 2.2216
Train Epoch: [24/200] Loss: 2.2356
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 3396
Step 61200 Loss: 2.1459
Step 61300 Loss: 2.1012
Step 61400 Loss: 2.0669
Step 61500 Loss: 2.0381
Step 61600 Loss: 2.0150
Step 61700 Loss: 1.9867
Step 61800 Loss: 1.9650
Step 61900 Loss: 1.9459
Step 62000 Loss: 1.9306
Step 62100 Loss: 1.9179
Step 62200 Loss: 1.9052
Step 62300 Loss: 1.8987
Step 62400 Loss: 1.8939
Step 62500 Loss: 1.8938
Step 62600 Loss: 1.8909
Step 62700 Loss: 1.8901
Step 62800 Loss: 1.8900
Step 62900 Loss: 1.8933
Step 63000 Loss: 1.8960
Step 63100 Loss: 1.9020
Step 63200 Loss: 1.9077
Step 63300 Loss: 1.9145
Step 63400 Loss: 1.9232
Step 63500 Loss: 1.9316
Step 63600 Loss: 1.9438
Step 63700 Loss: 1.9588
Step 63800 Loss: 1.9773
Step 63900 Loss: 1.9993
Step 64000 Loss: 2.0254
Step 64100 Loss: 2.0582
Step 64200 Loss: 2.0918
Step 64300 Loss: 2.1250
Step 64400 Loss: 2.1631
Step 64500 Loss: 2.2192
Train Epoch: [25/200] Loss: 2.2307
Model Saving at epoch 25
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 3396
Step 64600 Loss: 2.1526
Step 64700 Loss: 2.1108
Step 64800 Loss: 2.0750
Step 64900 Loss: 2.0421
Step 65000 Loss: 2.0159
Step 65100 Loss: 1.9874
Step 65200 Loss: 1.9658
Step 65300 Loss: 1.9456
Step 65400 Loss: 1.9301
Step 65500 Loss: 1.9161
Step 65600 Loss: 1.9038
Step 65700 Loss: 1.8976
Step 65800 Loss: 1.8918
Step 65900 Loss: 1.8907
Step 66000 Loss: 1.8886
Step 66100 Loss: 1.8877
Step 66200 Loss: 1.8875
Step 66300 Loss: 1.8905
Step 66400 Loss: 1.8926
Step 66500 Loss: 1.8986
Step 66600 Loss: 1.9041
Step 66700 Loss: 1.9104
Step 66800 Loss: 1.9185
Step 66900 Loss: 1.9279
Step 67000 Loss: 1.9395
Step 67100 Loss: 1.9547
Step 67200 Loss: 1.9734
Step 67300 Loss: 1.9955
Step 67400 Loss: 2.0219
Step 67500 Loss: 2.0548
Step 67600 Loss: 2.0886
Step 67700 Loss: 2.1217
Step 67800 Loss: 2.1602
Step 67900 Loss: 2.2156
Train Epoch: [26/200] Loss: 2.2247
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 3396
Step 68000 Loss: 2.1545
Step 68100 Loss: 2.1080
Step 68200 Loss: 2.0694
Step 68300 Loss: 2.0406
Step 68400 Loss: 2.0144
Step 68500 Loss: 1.9842
Step 68600 Loss: 1.9614
Step 68700 Loss: 1.9418
Step 68800 Loss: 1.9261
Step 68900 Loss: 1.9132
Step 69000 Loss: 1.9009
Step 69100 Loss: 1.8939
Step 69200 Loss: 1.8889
Step 69300 Loss: 1.8877
Step 69400 Loss: 1.8864
Step 69500 Loss: 1.8851
Step 69600 Loss: 1.8842
Step 69700 Loss: 1.8869
Step 69800 Loss: 1.8889
Step 69900 Loss: 1.8945
Step 70000 Loss: 1.9000
Step 70100 Loss: 1.9063
Step 70200 Loss: 1.9148
Step 70300 Loss: 1.9234
Step 70400 Loss: 1.9356
Step 70500 Loss: 1.9512
Step 70600 Loss: 1.9697
Step 70700 Loss: 1.9925
Step 70800 Loss: 2.0190
Step 70900 Loss: 2.0519
Step 71000 Loss: 2.0857
Step 71100 Loss: 2.1186
Step 71200 Loss: 2.1582
Step 71300 Loss: 2.2150
Train Epoch: [27/200] Loss: 2.2219
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 3396
Step 71400 Loss: 2.1850
Step 71500 Loss: 2.1307
Step 71600 Loss: 2.0885
Step 71700 Loss: 2.0565
Step 71800 Loss: 2.0272
Step 71900 Loss: 1.9966
Step 72000 Loss: 1.9731
Step 72100 Loss: 1.9521
Step 72200 Loss: 1.9338
Step 72300 Loss: 1.9192
Step 72400 Loss: 1.9061
Step 72500 Loss: 1.8986
Step 72600 Loss: 1.8927
Step 72700 Loss: 1.8902
Step 72800 Loss: 1.8876
Step 72900 Loss: 1.8855
Step 73000 Loss: 1.8848
Step 73100 Loss: 1.8873
Step 73200 Loss: 1.8894
Step 73300 Loss: 1.8947
Step 73400 Loss: 1.8996
Step 73500 Loss: 1.9069
Step 73600 Loss: 1.9150
Step 73700 Loss: 1.9240
Step 73800 Loss: 1.9358
Step 73900 Loss: 1.9511
Step 74000 Loss: 1.9694
Step 74100 Loss: 1.9914
Step 74200 Loss: 2.0179
Step 74300 Loss: 2.0512
Step 74400 Loss: 2.0842
Step 74500 Loss: 2.1171
Step 74600 Loss: 2.1562
Step 74700 Loss: 2.2116
Train Epoch: [28/200] Loss: 2.2167
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 3396
Step 74800 Loss: 2.1666
Step 74900 Loss: 2.1166
Step 75000 Loss: 2.0744
Step 75100 Loss: 2.0431
Step 75200 Loss: 2.0156
Step 75300 Loss: 1.9867
Step 75400 Loss: 1.9636
Step 75500 Loss: 1.9428
Step 75600 Loss: 1.9248
Step 75700 Loss: 1.9101
Step 75800 Loss: 1.8974
Step 75900 Loss: 1.8901
Step 76000 Loss: 1.8865
Step 76100 Loss: 1.8849
Step 76200 Loss: 1.8821
Step 76300 Loss: 1.8803
Step 76400 Loss: 1.8794
Step 76500 Loss: 1.8828
Step 76600 Loss: 1.8848
Step 76700 Loss: 1.8904
Step 76800 Loss: 1.8956
Step 76900 Loss: 1.9019
Step 77000 Loss: 1.9102
Step 77100 Loss: 1.9191
Step 77200 Loss: 1.9313
Step 77300 Loss: 1.9470
Step 77400 Loss: 1.9660
Step 77500 Loss: 1.9885
Step 77600 Loss: 2.0150
Step 77700 Loss: 2.0483
Step 77800 Loss: 2.0812
Step 77900 Loss: 2.1142
Step 78000 Loss: 2.1546
Step 78100 Loss: 2.2087
Train Epoch: [29/200] Loss: 2.2123
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 3396
Step 78200 Loss: 2.1744
Step 78300 Loss: 2.1267
Step 78400 Loss: 2.0824
Step 78500 Loss: 2.0490
Step 78600 Loss: 2.0193
Step 78700 Loss: 1.9867
Step 78800 Loss: 1.9661
Step 78900 Loss: 1.9450
Step 79000 Loss: 1.9278
Step 79100 Loss: 1.9130
Step 79200 Loss: 1.9000
Step 79300 Loss: 1.8913
Step 79400 Loss: 1.8865
Step 79500 Loss: 1.8848
Step 79600 Loss: 1.8814
Step 79700 Loss: 1.8804
Step 79800 Loss: 1.8797
Step 79900 Loss: 1.8817
Step 80000 Loss: 1.8845
Step 80100 Loss: 1.8889
Step 80200 Loss: 1.8937
Step 80300 Loss: 1.9004
Step 80400 Loss: 1.9083
Step 80500 Loss: 1.9172
Step 80600 Loss: 1.9299
Step 80700 Loss: 1.9461
Step 80800 Loss: 1.9645
Step 80900 Loss: 1.9870
Step 81000 Loss: 2.0137
Step 81100 Loss: 2.0470
Step 81200 Loss: 2.0797
Step 81300 Loss: 2.1124
Step 81400 Loss: 2.1524
Step 81500 Loss: 2.2072
Train Epoch: [30/200] Loss: 2.2087
Model Saving at epoch 30
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 3396
Step 81600 Loss: 2.1859
Step 81700 Loss: 2.1301
Step 81800 Loss: 2.0826
Step 81900 Loss: 2.0493
Step 82000 Loss: 2.0163
Step 82100 Loss: 1.9862
Step 82200 Loss: 1.9611
Step 82300 Loss: 1.9428
Step 82400 Loss: 1.9248
Step 82500 Loss: 1.9088
Step 82600 Loss: 1.8960
Step 82700 Loss: 1.8879
Step 82800 Loss: 1.8829
Step 82900 Loss: 1.8800
Step 83000 Loss: 1.8780
Step 83100 Loss: 1.8754
Step 83200 Loss: 1.8754
Step 83300 Loss: 1.8776
Step 83400 Loss: 1.8809
Step 83500 Loss: 1.8851
Step 83600 Loss: 1.8901
Step 83700 Loss: 1.8967
Step 83800 Loss: 1.9053
Step 83900 Loss: 1.9145
Step 84000 Loss: 1.9277
Step 84100 Loss: 1.9439
Step 84200 Loss: 1.9621
Step 84300 Loss: 1.9851
Step 84400 Loss: 2.0121
Step 84500 Loss: 2.0454
Step 84600 Loss: 2.0776
Step 84700 Loss: 2.1104
Step 84800 Loss: 2.1525
Step 84900 Loss: 2.2036
Train Epoch: [31/200] Loss: 2.2036
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 3396
Step 85000 Loss: 2.1776
Step 85100 Loss: 2.1238
Step 85200 Loss: 2.0766
Step 85300 Loss: 2.0450
Step 85400 Loss: 2.0145
Step 85500 Loss: 1.9833
Step 85600 Loss: 1.9613
Step 85700 Loss: 1.9409
Step 85800 Loss: 1.9232
Step 85900 Loss: 1.9085
Step 86000 Loss: 1.8964
Step 86100 Loss: 1.8893
Step 86200 Loss: 1.8842
Step 86300 Loss: 1.8799
Step 86400 Loss: 1.8778
Step 86500 Loss: 1.8745
Step 86600 Loss: 1.8737
Step 86700 Loss: 1.8753
Step 86800 Loss: 1.8778
Step 86900 Loss: 1.8814
Step 87000 Loss: 1.8862
Step 87100 Loss: 1.8930
Step 87200 Loss: 1.9007
Step 87300 Loss: 1.9098
Step 87400 Loss: 1.9228
Step 87500 Loss: 1.9390
Step 87600 Loss: 1.9581
Step 87700 Loss: 1.9806
Step 87800 Loss: 2.0079
Step 87900 Loss: 2.0411
Step 88000 Loss: 2.0729
Step 88100 Loss: 2.1062
Step 88200 Loss: 2.1476
Train Epoch: [32/200] Loss: 2.1978
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 3396
Step 88300 Loss: 2.5476
Step 88400 Loss: 2.1887
Step 88500 Loss: 2.1297
Step 88600 Loss: 2.0835
Step 88700 Loss: 2.0500
Step 88800 Loss: 2.0209
Step 88900 Loss: 1.9884
Step 89000 Loss: 1.9637
Step 89100 Loss: 1.9425
Step 89200 Loss: 1.9233
Step 89300 Loss: 1.9082
Step 89400 Loss: 1.8953
Step 89500 Loss: 1.8871
Step 89600 Loss: 1.8817
Step 89700 Loss: 1.8790
Step 89800 Loss: 1.8759
Step 89900 Loss: 1.8722
Step 90000 Loss: 1.8728
Step 90100 Loss: 1.8740
Step 90200 Loss: 1.8762
Step 90300 Loss: 1.8807
Step 90400 Loss: 1.8845
Step 90500 Loss: 1.8920
Step 90600 Loss: 1.8991
Step 90700 Loss: 1.9090
Step 90800 Loss: 1.9216
Step 90900 Loss: 1.9384
Step 91000 Loss: 1.9581
Step 91100 Loss: 1.9806
Step 91200 Loss: 2.0078
Step 91300 Loss: 2.0407
Step 91400 Loss: 2.0726
Step 91500 Loss: 2.1048
Step 91600 Loss: 2.1486
Train Epoch: [33/200] Loss: 2.1956
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 3396
Step 91700 Loss: 2.4013
Step 91800 Loss: 2.1898
Step 91900 Loss: 2.1350
Step 92000 Loss: 2.0857
Step 92100 Loss: 2.0489
Step 92200 Loss: 2.0176
Step 92300 Loss: 1.9827
Step 92400 Loss: 1.9567
Step 92500 Loss: 1.9342
Step 92600 Loss: 1.9162
Step 92700 Loss: 1.9009
Step 92800 Loss: 1.8883
Step 92900 Loss: 1.8802
Step 93000 Loss: 1.8747
Step 93100 Loss: 1.8723
Step 93200 Loss: 1.8704
Step 93300 Loss: 1.8672
Step 93400 Loss: 1.8678
Step 93500 Loss: 1.8697
Step 93600 Loss: 1.8720
Step 93700 Loss: 1.8772
Step 93800 Loss: 1.8810
Step 93900 Loss: 1.8872
Step 94000 Loss: 1.8957
Step 94100 Loss: 1.9054
Step 94200 Loss: 1.9177
Step 94300 Loss: 1.9342
Step 94400 Loss: 1.9533
Step 94500 Loss: 1.9765
Step 94600 Loss: 2.0038
Step 94700 Loss: 2.0370
Step 94800 Loss: 2.0686
Step 94900 Loss: 2.1005
Step 95000 Loss: 2.1433
Train Epoch: [34/200] Loss: 2.1894
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 3396
Step 95100 Loss: 2.3425
Step 95200 Loss: 2.1828
Step 95300 Loss: 2.1322
Step 95400 Loss: 2.0842
Step 95500 Loss: 2.0461
Step 95600 Loss: 2.0149
Step 95700 Loss: 1.9799
Step 95800 Loss: 1.9574
Step 95900 Loss: 1.9373
Step 96000 Loss: 1.9183
Step 96100 Loss: 1.9022
Step 96200 Loss: 1.8893
Step 96300 Loss: 1.8807
Step 96400 Loss: 1.8769
Step 96500 Loss: 1.8747
Step 96600 Loss: 1.8713
Step 96700 Loss: 1.8679
Step 96800 Loss: 1.8693
Step 96900 Loss: 1.8708
Step 97000 Loss: 1.8728
Step 97100 Loss: 1.8775
Step 97200 Loss: 1.8814
Step 97300 Loss: 1.8880
Step 97400 Loss: 1.8958
Step 97500 Loss: 1.9059
Step 97600 Loss: 1.9183
Step 97700 Loss: 1.9352
Step 97800 Loss: 1.9543
Step 97900 Loss: 1.9774
Step 98000 Loss: 2.0049
Step 98100 Loss: 2.0375
Step 98200 Loss: 2.0691
Step 98300 Loss: 2.1009
Step 98400 Loss: 2.1450
Train Epoch: [35/200] Loss: 2.1869
Model Saving at epoch 35
Calling G2SDataset.batch()
Done, time:  1.53 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.52 s, total batches: 3396
Step 98500 Loss: 2.3136
Step 98600 Loss: 2.1746
Step 98700 Loss: 2.1191
Step 98800 Loss: 2.0735
Step 98900 Loss: 2.0373
Step 99000 Loss: 2.0064
Step 99100 Loss: 1.9723
Step 99200 Loss: 1.9485
Step 99300 Loss: 1.9275
Step 99400 Loss: 1.9081
Step 99500 Loss: 1.8927
Step 99600 Loss: 1.8806
Step 99700 Loss: 1.8720
Step 99800 Loss: 1.8675
Step 99900 Loss: 1.8645
Step 100000 Loss: 1.8616
Step 100100 Loss: 1.8577
Step 100200 Loss: 1.8588
Step 100300 Loss: 1.8609
Step 100400 Loss: 1.8628
Step 100500 Loss: 1.8679
Step 100600 Loss: 1.8737
Step 100700 Loss: 1.8817
Step 100800 Loss: 1.8900
Step 100900 Loss: 1.8997
Step 101000 Loss: 1.9127
Step 101100 Loss: 1.9290
Step 101200 Loss: 1.9487
Step 101300 Loss: 1.9716
Step 101400 Loss: 1.9992
Step 101500 Loss: 2.0321
Step 101600 Loss: 2.0638
Step 101700 Loss: 2.0954
Step 101800 Loss: 2.1396
Train Epoch: [36/200] Loss: 2.1801
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 3396
Step 101900 Loss: 2.2957
Step 102000 Loss: 2.1772
Step 102100 Loss: 2.1200
Step 102200 Loss: 2.0749
Step 102300 Loss: 2.0354
Step 102400 Loss: 2.0030
Step 102500 Loss: 1.9678
Step 102600 Loss: 1.9441
Step 102700 Loss: 1.9239
Step 102800 Loss: 1.9074
Step 102900 Loss: 1.8917
Step 103000 Loss: 1.8804
Step 103100 Loss: 1.8717
Step 103200 Loss: 1.8690
Step 103300 Loss: 1.8653
Step 103400 Loss: 1.8631
Step 103500 Loss: 1.8594
Step 103600 Loss: 1.8599
Step 103700 Loss: 1.8620
Step 103800 Loss: 1.8631
Step 103900 Loss: 1.8676
Step 104000 Loss: 1.8724
Step 104100 Loss: 1.8799
Step 104200 Loss: 1.8872
Step 104300 Loss: 1.8971
Step 104400 Loss: 1.9091
Step 104500 Loss: 1.9264
Step 104600 Loss: 1.9460
Step 104700 Loss: 1.9692
Step 104800 Loss: 1.9975
Step 104900 Loss: 2.0299
Step 105000 Loss: 2.0619
Step 105100 Loss: 2.0935
Step 105200 Loss: 2.1380
Train Epoch: [37/200] Loss: 2.1760
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 3396
Step 105300 Loss: 2.2717
Step 105400 Loss: 2.1699
Step 105500 Loss: 2.1176
Step 105600 Loss: 2.0711
Step 105700 Loss: 2.0337
Step 105800 Loss: 2.0003
Step 105900 Loss: 1.9658
Step 106000 Loss: 1.9415
Step 106100 Loss: 1.9233
Step 106200 Loss: 1.9052
Step 106300 Loss: 1.8898
Step 106400 Loss: 1.8787
Step 106500 Loss: 1.8706
Step 106600 Loss: 1.8667
Step 106700 Loss: 1.8636
Step 106800 Loss: 1.8616
Step 106900 Loss: 1.8573
Step 107000 Loss: 1.8586
Step 107100 Loss: 1.8607
Step 107200 Loss: 1.8625
Step 107300 Loss: 1.8664
Step 107400 Loss: 1.8708
Step 107500 Loss: 1.8784
Step 107600 Loss: 1.8857
Step 107700 Loss: 1.8964
Step 107800 Loss: 1.9092
Step 107900 Loss: 1.9260
Step 108000 Loss: 1.9457
Step 108100 Loss: 1.9686
Step 108200 Loss: 1.9968
Step 108300 Loss: 2.0291
Step 108400 Loss: 2.0608
Step 108500 Loss: 2.0936
Step 108600 Loss: 2.1374
Train Epoch: [38/200] Loss: 2.1732
Calling G2SDataset.batch()
Done, time:  2.30 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  2.33 s, total batches: 3396
Step 108700 Loss: 2.2498
Step 108800 Loss: 2.1616
Step 108900 Loss: 2.1082
Step 109000 Loss: 2.0629
Step 109100 Loss: 2.0285
Step 109200 Loss: 1.9958
Step 109300 Loss: 1.9630
Step 109400 Loss: 1.9406
Step 109500 Loss: 1.9195
Step 109600 Loss: 1.9016
Step 109700 Loss: 1.8852
Step 109800 Loss: 1.8747
Step 109900 Loss: 1.8665
Step 110000 Loss: 1.8631
Step 110100 Loss: 1.8579
Step 110200 Loss: 1.8554
Step 110300 Loss: 1.8525
Step 110400 Loss: 1.8536
Step 110500 Loss: 1.8555
Step 110600 Loss: 1.8577
Step 110700 Loss: 1.8618
Step 110800 Loss: 1.8663
Step 110900 Loss: 1.8742
Step 111000 Loss: 1.8816
Step 111100 Loss: 1.8923
Step 111200 Loss: 1.9054
Step 111300 Loss: 1.9229
Step 111400 Loss: 1.9424
Step 111500 Loss: 1.9661
Step 111600 Loss: 1.9948
Step 111700 Loss: 2.0266
Step 111800 Loss: 2.0583
Step 111900 Loss: 2.0900
Step 112000 Loss: 2.1356
Train Epoch: [39/200] Loss: 2.1688
Calling G2SDataset.batch()
Done, time:  1.50 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.50 s, total batches: 3396
Step 112100 Loss: 2.2475
Step 112200 Loss: 2.1564
Step 112300 Loss: 2.0992
Step 112400 Loss: 2.0556
Step 112500 Loss: 2.0181
Step 112600 Loss: 1.9866
Step 112700 Loss: 1.9577
Step 112800 Loss: 1.9330
Step 112900 Loss: 1.9129
Step 113000 Loss: 1.8936
Step 113100 Loss: 1.8787
Step 113200 Loss: 1.8681
Step 113300 Loss: 1.8603
Step 113400 Loss: 1.8577
Step 113500 Loss: 1.8531
Step 113600 Loss: 1.8504
Step 113700 Loss: 1.8470
Step 113800 Loss: 1.8484
Step 113900 Loss: 1.8501
Step 114000 Loss: 1.8515
Step 114100 Loss: 1.8557
Step 114200 Loss: 1.8606
Step 114300 Loss: 1.8684
Step 114400 Loss: 1.8765
Step 114500 Loss: 1.8874
Step 114600 Loss: 1.9008
Step 114700 Loss: 1.9186
Step 114800 Loss: 1.9380
Step 114900 Loss: 1.9617
Step 115000 Loss: 1.9908
Step 115100 Loss: 2.0224
Step 115200 Loss: 2.0541
Step 115300 Loss: 2.0852
Step 115400 Loss: 2.1295
Train Epoch: [40/200] Loss: 2.1600
Model Saving at epoch 40
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Step 115500 Loss: 2.2246
Step 115600 Loss: 2.1315
Step 115700 Loss: 2.0796
Step 115800 Loss: 2.0341
Step 115900 Loss: 2.0014
Step 116000 Loss: 1.9703
Step 116100 Loss: 1.9382
Step 116200 Loss: 1.9141
Step 116300 Loss: 1.8949
Step 116400 Loss: 1.8791
Step 116500 Loss: 1.8620
Step 116600 Loss: 1.8537
Step 116700 Loss: 1.8456
Step 116800 Loss: 1.8443
Step 116900 Loss: 1.8407
Step 117000 Loss: 1.8402
Step 117100 Loss: 1.8372
Step 117200 Loss: 1.8394
Step 117300 Loss: 1.8426
Step 117400 Loss: 1.8452
Step 117500 Loss: 1.8506
Step 117600 Loss: 1.8545
Step 117700 Loss: 1.8624
Step 117800 Loss: 1.8708
Step 117900 Loss: 1.8816
Step 118000 Loss: 1.8957
Step 118100 Loss: 1.9130
Step 118200 Loss: 1.9331
Step 118300 Loss: 1.9564
Step 118400 Loss: 1.9854
Step 118500 Loss: 2.0172
Step 118600 Loss: 2.0490
Step 118700 Loss: 2.0809
Step 118800 Loss: 2.1278
Train Epoch: [41/200] Loss: 2.1566
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Step 118900 Loss: 2.2241
Step 119000 Loss: 2.1395
Step 119100 Loss: 2.0856
Step 119200 Loss: 2.0400
Step 119300 Loss: 2.0030
Step 119400 Loss: 1.9726
Step 119500 Loss: 1.9432
Step 119600 Loss: 1.9211
Step 119700 Loss: 1.9017
Step 119800 Loss: 1.8848
Step 119900 Loss: 1.8692
Step 120000 Loss: 1.8577
Step 120100 Loss: 1.8504
Step 120200 Loss: 1.8480
Step 120300 Loss: 1.8436
Step 120400 Loss: 1.8420
Step 120500 Loss: 1.8382
Step 120600 Loss: 1.8400
Step 120700 Loss: 1.8417
Step 120800 Loss: 1.8455
Step 120900 Loss: 1.8502
Step 121000 Loss: 1.8545
Step 121100 Loss: 1.8624
Step 121200 Loss: 1.8707
Step 121300 Loss: 1.8820
Step 121400 Loss: 1.8956
Step 121500 Loss: 1.9122
Step 121600 Loss: 1.9333
Step 121700 Loss: 1.9575
Step 121800 Loss: 1.9866
Step 121900 Loss: 2.0178
Step 122000 Loss: 2.0495
Step 122100 Loss: 2.0807
Step 122200 Loss: 2.1277
Train Epoch: [42/200] Loss: 2.1541
Calling G2SDataset.batch()
Done, time:  1.51 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Step 122300 Loss: 2.1984
Step 122400 Loss: 2.1239
Step 122500 Loss: 2.0724
Step 122600 Loss: 2.0304
Step 122700 Loss: 1.9988
Step 122800 Loss: 1.9650
Step 122900 Loss: 1.9358
Step 123000 Loss: 1.9128
Step 123100 Loss: 1.8952
Step 123200 Loss: 1.8782
Step 123300 Loss: 1.8625
Step 123400 Loss: 1.8538
Step 123500 Loss: 1.8431
Step 123600 Loss: 1.8412
Step 123700 Loss: 1.8363
Step 123800 Loss: 1.8339
Step 123900 Loss: 1.8320
Step 124000 Loss: 1.8324
Step 124100 Loss: 1.8339
Step 124200 Loss: 1.8378
Step 124300 Loss: 1.8422
Step 124400 Loss: 1.8470
Step 124500 Loss: 1.8548
Step 124600 Loss: 1.8631
Step 124700 Loss: 1.8742
Step 124800 Loss: 1.8885
Step 124900 Loss: 1.9062
Step 125000 Loss: 1.9269
Step 125100 Loss: 1.9506
Step 125200 Loss: 1.9800
Step 125300 Loss: 2.0113
Step 125400 Loss: 2.0432
Step 125500 Loss: 2.0746
Step 125600 Loss: 2.1206
Train Epoch: [43/200] Loss: 2.1445
Calling G2SDataset.batch()
Done, time:  1.40 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 125700 Loss: 2.1848
Step 125800 Loss: 2.1059
Step 125900 Loss: 2.0533
Step 126000 Loss: 2.0095
Step 126100 Loss: 1.9775
Step 126200 Loss: 1.9442
Step 126300 Loss: 1.9172
Step 126400 Loss: 1.8952
Step 126500 Loss: 1.8779
Step 126600 Loss: 1.8622
Step 126700 Loss: 1.8479
Step 126800 Loss: 1.8401
Step 126900 Loss: 1.8323
Step 127000 Loss: 1.8307
Step 127100 Loss: 1.8264
Step 127200 Loss: 1.8241
Step 127300 Loss: 1.8218
Step 127400 Loss: 1.8233
Step 127500 Loss: 1.8259
Step 127600 Loss: 1.8301
Step 127700 Loss: 1.8352
Step 127800 Loss: 1.8412
Step 127900 Loss: 1.8489
Step 128000 Loss: 1.8586
Step 128100 Loss: 1.8700
Step 128200 Loss: 1.8838
Step 128300 Loss: 1.9016
Step 128400 Loss: 1.9224
Step 128500 Loss: 1.9462
Step 128600 Loss: 1.9761
Step 128700 Loss: 2.0074
Step 128800 Loss: 2.0388
Step 128900 Loss: 2.0708
Step 129000 Loss: 2.1178
Train Epoch: [44/200] Loss: 2.1401
Calling G2SDataset.batch()
Done, time:  1.44 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 129100 Loss: 2.1661
Step 129200 Loss: 2.0974
Step 129300 Loss: 2.0484
Step 129400 Loss: 2.0092
Step 129500 Loss: 1.9774
Step 129600 Loss: 1.9423
Step 129700 Loss: 1.9140
Step 129800 Loss: 1.8919
Step 129900 Loss: 1.8743
Step 130000 Loss: 1.8589
Step 130100 Loss: 1.8442
Step 130200 Loss: 1.8356
Step 130300 Loss: 1.8280
Step 130400 Loss: 1.8269
Step 130500 Loss: 1.8241
Step 130600 Loss: 1.8219
Step 130700 Loss: 1.8198
Step 130800 Loss: 1.8216
Step 130900 Loss: 1.8243
Step 131000 Loss: 1.8291
Step 131100 Loss: 1.8338
Step 131200 Loss: 1.8397
Step 131300 Loss: 1.8471
Step 131400 Loss: 1.8553
Step 131500 Loss: 1.8664
Step 131600 Loss: 1.8815
Step 131700 Loss: 1.8996
Step 131800 Loss: 1.9204
Step 131900 Loss: 1.9449
Step 132000 Loss: 1.9748
Step 132100 Loss: 2.0057
Step 132200 Loss: 2.0370
Step 132300 Loss: 2.0685
Step 132400 Loss: 2.1142
Train Epoch: [45/200] Loss: 2.1349
Model Saving at epoch 45
Calling G2SDataset.batch()
Done, time:  1.80 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.49 s, total batches: 3396
Step 132500 Loss: 2.1607
Step 132600 Loss: 2.0963
Step 132700 Loss: 2.0490
Step 132800 Loss: 2.0083
Step 132900 Loss: 1.9759
Step 133000 Loss: 1.9441
Step 133100 Loss: 1.9165
Step 133200 Loss: 1.8948
Step 133300 Loss: 1.8764
Step 133400 Loss: 1.8600
Step 133500 Loss: 1.8461
Step 133600 Loss: 1.8377
Step 133700 Loss: 1.8294
Step 133800 Loss: 1.8276
Step 133900 Loss: 1.8244
Step 134000 Loss: 1.8216
Step 134100 Loss: 1.8197
Step 134200 Loss: 1.8213
Step 134300 Loss: 1.8237
Step 134400 Loss: 1.8277
Step 134500 Loss: 1.8330
Step 134600 Loss: 1.8384
Step 134700 Loss: 1.8462
Step 134800 Loss: 1.8542
Step 134900 Loss: 1.8652
Step 135000 Loss: 1.8791
Step 135100 Loss: 1.8965
Step 135200 Loss: 1.9172
Step 135300 Loss: 1.9420
Step 135400 Loss: 1.9715
Step 135500 Loss: 2.0024
Step 135600 Loss: 2.0339
Step 135700 Loss: 2.0654
Step 135800 Loss: 2.1125
Train Epoch: [46/200] Loss: 2.1304
Calling G2SDataset.batch()
Done, time:  1.41 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 135900 Loss: 2.1502
Step 136000 Loss: 2.0826
Step 136100 Loss: 2.0382
Step 136200 Loss: 1.9940
Step 136300 Loss: 1.9618
Step 136400 Loss: 1.9293
Step 136500 Loss: 1.9045
Step 136600 Loss: 1.8849
Step 136700 Loss: 1.8655
Step 136800 Loss: 1.8506
Step 136900 Loss: 1.8373
Step 137000 Loss: 1.8282
Step 137100 Loss: 1.8220
Step 137200 Loss: 1.8208
Step 137300 Loss: 1.8180
Step 137400 Loss: 1.8151
Step 137500 Loss: 1.8144
Step 137600 Loss: 1.8164
Step 137700 Loss: 1.8188
Step 137800 Loss: 1.8226
Step 137900 Loss: 1.8280
Step 138000 Loss: 1.8333
Step 138100 Loss: 1.8417
Step 138200 Loss: 1.8505
Step 138300 Loss: 1.8622
Step 138400 Loss: 1.8769
Step 138500 Loss: 1.8946
Step 138600 Loss: 1.9153
Step 138700 Loss: 1.9401
Step 138800 Loss: 1.9699
Step 138900 Loss: 2.0008
Step 139000 Loss: 2.0319
Step 139100 Loss: 2.0647
Step 139200 Loss: 2.1123
Train Epoch: [47/200] Loss: 2.1280
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.44 s, total batches: 3396
Step 139300 Loss: 2.1432
Step 139400 Loss: 2.0738
Step 139500 Loss: 2.0285
Step 139600 Loss: 1.9900
Step 139700 Loss: 1.9578
Step 139800 Loss: 1.9251
Step 139900 Loss: 1.8992
Step 140000 Loss: 1.8784
Step 140100 Loss: 1.8601
Step 140200 Loss: 1.8454
Step 140300 Loss: 1.8322
Step 140400 Loss: 1.8242
Step 140500 Loss: 1.8181
Step 140600 Loss: 1.8173
Step 140700 Loss: 1.8149
Step 140800 Loss: 1.8136
Step 140900 Loss: 1.8131
Step 141000 Loss: 1.8151
Step 141100 Loss: 1.8167
Step 141200 Loss: 1.8201
Step 141300 Loss: 1.8247
Step 141400 Loss: 1.8308
Step 141500 Loss: 1.8400
Step 141600 Loss: 1.8485
Step 141700 Loss: 1.8603
Step 141800 Loss: 1.8743
Step 141900 Loss: 1.8925
Step 142000 Loss: 1.9137
Step 142100 Loss: 1.9382
Step 142200 Loss: 1.9677
Step 142300 Loss: 1.9984
Step 142400 Loss: 2.0299
Step 142500 Loss: 2.0616
Step 142600 Loss: 2.1098
Train Epoch: [48/200] Loss: 2.1238
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Step 142700 Loss: 2.1330
Step 142800 Loss: 2.0655
Step 142900 Loss: 2.0240
Step 143000 Loss: 1.9853
Step 143100 Loss: 1.9551
Step 143200 Loss: 1.9202
Step 143300 Loss: 1.8947
Step 143400 Loss: 1.8746
Step 143500 Loss: 1.8592
Step 143600 Loss: 1.8435
Step 143700 Loss: 1.8278
Step 143800 Loss: 1.8200
Step 143900 Loss: 1.8136
Step 144000 Loss: 1.8134
Step 144100 Loss: 1.8099
Step 144200 Loss: 1.8084
Step 144300 Loss: 1.8069
Step 144400 Loss: 1.8104
Step 144500 Loss: 1.8130
Step 144600 Loss: 1.8184
Step 144700 Loss: 1.8234
Step 144800 Loss: 1.8290
Step 144900 Loss: 1.8367
Step 145000 Loss: 1.8464
Step 145100 Loss: 1.8584
Step 145200 Loss: 1.8738
Step 145300 Loss: 1.8921
Step 145400 Loss: 1.9135
Step 145500 Loss: 1.9382
Step 145600 Loss: 1.9676
Step 145700 Loss: 1.9985
Step 145800 Loss: 2.0294
Step 145900 Loss: 2.0613
Step 146000 Loss: 2.1104
Train Epoch: [49/200] Loss: 2.1225
Calling G2SDataset.batch()
Done, time:  1.42 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.41 s, total batches: 3396
Step 146100 Loss: 2.1207
Step 146200 Loss: 2.0552
Step 146300 Loss: 2.0109
Step 146400 Loss: 1.9720
Step 146500 Loss: 1.9462
Step 146600 Loss: 1.9112
Step 146700 Loss: 1.8885
Step 146800 Loss: 1.8687
Step 146900 Loss: 1.8514
Step 147000 Loss: 1.8370
Step 147100 Loss: 1.8230
Step 147200 Loss: 1.8158
Step 147300 Loss: 1.8098
Step 147400 Loss: 1.8087
Step 147500 Loss: 1.8054
Step 147600 Loss: 1.8039
Step 147700 Loss: 1.8031
Step 147800 Loss: 1.8065
Step 147900 Loss: 1.8084
Step 148000 Loss: 1.8139
Step 148100 Loss: 1.8193
Step 148200 Loss: 1.8246
Step 148300 Loss: 1.8323
Step 148400 Loss: 1.8416
Step 148500 Loss: 1.8536
Step 148600 Loss: 1.8692
Step 148700 Loss: 1.8872
Step 148800 Loss: 1.9084
Step 148900 Loss: 1.9333
Step 149000 Loss: 1.9626
Step 149100 Loss: 1.9934
Step 149200 Loss: 2.0243
Step 149300 Loss: 2.0566
Step 149400 Loss: 2.1049
Train Epoch: [50/200] Loss: 2.1150
Model Saving at epoch 50
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 3396
Step 149500 Loss: 2.0954
Step 149600 Loss: 2.0306
Step 149700 Loss: 1.9903
Step 149800 Loss: 1.9564
Step 149900 Loss: 1.9317
Step 150000 Loss: 1.8986
Step 150100 Loss: 1.8775
Step 150200 Loss: 1.8582
Step 150300 Loss: 1.8414
Step 150400 Loss: 1.8274
Step 150500 Loss: 1.8133
Step 150600 Loss: 1.8063
Step 150700 Loss: 1.8010
Step 150800 Loss: 1.8015
Step 150900 Loss: 1.7997
Step 151000 Loss: 1.7977
Step 151100 Loss: 1.7970
Step 151200 Loss: 1.7999
Step 151300 Loss: 1.8017
Step 151400 Loss: 1.8070
Step 151500 Loss: 1.8122
Step 151600 Loss: 1.8183
Step 151700 Loss: 1.8268
Step 151800 Loss: 1.8363
Step 151900 Loss: 1.8476
Step 152000 Loss: 1.8634
Step 152100 Loss: 1.8819
Step 152200 Loss: 1.9032
Step 152300 Loss: 1.9277
Step 152400 Loss: 1.9575
Step 152500 Loss: 1.9880
Step 152600 Loss: 2.0187
Step 152700 Loss: 2.0524
Step 152800 Loss: 2.1008
Train Epoch: [51/200] Loss: 2.1095
Calling G2SDataset.batch()
Done, time:  1.44 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.44 s, total batches: 3396
Step 152900 Loss: 2.0940
Step 153000 Loss: 2.0360
Step 153100 Loss: 1.9917
Step 153200 Loss: 1.9565
Step 153300 Loss: 1.9285
Step 153400 Loss: 1.8994
Step 153500 Loss: 1.8750
Step 153600 Loss: 1.8562
Step 153700 Loss: 1.8398
Step 153800 Loss: 1.8260
Step 153900 Loss: 1.8119
Step 154000 Loss: 1.8054
Step 154100 Loss: 1.8007
Step 154200 Loss: 1.8003
Step 154300 Loss: 1.7990
Step 154400 Loss: 1.7973
Step 154500 Loss: 1.7965
Step 154600 Loss: 1.7989
Step 154700 Loss: 1.8000
Step 154800 Loss: 1.8053
Step 154900 Loss: 1.8105
Step 155000 Loss: 1.8175
Step 155100 Loss: 1.8253
Step 155200 Loss: 1.8352
Step 155300 Loss: 1.8471
Step 155400 Loss: 1.8627
Step 155500 Loss: 1.8802
Step 155600 Loss: 1.9021
Step 155700 Loss: 1.9268
Step 155800 Loss: 1.9565
Step 155900 Loss: 1.9869
Step 156000 Loss: 2.0176
Step 156100 Loss: 2.0513
Step 156200 Loss: 2.1005
Train Epoch: [52/200] Loss: 2.1076
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.44 s, total batches: 3396
Step 156300 Loss: 2.0887
Step 156400 Loss: 2.0349
Step 156500 Loss: 1.9931
Step 156600 Loss: 1.9535
Step 156700 Loss: 1.9273
Step 156800 Loss: 1.8958
Step 156900 Loss: 1.8733
Step 157000 Loss: 1.8543
Step 157100 Loss: 1.8356
Step 157200 Loss: 1.8223
Step 157300 Loss: 1.8097
Step 157400 Loss: 1.8035
Step 157500 Loss: 1.7976
Step 157600 Loss: 1.7953
Step 157700 Loss: 1.7930
Step 157800 Loss: 1.7917
Step 157900 Loss: 1.7916
Step 158000 Loss: 1.7941
Step 158100 Loss: 1.7959
Step 158200 Loss: 1.8011
Step 158300 Loss: 1.8063
Step 158400 Loss: 1.8125
Step 158500 Loss: 1.8203
Step 158600 Loss: 1.8301
Step 158700 Loss: 1.8431
Step 158800 Loss: 1.8596
Step 158900 Loss: 1.8781
Step 159000 Loss: 1.8993
Step 159100 Loss: 1.9240
Step 159200 Loss: 1.9540
Step 159300 Loss: 1.9840
Step 159400 Loss: 2.0145
Step 159500 Loss: 2.0495
Step 159600 Loss: 2.0968
Train Epoch: [53/200] Loss: 2.1022
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 3396
Step 159700 Loss: 2.0740
Step 159800 Loss: 2.0165
Step 159900 Loss: 1.9739
Step 160000 Loss: 1.9425
Step 160100 Loss: 1.9162
Step 160200 Loss: 1.8866
Step 160300 Loss: 1.8680
Step 160400 Loss: 1.8501
Step 160500 Loss: 1.8349
Step 160600 Loss: 1.8205
Step 160700 Loss: 1.8102
Step 160800 Loss: 1.8025
Step 160900 Loss: 1.7971
Step 161000 Loss: 1.7950
Step 161100 Loss: 1.7927
Step 161200 Loss: 1.7921
Step 161300 Loss: 1.7921
Step 161400 Loss: 1.7943
Step 161500 Loss: 1.7966
Step 161600 Loss: 1.8024
Step 161700 Loss: 1.8074
Step 161800 Loss: 1.8142
Step 161900 Loss: 1.8222
Step 162000 Loss: 1.8317
Step 162100 Loss: 1.8445
Step 162200 Loss: 1.8603
Step 162300 Loss: 1.8780
Step 162400 Loss: 1.8997
Step 162500 Loss: 1.9246
Step 162600 Loss: 1.9543
Step 162700 Loss: 1.9839
Step 162800 Loss: 2.0142
Step 162900 Loss: 2.0505
Step 163000 Loss: 2.0988
Train Epoch: [54/200] Loss: 2.1019
Calling G2SDataset.batch()
Done, time:  1.44 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 163100 Loss: 2.0700
Step 163200 Loss: 2.0165
Step 163300 Loss: 1.9750
Step 163400 Loss: 1.9443
Step 163500 Loss: 1.9161
Step 163600 Loss: 1.8867
Step 163700 Loss: 1.8652
Step 163800 Loss: 1.8476
Step 163900 Loss: 1.8318
Step 164000 Loss: 1.8184
Step 164100 Loss: 1.8081
Step 164200 Loss: 1.7989
Step 164300 Loss: 1.7943
Step 164400 Loss: 1.7928
Step 164500 Loss: 1.7907
Step 164600 Loss: 1.7883
Step 164700 Loss: 1.7901
Step 164800 Loss: 1.7912
Step 164900 Loss: 1.7930
Step 165000 Loss: 1.7988
Step 165100 Loss: 1.8039
Step 165200 Loss: 1.8104
Step 165300 Loss: 1.8192
Step 165400 Loss: 1.8286
Step 165500 Loss: 1.8414
Step 165600 Loss: 1.8573
Step 165700 Loss: 1.8753
Step 165800 Loss: 1.8972
Step 165900 Loss: 1.9220
Step 166000 Loss: 1.9516
Step 166100 Loss: 1.9810
Step 166200 Loss: 2.0112
Step 166300 Loss: 2.0468
Step 166400 Loss: 2.0957
Train Epoch: [55/200] Loss: 2.0971
Model Saving at epoch 55
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 166500 Loss: 2.0644
Step 166600 Loss: 2.0100
Step 166700 Loss: 1.9683
Step 166800 Loss: 1.9366
Step 166900 Loss: 1.9086
Step 167000 Loss: 1.8823
Step 167100 Loss: 1.8615
Step 167200 Loss: 1.8458
Step 167300 Loss: 1.8266
Step 167400 Loss: 1.8119
Step 167500 Loss: 1.7999
Step 167600 Loss: 1.7940
Step 167700 Loss: 1.7890
Step 167800 Loss: 1.7879
Step 167900 Loss: 1.7861
Step 168000 Loss: 1.7830
Step 168100 Loss: 1.7832
Step 168200 Loss: 1.7848
Step 168300 Loss: 1.7888
Step 168400 Loss: 1.7935
Step 168500 Loss: 1.7994
Step 168600 Loss: 1.8069
Step 168700 Loss: 1.8153
Step 168800 Loss: 1.8238
Step 168900 Loss: 1.8372
Step 169000 Loss: 1.8532
Step 169100 Loss: 1.8717
Step 169200 Loss: 1.8939
Step 169300 Loss: 1.9184
Step 169400 Loss: 1.9482
Step 169500 Loss: 1.9772
Step 169600 Loss: 2.0073
Step 169700 Loss: 2.0439
Step 169800 Loss: 2.0902
Train Epoch: [56/200] Loss: 2.0902
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 169900 Loss: 2.0446
Step 170000 Loss: 1.9980
Step 170100 Loss: 1.9574
Step 170200 Loss: 1.9291
Step 170300 Loss: 1.9006
Step 170400 Loss: 1.8728
Step 170500 Loss: 1.8520
Step 170600 Loss: 1.8348
Step 170700 Loss: 1.8183
Step 170800 Loss: 1.8047
Step 170900 Loss: 1.7932
Step 171000 Loss: 1.7876
Step 171100 Loss: 1.7833
Step 171200 Loss: 1.7824
Step 171300 Loss: 1.7811
Step 171400 Loss: 1.7773
Step 171500 Loss: 1.7785
Step 171600 Loss: 1.7813
Step 171700 Loss: 1.7850
Step 171800 Loss: 1.7907
Step 171900 Loss: 1.7948
Step 172000 Loss: 1.8030
Step 172100 Loss: 1.8102
Step 172200 Loss: 1.8204
Step 172300 Loss: 1.8333
Step 172400 Loss: 1.8499
Step 172500 Loss: 1.8677
Step 172600 Loss: 1.8896
Step 172700 Loss: 1.9144
Step 172800 Loss: 1.9440
Step 172900 Loss: 1.9731
Step 173000 Loss: 2.0031
Step 173100 Loss: 2.0404
Train Epoch: [57/200] Loss: 2.0878
Calling G2SDataset.batch()
Done, time:  1.41 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.41 s, total batches: 3396
Step 173200 Loss: 2.3564
Step 173300 Loss: 2.0550
Step 173400 Loss: 2.0023
Step 173500 Loss: 1.9586
Step 173600 Loss: 1.9290
Step 173700 Loss: 1.9000
Step 173800 Loss: 1.8746
Step 173900 Loss: 1.8505
Step 174000 Loss: 1.8327
Step 174100 Loss: 1.8157
Step 174200 Loss: 1.8031
Step 174300 Loss: 1.7915
Step 174400 Loss: 1.7855
Step 174500 Loss: 1.7820
Step 174600 Loss: 1.7816
Step 174700 Loss: 1.7806
Step 174800 Loss: 1.7770
Step 174900 Loss: 1.7779
Step 175000 Loss: 1.7804
Step 175100 Loss: 1.7830
Step 175200 Loss: 1.7880
Step 175300 Loss: 1.7925
Step 175400 Loss: 1.8001
Step 175500 Loss: 1.8087
Step 175600 Loss: 1.8184
Step 175700 Loss: 1.8314
Step 175800 Loss: 1.8480
Step 175900 Loss: 1.8667
Step 176000 Loss: 1.8884
Step 176100 Loss: 1.9136
Step 176200 Loss: 1.9431
Step 176300 Loss: 1.9718
Step 176400 Loss: 2.0015
Step 176500 Loss: 2.0403
Train Epoch: [58/200] Loss: 2.0853
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 3396
Calling G2SDataset.batch()
Done, time:  1.43 s, total batches: 3396
Step 176600 Loss: 2.2272
Step 176700 Loss: 2.0518
Step 176800 Loss: 2.0036
Step 176900 Loss: 1.9613
Step 177000 Loss: 1.9275
Step 177100 Loss: 1.8956
Step 177200 Loss: 1.8669
Step 177300 Loss: 1.8466
Step 177400 Loss: 1.8292
Step 177500 Loss: 1.8128
Step 177600 Loss: 1.7996
Step 177700 Loss: 1.7887
Step 177800 Loss: 1.7831
Step 177900 Loss: 1.7798
Step 178000 Loss: 1.7778
Step 178100 Loss: 1.7766
Step 178200 Loss: 1.7729
Step 178300 Loss: 1.7748
Step 178400 Loss: 1.7766
Step 178500 Loss: 1.7803
Step 178600 Loss: 1.7861
Step 178700 Loss: 1.7910
Step 178800 Loss: 1.7986
Step 178900 Loss: 1.8070
Step 179000 Loss: 1.8176
Step 179100 Loss: 1.8299
Step 179200 Loss: 1.8471
Step 179300 Loss: 1.8656
Step 179400 Loss: 1.8877
Step 179500 Loss: 1.9124
Step 179600 Loss: 1.9415
Step 179700 Loss: 1.9704
