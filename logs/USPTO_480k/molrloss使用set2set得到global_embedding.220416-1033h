Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = **
**** embed_size = *256*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *256*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *256*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *256*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *100*
**** max_steps = *300000*
**** warmup_steps = *100*
**** lr = *0.0001*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=361, out_features=256, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=370, out_features=256, bias=True)
        (W_r): Linear(in_features=114, out_features=256, bias=False)
        (U_r): Linear(in_features=256, out_features=256, bias=True)
        (W_h): Linear(in_features=370, out_features=256, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=256, bias=True)
        (attn_W_k): Linear(in_features=256, out_features=256, bias=True)
        (attn_W_v): Linear(in_features=256, out_features=256, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=256, bias=True)
      (attn_W_k): Linear(in_features=256, out_features=256, bias=True)
      (attn_W_v): Linear(in_features=256, out_features=256, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (attention_encoder): AttnEncoderXL(
    (dropout): Dropout(p=0.3, inplace=False)
    (attention_layers): ModuleList(
      (0): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (1): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (2): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (3): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (4): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (5): SALayerXL(
        (self_attn): MultiHeadedRelAttention(
          (linear_keys): Linear(in_features=256, out_features=256, bias=True)
          (linear_values): Linear(in_features=256, out_features=256, bias=True)
          (linear_query): Linear(in_features=256, out_features=256, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.3, inplace=False)
          (final_linear): Linear(in_features=256, out_features=256, bias=True)
          (relative_pe): Embedding(12, 256, padding_idx=11)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=256, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (set2set): Set2Set(256, 512)
  (g): Sequential(
    (0): Linear(in_features=512, out_features=256, bias=False)
    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 9561232
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.55 s, total batches: 6756
Step 100 Loss: 8.3166, lr 0.0001
Step 200 Loss: 5.9281, lr 0.0001
Step 300 Loss: 4.8405, lr 0.0001
Step 400 Loss: 4.1771, lr 0.0001
Step 500 Loss: 3.7330, lr 0.0001
Step 600 Loss: 3.4103, lr 0.0001
Step 700 Loss: 3.1611, lr 0.0001
Step 800 Loss: 2.9616, lr 0.0001
Step 900 Loss: 2.7947, lr 0.0001
Step 1000 Loss: 2.6544, lr 0.0001
Step 1100 Loss: 2.5344, lr 0.0001
Step 1200 Loss: 2.4342, lr 0.0001
Step 1300 Loss: 2.3435, lr 0.0001
Step 1400 Loss: 2.2627, lr 0.0001
Step 1500 Loss: 2.1909, lr 0.0001
Step 1600 Loss: 2.1306, lr 0.0001
Step 1700 Loss: 2.0749, lr 0.0001
Step 1800 Loss: 2.0228, lr 0.0001
Step 1900 Loss: 1.9754, lr 0.0001
Step 2000 Loss: 1.9306, lr 0.0001
Step 2100 Loss: 1.8884, lr 0.0001
Step 2200 Loss: 1.8489, lr 0.0001
Step 2300 Loss: 1.8130, lr 0.0001
Step 2400 Loss: 1.7799, lr 0.0001
Step 2500 Loss: 1.7481, lr 0.0001
Step 2600 Loss: 1.7177, lr 0.0001
Step 2700 Loss: 1.6905, lr 0.0001
Step 2800 Loss: 1.6641, lr 0.0001
Step 2900 Loss: 1.6392, lr 0.0001
Step 3000 Loss: 1.6155, lr 0.0001
Step 3100 Loss: 1.5940, lr 0.0001
Step 3200 Loss: 1.5740, lr 0.0001
Step 3300 Loss: 1.5544, lr 0.0001
Step 3400 Loss: 1.5358, lr 0.0001
Step 3500 Loss: 1.5176, lr 0.0001
Step 3600 Loss: 1.5007, lr 0.0001
Step 3700 Loss: 1.4846, lr 0.0001
Step 3800 Loss: 1.4691, lr 0.0001
Step 3900 Loss: 1.4544, lr 0.0001
Step 4000 Loss: 1.4413, lr 0.0001
Step 4100 Loss: 1.4284, lr 0.0001
Step 4200 Loss: 1.4168, lr 0.0001
Step 4300 Loss: 1.4055, lr 0.0001
Step 4400 Loss: 1.3947, lr 0.0001
Step 4500 Loss: 1.3844, lr 0.0001
Step 4600 Loss: 1.3743, lr 0.0001
Step 4700 Loss: 1.3648, lr 0.0001
Step 4800 Loss: 1.3566, lr 0.0001
Step 4900 Loss: 1.3483, lr 0.0001
Step 5000 Loss: 1.3408, lr 0.0001
Step 5100 Loss: 1.3334, lr 0.0001
Step 5200 Loss: 1.3260, lr 0.0001
Step 5300 Loss: 1.3191, lr 0.0001
Step 5400 Loss: 1.3133, lr 0.0001
Step 5500 Loss: 1.3072, lr 0.0001
Step 5600 Loss: 1.3010, lr 0.0001
Step 5700 Loss: 1.2953, lr 0.0001
Step 5800 Loss: 1.2893, lr 0.0001
Step 5900 Loss: 1.2850, lr 0.0001
Step 6000 Loss: 1.2828, lr 0.0001
Step 6100 Loss: 1.2833, lr 0.0001
Step 6200 Loss: 1.2809, lr 0.0001
Step 6300 Loss: 1.2767, lr 0.0001
Step 6400 Loss: 1.2721, lr 0.0001
Step 6500 Loss: 1.2672, lr 0.0001
Step 6600 Loss: 1.2617, lr 0.0001
Step 6700 Loss: 1.2567, lr 0.0001
Train Epoch: [1/100] Loss: 1.2534,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6756
Step 6800 Loss: 1.9664, lr 9.997532801828658e-05
Step 6900 Loss: 1.3476, lr 9.997532801828658e-05
Step 7000 Loss: 1.1967, lr 9.997532801828658e-05
Step 7100 Loss: 1.1231, lr 9.997532801828658e-05
Step 7200 Loss: 1.0748, lr 9.997532801828658e-05
Step 7300 Loss: 1.0387, lr 9.997532801828658e-05
Step 7400 Loss: 1.0054, lr 9.997532801828658e-05
Step 7500 Loss: 0.9808, lr 9.997532801828658e-05
Step 7600 Loss: 0.9591, lr 9.997532801828658e-05
Step 7700 Loss: 0.9405, lr 9.997532801828658e-05
Step 7800 Loss: 0.9256, lr 9.997532801828658e-05
Step 7900 Loss: 0.9126, lr 9.997532801828658e-05
Step 8000 Loss: 0.9011, lr 9.997532801828658e-05
Step 8100 Loss: 0.8906, lr 9.997532801828658e-05
Step 8200 Loss: 0.8822, lr 9.997532801828658e-05
Step 8300 Loss: 0.8739, lr 9.997532801828658e-05
Step 8400 Loss: 0.8654, lr 9.997532801828658e-05
Step 8500 Loss: 0.8579, lr 9.997532801828658e-05
Step 8600 Loss: 0.8505, lr 9.997532801828658e-05
Step 8700 Loss: 0.8448, lr 9.997532801828658e-05
Step 8800 Loss: 0.8381, lr 9.997532801828658e-05
Step 8900 Loss: 0.8317, lr 9.997532801828658e-05
Step 9000 Loss: 0.8262, lr 9.997532801828658e-05
Step 9100 Loss: 0.8201, lr 9.997532801828658e-05
Step 9200 Loss: 0.8152, lr 9.997532801828658e-05
Step 9300 Loss: 0.8100, lr 9.997532801828658e-05
Step 9400 Loss: 0.8051, lr 9.997532801828658e-05
Step 9500 Loss: 0.8020, lr 9.997532801828658e-05
Step 9600 Loss: 0.7979, lr 9.997532801828658e-05
Step 9700 Loss: 0.7943, lr 9.997532801828658e-05
Step 9800 Loss: 0.7906, lr 9.997532801828658e-05
Step 9900 Loss: 0.7873, lr 9.997532801828658e-05
Step 10000 Loss: 0.7838, lr 9.997532801828658e-05
Step 10100 Loss: 0.7805, lr 9.997532801828658e-05
Step 10200 Loss: 0.7771, lr 9.997532801828658e-05
Step 10300 Loss: 0.7740, lr 9.997532801828658e-05
Step 10400 Loss: 0.7726, lr 9.997532801828658e-05
Step 10500 Loss: 0.7730, lr 9.997532801828658e-05
Step 10600 Loss: 0.7719, lr 9.997532801828658e-05
Step 10700 Loss: 0.7703, lr 9.997532801828658e-05
Step 10800 Loss: 0.7692, lr 9.997532801828658e-05
Step 10900 Loss: 0.7675, lr 9.997532801828658e-05
Step 11000 Loss: 0.7659, lr 9.997532801828658e-05
Step 11100 Loss: 0.7646, lr 9.997532801828658e-05
Step 11200 Loss: 0.7627, lr 9.997532801828658e-05
Step 11300 Loss: 0.7610, lr 9.997532801828658e-05
Step 11400 Loss: 0.7596, lr 9.997532801828658e-05
Step 11500 Loss: 0.7580, lr 9.997532801828658e-05
Step 11600 Loss: 0.7571, lr 9.997532801828658e-05
Step 11700 Loss: 0.7558, lr 9.997532801828658e-05
Step 11800 Loss: 0.7544, lr 9.997532801828658e-05
Step 11900 Loss: 0.7542, lr 9.997532801828658e-05
Step 12000 Loss: 0.7530, lr 9.997532801828658e-05
Step 12100 Loss: 0.7521, lr 9.997532801828658e-05
Step 12200 Loss: 0.7516, lr 9.997532801828658e-05
Step 12300 Loss: 0.7510, lr 9.997532801828658e-05
Step 12400 Loss: 0.7505, lr 9.997532801828658e-05
Step 12500 Loss: 0.7506, lr 9.997532801828658e-05
Step 12600 Loss: 0.7508, lr 9.997532801828658e-05
Step 12700 Loss: 0.7510, lr 9.997532801828658e-05
Step 12800 Loss: 0.7510, lr 9.997532801828658e-05
Step 12900 Loss: 0.7517, lr 9.997532801828658e-05
Step 13000 Loss: 0.7526, lr 9.997532801828658e-05
Step 13100 Loss: 0.7558, lr 9.997532801828658e-05
Step 13200 Loss: 0.7594, lr 9.997532801828658e-05
Step 13300 Loss: 0.7622, lr 9.997532801828658e-05
Step 13400 Loss: 0.7626, lr 9.997532801828658e-05
Step 13500 Loss: 0.7628, lr 9.997532801828658e-05
Train Epoch: [2/100] Loss: 0.7629,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 13600 Loss: 1.3667, lr 9.990133642141359e-05
Step 13700 Loss: 1.1360, lr 9.990133642141359e-05
Step 13800 Loss: 1.0395, lr 9.990133642141359e-05
Step 13900 Loss: 0.9823, lr 9.990133642141359e-05
Step 14000 Loss: 0.9437, lr 9.990133642141359e-05
Step 14100 Loss: 0.9104, lr 9.990133642141359e-05
Step 14200 Loss: 0.8850, lr 9.990133642141359e-05
Step 14300 Loss: 0.8635, lr 9.990133642141359e-05
Step 14400 Loss: 0.8443, lr 9.990133642141359e-05
Step 14500 Loss: 0.8278, lr 9.990133642141359e-05
Step 14600 Loss: 0.8137, lr 9.990133642141359e-05
Step 14700 Loss: 0.8011, lr 9.990133642141359e-05
Step 14800 Loss: 0.7920, lr 9.990133642141359e-05
Step 14900 Loss: 0.7829, lr 9.990133642141359e-05
Step 15000 Loss: 0.7750, lr 9.990133642141359e-05
Step 15100 Loss: 0.7673, lr 9.990133642141359e-05
Step 15200 Loss: 0.7597, lr 9.990133642141359e-05
Step 15300 Loss: 0.7567, lr 9.990133642141359e-05
Step 15400 Loss: 0.7518, lr 9.990133642141359e-05
Step 15500 Loss: 0.7469, lr 9.990133642141359e-05
Step 15600 Loss: 0.7418, lr 9.990133642141359e-05
Step 15700 Loss: 0.7385, lr 9.990133642141359e-05
Step 15800 Loss: 0.7349, lr 9.990133642141359e-05
Step 15900 Loss: 0.7316, lr 9.990133642141359e-05
Step 16000 Loss: 0.7273, lr 9.990133642141359e-05
Step 16100 Loss: 0.7232, lr 9.990133642141359e-05
Step 16200 Loss: 0.7198, lr 9.990133642141359e-05
Step 16300 Loss: 0.7175, lr 9.990133642141359e-05
Step 16400 Loss: 0.7143, lr 9.990133642141359e-05
Step 16500 Loss: 0.7117, lr 9.990133642141359e-05
Step 16600 Loss: 0.7089, lr 9.990133642141359e-05
Step 16700 Loss: 0.7062, lr 9.990133642141359e-05
Step 16800 Loss: 0.7034, lr 9.990133642141359e-05
Step 16900 Loss: 0.7006, lr 9.990133642141359e-05
Step 17000 Loss: 0.6976, lr 9.990133642141359e-05
Step 17100 Loss: 0.6946, lr 9.990133642141359e-05
Step 17200 Loss: 0.6923, lr 9.990133642141359e-05
Step 17300 Loss: 0.6899, lr 9.990133642141359e-05
Step 17400 Loss: 0.6873, lr 9.990133642141359e-05
Step 17500 Loss: 0.6855, lr 9.990133642141359e-05
Step 17600 Loss: 0.6837, lr 9.990133642141359e-05
Step 17700 Loss: 0.6821, lr 9.990133642141359e-05
Step 17800 Loss: 0.6803, lr 9.990133642141359e-05
Step 17900 Loss: 0.6787, lr 9.990133642141359e-05
Step 18000 Loss: 0.6769, lr 9.990133642141359e-05
Step 18100 Loss: 0.6746, lr 9.990133642141359e-05
Step 18200 Loss: 0.6731, lr 9.990133642141359e-05
Step 18300 Loss: 0.6723, lr 9.990133642141359e-05
Step 18400 Loss: 0.6712, lr 9.990133642141359e-05
Step 18500 Loss: 0.6706, lr 9.990133642141359e-05
Step 18600 Loss: 0.6697, lr 9.990133642141359e-05
Step 18700 Loss: 0.6692, lr 9.990133642141359e-05
Step 18800 Loss: 0.6690, lr 9.990133642141359e-05
Step 18900 Loss: 0.6687, lr 9.990133642141359e-05
Step 19000 Loss: 0.6684, lr 9.990133642141359e-05
Step 19100 Loss: 0.6685, lr 9.990133642141359e-05
Step 19200 Loss: 0.6687, lr 9.990133642141359e-05
Step 19300 Loss: 0.6692, lr 9.990133642141359e-05
Step 19400 Loss: 0.6700, lr 9.990133642141359e-05
Step 19500 Loss: 0.6708, lr 9.990133642141359e-05
Step 19600 Loss: 0.6723, lr 9.990133642141359e-05
Step 19700 Loss: 0.6732, lr 9.990133642141359e-05
Step 19800 Loss: 0.6743, lr 9.990133642141359e-05
Step 19900 Loss: 0.6752, lr 9.990133642141359e-05
Step 20000 Loss: 0.6754, lr 9.990133642141359e-05
Step 20100 Loss: 0.6747, lr 9.990133642141359e-05
Step 20200 Loss: 0.6740, lr 9.990133642141359e-05
Train Epoch: [3/100] Loss: 0.6738,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6755
Step 20300 Loss: 1.7610, lr 9.977809823015401e-05
Step 20400 Loss: 1.1817, lr 9.977809823015401e-05
Step 20500 Loss: 1.0433, lr 9.977809823015401e-05
Step 20600 Loss: 0.9742, lr 9.977809823015401e-05
Step 20700 Loss: 0.9307, lr 9.977809823015401e-05
Step 20800 Loss: 0.9007, lr 9.977809823015401e-05
Step 20900 Loss: 0.8755, lr 9.977809823015401e-05
Step 21000 Loss: 0.8550, lr 9.977809823015401e-05
Step 21100 Loss: 0.8371, lr 9.977809823015401e-05
Step 21200 Loss: 0.8229, lr 9.977809823015401e-05
Step 21300 Loss: 0.8100, lr 9.977809823015401e-05
Step 21400 Loss: 0.8007, lr 9.977809823015401e-05
Step 21500 Loss: 0.7912, lr 9.977809823015401e-05
Step 21600 Loss: 0.7830, lr 9.977809823015401e-05
Step 21700 Loss: 0.7752, lr 9.977809823015401e-05
Step 21800 Loss: 0.7673, lr 9.977809823015401e-05
Step 21900 Loss: 0.7593, lr 9.977809823015401e-05
Step 22000 Loss: 0.7523, lr 9.977809823015401e-05
Step 22100 Loss: 0.7451, lr 9.977809823015401e-05
Step 22200 Loss: 0.7382, lr 9.977809823015401e-05
Step 22300 Loss: 0.7308, lr 9.977809823015401e-05
Step 22400 Loss: 0.7248, lr 9.977809823015401e-05
Step 22500 Loss: 0.7188, lr 9.977809823015401e-05
Step 22600 Loss: 0.7128, lr 9.977809823015401e-05
Step 22700 Loss: 0.7079, lr 9.977809823015401e-05
Step 22800 Loss: 0.7023, lr 9.977809823015401e-05
Step 22900 Loss: 0.6971, lr 9.977809823015401e-05
Step 23000 Loss: 0.6928, lr 9.977809823015401e-05
Step 23100 Loss: 0.6884, lr 9.977809823015401e-05
Step 23200 Loss: 0.6843, lr 9.977809823015401e-05
Step 23300 Loss: 0.6804, lr 9.977809823015401e-05
Step 23400 Loss: 0.6760, lr 9.977809823015401e-05
Step 23500 Loss: 0.6721, lr 9.977809823015401e-05
Step 23600 Loss: 0.6680, lr 9.977809823015401e-05
Step 23700 Loss: 0.6642, lr 9.977809823015401e-05
Step 23800 Loss: 0.6605, lr 9.977809823015401e-05
Step 23900 Loss: 0.6574, lr 9.977809823015401e-05
Step 24000 Loss: 0.6539, lr 9.977809823015401e-05
Step 24100 Loss: 0.6511, lr 9.977809823015401e-05
Step 24200 Loss: 0.6480, lr 9.977809823015401e-05
Step 24300 Loss: 0.6461, lr 9.977809823015401e-05
Step 24400 Loss: 0.6441, lr 9.977809823015401e-05
Step 24500 Loss: 0.6424, lr 9.977809823015401e-05
Step 24600 Loss: 0.6406, lr 9.977809823015401e-05
Step 24700 Loss: 0.6387, lr 9.977809823015401e-05
Step 24800 Loss: 0.6365, lr 9.977809823015401e-05
Step 24900 Loss: 0.6348, lr 9.977809823015401e-05
Step 25000 Loss: 0.6331, lr 9.977809823015401e-05
Step 25100 Loss: 0.6320, lr 9.977809823015401e-05
Step 25200 Loss: 0.6311, lr 9.977809823015401e-05
Step 25300 Loss: 0.6301, lr 9.977809823015401e-05
Step 25400 Loss: 0.6299, lr 9.977809823015401e-05
Step 25500 Loss: 0.6295, lr 9.977809823015401e-05
Step 25600 Loss: 0.6291, lr 9.977809823015401e-05
Step 25700 Loss: 0.6288, lr 9.977809823015401e-05
Step 25800 Loss: 0.6288, lr 9.977809823015401e-05
Step 25900 Loss: 0.6290, lr 9.977809823015401e-05
Step 26000 Loss: 0.6292, lr 9.977809823015401e-05
Step 26100 Loss: 0.6296, lr 9.977809823015401e-05
Step 26200 Loss: 0.6307, lr 9.977809823015401e-05
Step 26300 Loss: 0.6312, lr 9.977809823015401e-05
Step 26400 Loss: 0.6327, lr 9.977809823015401e-05
Step 26500 Loss: 0.6335, lr 9.977809823015401e-05
Step 26600 Loss: 0.6344, lr 9.977809823015401e-05
Step 26700 Loss: 0.6358, lr 9.977809823015401e-05
Step 26800 Loss: 0.6372, lr 9.977809823015401e-05
Step 26900 Loss: 0.6364, lr 9.977809823015401e-05
Step 27000 Loss: 0.6361, lr 9.977809823015401e-05
Train Epoch: [4/100] Loss: 0.6363,lr 0.000100
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6755
Step 27100 Loss: 1.2052, lr 9.960573506572391e-05
Step 27200 Loss: 0.9832, lr 9.960573506572391e-05
Step 27300 Loss: 0.8887, lr 9.960573506572391e-05
Step 27400 Loss: 0.8332, lr 9.960573506572391e-05
Step 27500 Loss: 0.8038, lr 9.960573506572391e-05
Step 27600 Loss: 0.7790, lr 9.960573506572391e-05
Step 27700 Loss: 0.7597, lr 9.960573506572391e-05
Step 27800 Loss: 0.7432, lr 9.960573506572391e-05
Step 27900 Loss: 0.7275, lr 9.960573506572391e-05
Step 28000 Loss: 0.7150, lr 9.960573506572391e-05
Step 28100 Loss: 0.7053, lr 9.960573506572391e-05
Step 28200 Loss: 0.6964, lr 9.960573506572391e-05
Step 28300 Loss: 0.6892, lr 9.960573506572391e-05
Step 28400 Loss: 0.6810, lr 9.960573506572391e-05
Step 28500 Loss: 0.6744, lr 9.960573506572391e-05
Step 28600 Loss: 0.6677, lr 9.960573506572391e-05
Step 28700 Loss: 0.6609, lr 9.960573506572391e-05
Step 28800 Loss: 0.6550, lr 9.960573506572391e-05
Step 28900 Loss: 0.6489, lr 9.960573506572391e-05
Step 29000 Loss: 0.6432, lr 9.960573506572391e-05
Step 29100 Loss: 0.6372, lr 9.960573506572391e-05
Step 29200 Loss: 0.6320, lr 9.960573506572391e-05
Step 29300 Loss: 0.6271, lr 9.960573506572391e-05
Step 29400 Loss: 0.6255, lr 9.960573506572391e-05
Step 29500 Loss: 0.6223, lr 9.960573506572391e-05
Step 29600 Loss: 0.6194, lr 9.960573506572391e-05
Step 29700 Loss: 0.6169, lr 9.960573506572391e-05
Step 29800 Loss: 0.6149, lr 9.960573506572391e-05
Step 29900 Loss: 0.6119, lr 9.960573506572391e-05
Step 30000 Loss: 0.6104, lr 9.960573506572391e-05
Step 30100 Loss: 0.6076, lr 9.960573506572391e-05
Step 30200 Loss: 0.6048, lr 9.960573506572391e-05
Step 30300 Loss: 0.6027, lr 9.960573506572391e-05
Step 30400 Loss: 0.6001, lr 9.960573506572391e-05
Step 30500 Loss: 0.5975, lr 9.960573506572391e-05
Step 30600 Loss: 0.5950, lr 9.960573506572391e-05
Step 30700 Loss: 0.5924, lr 9.960573506572391e-05
Step 30800 Loss: 0.5903, lr 9.960573506572391e-05
Step 30900 Loss: 0.5880, lr 9.960573506572391e-05
Step 31000 Loss: 0.5862, lr 9.960573506572391e-05
Step 31100 Loss: 0.5849, lr 9.960573506572391e-05
Step 31200 Loss: 0.5836, lr 9.960573506572391e-05
Step 31300 Loss: 0.5821, lr 9.960573506572391e-05
Step 31400 Loss: 0.5808, lr 9.960573506572391e-05
Step 31500 Loss: 0.5789, lr 9.960573506572391e-05
Step 31600 Loss: 0.5776, lr 9.960573506572391e-05
Step 31700 Loss: 0.5779, lr 9.960573506572391e-05
Step 31800 Loss: 0.5772, lr 9.960573506572391e-05
Step 31900 Loss: 0.5766, lr 9.960573506572391e-05
Step 32000 Loss: 0.5762, lr 9.960573506572391e-05
Step 32100 Loss: 0.5756, lr 9.960573506572391e-05
Step 32200 Loss: 0.5753, lr 9.960573506572391e-05
Step 32300 Loss: 0.5745, lr 9.960573506572391e-05
Step 32400 Loss: 0.5745, lr 9.960573506572391e-05
Step 32500 Loss: 0.5744, lr 9.960573506572391e-05
Step 32600 Loss: 0.5744, lr 9.960573506572391e-05
Step 32700 Loss: 0.5746, lr 9.960573506572391e-05
Step 32800 Loss: 0.5752, lr 9.960573506572391e-05
Step 32900 Loss: 0.5762, lr 9.960573506572391e-05
Step 33000 Loss: 0.5771, lr 9.960573506572391e-05
Step 33100 Loss: 0.5788, lr 9.960573506572391e-05
Step 33200 Loss: 0.5803, lr 9.960573506572391e-05
Step 33300 Loss: 0.5817, lr 9.960573506572391e-05
Step 33400 Loss: 0.5825, lr 9.960573506572391e-05
Step 33500 Loss: 0.5827, lr 9.960573506572391e-05
Step 33600 Loss: 0.5825, lr 9.960573506572391e-05
Step 33700 Loss: 0.5822, lr 9.960573506572391e-05
Train Epoch: [5/100] Loss: 0.5826,lr 0.000100
Model Saving at epoch 5
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6755
Step 33800 Loss: 2.0549, lr 9.93844170297569e-05
Step 33900 Loss: 1.3050, lr 9.93844170297569e-05
Step 34000 Loss: 1.0877, lr 9.93844170297569e-05
Step 34100 Loss: 0.9744, lr 9.93844170297569e-05
Step 34200 Loss: 0.8986, lr 9.93844170297569e-05
Step 34300 Loss: 0.8491, lr 9.93844170297569e-05
Step 34400 Loss: 0.8100, lr 9.93844170297569e-05
Step 34500 Loss: 0.7808, lr 9.93844170297569e-05
Step 34600 Loss: 0.7575, lr 9.93844170297569e-05
Step 34700 Loss: 0.7364, lr 9.93844170297569e-05
Step 34800 Loss: 0.7190, lr 9.93844170297569e-05
Step 34900 Loss: 0.7061, lr 9.93844170297569e-05
Step 35000 Loss: 0.6937, lr 9.93844170297569e-05
Step 35100 Loss: 0.6835, lr 9.93844170297569e-05
Step 35200 Loss: 0.6744, lr 9.93844170297569e-05
Step 35300 Loss: 0.6658, lr 9.93844170297569e-05
Step 35400 Loss: 0.6582, lr 9.93844170297569e-05
Step 35500 Loss: 0.6513, lr 9.93844170297569e-05
Step 35600 Loss: 0.6445, lr 9.93844170297569e-05
Step 35700 Loss: 0.6380, lr 9.93844170297569e-05
Step 35800 Loss: 0.6310, lr 9.93844170297569e-05
Step 35900 Loss: 0.6252, lr 9.93844170297569e-05
Step 36000 Loss: 0.6195, lr 9.93844170297569e-05
Step 36100 Loss: 0.6141, lr 9.93844170297569e-05
Step 36200 Loss: 0.6102, lr 9.93844170297569e-05
Step 36300 Loss: 0.6062, lr 9.93844170297569e-05
Step 36400 Loss: 0.6050, lr 9.93844170297569e-05
Step 36500 Loss: 0.6038, lr 9.93844170297569e-05
Step 36600 Loss: 0.6014, lr 9.93844170297569e-05
Step 36700 Loss: 0.5979, lr 9.93844170297569e-05
Step 36800 Loss: 0.5949, lr 9.93844170297569e-05
Step 36900 Loss: 0.5911, lr 9.93844170297569e-05
Step 37000 Loss: 0.5874, lr 9.93844170297569e-05
Step 37100 Loss: 0.5838, lr 9.93844170297569e-05
Step 37200 Loss: 0.5801, lr 9.93844170297569e-05
Step 37300 Loss: 0.5766, lr 9.93844170297569e-05
Step 37400 Loss: 0.5735, lr 9.93844170297569e-05
Step 37500 Loss: 0.5699, lr 9.93844170297569e-05
Step 37600 Loss: 0.5674, lr 9.93844170297569e-05
Step 37700 Loss: 0.5650, lr 9.93844170297569e-05
Step 37800 Loss: 0.5631, lr 9.93844170297569e-05
Step 37900 Loss: 0.5611, lr 9.93844170297569e-05
Step 38000 Loss: 0.5596, lr 9.93844170297569e-05
Step 38100 Loss: 0.5578, lr 9.93844170297569e-05
Step 38200 Loss: 0.5561, lr 9.93844170297569e-05
Step 38300 Loss: 0.5541, lr 9.93844170297569e-05
Step 38400 Loss: 0.5531, lr 9.93844170297569e-05
Step 38500 Loss: 0.5530, lr 9.93844170297569e-05
Step 38600 Loss: 0.5530, lr 9.93844170297569e-05
Step 38700 Loss: 0.5523, lr 9.93844170297569e-05
Step 38800 Loss: 0.5516, lr 9.93844170297569e-05
Step 38900 Loss: 0.5516, lr 9.93844170297569e-05
Step 39000 Loss: 0.5511, lr 9.93844170297569e-05
Step 39100 Loss: 0.5508, lr 9.93844170297569e-05
Step 39200 Loss: 0.5506, lr 9.93844170297569e-05
Step 39300 Loss: 0.5505, lr 9.93844170297569e-05
Step 39400 Loss: 0.5504, lr 9.93844170297569e-05
Step 39500 Loss: 0.5506, lr 9.93844170297569e-05
Step 39600 Loss: 0.5510, lr 9.93844170297569e-05
Step 39700 Loss: 0.5522, lr 9.93844170297569e-05
Step 39800 Loss: 0.5529, lr 9.93844170297569e-05
Step 39900 Loss: 0.5550, lr 9.93844170297569e-05
Step 40000 Loss: 0.5562, lr 9.93844170297569e-05
Step 40100 Loss: 0.5572, lr 9.93844170297569e-05
Step 40200 Loss: 0.5581, lr 9.93844170297569e-05
Step 40300 Loss: 0.5581, lr 9.93844170297569e-05
Step 40400 Loss: 0.5573, lr 9.93844170297569e-05
Step 40500 Loss: 0.5570, lr 9.93844170297569e-05
Train Epoch: [6/100] Loss: 0.5571,lr 0.000099
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 40600 Loss: 1.3224, lr 9.911436253643445e-05
Step 40700 Loss: 1.0211, lr 9.911436253643445e-05
Step 40800 Loss: 0.9090, lr 9.911436253643445e-05
Step 40900 Loss: 0.8465, lr 9.911436253643445e-05
Step 41000 Loss: 0.8091, lr 9.911436253643445e-05
Step 41100 Loss: 0.7780, lr 9.911436253643445e-05
Step 41200 Loss: 0.7518, lr 9.911436253643445e-05
Step 41300 Loss: 0.7310, lr 9.911436253643445e-05
Step 41400 Loss: 0.7133, lr 9.911436253643445e-05
Step 41500 Loss: 0.6969, lr 9.911436253643445e-05
Step 41600 Loss: 0.6837, lr 9.911436253643445e-05
Step 41700 Loss: 0.6716, lr 9.911436253643445e-05
Step 41800 Loss: 0.6606, lr 9.911436253643445e-05
Step 41900 Loss: 0.6500, lr 9.911436253643445e-05
Step 42000 Loss: 0.6412, lr 9.911436253643445e-05
Step 42100 Loss: 0.6331, lr 9.911436253643445e-05
Step 42200 Loss: 0.6247, lr 9.911436253643445e-05
Step 42300 Loss: 0.6178, lr 9.911436253643445e-05
Step 42400 Loss: 0.6113, lr 9.911436253643445e-05
Step 42500 Loss: 0.6048, lr 9.911436253643445e-05
Step 42600 Loss: 0.5982, lr 9.911436253643445e-05
Step 42700 Loss: 0.5927, lr 9.911436253643445e-05
Step 42800 Loss: 0.5875, lr 9.911436253643445e-05
Step 42900 Loss: 0.5820, lr 9.911436253643445e-05
Step 43000 Loss: 0.5768, lr 9.911436253643445e-05
Step 43100 Loss: 0.5722, lr 9.911436253643445e-05
Step 43200 Loss: 0.5681, lr 9.911436253643445e-05
Step 43300 Loss: 0.5651, lr 9.911436253643445e-05
Step 43400 Loss: 0.5607, lr 9.911436253643445e-05
Step 43500 Loss: 0.5578, lr 9.911436253643445e-05
Step 43600 Loss: 0.5545, lr 9.911436253643445e-05
Step 43700 Loss: 0.5506, lr 9.911436253643445e-05
Step 43800 Loss: 0.5478, lr 9.911436253643445e-05
Step 43900 Loss: 0.5448, lr 9.911436253643445e-05
Step 44000 Loss: 0.5417, lr 9.911436253643445e-05
Step 44100 Loss: 0.5381, lr 9.911436253643445e-05
Step 44200 Loss: 0.5358, lr 9.911436253643445e-05
Step 44300 Loss: 0.5334, lr 9.911436253643445e-05
Step 44400 Loss: 0.5308, lr 9.911436253643445e-05
Step 44500 Loss: 0.5287, lr 9.911436253643445e-05
Step 44600 Loss: 0.5271, lr 9.911436253643445e-05
Step 44700 Loss: 0.5260, lr 9.911436253643445e-05
Step 44800 Loss: 0.5248, lr 9.911436253643445e-05
Step 44900 Loss: 0.5231, lr 9.911436253643445e-05
Step 45000 Loss: 0.5218, lr 9.911436253643445e-05
Step 45100 Loss: 0.5195, lr 9.911436253643445e-05
Step 45200 Loss: 0.5180, lr 9.911436253643445e-05
Step 45300 Loss: 0.5172, lr 9.911436253643445e-05
Step 45400 Loss: 0.5169, lr 9.911436253643445e-05
Step 45500 Loss: 0.5167, lr 9.911436253643445e-05
Step 45600 Loss: 0.5160, lr 9.911436253643445e-05
Step 45700 Loss: 0.5158, lr 9.911436253643445e-05
Step 45800 Loss: 0.5154, lr 9.911436253643445e-05
Step 45900 Loss: 0.5152, lr 9.911436253643445e-05
Step 46000 Loss: 0.5154, lr 9.911436253643445e-05
Step 46100 Loss: 0.5148, lr 9.911436253643445e-05
Step 46200 Loss: 0.5153, lr 9.911436253643445e-05
Step 46300 Loss: 0.5157, lr 9.911436253643445e-05
Step 46400 Loss: 0.5173, lr 9.911436253643445e-05
Step 46500 Loss: 0.5186, lr 9.911436253643445e-05
Step 46600 Loss: 0.5199, lr 9.911436253643445e-05
Step 46700 Loss: 0.5217, lr 9.911436253643445e-05
Step 46800 Loss: 0.5231, lr 9.911436253643445e-05
Step 46900 Loss: 0.5236, lr 9.911436253643445e-05
Step 47000 Loss: 0.5238, lr 9.911436253643445e-05
Step 47100 Loss: 0.5241, lr 9.911436253643445e-05
Step 47200 Loss: 0.5232, lr 9.911436253643445e-05
Train Epoch: [7/100] Loss: 0.5233,lr 0.000099
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 47300 Loss: 1.8399, lr 9.879583809693738e-05
Step 47400 Loss: 1.0039, lr 9.879583809693738e-05
Step 47500 Loss: 0.8720, lr 9.879583809693738e-05
Step 47600 Loss: 0.8102, lr 9.879583809693738e-05
Step 47700 Loss: 0.7698, lr 9.879583809693738e-05
Step 47800 Loss: 0.7412, lr 9.879583809693738e-05
Step 47900 Loss: 0.7158, lr 9.879583809693738e-05
Step 48000 Loss: 0.6979, lr 9.879583809693738e-05
Step 48100 Loss: 0.6808, lr 9.879583809693738e-05
Step 48200 Loss: 0.6666, lr 9.879583809693738e-05
Step 48300 Loss: 0.6534, lr 9.879583809693738e-05
Step 48400 Loss: 0.6436, lr 9.879583809693738e-05
Step 48500 Loss: 0.6325, lr 9.879583809693738e-05
Step 48600 Loss: 0.6233, lr 9.879583809693738e-05
Step 48700 Loss: 0.6148, lr 9.879583809693738e-05
Step 48800 Loss: 0.6077, lr 9.879583809693738e-05
Step 48900 Loss: 0.6005, lr 9.879583809693738e-05
Step 49000 Loss: 0.5943, lr 9.879583809693738e-05
Step 49100 Loss: 0.5884, lr 9.879583809693738e-05
Step 49200 Loss: 0.5826, lr 9.879583809693738e-05
Step 49300 Loss: 0.5767, lr 9.879583809693738e-05
Step 49400 Loss: 0.5718, lr 9.879583809693738e-05
Step 49500 Loss: 0.5668, lr 9.879583809693738e-05
Step 49600 Loss: 0.5619, lr 9.879583809693738e-05
Step 49700 Loss: 0.5578, lr 9.879583809693738e-05
Step 49800 Loss: 0.5531, lr 9.879583809693738e-05
Step 49900 Loss: 0.5487, lr 9.879583809693738e-05
Step 50000 Loss: 0.5452, lr 9.879583809693738e-05
Step 50100 Loss: 0.5425, lr 9.879583809693738e-05
Step 50200 Loss: 0.5390, lr 9.879583809693738e-05
Step 50300 Loss: 0.5367, lr 9.879583809693738e-05
Step 50400 Loss: 0.5332, lr 9.879583809693738e-05
Step 50500 Loss: 0.5302, lr 9.879583809693738e-05
Step 50600 Loss: 0.5275, lr 9.879583809693738e-05
Step 50700 Loss: 0.5245, lr 9.879583809693738e-05
Step 50800 Loss: 0.5213, lr 9.879583809693738e-05
Step 50900 Loss: 0.5182, lr 9.879583809693738e-05
Step 51000 Loss: 0.5153, lr 9.879583809693738e-05
Step 51100 Loss: 0.5132, lr 9.879583809693738e-05
Step 51200 Loss: 0.5111, lr 9.879583809693738e-05
Step 51300 Loss: 0.5094, lr 9.879583809693738e-05
Step 51400 Loss: 0.5077, lr 9.879583809693738e-05
Step 51500 Loss: 0.5065, lr 9.879583809693738e-05
Step 51600 Loss: 0.5050, lr 9.879583809693738e-05
Step 51700 Loss: 0.5035, lr 9.879583809693738e-05
Step 51800 Loss: 0.5023, lr 9.879583809693738e-05
Step 51900 Loss: 0.5010, lr 9.879583809693738e-05
Step 52000 Loss: 0.4997, lr 9.879583809693738e-05
Step 52100 Loss: 0.4995, lr 9.879583809693738e-05
Step 52200 Loss: 0.4988, lr 9.879583809693738e-05
Step 52300 Loss: 0.4982, lr 9.879583809693738e-05
Step 52400 Loss: 0.4986, lr 9.879583809693738e-05
Step 52500 Loss: 0.4984, lr 9.879583809693738e-05
Step 52600 Loss: 0.4985, lr 9.879583809693738e-05
Step 52700 Loss: 0.4986, lr 9.879583809693738e-05
Step 52800 Loss: 0.4986, lr 9.879583809693738e-05
Step 52900 Loss: 0.4992, lr 9.879583809693738e-05
Step 53000 Loss: 0.4996, lr 9.879583809693738e-05
Step 53100 Loss: 0.5005, lr 9.879583809693738e-05
Step 53200 Loss: 0.5019, lr 9.879583809693738e-05
Step 53300 Loss: 0.5028, lr 9.879583809693738e-05
Step 53400 Loss: 0.5052, lr 9.879583809693738e-05
Step 53500 Loss: 0.5067, lr 9.879583809693738e-05
Step 53600 Loss: 0.5081, lr 9.879583809693738e-05
Step 53700 Loss: 0.5086, lr 9.879583809693738e-05
Step 53800 Loss: 0.5086, lr 9.879583809693738e-05
Step 53900 Loss: 0.5083, lr 9.879583809693738e-05
Step 54000 Loss: 0.5085, lr 9.879583809693738e-05
Train Epoch: [8/100] Loss: 0.5091,lr 0.000099
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6756
Step 54100 Loss: 1.4598, lr 9.842915805643157e-05
Step 54200 Loss: 1.1279, lr 9.842915805643157e-05
Step 54300 Loss: 1.0001, lr 9.842915805643157e-05
Step 54400 Loss: 0.9124, lr 9.842915805643157e-05
Step 54500 Loss: 0.8586, lr 9.842915805643157e-05
Step 54600 Loss: 0.8114, lr 9.842915805643157e-05
Step 54700 Loss: 0.7726, lr 9.842915805643157e-05
Step 54800 Loss: 0.7416, lr 9.842915805643157e-05
Step 54900 Loss: 0.7172, lr 9.842915805643157e-05
Step 55000 Loss: 0.6949, lr 9.842915805643157e-05
Step 55100 Loss: 0.6769, lr 9.842915805643157e-05
Step 55200 Loss: 0.6614, lr 9.842915805643157e-05
Step 55300 Loss: 0.6474, lr 9.842915805643157e-05
Step 55400 Loss: 0.6350, lr 9.842915805643157e-05
Step 55500 Loss: 0.6244, lr 9.842915805643157e-05
Step 55600 Loss: 0.6147, lr 9.842915805643157e-05
Step 55700 Loss: 0.6052, lr 9.842915805643157e-05
Step 55800 Loss: 0.5976, lr 9.842915805643157e-05
Step 55900 Loss: 0.5900, lr 9.842915805643157e-05
Step 56000 Loss: 0.5828, lr 9.842915805643157e-05
Step 56100 Loss: 0.5755, lr 9.842915805643157e-05
Step 56200 Loss: 0.5698, lr 9.842915805643157e-05
Step 56300 Loss: 0.5651, lr 9.842915805643157e-05
Step 56400 Loss: 0.5605, lr 9.842915805643157e-05
Step 56500 Loss: 0.5561, lr 9.842915805643157e-05
Step 56600 Loss: 0.5517, lr 9.842915805643157e-05
Step 56700 Loss: 0.5472, lr 9.842915805643157e-05
Step 56800 Loss: 0.5438, lr 9.842915805643157e-05
Step 56900 Loss: 0.5395, lr 9.842915805643157e-05
Step 57000 Loss: 0.5362, lr 9.842915805643157e-05
Step 57100 Loss: 0.5328, lr 9.842915805643157e-05
Step 57200 Loss: 0.5298, lr 9.842915805643157e-05
Step 57300 Loss: 0.5331, lr 9.842915805643157e-05
Step 57400 Loss: 0.5314, lr 9.842915805643157e-05
Step 57500 Loss: 0.5288, lr 9.842915805643157e-05
Step 57600 Loss: 0.5258, lr 9.842915805643157e-05
Step 57700 Loss: 0.5235, lr 9.842915805643157e-05
Step 57800 Loss: 0.5206, lr 9.842915805643157e-05
Step 57900 Loss: 0.5183, lr 9.842915805643157e-05
Step 58000 Loss: 0.5157, lr 9.842915805643157e-05
Step 58100 Loss: 0.5140, lr 9.842915805643157e-05
Step 58200 Loss: 0.5120, lr 9.842915805643157e-05
Step 58300 Loss: 0.5106, lr 9.842915805643157e-05
Step 58400 Loss: 0.5088, lr 9.842915805643157e-05
Step 58500 Loss: 0.5078, lr 9.842915805643157e-05
Step 58600 Loss: 0.5060, lr 9.842915805643157e-05
Step 58700 Loss: 0.5047, lr 9.842915805643157e-05
Step 58800 Loss: 0.5037, lr 9.842915805643157e-05
Step 58900 Loss: 0.5031, lr 9.842915805643157e-05
Step 59000 Loss: 0.5027, lr 9.842915805643157e-05
Step 59100 Loss: 0.5022, lr 9.842915805643157e-05
Step 59200 Loss: 0.5020, lr 9.842915805643157e-05
Step 59300 Loss: 0.5013, lr 9.842915805643157e-05
Step 59400 Loss: 0.5011, lr 9.842915805643157e-05
Step 59500 Loss: 0.5008, lr 9.842915805643157e-05
Step 59600 Loss: 0.5005, lr 9.842915805643157e-05
Step 59700 Loss: 0.5007, lr 9.842915805643157e-05
Step 59800 Loss: 0.5014, lr 9.842915805643157e-05
Step 59900 Loss: 0.5022, lr 9.842915805643157e-05
Step 60000 Loss: 0.5032, lr 9.842915805643157e-05
Step 60100 Loss: 0.5046, lr 9.842915805643157e-05
Step 60200 Loss: 0.5064, lr 9.842915805643157e-05
Step 60300 Loss: 0.5083, lr 9.842915805643157e-05
Step 60400 Loss: 0.5090, lr 9.842915805643157e-05
Step 60500 Loss: 0.5097, lr 9.842915805643157e-05
Step 60600 Loss: 0.5099, lr 9.842915805643157e-05
Step 60700 Loss: 0.5090, lr 9.842915805643157e-05
Step 60800 Loss: 0.5091, lr 9.842915805643157e-05
Train Epoch: [9/100] Loss: 0.5092,lr 0.000098
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 60900 Loss: 1.0010, lr 9.801468428384717e-05
Step 61000 Loss: 0.8644, lr 9.801468428384717e-05
Step 61100 Loss: 0.7971, lr 9.801468428384717e-05
Step 61200 Loss: 0.7525, lr 9.801468428384717e-05
Step 61300 Loss: 0.7222, lr 9.801468428384717e-05
Step 61400 Loss: 0.6969, lr 9.801468428384717e-05
Step 61500 Loss: 0.6783, lr 9.801468428384717e-05
Step 61600 Loss: 0.6609, lr 9.801468428384717e-05
Step 61700 Loss: 0.6463, lr 9.801468428384717e-05
Step 61800 Loss: 0.6329, lr 9.801468428384717e-05
Step 61900 Loss: 0.6221, lr 9.801468428384717e-05
Step 62000 Loss: 0.6111, lr 9.801468428384717e-05
Step 62100 Loss: 0.6020, lr 9.801468428384717e-05
Step 62200 Loss: 0.5923, lr 9.801468428384717e-05
Step 62300 Loss: 0.5846, lr 9.801468428384717e-05
Step 62400 Loss: 0.5765, lr 9.801468428384717e-05
Step 62500 Loss: 0.5695, lr 9.801468428384717e-05
Step 62600 Loss: 0.5639, lr 9.801468428384717e-05
Step 62700 Loss: 0.5575, lr 9.801468428384717e-05
Step 62800 Loss: 0.5516, lr 9.801468428384717e-05
Step 62900 Loss: 0.5461, lr 9.801468428384717e-05
Step 63000 Loss: 0.5408, lr 9.801468428384717e-05
Step 63100 Loss: 0.5356, lr 9.801468428384717e-05
Step 63200 Loss: 0.5313, lr 9.801468428384717e-05
Step 63300 Loss: 0.5269, lr 9.801468428384717e-05
Step 63400 Loss: 0.5227, lr 9.801468428384717e-05
Step 63500 Loss: 0.5192, lr 9.801468428384717e-05
Step 63600 Loss: 0.5160, lr 9.801468428384717e-05
Step 63700 Loss: 0.5125, lr 9.801468428384717e-05
Step 63800 Loss: 0.5096, lr 9.801468428384717e-05
Step 63900 Loss: 0.5061, lr 9.801468428384717e-05
Step 64000 Loss: 0.5031, lr 9.801468428384717e-05
Step 64100 Loss: 0.5004, lr 9.801468428384717e-05
Step 64200 Loss: 0.4974, lr 9.801468428384717e-05
Step 64300 Loss: 0.4948, lr 9.801468428384717e-05
Step 64400 Loss: 0.4917, lr 9.801468428384717e-05
Step 64500 Loss: 0.4892, lr 9.801468428384717e-05
Step 64600 Loss: 0.4868, lr 9.801468428384717e-05
Step 64700 Loss: 0.4851, lr 9.801468428384717e-05
Step 64800 Loss: 0.4834, lr 9.801468428384717e-05
Step 64900 Loss: 0.4816, lr 9.801468428384717e-05
Step 65000 Loss: 0.4803, lr 9.801468428384717e-05
Step 65100 Loss: 0.4792, lr 9.801468428384717e-05
Step 65200 Loss: 0.4783, lr 9.801468428384717e-05
Step 65300 Loss: 0.4774, lr 9.801468428384717e-05
Step 65400 Loss: 0.4762, lr 9.801468428384717e-05
Step 65500 Loss: 0.4754, lr 9.801468428384717e-05
Step 65600 Loss: 0.4751, lr 9.801468428384717e-05
Step 65700 Loss: 0.4748, lr 9.801468428384717e-05
Step 65800 Loss: 0.4747, lr 9.801468428384717e-05
Step 65900 Loss: 0.4744, lr 9.801468428384717e-05
Step 66000 Loss: 0.4742, lr 9.801468428384717e-05
Step 66100 Loss: 0.4742, lr 9.801468428384717e-05
Step 66200 Loss: 0.4741, lr 9.801468428384717e-05
Step 66300 Loss: 0.4743, lr 9.801468428384717e-05
Step 66400 Loss: 0.4748, lr 9.801468428384717e-05
Step 66500 Loss: 0.4751, lr 9.801468428384717e-05
Step 66600 Loss: 0.4758, lr 9.801468428384717e-05
Step 66700 Loss: 0.4770, lr 9.801468428384717e-05
Step 66800 Loss: 0.4779, lr 9.801468428384717e-05
Step 66900 Loss: 0.4803, lr 9.801468428384717e-05
Step 67000 Loss: 0.4818, lr 9.801468428384717e-05
Step 67100 Loss: 0.4834, lr 9.801468428384717e-05
Step 67200 Loss: 0.4848, lr 9.801468428384717e-05
Step 67300 Loss: 0.4854, lr 9.801468428384717e-05
Step 67400 Loss: 0.4857, lr 9.801468428384717e-05
Step 67500 Loss: 0.4856, lr 9.801468428384717e-05
Train Epoch: [10/100] Loss: 0.4859,lr 0.000098
Model Saving at epoch 10
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6755
Step 67600 Loss: 1.1373, lr 9.75528258147577e-05
Step 67700 Loss: 0.9203, lr 9.75528258147577e-05
Step 67800 Loss: 0.8452, lr 9.75528258147577e-05
Step 67900 Loss: 0.8235, lr 9.75528258147577e-05
Step 68000 Loss: 0.8179, lr 9.75528258147577e-05
Step 68100 Loss: 0.7961, lr 9.75528258147577e-05
Step 68200 Loss: 0.7704, lr 9.75528258147577e-05
Step 68300 Loss: 0.7486, lr 9.75528258147577e-05
Step 68400 Loss: 0.7281, lr 9.75528258147577e-05
Step 68500 Loss: 0.7084, lr 9.75528258147577e-05
Step 68600 Loss: 0.6904, lr 9.75528258147577e-05
Step 68700 Loss: 0.6749, lr 9.75528258147577e-05
Step 68800 Loss: 0.6602, lr 9.75528258147577e-05
Step 68900 Loss: 0.6490, lr 9.75528258147577e-05
Step 69000 Loss: 0.6375, lr 9.75528258147577e-05
Step 69100 Loss: 0.6272, lr 9.75528258147577e-05
Step 69200 Loss: 0.6169, lr 9.75528258147577e-05
Step 69300 Loss: 0.6084, lr 9.75528258147577e-05
Step 69400 Loss: 0.6000, lr 9.75528258147577e-05
Step 69500 Loss: 0.5922, lr 9.75528258147577e-05
Step 69600 Loss: 0.5848, lr 9.75528258147577e-05
Step 69700 Loss: 0.5780, lr 9.75528258147577e-05
Step 69800 Loss: 0.5714, lr 9.75528258147577e-05
Step 69900 Loss: 0.5656, lr 9.75528258147577e-05
Step 70000 Loss: 0.5603, lr 9.75528258147577e-05
Step 70100 Loss: 0.5547, lr 9.75528258147577e-05
Step 70200 Loss: 0.5497, lr 9.75528258147577e-05
Step 70300 Loss: 0.5451, lr 9.75528258147577e-05
Step 70400 Loss: 0.5400, lr 9.75528258147577e-05
Step 70500 Loss: 0.5359, lr 9.75528258147577e-05
Step 70600 Loss: 0.5319, lr 9.75528258147577e-05
Step 70700 Loss: 0.5278, lr 9.75528258147577e-05
Step 70800 Loss: 0.5241, lr 9.75528258147577e-05
Step 70900 Loss: 0.5201, lr 9.75528258147577e-05
Step 71000 Loss: 0.5165, lr 9.75528258147577e-05
Step 71100 Loss: 0.5127, lr 9.75528258147577e-05
Step 71200 Loss: 0.5092, lr 9.75528258147577e-05
Step 71300 Loss: 0.5056, lr 9.75528258147577e-05
Step 71400 Loss: 0.5027, lr 9.75528258147577e-05
Step 71500 Loss: 0.4996, lr 9.75528258147577e-05
Step 71600 Loss: 0.4978, lr 9.75528258147577e-05
Step 71700 Loss: 0.4955, lr 9.75528258147577e-05
Step 71800 Loss: 0.4940, lr 9.75528258147577e-05
Step 71900 Loss: 0.4926, lr 9.75528258147577e-05
Step 72000 Loss: 0.4919, lr 9.75528258147577e-05
Step 72100 Loss: 0.4907, lr 9.75528258147577e-05
Step 72200 Loss: 0.4899, lr 9.75528258147577e-05
Step 72300 Loss: 0.4889, lr 9.75528258147577e-05
Step 72400 Loss: 0.4888, lr 9.75528258147577e-05
Step 72500 Loss: 0.4886, lr 9.75528258147577e-05
Step 72600 Loss: 0.4882, lr 9.75528258147577e-05
Step 72700 Loss: 0.4881, lr 9.75528258147577e-05
Step 72800 Loss: 0.4875, lr 9.75528258147577e-05
Step 72900 Loss: 0.4874, lr 9.75528258147577e-05
Step 73000 Loss: 0.4873, lr 9.75528258147577e-05
Step 73100 Loss: 0.4876, lr 9.75528258147577e-05
Step 73200 Loss: 0.4885, lr 9.75528258147577e-05
Step 73300 Loss: 0.4901, lr 9.75528258147577e-05
Step 73400 Loss: 0.4914, lr 9.75528258147577e-05
Step 73500 Loss: 0.4933, lr 9.75528258147577e-05
Step 73600 Loss: 0.4946, lr 9.75528258147577e-05
Step 73700 Loss: 0.4967, lr 9.75528258147577e-05
Step 73800 Loss: 0.4978, lr 9.75528258147577e-05
Step 73900 Loss: 0.4991, lr 9.75528258147577e-05
Step 74000 Loss: 0.4997, lr 9.75528258147577e-05
Step 74100 Loss: 0.5002, lr 9.75528258147577e-05
Step 74200 Loss: 0.5001, lr 9.75528258147577e-05
Step 74300 Loss: 0.5000, lr 9.75528258147577e-05
Train Epoch: [11/100] Loss: 0.5001,lr 0.000098
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 74400 Loss: 1.0239, lr 9.70440384477113e-05
Step 74500 Loss: 0.8553, lr 9.70440384477113e-05
Step 74600 Loss: 0.7725, lr 9.70440384477113e-05
Step 74700 Loss: 0.7231, lr 9.70440384477113e-05
Step 74800 Loss: 0.6931, lr 9.70440384477113e-05
Step 74900 Loss: 0.6685, lr 9.70440384477113e-05
Step 75000 Loss: 0.6488, lr 9.70440384477113e-05
Step 75100 Loss: 0.6324, lr 9.70440384477113e-05
Step 75200 Loss: 0.6181, lr 9.70440384477113e-05
Step 75300 Loss: 0.6068, lr 9.70440384477113e-05
Step 75400 Loss: 0.5976, lr 9.70440384477113e-05
Step 75500 Loss: 0.5895, lr 9.70440384477113e-05
Step 75600 Loss: 0.5820, lr 9.70440384477113e-05
Step 75700 Loss: 0.5745, lr 9.70440384477113e-05
Step 75800 Loss: 0.5686, lr 9.70440384477113e-05
Step 75900 Loss: 0.5629, lr 9.70440384477113e-05
Step 76000 Loss: 0.5567, lr 9.70440384477113e-05
Step 76100 Loss: 0.5522, lr 9.70440384477113e-05
Step 76200 Loss: 0.5461, lr 9.70440384477113e-05
Step 76300 Loss: 0.5405, lr 9.70440384477113e-05
Step 76400 Loss: 0.5350, lr 9.70440384477113e-05
Step 76500 Loss: 0.5302, lr 9.70440384477113e-05
Step 76600 Loss: 0.5256, lr 9.70440384477113e-05
Step 76700 Loss: 0.5213, lr 9.70440384477113e-05
Step 76800 Loss: 0.5166, lr 9.70440384477113e-05
Step 76900 Loss: 0.5125, lr 9.70440384477113e-05
Step 77000 Loss: 0.5089, lr 9.70440384477113e-05
Step 77100 Loss: 0.5063, lr 9.70440384477113e-05
Step 77200 Loss: 0.5034, lr 9.70440384477113e-05
Step 77300 Loss: 0.5006, lr 9.70440384477113e-05
Step 77400 Loss: 0.4971, lr 9.70440384477113e-05
Step 77500 Loss: 0.4941, lr 9.70440384477113e-05
Step 77600 Loss: 0.4919, lr 9.70440384477113e-05
Step 77700 Loss: 0.4891, lr 9.70440384477113e-05
Step 77800 Loss: 0.4872, lr 9.70440384477113e-05
Step 77900 Loss: 0.4854, lr 9.70440384477113e-05
Step 78000 Loss: 0.4834, lr 9.70440384477113e-05
Step 78100 Loss: 0.4815, lr 9.70440384477113e-05
Step 78200 Loss: 0.4797, lr 9.70440384477113e-05
Step 78300 Loss: 0.4782, lr 9.70440384477113e-05
Step 78400 Loss: 0.4768, lr 9.70440384477113e-05
Step 78500 Loss: 0.4761, lr 9.70440384477113e-05
Step 78600 Loss: 0.4745, lr 9.70440384477113e-05
Step 78700 Loss: 0.4733, lr 9.70440384477113e-05
Step 78800 Loss: 0.4719, lr 9.70440384477113e-05
Step 78900 Loss: 0.4699, lr 9.70440384477113e-05
Step 79000 Loss: 0.4684, lr 9.70440384477113e-05
Step 79100 Loss: 0.4676, lr 9.70440384477113e-05
Step 79200 Loss: 0.4669, lr 9.70440384477113e-05
Step 79300 Loss: 0.4666, lr 9.70440384477113e-05
Step 79400 Loss: 0.4660, lr 9.70440384477113e-05
Step 79500 Loss: 0.4655, lr 9.70440384477113e-05
Step 79600 Loss: 0.4649, lr 9.70440384477113e-05
Step 79700 Loss: 0.4648, lr 9.70440384477113e-05
Step 79800 Loss: 0.4646, lr 9.70440384477113e-05
Step 79900 Loss: 0.4644, lr 9.70440384477113e-05
Step 80000 Loss: 0.4646, lr 9.70440384477113e-05
Step 80100 Loss: 0.4650, lr 9.70440384477113e-05
Step 80200 Loss: 0.4657, lr 9.70440384477113e-05
Step 80300 Loss: 0.4666, lr 9.70440384477113e-05
Step 80400 Loss: 0.4681, lr 9.70440384477113e-05
Step 80500 Loss: 0.4692, lr 9.70440384477113e-05
Step 80600 Loss: 0.4701, lr 9.70440384477113e-05
Step 80700 Loss: 0.4707, lr 9.70440384477113e-05
Step 80800 Loss: 0.4713, lr 9.70440384477113e-05
Step 80900 Loss: 0.4717, lr 9.70440384477113e-05
Step 81000 Loss: 0.4715, lr 9.70440384477113e-05
Train Epoch: [12/100] Loss: 0.4717,lr 0.000097
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 81100 Loss: 1.2405, lr 9.64888242944126e-05
Step 81200 Loss: 0.8643, lr 9.64888242944126e-05
Step 81300 Loss: 0.7847, lr 9.64888242944126e-05
Step 81400 Loss: 0.7420, lr 9.64888242944126e-05
Step 81500 Loss: 0.7087, lr 9.64888242944126e-05
Step 81600 Loss: 0.6880, lr 9.64888242944126e-05
Step 81700 Loss: 0.6681, lr 9.64888242944126e-05
Step 81800 Loss: 0.6521, lr 9.64888242944126e-05
Step 81900 Loss: 0.6384, lr 9.64888242944126e-05
Step 82000 Loss: 0.6265, lr 9.64888242944126e-05
Step 82100 Loss: 0.6166, lr 9.64888242944126e-05
Step 82200 Loss: 0.6073, lr 9.64888242944126e-05
Step 82300 Loss: 0.5974, lr 9.64888242944126e-05
Step 82400 Loss: 0.5897, lr 9.64888242944126e-05
Step 82500 Loss: 0.5826, lr 9.64888242944126e-05
Step 82600 Loss: 0.5769, lr 9.64888242944126e-05
Step 82700 Loss: 0.5705, lr 9.64888242944126e-05
Step 82800 Loss: 0.5646, lr 9.64888242944126e-05
Step 82900 Loss: 0.5593, lr 9.64888242944126e-05
Step 83000 Loss: 0.5546, lr 9.64888242944126e-05
Step 83100 Loss: 0.5489, lr 9.64888242944126e-05
Step 83200 Loss: 0.5441, lr 9.64888242944126e-05
Step 83300 Loss: 0.5389, lr 9.64888242944126e-05
Step 83400 Loss: 0.5341, lr 9.64888242944126e-05
Step 83500 Loss: 0.5298, lr 9.64888242944126e-05
Step 83600 Loss: 0.5251, lr 9.64888242944126e-05
Step 83700 Loss: 0.5209, lr 9.64888242944126e-05
Step 83800 Loss: 0.5177, lr 9.64888242944126e-05
Step 83900 Loss: 0.5140, lr 9.64888242944126e-05
Step 84000 Loss: 0.5105, lr 9.64888242944126e-05
Step 84100 Loss: 0.5070, lr 9.64888242944126e-05
Step 84200 Loss: 0.5035, lr 9.64888242944126e-05
Step 84300 Loss: 0.5006, lr 9.64888242944126e-05
Step 84400 Loss: 0.4971, lr 9.64888242944126e-05
Step 84500 Loss: 0.4937, lr 9.64888242944126e-05
Step 84600 Loss: 0.4904, lr 9.64888242944126e-05
Step 84700 Loss: 0.4874, lr 9.64888242944126e-05
Step 84800 Loss: 0.4841, lr 9.64888242944126e-05
Step 84900 Loss: 0.4814, lr 9.64888242944126e-05
Step 85000 Loss: 0.4789, lr 9.64888242944126e-05
Step 85100 Loss: 0.4773, lr 9.64888242944126e-05
Step 85200 Loss: 0.4751, lr 9.64888242944126e-05
Step 85300 Loss: 0.4736, lr 9.64888242944126e-05
Step 85400 Loss: 0.4716, lr 9.64888242944126e-05
Step 85500 Loss: 0.4699, lr 9.64888242944126e-05
Step 85600 Loss: 0.4684, lr 9.64888242944126e-05
Step 85700 Loss: 0.4674, lr 9.64888242944126e-05
Step 85800 Loss: 0.4662, lr 9.64888242944126e-05
Step 85900 Loss: 0.4658, lr 9.64888242944126e-05
Step 86000 Loss: 0.4658, lr 9.64888242944126e-05
Step 86100 Loss: 0.4650, lr 9.64888242944126e-05
Step 86200 Loss: 0.4647, lr 9.64888242944126e-05
Step 86300 Loss: 0.4638, lr 9.64888242944126e-05
Step 86400 Loss: 0.4637, lr 9.64888242944126e-05
Step 86500 Loss: 0.4633, lr 9.64888242944126e-05
Step 86600 Loss: 0.4632, lr 9.64888242944126e-05
Step 86700 Loss: 0.4631, lr 9.64888242944126e-05
Step 86800 Loss: 0.4630, lr 9.64888242944126e-05
Step 86900 Loss: 0.4635, lr 9.64888242944126e-05
Step 87000 Loss: 0.4642, lr 9.64888242944126e-05
Step 87100 Loss: 0.4649, lr 9.64888242944126e-05
Step 87200 Loss: 0.4662, lr 9.64888242944126e-05
Step 87300 Loss: 0.4668, lr 9.64888242944126e-05
Step 87400 Loss: 0.4677, lr 9.64888242944126e-05
Step 87500 Loss: 0.4683, lr 9.64888242944126e-05
Step 87600 Loss: 0.4686, lr 9.64888242944126e-05
Step 87700 Loss: 0.4677, lr 9.64888242944126e-05
Step 87800 Loss: 0.4669, lr 9.64888242944126e-05
Train Epoch: [13/100] Loss: 0.4671,lr 0.000096
Calling G2SDataset.batch()
Done, time:  2.02 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 87900 Loss: 1.1387, lr 9.588773128419907e-05
Step 88000 Loss: 0.8942, lr 9.588773128419907e-05
Step 88100 Loss: 0.7986, lr 9.588773128419907e-05
Step 88200 Loss: 0.7544, lr 9.588773128419907e-05
Step 88300 Loss: 0.7269, lr 9.588773128419907e-05
Step 88400 Loss: 0.6976, lr 9.588773128419907e-05
Step 88500 Loss: 0.6701, lr 9.588773128419907e-05
Step 88600 Loss: 0.6495, lr 9.588773128419907e-05
Step 88700 Loss: 0.6304, lr 9.588773128419907e-05
Step 88800 Loss: 0.6155, lr 9.588773128419907e-05
Step 88900 Loss: 0.6034, lr 9.588773128419907e-05
Step 89000 Loss: 0.5920, lr 9.588773128419907e-05
Step 89100 Loss: 0.5836, lr 9.588773128419907e-05
Step 89200 Loss: 0.5746, lr 9.588773128419907e-05
Step 89300 Loss: 0.5669, lr 9.588773128419907e-05
Step 89400 Loss: 0.5599, lr 9.588773128419907e-05
Step 89500 Loss: 0.5537, lr 9.588773128419907e-05
Step 89600 Loss: 0.5487, lr 9.588773128419907e-05
Step 89700 Loss: 0.5441, lr 9.588773128419907e-05
Step 89800 Loss: 0.5396, lr 9.588773128419907e-05
Step 89900 Loss: 0.5347, lr 9.588773128419907e-05
Step 90000 Loss: 0.5302, lr 9.588773128419907e-05
Step 90100 Loss: 0.5265, lr 9.588773128419907e-05
Step 90200 Loss: 0.5233, lr 9.588773128419907e-05
Step 90300 Loss: 0.5191, lr 9.588773128419907e-05
Step 90400 Loss: 0.5154, lr 9.588773128419907e-05
Step 90500 Loss: 0.5128, lr 9.588773128419907e-05
Step 90600 Loss: 0.5108, lr 9.588773128419907e-05
Step 90700 Loss: 0.5081, lr 9.588773128419907e-05
Step 90800 Loss: 0.5064, lr 9.588773128419907e-05
Step 90900 Loss: 0.5034, lr 9.588773128419907e-05
Step 91000 Loss: 0.4999, lr 9.588773128419907e-05
Step 91100 Loss: 0.4971, lr 9.588773128419907e-05
Step 91200 Loss: 0.4936, lr 9.588773128419907e-05
Step 91300 Loss: 0.4903, lr 9.588773128419907e-05
Step 91400 Loss: 0.4865, lr 9.588773128419907e-05
Step 91500 Loss: 0.4833, lr 9.588773128419907e-05
Step 91600 Loss: 0.4804, lr 9.588773128419907e-05
Step 91700 Loss: 0.4777, lr 9.588773128419907e-05
Step 91800 Loss: 0.4754, lr 9.588773128419907e-05
Step 91900 Loss: 0.4735, lr 9.588773128419907e-05
Step 92000 Loss: 0.4719, lr 9.588773128419907e-05
Step 92100 Loss: 0.4707, lr 9.588773128419907e-05
Step 92200 Loss: 0.4689, lr 9.588773128419907e-05
Step 92300 Loss: 0.4674, lr 9.588773128419907e-05
Step 92400 Loss: 0.4656, lr 9.588773128419907e-05
Step 92500 Loss: 0.4642, lr 9.588773128419907e-05
Step 92600 Loss: 0.4630, lr 9.588773128419907e-05
Step 92700 Loss: 0.4625, lr 9.588773128419907e-05
Step 92800 Loss: 0.4621, lr 9.588773128419907e-05
Step 92900 Loss: 0.4610, lr 9.588773128419907e-05
Step 93000 Loss: 0.4605, lr 9.588773128419907e-05
Step 93100 Loss: 0.4596, lr 9.588773128419907e-05
Step 93200 Loss: 0.4591, lr 9.588773128419907e-05
Step 93300 Loss: 0.4588, lr 9.588773128419907e-05
Step 93400 Loss: 0.4582, lr 9.588773128419907e-05
Step 93500 Loss: 0.4581, lr 9.588773128419907e-05
Step 93600 Loss: 0.4581, lr 9.588773128419907e-05
Step 93700 Loss: 0.4585, lr 9.588773128419907e-05
Step 93800 Loss: 0.4590, lr 9.588773128419907e-05
Step 93900 Loss: 0.4599, lr 9.588773128419907e-05
Step 94000 Loss: 0.4607, lr 9.588773128419907e-05
Step 94100 Loss: 0.4615, lr 9.588773128419907e-05
Step 94200 Loss: 0.4615, lr 9.588773128419907e-05
Step 94300 Loss: 0.4614, lr 9.588773128419907e-05
Step 94400 Loss: 0.4612, lr 9.588773128419907e-05
Step 94500 Loss: 0.4603, lr 9.588773128419907e-05
Train Epoch: [14/100] Loss: 0.4603,lr 0.000096
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 94600 Loss: 1.4330, lr 9.5241352623301e-05
Step 94700 Loss: 0.8606, lr 9.5241352623301e-05
Step 94800 Loss: 0.7539, lr 9.5241352623301e-05
Step 94900 Loss: 0.7012, lr 9.5241352623301e-05
Step 95000 Loss: 0.6661, lr 9.5241352623301e-05
Step 95100 Loss: 0.6426, lr 9.5241352623301e-05
Step 95200 Loss: 0.6205, lr 9.5241352623301e-05
Step 95300 Loss: 0.6023, lr 9.5241352623301e-05
Step 95400 Loss: 0.5895, lr 9.5241352623301e-05
Step 95500 Loss: 0.5766, lr 9.5241352623301e-05
Step 95600 Loss: 0.5661, lr 9.5241352623301e-05
Step 95700 Loss: 0.5561, lr 9.5241352623301e-05
Step 95800 Loss: 0.5462, lr 9.5241352623301e-05
Step 95900 Loss: 0.5370, lr 9.5241352623301e-05
Step 96000 Loss: 0.5291, lr 9.5241352623301e-05
Step 96100 Loss: 0.5223, lr 9.5241352623301e-05
Step 96200 Loss: 0.5154, lr 9.5241352623301e-05
Step 96300 Loss: 0.5102, lr 9.5241352623301e-05
Step 96400 Loss: 0.5049, lr 9.5241352623301e-05
Step 96500 Loss: 0.4995, lr 9.5241352623301e-05
Step 96600 Loss: 0.4946, lr 9.5241352623301e-05
Step 96700 Loss: 0.4898, lr 9.5241352623301e-05
Step 96800 Loss: 0.4849, lr 9.5241352623301e-05
Step 96900 Loss: 0.4807, lr 9.5241352623301e-05
Step 97000 Loss: 0.4776, lr 9.5241352623301e-05
Step 97100 Loss: 0.4733, lr 9.5241352623301e-05
Step 97200 Loss: 0.4698, lr 9.5241352623301e-05
Step 97300 Loss: 0.4669, lr 9.5241352623301e-05
Step 97400 Loss: 0.4647, lr 9.5241352623301e-05
Step 97500 Loss: 0.4628, lr 9.5241352623301e-05
Step 97600 Loss: 0.4606, lr 9.5241352623301e-05
Step 97700 Loss: 0.4574, lr 9.5241352623301e-05
Step 97800 Loss: 0.4549, lr 9.5241352623301e-05
Step 97900 Loss: 0.4523, lr 9.5241352623301e-05
Step 98000 Loss: 0.4497, lr 9.5241352623301e-05
Step 98100 Loss: 0.4468, lr 9.5241352623301e-05
Step 98200 Loss: 0.4442, lr 9.5241352623301e-05
Step 98300 Loss: 0.4414, lr 9.5241352623301e-05
Step 98400 Loss: 0.4393, lr 9.5241352623301e-05
Step 98500 Loss: 0.4372, lr 9.5241352623301e-05
Step 98600 Loss: 0.4359, lr 9.5241352623301e-05
Step 98700 Loss: 0.4339, lr 9.5241352623301e-05
Step 98800 Loss: 0.4329, lr 9.5241352623301e-05
Step 98900 Loss: 0.4314, lr 9.5241352623301e-05
Step 99000 Loss: 0.4306, lr 9.5241352623301e-05
Step 99100 Loss: 0.4294, lr 9.5241352623301e-05
Step 99200 Loss: 0.4280, lr 9.5241352623301e-05
Step 99300 Loss: 0.4266, lr 9.5241352623301e-05
Step 99400 Loss: 0.4262, lr 9.5241352623301e-05
Step 99500 Loss: 0.4256, lr 9.5241352623301e-05
Step 99600 Loss: 0.4249, lr 9.5241352623301e-05
Step 99700 Loss: 0.4245, lr 9.5241352623301e-05
Step 99800 Loss: 0.4238, lr 9.5241352623301e-05
Step 99900 Loss: 0.4233, lr 9.5241352623301e-05
Step 100000 Loss: 0.4231, lr 9.5241352623301e-05
Step 100100 Loss: 0.4228, lr 9.5241352623301e-05
Step 100200 Loss: 0.4229, lr 9.5241352623301e-05
Step 100300 Loss: 0.4231, lr 9.5241352623301e-05
Step 100400 Loss: 0.4235, lr 9.5241352623301e-05
Step 100500 Loss: 0.4243, lr 9.5241352623301e-05
Step 100600 Loss: 0.4249, lr 9.5241352623301e-05
Step 100700 Loss: 0.4264, lr 9.5241352623301e-05
Step 100800 Loss: 0.4274, lr 9.5241352623301e-05
Step 100900 Loss: 0.4282, lr 9.5241352623301e-05
Step 101000 Loss: 0.4287, lr 9.5241352623301e-05
Step 101100 Loss: 0.4296, lr 9.5241352623301e-05
Step 101200 Loss: 0.4292, lr 9.5241352623301e-05
Step 101300 Loss: 0.4291, lr 9.5241352623301e-05
Train Epoch: [15/100] Loss: 0.4291,lr 0.000095
Model Saving at epoch 15
Calling G2SDataset.batch()
Done, time:  2.23 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.36 s, total batches: 6755
Step 101400 Loss: 0.9722, lr 9.455032620941843e-05
Step 101500 Loss: 0.7915, lr 9.455032620941843e-05
Step 101600 Loss: 0.7178, lr 9.455032620941843e-05
Step 101700 Loss: 0.6688, lr 9.455032620941843e-05
Step 101800 Loss: 0.6420, lr 9.455032620941843e-05
Step 101900 Loss: 0.6184, lr 9.455032620941843e-05
Step 102000 Loss: 0.5969, lr 9.455032620941843e-05
Step 102100 Loss: 0.5824, lr 9.455032620941843e-05
Step 102200 Loss: 0.5691, lr 9.455032620941843e-05
Step 102300 Loss: 0.5568, lr 9.455032620941843e-05
Step 102400 Loss: 0.5472, lr 9.455032620941843e-05
Step 102500 Loss: 0.5383, lr 9.455032620941843e-05
Step 102600 Loss: 0.5296, lr 9.455032620941843e-05
Step 102700 Loss: 0.5210, lr 9.455032620941843e-05
Step 102800 Loss: 0.5142, lr 9.455032620941843e-05
Step 102900 Loss: 0.5081, lr 9.455032620941843e-05
Step 103000 Loss: 0.5019, lr 9.455032620941843e-05
Step 103100 Loss: 0.4965, lr 9.455032620941843e-05
Step 103200 Loss: 0.4915, lr 9.455032620941843e-05
Step 103300 Loss: 0.4863, lr 9.455032620941843e-05
Step 103400 Loss: 0.4817, lr 9.455032620941843e-05
Step 103500 Loss: 0.4771, lr 9.455032620941843e-05
Step 103600 Loss: 0.4725, lr 9.455032620941843e-05
Step 103700 Loss: 0.4680, lr 9.455032620941843e-05
Step 103800 Loss: 0.4638, lr 9.455032620941843e-05
Step 103900 Loss: 0.4607, lr 9.455032620941843e-05
Step 104000 Loss: 0.4572, lr 9.455032620941843e-05
Step 104100 Loss: 0.4551, lr 9.455032620941843e-05
Step 104200 Loss: 0.4520, lr 9.455032620941843e-05
Step 104300 Loss: 0.4495, lr 9.455032620941843e-05
Step 104400 Loss: 0.4468, lr 9.455032620941843e-05
Step 104500 Loss: 0.4440, lr 9.455032620941843e-05
Step 104600 Loss: 0.4416, lr 9.455032620941843e-05
Step 104700 Loss: 0.4390, lr 9.455032620941843e-05
Step 104800 Loss: 0.4363, lr 9.455032620941843e-05
Step 104900 Loss: 0.4334, lr 9.455032620941843e-05
Step 105000 Loss: 0.4313, lr 9.455032620941843e-05
Step 105100 Loss: 0.4289, lr 9.455032620941843e-05
Step 105200 Loss: 0.4267, lr 9.455032620941843e-05
Step 105300 Loss: 0.4250, lr 9.455032620941843e-05
Step 105400 Loss: 0.4240, lr 9.455032620941843e-05
Step 105500 Loss: 0.4226, lr 9.455032620941843e-05
Step 105600 Loss: 0.4213, lr 9.455032620941843e-05
Step 105700 Loss: 0.4197, lr 9.455032620941843e-05
Step 105800 Loss: 0.4187, lr 9.455032620941843e-05
Step 105900 Loss: 0.4168, lr 9.455032620941843e-05
Step 106000 Loss: 0.4155, lr 9.455032620941843e-05
Step 106100 Loss: 0.4145, lr 9.455032620941843e-05
Step 106200 Loss: 0.4141, lr 9.455032620941843e-05
Step 106300 Loss: 0.4138, lr 9.455032620941843e-05
Step 106400 Loss: 0.4130, lr 9.455032620941843e-05
Step 106500 Loss: 0.4126, lr 9.455032620941843e-05
Step 106600 Loss: 0.4118, lr 9.455032620941843e-05
Step 106700 Loss: 0.4116, lr 9.455032620941843e-05
Step 106800 Loss: 0.4116, lr 9.455032620941843e-05
Step 106900 Loss: 0.4113, lr 9.455032620941843e-05
Step 107000 Loss: 0.4111, lr 9.455032620941843e-05
Step 107100 Loss: 0.4120, lr 9.455032620941843e-05
Step 107200 Loss: 0.4135, lr 9.455032620941843e-05
Step 107300 Loss: 0.4147, lr 9.455032620941843e-05
Step 107400 Loss: 0.4161, lr 9.455032620941843e-05
Step 107500 Loss: 0.4176, lr 9.455032620941843e-05
Step 107600 Loss: 0.4193, lr 9.455032620941843e-05
Step 107700 Loss: 0.4201, lr 9.455032620941843e-05
Step 107800 Loss: 0.4208, lr 9.455032620941843e-05
Step 107900 Loss: 0.4212, lr 9.455032620941843e-05
Step 108000 Loss: 0.4209, lr 9.455032620941843e-05
Train Epoch: [16/100] Loss: 0.4217,lr 0.000095
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Step 108100 Loss: 1.7306, lr 9.381533400219321e-05
Step 108200 Loss: 0.8584, lr 9.381533400219321e-05
Step 108300 Loss: 0.7423, lr 9.381533400219321e-05
Step 108400 Loss: 0.6861, lr 9.381533400219321e-05
Step 108500 Loss: 0.6513, lr 9.381533400219321e-05
Step 108600 Loss: 0.6265, lr 9.381533400219321e-05
Step 108700 Loss: 0.6046, lr 9.381533400219321e-05
Step 108800 Loss: 0.5889, lr 9.381533400219321e-05
Step 108900 Loss: 0.5744, lr 9.381533400219321e-05
Step 109000 Loss: 0.5636, lr 9.381533400219321e-05
Step 109100 Loss: 0.5529, lr 9.381533400219321e-05
Step 109200 Loss: 0.5468, lr 9.381533400219321e-05
Step 109300 Loss: 0.5383, lr 9.381533400219321e-05
Step 109400 Loss: 0.5323, lr 9.381533400219321e-05
Step 109500 Loss: 0.5265, lr 9.381533400219321e-05
Step 109600 Loss: 0.5209, lr 9.381533400219321e-05
Step 109700 Loss: 0.5155, lr 9.381533400219321e-05
Step 109800 Loss: 0.5111, lr 9.381533400219321e-05
Step 109900 Loss: 0.5066, lr 9.381533400219321e-05
Step 110000 Loss: 0.5020, lr 9.381533400219321e-05
Step 110100 Loss: 0.4981, lr 9.381533400219321e-05
Step 110200 Loss: 0.4937, lr 9.381533400219321e-05
Step 110300 Loss: 0.4899, lr 9.381533400219321e-05
Step 110400 Loss: 0.4868, lr 9.381533400219321e-05
Step 110500 Loss: 0.4842, lr 9.381533400219321e-05
Step 110600 Loss: 0.4812, lr 9.381533400219321e-05
Step 110700 Loss: 0.4784, lr 9.381533400219321e-05
Step 110800 Loss: 0.4758, lr 9.381533400219321e-05
Step 110900 Loss: 0.4737, lr 9.381533400219321e-05
Step 111000 Loss: 0.4711, lr 9.381533400219321e-05
Step 111100 Loss: 0.4695, lr 9.381533400219321e-05
Step 111200 Loss: 0.4667, lr 9.381533400219321e-05
Step 111300 Loss: 0.4650, lr 9.381533400219321e-05
Step 111400 Loss: 0.4628, lr 9.381533400219321e-05
Step 111500 Loss: 0.4606, lr 9.381533400219321e-05
Step 111600 Loss: 0.4586, lr 9.381533400219321e-05
Step 111700 Loss: 0.4564, lr 9.381533400219321e-05
Step 111800 Loss: 0.4536, lr 9.381533400219321e-05
Step 111900 Loss: 0.4515, lr 9.381533400219321e-05
Step 112000 Loss: 0.4494, lr 9.381533400219321e-05
Step 112100 Loss: 0.4480, lr 9.381533400219321e-05
Step 112200 Loss: 0.4463, lr 9.381533400219321e-05
Step 112300 Loss: 0.4451, lr 9.381533400219321e-05
Step 112400 Loss: 0.4437, lr 9.381533400219321e-05
Step 112500 Loss: 0.4423, lr 9.381533400219321e-05
Step 112600 Loss: 0.4410, lr 9.381533400219321e-05
Step 112700 Loss: 0.4391, lr 9.381533400219321e-05
Step 112800 Loss: 0.4378, lr 9.381533400219321e-05
Step 112900 Loss: 0.4369, lr 9.381533400219321e-05
Step 113000 Loss: 0.4359, lr 9.381533400219321e-05
Step 113100 Loss: 0.4354, lr 9.381533400219321e-05
Step 113200 Loss: 0.4347, lr 9.381533400219321e-05
Step 113300 Loss: 0.4337, lr 9.381533400219321e-05
Step 113400 Loss: 0.4332, lr 9.381533400219321e-05
Step 113500 Loss: 0.4345, lr 9.381533400219321e-05
Step 113600 Loss: 0.4353, lr 9.381533400219321e-05
Step 113700 Loss: 0.4355, lr 9.381533400219321e-05
Step 113800 Loss: 0.4353, lr 9.381533400219321e-05
Step 113900 Loss: 0.4356, lr 9.381533400219321e-05
Step 114000 Loss: 0.4360, lr 9.381533400219321e-05
Step 114100 Loss: 0.4363, lr 9.381533400219321e-05
Step 114200 Loss: 0.4373, lr 9.381533400219321e-05
Step 114300 Loss: 0.4381, lr 9.381533400219321e-05
Step 114400 Loss: 0.4393, lr 9.381533400219321e-05
Step 114500 Loss: 0.4402, lr 9.381533400219321e-05
Step 114600 Loss: 0.4408, lr 9.381533400219321e-05
Step 114700 Loss: 0.4402, lr 9.381533400219321e-05
Step 114800 Loss: 0.4399, lr 9.381533400219321e-05
Train Epoch: [17/100] Loss: 0.4402,lr 0.000094
Calling G2SDataset.batch()
Done, time:  2.36 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6755
Step 114900 Loss: 1.0157, lr 9.303710135019722e-05
Step 115000 Loss: 0.8001, lr 9.303710135019722e-05
Step 115100 Loss: 0.7344, lr 9.303710135019722e-05
Step 115200 Loss: 0.6943, lr 9.303710135019722e-05
Step 115300 Loss: 0.6684, lr 9.303710135019722e-05
Step 115400 Loss: 0.6478, lr 9.303710135019722e-05
Step 115500 Loss: 0.6293, lr 9.303710135019722e-05
Step 115600 Loss: 0.6140, lr 9.303710135019722e-05
Step 115700 Loss: 0.6009, lr 9.303710135019722e-05
Step 115800 Loss: 0.5885, lr 9.303710135019722e-05
Step 115900 Loss: 0.5799, lr 9.303710135019722e-05
Step 116000 Loss: 0.5709, lr 9.303710135019722e-05
Step 116100 Loss: 0.5623, lr 9.303710135019722e-05
Step 116200 Loss: 0.5548, lr 9.303710135019722e-05
Step 116300 Loss: 0.5489, lr 9.303710135019722e-05
Step 116400 Loss: 0.5433, lr 9.303710135019722e-05
Step 116500 Loss: 0.5374, lr 9.303710135019722e-05
Step 116600 Loss: 0.5325, lr 9.303710135019722e-05
Step 116700 Loss: 0.5278, lr 9.303710135019722e-05
Step 116800 Loss: 0.5229, lr 9.303710135019722e-05
Step 116900 Loss: 0.5184, lr 9.303710135019722e-05
Step 117000 Loss: 0.5140, lr 9.303710135019722e-05
Step 117100 Loss: 0.5097, lr 9.303710135019722e-05
Step 117200 Loss: 0.5057, lr 9.303710135019722e-05
Step 117300 Loss: 0.5019, lr 9.303710135019722e-05
Step 117400 Loss: 0.4981, lr 9.303710135019722e-05
Step 117500 Loss: 0.4946, lr 9.303710135019722e-05
Step 117600 Loss: 0.4922, lr 9.303710135019722e-05
Step 117700 Loss: 0.4890, lr 9.303710135019722e-05
Step 117800 Loss: 0.4874, lr 9.303710135019722e-05
Step 117900 Loss: 0.4849, lr 9.303710135019722e-05
Step 118000 Loss: 0.4821, lr 9.303710135019722e-05
Step 118100 Loss: 0.4802, lr 9.303710135019722e-05
Step 118200 Loss: 0.4773, lr 9.303710135019722e-05
Step 118300 Loss: 0.4746, lr 9.303710135019722e-05
Step 118400 Loss: 0.4721, lr 9.303710135019722e-05
Step 118500 Loss: 0.4697, lr 9.303710135019722e-05
Step 118600 Loss: 0.4672, lr 9.303710135019722e-05
Step 118700 Loss: 0.4651, lr 9.303710135019722e-05
Step 118800 Loss: 0.4628, lr 9.303710135019722e-05
Step 118900 Loss: 0.4608, lr 9.303710135019722e-05
Step 119000 Loss: 0.4592, lr 9.303710135019722e-05
Step 119100 Loss: 0.4578, lr 9.303710135019722e-05
Step 119200 Loss: 0.4557, lr 9.303710135019722e-05
Step 119300 Loss: 0.4542, lr 9.303710135019722e-05
Step 119400 Loss: 0.4522, lr 9.303710135019722e-05
Step 119500 Loss: 0.4505, lr 9.303710135019722e-05
Step 119600 Loss: 0.4489, lr 9.303710135019722e-05
Step 119700 Loss: 0.4480, lr 9.303710135019722e-05
Step 119800 Loss: 0.4475, lr 9.303710135019722e-05
Step 119900 Loss: 0.4464, lr 9.303710135019722e-05
Step 120000 Loss: 0.4459, lr 9.303710135019722e-05
Step 120100 Loss: 0.4452, lr 9.303710135019722e-05
Step 120200 Loss: 0.4446, lr 9.303710135019722e-05
Step 120300 Loss: 0.4442, lr 9.303710135019722e-05
Step 120400 Loss: 0.4439, lr 9.303710135019722e-05
Step 120500 Loss: 0.4438, lr 9.303710135019722e-05
Step 120600 Loss: 0.4437, lr 9.303710135019722e-05
Step 120700 Loss: 0.4442, lr 9.303710135019722e-05
Step 120800 Loss: 0.4448, lr 9.303710135019722e-05
Step 120900 Loss: 0.4459, lr 9.303710135019722e-05
Step 121000 Loss: 0.4471, lr 9.303710135019722e-05
Step 121100 Loss: 0.4477, lr 9.303710135019722e-05
Step 121200 Loss: 0.4475, lr 9.303710135019722e-05
Step 121300 Loss: 0.4480, lr 9.303710135019722e-05
Step 121400 Loss: 0.4486, lr 9.303710135019722e-05
Step 121500 Loss: 0.4487, lr 9.303710135019722e-05
Step 121600 Loss: 0.4487, lr 9.303710135019722e-05
Train Epoch: [18/100] Loss: 0.4492,lr 0.000093
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 121700 Loss: 0.8617, lr 9.22163962751008e-05
Step 121800 Loss: 0.7495, lr 9.22163962751008e-05
Step 121900 Loss: 0.6967, lr 9.22163962751008e-05
Step 122000 Loss: 0.6602, lr 9.22163962751008e-05
Step 122100 Loss: 0.6364, lr 9.22163962751008e-05
Step 122200 Loss: 0.6151, lr 9.22163962751008e-05
Step 122300 Loss: 0.5974, lr 9.22163962751008e-05
Step 122400 Loss: 0.5816, lr 9.22163962751008e-05
Step 122500 Loss: 0.5686, lr 9.22163962751008e-05
Step 122600 Loss: 0.5573, lr 9.22163962751008e-05
Step 122700 Loss: 0.5482, lr 9.22163962751008e-05
Step 122800 Loss: 0.5387, lr 9.22163962751008e-05
Step 122900 Loss: 0.5310, lr 9.22163962751008e-05
Step 123000 Loss: 0.5223, lr 9.22163962751008e-05
Step 123100 Loss: 0.5160, lr 9.22163962751008e-05
Step 123200 Loss: 0.5094, lr 9.22163962751008e-05
Step 123300 Loss: 0.5035, lr 9.22163962751008e-05
Step 123400 Loss: 0.4990, lr 9.22163962751008e-05
Step 123500 Loss: 0.4932, lr 9.22163962751008e-05
Step 123600 Loss: 0.4883, lr 9.22163962751008e-05
Step 123700 Loss: 0.4834, lr 9.22163962751008e-05
Step 123800 Loss: 0.4786, lr 9.22163962751008e-05
Step 123900 Loss: 0.4740, lr 9.22163962751008e-05
Step 124000 Loss: 0.4706, lr 9.22163962751008e-05
Step 124100 Loss: 0.4664, lr 9.22163962751008e-05
Step 124200 Loss: 0.4624, lr 9.22163962751008e-05
Step 124300 Loss: 0.4603, lr 9.22163962751008e-05
Step 124400 Loss: 0.4581, lr 9.22163962751008e-05
Step 124500 Loss: 0.4553, lr 9.22163962751008e-05
Step 124600 Loss: 0.4530, lr 9.22163962751008e-05
Step 124700 Loss: 0.4502, lr 9.22163962751008e-05
Step 124800 Loss: 0.4475, lr 9.22163962751008e-05
Step 124900 Loss: 0.4449, lr 9.22163962751008e-05
Step 125000 Loss: 0.4422, lr 9.22163962751008e-05
Step 125100 Loss: 0.4397, lr 9.22163962751008e-05
Step 125200 Loss: 0.4369, lr 9.22163962751008e-05
Step 125300 Loss: 0.4346, lr 9.22163962751008e-05
Step 125400 Loss: 0.4326, lr 9.22163962751008e-05
Step 125500 Loss: 0.4305, lr 9.22163962751008e-05
Step 125600 Loss: 0.4292, lr 9.22163962751008e-05
Step 125700 Loss: 0.4275, lr 9.22163962751008e-05
Step 125800 Loss: 0.4264, lr 9.22163962751008e-05
Step 125900 Loss: 0.4249, lr 9.22163962751008e-05
Step 126000 Loss: 0.4238, lr 9.22163962751008e-05
Step 126100 Loss: 0.4227, lr 9.22163962751008e-05
Step 126200 Loss: 0.4207, lr 9.22163962751008e-05
Step 126300 Loss: 0.4195, lr 9.22163962751008e-05
Step 126400 Loss: 0.4187, lr 9.22163962751008e-05
Step 126500 Loss: 0.4180, lr 9.22163962751008e-05
Step 126600 Loss: 0.4180, lr 9.22163962751008e-05
Step 126700 Loss: 0.4174, lr 9.22163962751008e-05
Step 126800 Loss: 0.4170, lr 9.22163962751008e-05
Step 126900 Loss: 0.4164, lr 9.22163962751008e-05
Step 127000 Loss: 0.4165, lr 9.22163962751008e-05
Step 127100 Loss: 0.4164, lr 9.22163962751008e-05
Step 127200 Loss: 0.4163, lr 9.22163962751008e-05
Step 127300 Loss: 0.4163, lr 9.22163962751008e-05
Step 127400 Loss: 0.4172, lr 9.22163962751008e-05
Step 127500 Loss: 0.4180, lr 9.22163962751008e-05
Step 127600 Loss: 0.4183, lr 9.22163962751008e-05
Step 127700 Loss: 0.4194, lr 9.22163962751008e-05
Step 127800 Loss: 0.4197, lr 9.22163962751008e-05
Step 127900 Loss: 0.4198, lr 9.22163962751008e-05
Step 128000 Loss: 0.4200, lr 9.22163962751008e-05
Step 128100 Loss: 0.4196, lr 9.22163962751008e-05
Step 128200 Loss: 0.4197, lr 9.22163962751008e-05
Step 128300 Loss: 0.4193, lr 9.22163962751008e-05
Train Epoch: [19/100] Loss: 0.4195,lr 0.000092
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 128400 Loss: 1.2530, lr 9.135402871372814e-05
Step 128500 Loss: 0.9219, lr 9.135402871372814e-05
Step 128600 Loss: 0.7953, lr 9.135402871372814e-05
Step 128700 Loss: 0.7234, lr 9.135402871372814e-05
Step 128800 Loss: 0.6772, lr 9.135402871372814e-05
Step 128900 Loss: 0.6473, lr 9.135402871372814e-05
Step 129000 Loss: 0.6190, lr 9.135402871372814e-05
Step 129100 Loss: 0.5971, lr 9.135402871372814e-05
Step 129200 Loss: 0.5809, lr 9.135402871372814e-05
Step 129300 Loss: 0.5659, lr 9.135402871372814e-05
Step 129400 Loss: 0.5535, lr 9.135402871372814e-05
Step 129500 Loss: 0.5433, lr 9.135402871372814e-05
Step 129600 Loss: 0.5323, lr 9.135402871372814e-05
Step 129700 Loss: 0.5230, lr 9.135402871372814e-05
Step 129800 Loss: 0.5153, lr 9.135402871372814e-05
Step 129900 Loss: 0.5093, lr 9.135402871372814e-05
Step 130000 Loss: 0.5028, lr 9.135402871372814e-05
Step 130100 Loss: 0.4973, lr 9.135402871372814e-05
Step 130200 Loss: 0.4919, lr 9.135402871372814e-05
Step 130300 Loss: 0.4867, lr 9.135402871372814e-05
Step 130400 Loss: 0.4813, lr 9.135402871372814e-05
Step 130500 Loss: 0.4762, lr 9.135402871372814e-05
Step 130600 Loss: 0.4709, lr 9.135402871372814e-05
Step 130700 Loss: 0.4663, lr 9.135402871372814e-05
Step 130800 Loss: 0.4624, lr 9.135402871372814e-05
Step 130900 Loss: 0.4582, lr 9.135402871372814e-05
Step 131000 Loss: 0.4538, lr 9.135402871372814e-05
Step 131100 Loss: 0.4510, lr 9.135402871372814e-05
Step 131200 Loss: 0.4480, lr 9.135402871372814e-05
Step 131300 Loss: 0.4453, lr 9.135402871372814e-05
Step 131400 Loss: 0.4421, lr 9.135402871372814e-05
Step 131500 Loss: 0.4389, lr 9.135402871372814e-05
Step 131600 Loss: 0.4365, lr 9.135402871372814e-05
Step 131700 Loss: 0.4334, lr 9.135402871372814e-05
Step 131800 Loss: 0.4305, lr 9.135402871372814e-05
Step 131900 Loss: 0.4274, lr 9.135402871372814e-05
Step 132000 Loss: 0.4250, lr 9.135402871372814e-05
Step 132100 Loss: 0.4222, lr 9.135402871372814e-05
Step 132200 Loss: 0.4199, lr 9.135402871372814e-05
Step 132300 Loss: 0.4179, lr 9.135402871372814e-05
Step 132400 Loss: 0.4165, lr 9.135402871372814e-05
Step 132500 Loss: 0.4147, lr 9.135402871372814e-05
Step 132600 Loss: 0.4135, lr 9.135402871372814e-05
Step 132700 Loss: 0.4116, lr 9.135402871372814e-05
Step 132800 Loss: 0.4101, lr 9.135402871372814e-05
Step 132900 Loss: 0.4082, lr 9.135402871372814e-05
Step 133000 Loss: 0.4069, lr 9.135402871372814e-05
Step 133100 Loss: 0.4054, lr 9.135402871372814e-05
Step 133200 Loss: 0.4043, lr 9.135402871372814e-05
Step 133300 Loss: 0.4042, lr 9.135402871372814e-05
Step 133400 Loss: 0.4037, lr 9.135402871372814e-05
Step 133500 Loss: 0.4032, lr 9.135402871372814e-05
Step 133600 Loss: 0.4025, lr 9.135402871372814e-05
Step 133700 Loss: 0.4021, lr 9.135402871372814e-05
Step 133800 Loss: 0.4017, lr 9.135402871372814e-05
Step 133900 Loss: 0.4014, lr 9.135402871372814e-05
Step 134000 Loss: 0.4010, lr 9.135402871372814e-05
Step 134100 Loss: 0.4014, lr 9.135402871372814e-05
Step 134200 Loss: 0.4019, lr 9.135402871372814e-05
Step 134300 Loss: 0.4024, lr 9.135402871372814e-05
Step 134400 Loss: 0.4033, lr 9.135402871372814e-05
Step 134500 Loss: 0.4043, lr 9.135402871372814e-05
Step 134600 Loss: 0.4056, lr 9.135402871372814e-05
Step 134700 Loss: 0.4056, lr 9.135402871372814e-05
Step 134800 Loss: 0.4059, lr 9.135402871372814e-05
Step 134900 Loss: 0.4063, lr 9.135402871372814e-05
Step 135000 Loss: 0.4052, lr 9.135402871372814e-05
Step 135100 Loss: 0.4048, lr 9.135402871372814e-05
Train Epoch: [20/100] Loss: 0.4051,lr 0.000091
Model Saving at epoch 20
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6756
Step 135200 Loss: 0.9027, lr 9.045084971874742e-05
Step 135300 Loss: 0.7723, lr 9.045084971874742e-05
Step 135400 Loss: 0.7081, lr 9.045084971874742e-05
Step 135500 Loss: 0.6684, lr 9.045084971874742e-05
Step 135600 Loss: 0.6433, lr 9.045084971874742e-05
Step 135700 Loss: 0.6208, lr 9.045084971874742e-05
Step 135800 Loss: 0.6012, lr 9.045084971874742e-05
Step 135900 Loss: 0.5857, lr 9.045084971874742e-05
Step 136000 Loss: 0.5714, lr 9.045084971874742e-05
Step 136100 Loss: 0.5596, lr 9.045084971874742e-05
Step 136200 Loss: 0.5505, lr 9.045084971874742e-05
Step 136300 Loss: 0.5408, lr 9.045084971874742e-05
Step 136400 Loss: 0.5320, lr 9.045084971874742e-05
Step 136500 Loss: 0.5238, lr 9.045084971874742e-05
Step 136600 Loss: 0.5177, lr 9.045084971874742e-05
Step 136700 Loss: 0.5109, lr 9.045084971874742e-05
Step 136800 Loss: 0.5041, lr 9.045084971874742e-05
Step 136900 Loss: 0.4990, lr 9.045084971874742e-05
Step 137000 Loss: 0.4933, lr 9.045084971874742e-05
Step 137100 Loss: 0.4877, lr 9.045084971874742e-05
Step 137200 Loss: 0.4823, lr 9.045084971874742e-05
Step 137300 Loss: 0.4773, lr 9.045084971874742e-05
Step 137400 Loss: 0.4727, lr 9.045084971874742e-05
Step 137500 Loss: 0.4684, lr 9.045084971874742e-05
Step 137600 Loss: 0.4638, lr 9.045084971874742e-05
Step 137700 Loss: 0.4593, lr 9.045084971874742e-05
Step 137800 Loss: 0.4560, lr 9.045084971874742e-05
Step 137900 Loss: 0.4537, lr 9.045084971874742e-05
Step 138000 Loss: 0.4510, lr 9.045084971874742e-05
Step 138100 Loss: 0.4484, lr 9.045084971874742e-05
Step 138200 Loss: 0.4452, lr 9.045084971874742e-05
Step 138300 Loss: 0.4424, lr 9.045084971874742e-05
Step 138400 Loss: 0.4421, lr 9.045084971874742e-05
Step 138500 Loss: 0.4418, lr 9.045084971874742e-05
Step 138600 Loss: 0.4414, lr 9.045084971874742e-05
Step 138700 Loss: 0.4397, lr 9.045084971874742e-05
Step 138800 Loss: 0.4374, lr 9.045084971874742e-05
Step 138900 Loss: 0.4348, lr 9.045084971874742e-05
Step 139000 Loss: 0.4330, lr 9.045084971874742e-05
Step 139100 Loss: 0.4314, lr 9.045084971874742e-05
Step 139200 Loss: 0.4294, lr 9.045084971874742e-05
Step 139300 Loss: 0.4278, lr 9.045084971874742e-05
Step 139400 Loss: 0.4258, lr 9.045084971874742e-05
Step 139500 Loss: 0.4239, lr 9.045084971874742e-05
Step 139600 Loss: 0.4220, lr 9.045084971874742e-05
Step 139700 Loss: 0.4197, lr 9.045084971874742e-05
Step 139800 Loss: 0.4181, lr 9.045084971874742e-05
Step 139900 Loss: 0.4168, lr 9.045084971874742e-05
Step 140000 Loss: 0.4156, lr 9.045084971874742e-05
Step 140100 Loss: 0.4148, lr 9.045084971874742e-05
Step 140200 Loss: 0.4138, lr 9.045084971874742e-05
Step 140300 Loss: 0.4135, lr 9.045084971874742e-05
Step 140400 Loss: 0.4128, lr 9.045084971874742e-05
Step 140500 Loss: 0.4127, lr 9.045084971874742e-05
Step 140600 Loss: 0.4126, lr 9.045084971874742e-05
Step 140700 Loss: 0.4122, lr 9.045084971874742e-05
Step 140800 Loss: 0.4118, lr 9.045084971874742e-05
Step 140900 Loss: 0.4119, lr 9.045084971874742e-05
Step 141000 Loss: 0.4121, lr 9.045084971874742e-05
Step 141100 Loss: 0.4122, lr 9.045084971874742e-05
Step 141200 Loss: 0.4127, lr 9.045084971874742e-05
Step 141300 Loss: 0.4132, lr 9.045084971874742e-05
Step 141400 Loss: 0.4137, lr 9.045084971874742e-05
Step 141500 Loss: 0.4142, lr 9.045084971874742e-05
Step 141600 Loss: 0.4143, lr 9.045084971874742e-05
Step 141700 Loss: 0.4137, lr 9.045084971874742e-05
Step 141800 Loss: 0.4127, lr 9.045084971874742e-05
Train Epoch: [21/100] Loss: 0.4128,lr 0.000090
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6756
Step 141900 Loss: 1.1834, lr 8.950775061878456e-05
Step 142000 Loss: 0.8269, lr 8.950775061878456e-05
Step 142100 Loss: 0.7378, lr 8.950775061878456e-05
Step 142200 Loss: 0.6872, lr 8.950775061878456e-05
Step 142300 Loss: 0.6518, lr 8.950775061878456e-05
Step 142400 Loss: 0.6316, lr 8.950775061878456e-05
Step 142500 Loss: 0.6104, lr 8.950775061878456e-05
Step 142600 Loss: 0.5925, lr 8.950775061878456e-05
Step 142700 Loss: 0.5778, lr 8.950775061878456e-05
Step 142800 Loss: 0.5658, lr 8.950775061878456e-05
Step 142900 Loss: 0.5562, lr 8.950775061878456e-05
Step 143000 Loss: 0.5474, lr 8.950775061878456e-05
Step 143100 Loss: 0.5382, lr 8.950775061878456e-05
Step 143200 Loss: 0.5311, lr 8.950775061878456e-05
Step 143300 Loss: 0.5243, lr 8.950775061878456e-05
Step 143400 Loss: 0.5183, lr 8.950775061878456e-05
Step 143500 Loss: 0.5108, lr 8.950775061878456e-05
Step 143600 Loss: 0.5043, lr 8.950775061878456e-05
Step 143700 Loss: 0.4984, lr 8.950775061878456e-05
Step 143800 Loss: 0.4930, lr 8.950775061878456e-05
Step 143900 Loss: 0.4870, lr 8.950775061878456e-05
Step 144000 Loss: 0.4821, lr 8.950775061878456e-05
Step 144100 Loss: 0.4766, lr 8.950775061878456e-05
Step 144200 Loss: 0.4716, lr 8.950775061878456e-05
Step 144300 Loss: 0.4675, lr 8.950775061878456e-05
Step 144400 Loss: 0.4626, lr 8.950775061878456e-05
Step 144500 Loss: 0.4585, lr 8.950775061878456e-05
Step 144600 Loss: 0.4555, lr 8.950775061878456e-05
Step 144700 Loss: 0.4543, lr 8.950775061878456e-05
Step 144800 Loss: 0.4524, lr 8.950775061878456e-05
Step 144900 Loss: 0.4501, lr 8.950775061878456e-05
Step 145000 Loss: 0.4474, lr 8.950775061878456e-05
Step 145100 Loss: 0.4452, lr 8.950775061878456e-05
Step 145200 Loss: 0.4422, lr 8.950775061878456e-05
Step 145300 Loss: 0.4389, lr 8.950775061878456e-05
Step 145400 Loss: 0.4357, lr 8.950775061878456e-05
Step 145500 Loss: 0.4330, lr 8.950775061878456e-05
Step 145600 Loss: 0.4296, lr 8.950775061878456e-05
Step 145700 Loss: 0.4271, lr 8.950775061878456e-05
Step 145800 Loss: 0.4247, lr 8.950775061878456e-05
Step 145900 Loss: 0.4229, lr 8.950775061878456e-05
Step 146000 Loss: 0.4208, lr 8.950775061878456e-05
Step 146100 Loss: 0.4193, lr 8.950775061878456e-05
Step 146200 Loss: 0.4173, lr 8.950775061878456e-05
Step 146300 Loss: 0.4154, lr 8.950775061878456e-05
Step 146400 Loss: 0.4133, lr 8.950775061878456e-05
Step 146500 Loss: 0.4114, lr 8.950775061878456e-05
Step 146600 Loss: 0.4098, lr 8.950775061878456e-05
Step 146700 Loss: 0.4087, lr 8.950775061878456e-05
Step 146800 Loss: 0.4077, lr 8.950775061878456e-05
Step 146900 Loss: 0.4066, lr 8.950775061878456e-05
Step 147000 Loss: 0.4062, lr 8.950775061878456e-05
Step 147100 Loss: 0.4056, lr 8.950775061878456e-05
Step 147200 Loss: 0.4051, lr 8.950775061878456e-05
Step 147300 Loss: 0.4046, lr 8.950775061878456e-05
Step 147400 Loss: 0.4043, lr 8.950775061878456e-05
Step 147500 Loss: 0.4040, lr 8.950775061878456e-05
Step 147600 Loss: 0.4037, lr 8.950775061878456e-05
Step 147700 Loss: 0.4038, lr 8.950775061878456e-05
Step 147800 Loss: 0.4041, lr 8.950775061878456e-05
Step 147900 Loss: 0.4039, lr 8.950775061878456e-05
Step 148000 Loss: 0.4049, lr 8.950775061878456e-05
Step 148100 Loss: 0.4051, lr 8.950775061878456e-05
Step 148200 Loss: 0.4052, lr 8.950775061878456e-05
Step 148300 Loss: 0.4050, lr 8.950775061878456e-05
Step 148400 Loss: 0.4046, lr 8.950775061878456e-05
Step 148500 Loss: 0.4038, lr 8.950775061878456e-05
Step 148600 Loss: 0.4027, lr 8.950775061878456e-05
Train Epoch: [22/100] Loss: 0.4033,lr 0.000090
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Step 148700 Loss: 0.9153, lr 8.852566213878951e-05
Step 148800 Loss: 0.7614, lr 8.852566213878951e-05
Step 148900 Loss: 0.6975, lr 8.852566213878951e-05
Step 149000 Loss: 0.6522, lr 8.852566213878951e-05
Step 149100 Loss: 0.6277, lr 8.852566213878951e-05
Step 149200 Loss: 0.6052, lr 8.852566213878951e-05
Step 149300 Loss: 0.5852, lr 8.852566213878951e-05
Step 149400 Loss: 0.5699, lr 8.852566213878951e-05
Step 149500 Loss: 0.5554, lr 8.852566213878951e-05
Step 149600 Loss: 0.5433, lr 8.852566213878951e-05
Step 149700 Loss: 0.5340, lr 8.852566213878951e-05
Step 149800 Loss: 0.5245, lr 8.852566213878951e-05
Step 149900 Loss: 0.5157, lr 8.852566213878951e-05
Step 150000 Loss: 0.5074, lr 8.852566213878951e-05
Step 150100 Loss: 0.5010, lr 8.852566213878951e-05
Step 150200 Loss: 0.4956, lr 8.852566213878951e-05
Step 150300 Loss: 0.4890, lr 8.852566213878951e-05
Step 150400 Loss: 0.4846, lr 8.852566213878951e-05
Step 150500 Loss: 0.4800, lr 8.852566213878951e-05
Step 150600 Loss: 0.4751, lr 8.852566213878951e-05
Step 150700 Loss: 0.4700, lr 8.852566213878951e-05
Step 150800 Loss: 0.4658, lr 8.852566213878951e-05
Step 150900 Loss: 0.4611, lr 8.852566213878951e-05
Step 151000 Loss: 0.4572, lr 8.852566213878951e-05
Step 151100 Loss: 0.4531, lr 8.852566213878951e-05
Step 151200 Loss: 0.4490, lr 8.852566213878951e-05
Step 151300 Loss: 0.4454, lr 8.852566213878951e-05
Step 151400 Loss: 0.4432, lr 8.852566213878951e-05
Step 151500 Loss: 0.4408, lr 8.852566213878951e-05
Step 151600 Loss: 0.4387, lr 8.852566213878951e-05
Step 151700 Loss: 0.4359, lr 8.852566213878951e-05
Step 151800 Loss: 0.4330, lr 8.852566213878951e-05
Step 151900 Loss: 0.4306, lr 8.852566213878951e-05
Step 152000 Loss: 0.4276, lr 8.852566213878951e-05
Step 152100 Loss: 0.4249, lr 8.852566213878951e-05
Step 152200 Loss: 0.4219, lr 8.852566213878951e-05
Step 152300 Loss: 0.4198, lr 8.852566213878951e-05
Step 152400 Loss: 0.4173, lr 8.852566213878951e-05
Step 152500 Loss: 0.4151, lr 8.852566213878951e-05
Step 152600 Loss: 0.4133, lr 8.852566213878951e-05
Step 152700 Loss: 0.4116, lr 8.852566213878951e-05
Step 152800 Loss: 0.4101, lr 8.852566213878951e-05
Step 152900 Loss: 0.4089, lr 8.852566213878951e-05
Step 153000 Loss: 0.4073, lr 8.852566213878951e-05
Step 153100 Loss: 0.4057, lr 8.852566213878951e-05
Step 153200 Loss: 0.4038, lr 8.852566213878951e-05
Step 153300 Loss: 0.4024, lr 8.852566213878951e-05
Step 153400 Loss: 0.4016, lr 8.852566213878951e-05
Step 153500 Loss: 0.4014, lr 8.852566213878951e-05
Step 153600 Loss: 0.4010, lr 8.852566213878951e-05
Step 153700 Loss: 0.4005, lr 8.852566213878951e-05
Step 153800 Loss: 0.4001, lr 8.852566213878951e-05
Step 153900 Loss: 0.4000, lr 8.852566213878951e-05
Step 154000 Loss: 0.3998, lr 8.852566213878951e-05
Step 154100 Loss: 0.3999, lr 8.852566213878951e-05
Step 154200 Loss: 0.3997, lr 8.852566213878951e-05
Step 154300 Loss: 0.3993, lr 8.852566213878951e-05
Step 154400 Loss: 0.3997, lr 8.852566213878951e-05
Step 154500 Loss: 0.4000, lr 8.852566213878951e-05
Step 154600 Loss: 0.4005, lr 8.852566213878951e-05
Step 154700 Loss: 0.4010, lr 8.852566213878951e-05
Step 154800 Loss: 0.4021, lr 8.852566213878951e-05
Step 154900 Loss: 0.4025, lr 8.852566213878951e-05
Step 155000 Loss: 0.4031, lr 8.852566213878951e-05
Step 155100 Loss: 0.4034, lr 8.852566213878951e-05
Step 155200 Loss: 0.4030, lr 8.852566213878951e-05
Step 155300 Loss: 0.4017, lr 8.852566213878951e-05
Train Epoch: [23/100] Loss: 0.4012,lr 0.000089
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Step 155400 Loss: 1.2917, lr 8.750555348152303e-05
Step 155500 Loss: 0.7993, lr 8.750555348152303e-05
Step 155600 Loss: 0.6997, lr 8.750555348152303e-05
Step 155700 Loss: 0.6498, lr 8.750555348152303e-05
Step 155800 Loss: 0.6171, lr 8.750555348152303e-05
Step 155900 Loss: 0.5964, lr 8.750555348152303e-05
Step 156000 Loss: 0.5761, lr 8.750555348152303e-05
Step 156100 Loss: 0.5609, lr 8.750555348152303e-05
Step 156200 Loss: 0.5488, lr 8.750555348152303e-05
Step 156300 Loss: 0.5368, lr 8.750555348152303e-05
Step 156400 Loss: 0.5270, lr 8.750555348152303e-05
Step 156500 Loss: 0.5202, lr 8.750555348152303e-05
Step 156600 Loss: 0.5109, lr 8.750555348152303e-05
Step 156700 Loss: 0.5033, lr 8.750555348152303e-05
Step 156800 Loss: 0.4960, lr 8.750555348152303e-05
Step 156900 Loss: 0.4905, lr 8.750555348152303e-05
Step 157000 Loss: 0.4845, lr 8.750555348152303e-05
Step 157100 Loss: 0.4800, lr 8.750555348152303e-05
Step 157200 Loss: 0.4753, lr 8.750555348152303e-05
Step 157300 Loss: 0.4709, lr 8.750555348152303e-05
Step 157400 Loss: 0.4665, lr 8.750555348152303e-05
Step 157500 Loss: 0.4622, lr 8.750555348152303e-05
Step 157600 Loss: 0.4580, lr 8.750555348152303e-05
Step 157700 Loss: 0.4545, lr 8.750555348152303e-05
Step 157800 Loss: 0.4515, lr 8.750555348152303e-05
Step 157900 Loss: 0.4474, lr 8.750555348152303e-05
Step 158000 Loss: 0.4437, lr 8.750555348152303e-05
Step 158100 Loss: 0.4407, lr 8.750555348152303e-05
Step 158200 Loss: 0.4385, lr 8.750555348152303e-05
Step 158300 Loss: 0.4356, lr 8.750555348152303e-05
Step 158400 Loss: 0.4334, lr 8.750555348152303e-05
Step 158500 Loss: 0.4303, lr 8.750555348152303e-05
Step 158600 Loss: 0.4283, lr 8.750555348152303e-05
Step 158700 Loss: 0.4260, lr 8.750555348152303e-05
Step 158800 Loss: 0.4231, lr 8.750555348152303e-05
Step 158900 Loss: 0.4204, lr 8.750555348152303e-05
Step 159000 Loss: 0.4178, lr 8.750555348152303e-05
Step 159100 Loss: 0.4150, lr 8.750555348152303e-05
Step 159200 Loss: 0.4126, lr 8.750555348152303e-05
Step 159300 Loss: 0.4105, lr 8.750555348152303e-05
Step 159400 Loss: 0.4089, lr 8.750555348152303e-05
Step 159500 Loss: 0.4071, lr 8.750555348152303e-05
Step 159600 Loss: 0.4059, lr 8.750555348152303e-05
Step 159700 Loss: 0.4042, lr 8.750555348152303e-05
Step 159800 Loss: 0.4024, lr 8.750555348152303e-05
Step 159900 Loss: 0.4009, lr 8.750555348152303e-05
Step 160000 Loss: 0.3991, lr 8.750555348152303e-05
Step 160100 Loss: 0.3977, lr 8.750555348152303e-05
Step 160200 Loss: 0.3967, lr 8.750555348152303e-05
Step 160300 Loss: 0.3958, lr 8.750555348152303e-05
Step 160400 Loss: 0.3950, lr 8.750555348152303e-05
Step 160500 Loss: 0.3945, lr 8.750555348152303e-05
Step 160600 Loss: 0.3939, lr 8.750555348152303e-05
Step 160700 Loss: 0.3935, lr 8.750555348152303e-05
Step 160800 Loss: 0.3933, lr 8.750555348152303e-05
Step 160900 Loss: 0.3939, lr 8.750555348152303e-05
Step 161000 Loss: 0.3949, lr 8.750555348152303e-05
Step 161100 Loss: 0.3951, lr 8.750555348152303e-05
Step 161200 Loss: 0.3955, lr 8.750555348152303e-05
Step 161300 Loss: 0.3962, lr 8.750555348152303e-05
Step 161400 Loss: 0.3963, lr 8.750555348152303e-05
Step 161500 Loss: 0.3975, lr 8.750555348152303e-05
Step 161600 Loss: 0.3976, lr 8.750555348152303e-05
Step 161700 Loss: 0.3978, lr 8.750555348152303e-05
Step 161800 Loss: 0.3979, lr 8.750555348152303e-05
Step 161900 Loss: 0.3977, lr 8.750555348152303e-05
Step 162000 Loss: 0.3964, lr 8.750555348152303e-05
Step 162100 Loss: 0.3949, lr 8.750555348152303e-05
Train Epoch: [24/100] Loss: 0.3947,lr 0.000088
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6755
Step 162200 Loss: 0.9423, lr 8.644843137107063e-05
Step 162300 Loss: 0.7688, lr 8.644843137107063e-05
Step 162400 Loss: 0.6996, lr 8.644843137107063e-05
Step 162500 Loss: 0.6549, lr 8.644843137107063e-05
Step 162600 Loss: 0.6293, lr 8.644843137107063e-05
Step 162700 Loss: 0.6086, lr 8.644843137107063e-05
Step 162800 Loss: 0.5872, lr 8.644843137107063e-05
Step 162900 Loss: 0.5710, lr 8.644843137107063e-05
Step 163000 Loss: 0.5580, lr 8.644843137107063e-05
Step 163100 Loss: 0.5457, lr 8.644843137107063e-05
Step 163200 Loss: 0.5360, lr 8.644843137107063e-05
Step 163300 Loss: 0.5266, lr 8.644843137107063e-05
Step 163400 Loss: 0.5178, lr 8.644843137107063e-05
Step 163500 Loss: 0.5097, lr 8.644843137107063e-05
Step 163600 Loss: 0.5030, lr 8.644843137107063e-05
Step 163700 Loss: 0.4966, lr 8.644843137107063e-05
Step 163800 Loss: 0.4901, lr 8.644843137107063e-05
Step 163900 Loss: 0.4849, lr 8.644843137107063e-05
Step 164000 Loss: 0.4796, lr 8.644843137107063e-05
Step 164100 Loss: 0.4740, lr 8.644843137107063e-05
Step 164200 Loss: 0.4691, lr 8.644843137107063e-05
Step 164300 Loss: 0.4643, lr 8.644843137107063e-05
Step 164400 Loss: 0.4600, lr 8.644843137107063e-05
Step 164500 Loss: 0.4560, lr 8.644843137107063e-05
Step 164600 Loss: 0.4525, lr 8.644843137107063e-05
Step 164700 Loss: 0.4493, lr 8.644843137107063e-05
Step 164800 Loss: 0.4461, lr 8.644843137107063e-05
Step 164900 Loss: 0.4439, lr 8.644843137107063e-05
Step 165000 Loss: 0.4413, lr 8.644843137107063e-05
Step 165100 Loss: 0.4391, lr 8.644843137107063e-05
Step 165200 Loss: 0.4368, lr 8.644843137107063e-05
Step 165300 Loss: 0.4348, lr 8.644843137107063e-05
Step 165400 Loss: 0.4331, lr 8.644843137107063e-05
Step 165500 Loss: 0.4309, lr 8.644843137107063e-05
Step 165600 Loss: 0.4289, lr 8.644843137107063e-05
Step 165700 Loss: 0.4269, lr 8.644843137107063e-05
Step 165800 Loss: 0.4252, lr 8.644843137107063e-05
Step 165900 Loss: 0.4230, lr 8.644843137107063e-05
Step 166000 Loss: 0.4205, lr 8.644843137107063e-05
Step 166100 Loss: 0.4179, lr 8.644843137107063e-05
Step 166200 Loss: 0.4164, lr 8.644843137107063e-05
Step 166300 Loss: 0.4148, lr 8.644843137107063e-05
Step 166400 Loss: 0.4134, lr 8.644843137107063e-05
Step 166500 Loss: 0.4114, lr 8.644843137107063e-05
Step 166600 Loss: 0.4098, lr 8.644843137107063e-05
Step 166700 Loss: 0.4075, lr 8.644843137107063e-05
Step 166800 Loss: 0.4058, lr 8.644843137107063e-05
Step 166900 Loss: 0.4044, lr 8.644843137107063e-05
Step 167000 Loss: 0.4032, lr 8.644843137107063e-05
Step 167100 Loss: 0.4028, lr 8.644843137107063e-05
Step 167200 Loss: 0.4015, lr 8.644843137107063e-05
Step 167300 Loss: 0.4009, lr 8.644843137107063e-05
Step 167400 Loss: 0.4012, lr 8.644843137107063e-05
Step 167500 Loss: 0.4028, lr 8.644843137107063e-05
Step 167600 Loss: 0.4040, lr 8.644843137107063e-05
Step 167700 Loss: 0.4051, lr 8.644843137107063e-05
Step 167800 Loss: 0.4062, lr 8.644843137107063e-05
Step 167900 Loss: 0.4074, lr 8.644843137107063e-05
Step 168000 Loss: 0.4093, lr 8.644843137107063e-05
Step 168100 Loss: 0.4112, lr 8.644843137107063e-05
Step 168200 Loss: 0.4147, lr 8.644843137107063e-05
Step 168300 Loss: 0.4169, lr 8.644843137107063e-05
Step 168400 Loss: 0.4187, lr 8.644843137107063e-05
Step 168500 Loss: 0.4202, lr 8.644843137107063e-05
Step 168600 Loss: 0.4242, lr 8.644843137107063e-05
Step 168700 Loss: 0.4275, lr 8.644843137107063e-05
Step 168800 Loss: 0.4301, lr 8.644843137107063e-05
Train Epoch: [25/100] Loss: 0.4320,lr 0.000086
Model Saving at epoch 25
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 168900 Loss: 1.8085, lr 8.535533905932742e-05
Step 169000 Loss: 1.0306, lr 8.535533905932742e-05
Step 169100 Loss: 0.9068, lr 8.535533905932742e-05
Step 169200 Loss: 0.8200, lr 8.535533905932742e-05
Step 169300 Loss: 0.7579, lr 8.535533905932742e-05
Step 169400 Loss: 0.7161, lr 8.535533905932742e-05
Step 169500 Loss: 0.6797, lr 8.535533905932742e-05
Step 169600 Loss: 0.6543, lr 8.535533905932742e-05
Step 169700 Loss: 0.6336, lr 8.535533905932742e-05
Step 169800 Loss: 0.6169, lr 8.535533905932742e-05
Step 169900 Loss: 0.6029, lr 8.535533905932742e-05
Step 170000 Loss: 0.5912, lr 8.535533905932742e-05
Step 170100 Loss: 0.5791, lr 8.535533905932742e-05
Step 170200 Loss: 0.5689, lr 8.535533905932742e-05
Step 170300 Loss: 0.5589, lr 8.535533905932742e-05
Step 170400 Loss: 0.5531, lr 8.535533905932742e-05
Step 170500 Loss: 0.5462, lr 8.535533905932742e-05
Step 170600 Loss: 0.5398, lr 8.535533905932742e-05
Step 170700 Loss: 0.5349, lr 8.535533905932742e-05
Step 170800 Loss: 0.5286, lr 8.535533905932742e-05
Step 170900 Loss: 0.5229, lr 8.535533905932742e-05
Step 171000 Loss: 0.5176, lr 8.535533905932742e-05
Step 171100 Loss: 0.5128, lr 8.535533905932742e-05
Step 171200 Loss: 0.5078, lr 8.535533905932742e-05
Step 171300 Loss: 0.5038, lr 8.535533905932742e-05
Step 171400 Loss: 0.4996, lr 8.535533905932742e-05
Step 171500 Loss: 0.4949, lr 8.535533905932742e-05
Step 171600 Loss: 0.4918, lr 8.535533905932742e-05
Step 171700 Loss: 0.4897, lr 8.535533905932742e-05
Step 171800 Loss: 0.4865, lr 8.535533905932742e-05
Step 171900 Loss: 0.4842, lr 8.535533905932742e-05
Step 172000 Loss: 0.4809, lr 8.535533905932742e-05
Step 172100 Loss: 0.4783, lr 8.535533905932742e-05
Step 172200 Loss: 0.4759, lr 8.535533905932742e-05
Step 172300 Loss: 0.4735, lr 8.535533905932742e-05
Step 172400 Loss: 0.4704, lr 8.535533905932742e-05
Step 172500 Loss: 0.4677, lr 8.535533905932742e-05
Step 172600 Loss: 0.4651, lr 8.535533905932742e-05
Step 172700 Loss: 0.4632, lr 8.535533905932742e-05
Step 172800 Loss: 0.4610, lr 8.535533905932742e-05
Step 172900 Loss: 0.4594, lr 8.535533905932742e-05
Step 173000 Loss: 0.4579, lr 8.535533905932742e-05
Step 173100 Loss: 0.4565, lr 8.535533905932742e-05
Step 173200 Loss: 0.4553, lr 8.535533905932742e-05
Step 173300 Loss: 0.4554, lr 8.535533905932742e-05
Step 173400 Loss: 0.4545, lr 8.535533905932742e-05
Step 173500 Loss: 0.4558, lr 8.535533905932742e-05
Step 173600 Loss: 0.4585, lr 8.535533905932742e-05
Step 173700 Loss: 0.4608, lr 8.535533905932742e-05
Step 173800 Loss: 0.4632, lr 8.535533905932742e-05
Step 173900 Loss: 0.4650, lr 8.535533905932742e-05
Step 174000 Loss: 0.4663, lr 8.535533905932742e-05
Step 174100 Loss: 0.4662, lr 8.535533905932742e-05
Step 174200 Loss: 0.4664, lr 8.535533905932742e-05
Step 174300 Loss: 0.4672, lr 8.535533905932742e-05
Step 174400 Loss: 0.4676, lr 8.535533905932742e-05
Step 174500 Loss: 0.4676, lr 8.535533905932742e-05
Step 174600 Loss: 0.4673, lr 8.535533905932742e-05
Step 174700 Loss: 0.4679, lr 8.535533905932742e-05
Step 174800 Loss: 0.4682, lr 8.535533905932742e-05
Step 174900 Loss: 0.4687, lr 8.535533905932742e-05
Step 175000 Loss: 0.4700, lr 8.535533905932742e-05
Step 175100 Loss: 0.4705, lr 8.535533905932742e-05
Step 175200 Loss: 0.4710, lr 8.535533905932742e-05
Step 175300 Loss: 0.4712, lr 8.535533905932742e-05
Step 175400 Loss: 0.4712, lr 8.535533905932742e-05
Step 175500 Loss: 0.4703, lr 8.535533905932742e-05
Step 175600 Loss: 0.4694, lr 8.535533905932742e-05
Train Epoch: [26/100] Loss: 0.4695,lr 0.000085
Calling G2SDataset.batch()
Done, time:  2.23 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6756
Step 175700 Loss: 0.9944, lr 8.422735529643448e-05
Step 175800 Loss: 0.7900, lr 8.422735529643448e-05
Step 175900 Loss: 0.7192, lr 8.422735529643448e-05
Step 176000 Loss: 0.6718, lr 8.422735529643448e-05
Step 176100 Loss: 0.6438, lr 8.422735529643448e-05
Step 176200 Loss: 0.6192, lr 8.422735529643448e-05
Step 176300 Loss: 0.5996, lr 8.422735529643448e-05
Step 176400 Loss: 0.5842, lr 8.422735529643448e-05
Step 176500 Loss: 0.5721, lr 8.422735529643448e-05
Step 176600 Loss: 0.5615, lr 8.422735529643448e-05
Step 176700 Loss: 0.5546, lr 8.422735529643448e-05
Step 176800 Loss: 0.5453, lr 8.422735529643448e-05
Step 176900 Loss: 0.5367, lr 8.422735529643448e-05
Step 177000 Loss: 0.5292, lr 8.422735529643448e-05
Step 177100 Loss: 0.5232, lr 8.422735529643448e-05
Step 177200 Loss: 0.5190, lr 8.422735529643448e-05
Step 177300 Loss: 0.5130, lr 8.422735529643448e-05
Step 177400 Loss: 0.5073, lr 8.422735529643448e-05
Step 177500 Loss: 0.5033, lr 8.422735529643448e-05
Step 177600 Loss: 0.4986, lr 8.422735529643448e-05
Step 177700 Loss: 0.4944, lr 8.422735529643448e-05
Step 177800 Loss: 0.4899, lr 8.422735529643448e-05
Step 177900 Loss: 0.4856, lr 8.422735529643448e-05
Step 178000 Loss: 0.4809, lr 8.422735529643448e-05
Step 178100 Loss: 0.4775, lr 8.422735529643448e-05
Step 178200 Loss: 0.4734, lr 8.422735529643448e-05
Step 178300 Loss: 0.4697, lr 8.422735529643448e-05
Step 178400 Loss: 0.4670, lr 8.422735529643448e-05
Step 178500 Loss: 0.4641, lr 8.422735529643448e-05
Step 178600 Loss: 0.4614, lr 8.422735529643448e-05
Step 178700 Loss: 0.4595, lr 8.422735529643448e-05
Step 178800 Loss: 0.4577, lr 8.422735529643448e-05
Step 178900 Loss: 0.4561, lr 8.422735529643448e-05
Step 179000 Loss: 0.4529, lr 8.422735529643448e-05
Step 179100 Loss: 0.4502, lr 8.422735529643448e-05
Step 179200 Loss: 0.4475, lr 8.422735529643448e-05
Step 179300 Loss: 0.4450, lr 8.422735529643448e-05
Step 179400 Loss: 0.4425, lr 8.422735529643448e-05
Step 179500 Loss: 0.4407, lr 8.422735529643448e-05
Step 179600 Loss: 0.4385, lr 8.422735529643448e-05
Step 179700 Loss: 0.4374, lr 8.422735529643448e-05
Step 179800 Loss: 0.4359, lr 8.422735529643448e-05
Step 179900 Loss: 0.4350, lr 8.422735529643448e-05
Step 180000 Loss: 0.4337, lr 8.422735529643448e-05
Step 180100 Loss: 0.4334, lr 8.422735529643448e-05
Step 180200 Loss: 0.4336, lr 8.422735529643448e-05
Step 180300 Loss: 0.4351, lr 8.422735529643448e-05
Step 180400 Loss: 0.4352, lr 8.422735529643448e-05
Step 180500 Loss: 0.4359, lr 8.422735529643448e-05
Step 180600 Loss: 0.4368, lr 8.422735529643448e-05
Step 180700 Loss: 0.4365, lr 8.422735529643448e-05
Step 180800 Loss: 0.4366, lr 8.422735529643448e-05
Step 180900 Loss: 0.4361, lr 8.422735529643448e-05
Step 181000 Loss: 0.4359, lr 8.422735529643448e-05
Step 181100 Loss: 0.4354, lr 8.422735529643448e-05
Step 181200 Loss: 0.4359, lr 8.422735529643448e-05
Step 181300 Loss: 0.4359, lr 8.422735529643448e-05
Step 181400 Loss: 0.4360, lr 8.422735529643448e-05
Step 181500 Loss: 0.4367, lr 8.422735529643448e-05
Step 181600 Loss: 0.4371, lr 8.422735529643448e-05
Step 181700 Loss: 0.4378, lr 8.422735529643448e-05
Step 181800 Loss: 0.4385, lr 8.422735529643448e-05
Step 181900 Loss: 0.4391, lr 8.422735529643448e-05
Step 182000 Loss: 0.4395, lr 8.422735529643448e-05
Step 182100 Loss: 0.4402, lr 8.422735529643448e-05
Step 182200 Loss: 0.4397, lr 8.422735529643448e-05
Step 182300 Loss: 0.4385, lr 8.422735529643448e-05
Step 182400 Loss: 0.4386, lr 8.422735529643448e-05
Train Epoch: [27/100] Loss: 0.4389,lr 0.000084
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6756
Step 182500 Loss: 1.0004, lr 8.306559326618263e-05
Step 182600 Loss: 0.8560, lr 8.306559326618263e-05
Step 182700 Loss: 0.7715, lr 8.306559326618263e-05
Step 182800 Loss: 0.7135, lr 8.306559326618263e-05
Step 182900 Loss: 0.6735, lr 8.306559326618263e-05
Step 183000 Loss: 0.6399, lr 8.306559326618263e-05
Step 183100 Loss: 0.6175, lr 8.306559326618263e-05
Step 183200 Loss: 0.5982, lr 8.306559326618263e-05
Step 183300 Loss: 0.5819, lr 8.306559326618263e-05
Step 183400 Loss: 0.5683, lr 8.306559326618263e-05
Step 183500 Loss: 0.5578, lr 8.306559326618263e-05
Step 183600 Loss: 0.5457, lr 8.306559326618263e-05
Step 183700 Loss: 0.5368, lr 8.306559326618263e-05
Step 183800 Loss: 0.5274, lr 8.306559326618263e-05
Step 183900 Loss: 0.5211, lr 8.306559326618263e-05
Step 184000 Loss: 0.5143, lr 8.306559326618263e-05
Step 184100 Loss: 0.5076, lr 8.306559326618263e-05
Step 184200 Loss: 0.5024, lr 8.306559326618263e-05
Step 184300 Loss: 0.4963, lr 8.306559326618263e-05
Step 184400 Loss: 0.4905, lr 8.306559326618263e-05
Step 184500 Loss: 0.4849, lr 8.306559326618263e-05
Step 184600 Loss: 0.4808, lr 8.306559326618263e-05
Step 184700 Loss: 0.4766, lr 8.306559326618263e-05
Step 184800 Loss: 0.4730, lr 8.306559326618263e-05
Step 184900 Loss: 0.4688, lr 8.306559326618263e-05
Step 185000 Loss: 0.4645, lr 8.306559326618263e-05
Step 185100 Loss: 0.4613, lr 8.306559326618263e-05
Step 185200 Loss: 0.4588, lr 8.306559326618263e-05
Step 185300 Loss: 0.4558, lr 8.306559326618263e-05
Step 185400 Loss: 0.4534, lr 8.306559326618263e-05
Step 185500 Loss: 0.4504, lr 8.306559326618263e-05
Step 185600 Loss: 0.4473, lr 8.306559326618263e-05
Step 185700 Loss: 0.4445, lr 8.306559326618263e-05
Step 185800 Loss: 0.4418, lr 8.306559326618263e-05
Step 185900 Loss: 0.4385, lr 8.306559326618263e-05
Step 186000 Loss: 0.4355, lr 8.306559326618263e-05
Step 186100 Loss: 0.4331, lr 8.306559326618263e-05
Step 186200 Loss: 0.4311, lr 8.306559326618263e-05
Step 186300 Loss: 0.4290, lr 8.306559326618263e-05
Step 186400 Loss: 0.4273, lr 8.306559326618263e-05
Step 186500 Loss: 0.4256, lr 8.306559326618263e-05
Step 186600 Loss: 0.4243, lr 8.306559326618263e-05
Step 186700 Loss: 0.4226, lr 8.306559326618263e-05
Step 186800 Loss: 0.4211, lr 8.306559326618263e-05
Step 186900 Loss: 0.4191, lr 8.306559326618263e-05
Step 187000 Loss: 0.4173, lr 8.306559326618263e-05
Step 187100 Loss: 0.4160, lr 8.306559326618263e-05
Step 187200 Loss: 0.4152, lr 8.306559326618263e-05
Step 187300 Loss: 0.4142, lr 8.306559326618263e-05
Step 187400 Loss: 0.4136, lr 8.306559326618263e-05
Step 187500 Loss: 0.4128, lr 8.306559326618263e-05
Step 187600 Loss: 0.4120, lr 8.306559326618263e-05
Step 187700 Loss: 0.4108, lr 8.306559326618263e-05
Step 187800 Loss: 0.4105, lr 8.306559326618263e-05
Step 187900 Loss: 0.4118, lr 8.306559326618263e-05
Step 188000 Loss: 0.4125, lr 8.306559326618263e-05
Step 188100 Loss: 0.4126, lr 8.306559326618263e-05
Step 188200 Loss: 0.4132, lr 8.306559326618263e-05
Step 188300 Loss: 0.4143, lr 8.306559326618263e-05
Step 188400 Loss: 0.4151, lr 8.306559326618263e-05
Step 188500 Loss: 0.4161, lr 8.306559326618263e-05
Step 188600 Loss: 0.4175, lr 8.306559326618263e-05
Step 188700 Loss: 0.4188, lr 8.306559326618263e-05
Step 188800 Loss: 0.4207, lr 8.306559326618263e-05
Step 188900 Loss: 0.4216, lr 8.306559326618263e-05
Step 189000 Loss: 0.4219, lr 8.306559326618263e-05
Step 189100 Loss: 0.4215, lr 8.306559326618263e-05
Train Epoch: [28/100] Loss: 0.4217,lr 0.000083
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6755
Step 189200 Loss: 1.0432, lr 8.187119948743452e-05
Step 189300 Loss: 0.7663, lr 8.187119948743452e-05
Step 189400 Loss: 0.6945, lr 8.187119948743452e-05
Step 189500 Loss: 0.6548, lr 8.187119948743452e-05
Step 189600 Loss: 0.6285, lr 8.187119948743452e-05
Step 189700 Loss: 0.6116, lr 8.187119948743452e-05
Step 189800 Loss: 0.5952, lr 8.187119948743452e-05
Step 189900 Loss: 0.5809, lr 8.187119948743452e-05
Step 190000 Loss: 0.5711, lr 8.187119948743452e-05
Step 190100 Loss: 0.5607, lr 8.187119948743452e-05
Step 190200 Loss: 0.5520, lr 8.187119948743452e-05
Step 190300 Loss: 0.5446, lr 8.187119948743452e-05
Step 190400 Loss: 0.5372, lr 8.187119948743452e-05
Step 190500 Loss: 0.5306, lr 8.187119948743452e-05
Step 190600 Loss: 0.5262, lr 8.187119948743452e-05
Step 190700 Loss: 0.5249, lr 8.187119948743452e-05
Step 190800 Loss: 0.5211, lr 8.187119948743452e-05
Step 190900 Loss: 0.5183, lr 8.187119948743452e-05
Step 191000 Loss: 0.5151, lr 8.187119948743452e-05
Step 191100 Loss: 0.5118, lr 8.187119948743452e-05
Step 191200 Loss: 0.5072, lr 8.187119948743452e-05
Step 191300 Loss: 0.5035, lr 8.187119948743452e-05
Step 191400 Loss: 0.4995, lr 8.187119948743452e-05
Step 191500 Loss: 0.4962, lr 8.187119948743452e-05
Step 191600 Loss: 0.4922, lr 8.187119948743452e-05
Step 191700 Loss: 0.4889, lr 8.187119948743452e-05
Step 191800 Loss: 0.4860, lr 8.187119948743452e-05
Step 191900 Loss: 0.4838, lr 8.187119948743452e-05
Step 192000 Loss: 0.4806, lr 8.187119948743452e-05
Step 192100 Loss: 0.4784, lr 8.187119948743452e-05
Step 192200 Loss: 0.4762, lr 8.187119948743452e-05
Step 192300 Loss: 0.4735, lr 8.187119948743452e-05
Step 192400 Loss: 0.4713, lr 8.187119948743452e-05
Step 192500 Loss: 0.4689, lr 8.187119948743452e-05
Step 192600 Loss: 0.4668, lr 8.187119948743452e-05
Step 192700 Loss: 0.4643, lr 8.187119948743452e-05
Step 192800 Loss: 0.4622, lr 8.187119948743452e-05
Step 192900 Loss: 0.4596, lr 8.187119948743452e-05
Step 193000 Loss: 0.4576, lr 8.187119948743452e-05
Step 193100 Loss: 0.4557, lr 8.187119948743452e-05
Step 193200 Loss: 0.4544, lr 8.187119948743452e-05
Step 193300 Loss: 0.4526, lr 8.187119948743452e-05
Step 193400 Loss: 0.4515, lr 8.187119948743452e-05
Step 193500 Loss: 0.4497, lr 8.187119948743452e-05
Step 193600 Loss: 0.4483, lr 8.187119948743452e-05
Step 193700 Loss: 0.4464, lr 8.187119948743452e-05
Step 193800 Loss: 0.4454, lr 8.187119948743452e-05
Step 193900 Loss: 0.4442, lr 8.187119948743452e-05
Step 194000 Loss: 0.4434, lr 8.187119948743452e-05
Step 194100 Loss: 0.4428, lr 8.187119948743452e-05
Step 194200 Loss: 0.4422, lr 8.187119948743452e-05
Step 194300 Loss: 0.4413, lr 8.187119948743452e-05
Step 194400 Loss: 0.4410, lr 8.187119948743452e-05
Step 194500 Loss: 0.4406, lr 8.187119948743452e-05
Step 194600 Loss: 0.4400, lr 8.187119948743452e-05
Step 194700 Loss: 0.4398, lr 8.187119948743452e-05
Step 194800 Loss: 0.4393, lr 8.187119948743452e-05
Step 194900 Loss: 0.4394, lr 8.187119948743452e-05
Step 195000 Loss: 0.4395, lr 8.187119948743452e-05
Step 195100 Loss: 0.4399, lr 8.187119948743452e-05
Step 195200 Loss: 0.4407, lr 8.187119948743452e-05
Step 195300 Loss: 0.4414, lr 8.187119948743452e-05
Step 195400 Loss: 0.4419, lr 8.187119948743452e-05
Step 195500 Loss: 0.4427, lr 8.187119948743452e-05
Step 195600 Loss: 0.4431, lr 8.187119948743452e-05
Step 195700 Loss: 0.4431, lr 8.187119948743452e-05
Step 195800 Loss: 0.4426, lr 8.187119948743452e-05
Step 195900 Loss: 0.4425, lr 8.187119948743452e-05
Train Epoch: [29/100] Loss: 0.4426,lr 0.000082
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Step 196000 Loss: 0.9448, lr 8.064535268264887e-05
Step 196100 Loss: 0.7998, lr 8.064535268264887e-05
Step 196200 Loss: 0.7405, lr 8.064535268264887e-05
Step 196300 Loss: 0.7007, lr 8.064535268264887e-05
Step 196400 Loss: 0.6741, lr 8.064535268264887e-05
Step 196500 Loss: 0.6489, lr 8.064535268264887e-05
Step 196600 Loss: 0.6284, lr 8.064535268264887e-05
Step 196700 Loss: 0.6125, lr 8.064535268264887e-05
Step 196800 Loss: 0.5979, lr 8.064535268264887e-05
Step 196900 Loss: 0.5853, lr 8.064535268264887e-05
Step 197000 Loss: 0.5755, lr 8.064535268264887e-05
Step 197100 Loss: 0.5648, lr 8.064535268264887e-05
Step 197200 Loss: 0.5578, lr 8.064535268264887e-05
Step 197300 Loss: 0.5504, lr 8.064535268264887e-05
Step 197400 Loss: 0.5445, lr 8.064535268264887e-05
Step 197500 Loss: 0.5384, lr 8.064535268264887e-05
Step 197600 Loss: 0.5320, lr 8.064535268264887e-05
Step 197700 Loss: 0.5273, lr 8.064535268264887e-05
Step 197800 Loss: 0.5214, lr 8.064535268264887e-05
Step 197900 Loss: 0.5155, lr 8.064535268264887e-05
Step 198000 Loss: 0.5102, lr 8.064535268264887e-05
Step 198100 Loss: 0.5056, lr 8.064535268264887e-05
Step 198200 Loss: 0.5009, lr 8.064535268264887e-05
Step 198300 Loss: 0.4970, lr 8.064535268264887e-05
Step 198400 Loss: 0.4924, lr 8.064535268264887e-05
Step 198500 Loss: 0.4882, lr 8.064535268264887e-05
Step 198600 Loss: 0.4843, lr 8.064535268264887e-05
Step 198700 Loss: 0.4812, lr 8.064535268264887e-05
Step 198800 Loss: 0.4781, lr 8.064535268264887e-05
Step 198900 Loss: 0.4756, lr 8.064535268264887e-05
Step 199000 Loss: 0.4731, lr 8.064535268264887e-05
Step 199100 Loss: 0.4707, lr 8.064535268264887e-05
Step 199200 Loss: 0.4691, lr 8.064535268264887e-05
Step 199300 Loss: 0.4664, lr 8.064535268264887e-05
Step 199400 Loss: 0.4634, lr 8.064535268264887e-05
Step 199500 Loss: 0.4606, lr 8.064535268264887e-05
Step 199600 Loss: 0.4576, lr 8.064535268264887e-05
Step 199700 Loss: 0.4550, lr 8.064535268264887e-05
Step 199800 Loss: 0.4522, lr 8.064535268264887e-05
Step 199900 Loss: 0.4501, lr 8.064535268264887e-05
Step 200000 Loss: 0.4484, lr 8.064535268264887e-05
Step 200100 Loss: 0.4464, lr 8.064535268264887e-05
Step 200200 Loss: 0.4441, lr 8.064535268264887e-05
Step 200300 Loss: 0.4421, lr 8.064535268264887e-05
Step 200400 Loss: 0.4396, lr 8.064535268264887e-05
Step 200500 Loss: 0.4370, lr 8.064535268264887e-05
Step 200600 Loss: 0.4350, lr 8.064535268264887e-05
Step 200700 Loss: 0.4334, lr 8.064535268264887e-05
Step 200800 Loss: 0.4321, lr 8.064535268264887e-05
Step 200900 Loss: 0.4311, lr 8.064535268264887e-05
Step 201000 Loss: 0.4298, lr 8.064535268264887e-05
Step 201100 Loss: 0.4284, lr 8.064535268264887e-05
Step 201200 Loss: 0.4271, lr 8.064535268264887e-05
Step 201300 Loss: 0.4265, lr 8.064535268264887e-05
Step 201400 Loss: 0.4258, lr 8.064535268264887e-05
Step 201500 Loss: 0.4246, lr 8.064535268264887e-05
Step 201600 Loss: 0.4241, lr 8.064535268264887e-05
Step 201700 Loss: 0.4240, lr 8.064535268264887e-05
Step 201800 Loss: 0.4243, lr 8.064535268264887e-05
Step 201900 Loss: 0.4244, lr 8.064535268264887e-05
Step 202000 Loss: 0.4245, lr 8.064535268264887e-05
Step 202100 Loss: 0.4252, lr 8.064535268264887e-05
Step 202200 Loss: 0.4257, lr 8.064535268264887e-05
Step 202300 Loss: 0.4270, lr 8.064535268264887e-05
Step 202400 Loss: 0.4270, lr 8.064535268264887e-05
Step 202500 Loss: 0.4262, lr 8.064535268264887e-05
Step 202600 Loss: 0.4248, lr 8.064535268264887e-05
Train Epoch: [30/100] Loss: 0.4246,lr 0.000081
Model Saving at epoch 30
Calling G2SDataset.batch()
Done, time:  1.83 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Step 202700 Loss: 1.1530, lr 7.938926261462371e-05
Step 202800 Loss: 0.8128, lr 7.938926261462371e-05
Step 202900 Loss: 0.7270, lr 7.938926261462371e-05
Step 203000 Loss: 0.6820, lr 7.938926261462371e-05
Step 203100 Loss: 0.6500, lr 7.938926261462371e-05
Step 203200 Loss: 0.6320, lr 7.938926261462371e-05
Step 203300 Loss: 0.6103, lr 7.938926261462371e-05
Step 203400 Loss: 0.5931, lr 7.938926261462371e-05
Step 203500 Loss: 0.5795, lr 7.938926261462371e-05
Step 203600 Loss: 0.5660, lr 7.938926261462371e-05
Step 203700 Loss: 0.5559, lr 7.938926261462371e-05
Step 203800 Loss: 0.5475, lr 7.938926261462371e-05
Step 203900 Loss: 0.5379, lr 7.938926261462371e-05
Step 204000 Loss: 0.5302, lr 7.938926261462371e-05
Step 204100 Loss: 0.5232, lr 7.938926261462371e-05
Step 204200 Loss: 0.5177, lr 7.938926261462371e-05
Step 204300 Loss: 0.5116, lr 7.938926261462371e-05
Step 204400 Loss: 0.5060, lr 7.938926261462371e-05
Step 204500 Loss: 0.5015, lr 7.938926261462371e-05
Step 204600 Loss: 0.4963, lr 7.938926261462371e-05
Step 204700 Loss: 0.4902, lr 7.938926261462371e-05
Step 204800 Loss: 0.4857, lr 7.938926261462371e-05
Step 204900 Loss: 0.4804, lr 7.938926261462371e-05
Step 205000 Loss: 0.4757, lr 7.938926261462371e-05
Step 205100 Loss: 0.4720, lr 7.938926261462371e-05
Step 205200 Loss: 0.4685, lr 7.938926261462371e-05
Step 205300 Loss: 0.4664, lr 7.938926261462371e-05
Step 205400 Loss: 0.4648, lr 7.938926261462371e-05
Step 205500 Loss: 0.4632, lr 7.938926261462371e-05
Step 205600 Loss: 0.4607, lr 7.938926261462371e-05
Step 205700 Loss: 0.4583, lr 7.938926261462371e-05
Step 205800 Loss: 0.4553, lr 7.938926261462371e-05
Step 205900 Loss: 0.4532, lr 7.938926261462371e-05
Step 206000 Loss: 0.4506, lr 7.938926261462371e-05
Step 206100 Loss: 0.4480, lr 7.938926261462371e-05
Step 206200 Loss: 0.4450, lr 7.938926261462371e-05
Step 206300 Loss: 0.4427, lr 7.938926261462371e-05
Step 206400 Loss: 0.4400, lr 7.938926261462371e-05
Step 206500 Loss: 0.4378, lr 7.938926261462371e-05
Step 206600 Loss: 0.4357, lr 7.938926261462371e-05
Step 206700 Loss: 0.4343, lr 7.938926261462371e-05
Step 206800 Loss: 0.4323, lr 7.938926261462371e-05
Step 206900 Loss: 0.4309, lr 7.938926261462371e-05
Step 207000 Loss: 0.4285, lr 7.938926261462371e-05
Step 207100 Loss: 0.4273, lr 7.938926261462371e-05
Step 207200 Loss: 0.4252, lr 7.938926261462371e-05
Step 207300 Loss: 0.4235, lr 7.938926261462371e-05
Step 207400 Loss: 0.4221, lr 7.938926261462371e-05
Step 207500 Loss: 0.4211, lr 7.938926261462371e-05
Step 207600 Loss: 0.4205, lr 7.938926261462371e-05
Step 207700 Loss: 0.4196, lr 7.938926261462371e-05
Step 207800 Loss: 0.4189, lr 7.938926261462371e-05
Step 207900 Loss: 0.4182, lr 7.938926261462371e-05
Step 208000 Loss: 0.4180, lr 7.938926261462371e-05
Step 208100 Loss: 0.4177, lr 7.938926261462371e-05
Step 208200 Loss: 0.4174, lr 7.938926261462371e-05
Step 208300 Loss: 0.4169, lr 7.938926261462371e-05
Step 208400 Loss: 0.4168, lr 7.938926261462371e-05
Step 208500 Loss: 0.4170, lr 7.938926261462371e-05
Step 208600 Loss: 0.4177, lr 7.938926261462371e-05
Step 208700 Loss: 0.4181, lr 7.938926261462371e-05
Step 208800 Loss: 0.4190, lr 7.938926261462371e-05
Step 208900 Loss: 0.4194, lr 7.938926261462371e-05
Step 209000 Loss: 0.4201, lr 7.938926261462371e-05
Step 209100 Loss: 0.4210, lr 7.938926261462371e-05
Step 209200 Loss: 0.4212, lr 7.938926261462371e-05
Step 209300 Loss: 0.4203, lr 7.938926261462371e-05
Step 209400 Loss: 0.4199, lr 7.938926261462371e-05
