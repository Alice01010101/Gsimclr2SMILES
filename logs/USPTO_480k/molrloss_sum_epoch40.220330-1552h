Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = **
**** embed_size = *1024*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *1024*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *1024*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *1024*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *100*
**** max_steps = *300000*
**** warmup_steps = *100*
**** lr = *0.0001*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=1129, out_features=1024, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=1138, out_features=1024, bias=True)
        (W_r): Linear(in_features=114, out_features=1024, bias=False)
        (U_r): Linear(in_features=1024, out_features=1024, bias=True)
        (W_h): Linear(in_features=1138, out_features=1024, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=1024, bias=True)
        (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
        (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=1024, bias=True)
      (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
      (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (g): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=256, bias=False)
    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 9774736
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.00 s, total batches: 6756
Step 100 Loss: 2.7507, lr 0.0001
Step 200 Loss: 2.2120, lr 0.0001
Step 300 Loss: 1.9332, lr 0.0001
Step 400 Loss: 1.7532, lr 0.0001
Step 500 Loss: 1.6245, lr 0.0001
Step 600 Loss: 1.5224, lr 0.0001
Step 700 Loss: 1.4440, lr 0.0001
Step 800 Loss: 1.3719, lr 0.0001
Step 900 Loss: 1.3108, lr 0.0001
Step 1000 Loss: 1.2581, lr 0.0001
Step 1100 Loss: 1.2119, lr 0.0001
Step 1200 Loss: 1.1708, lr 0.0001
Step 1300 Loss: 1.1353, lr 0.0001
Step 1400 Loss: 1.1017, lr 0.0001
Step 1500 Loss: 1.0738, lr 0.0001
Step 1600 Loss: 1.0456, lr 0.0001
Step 1700 Loss: 1.0221, lr 0.0001
Step 1800 Loss: 1.0012, lr 0.0001
Step 1900 Loss: 0.9822, lr 0.0001
Step 2000 Loss: 0.9642, lr 0.0001
Step 2100 Loss: 0.9470, lr 0.0001
Step 2200 Loss: 0.9320, lr 0.0001
Step 2300 Loss: 0.9178, lr 0.0001
Step 2400 Loss: 0.9061, lr 0.0001
Step 2500 Loss: 0.8940, lr 0.0001
Step 2600 Loss: 0.8816, lr 0.0001
Step 2700 Loss: 0.8708, lr 0.0001
Step 2800 Loss: 0.8607, lr 0.0001
Step 2900 Loss: 0.8512, lr 0.0001
Step 3000 Loss: 0.8426, lr 0.0001
Step 3100 Loss: 0.8340, lr 0.0001
Step 3200 Loss: 0.8253, lr 0.0001
Step 3300 Loss: 0.8173, lr 0.0001
Step 3400 Loss: 0.8096, lr 0.0001
Step 3500 Loss: 0.8018, lr 0.0001
Step 3600 Loss: 0.7953, lr 0.0001
Step 3700 Loss: 0.7892, lr 0.0001
Step 3800 Loss: 0.7835, lr 0.0001
Step 3900 Loss: 0.7774, lr 0.0001
Step 4000 Loss: 0.7728, lr 0.0001
Step 4100 Loss: 0.7680, lr 0.0001
Step 4200 Loss: 0.7647, lr 0.0001
Step 4300 Loss: 0.7610, lr 0.0001
Step 4400 Loss: 0.7570, lr 0.0001
Step 4500 Loss: 0.7529, lr 0.0001
Step 4600 Loss: 0.7487, lr 0.0001
Step 4700 Loss: 0.7451, lr 0.0001
Step 4800 Loss: 0.7427, lr 0.0001
Step 4900 Loss: 0.7401, lr 0.0001
Step 5000 Loss: 0.7388, lr 0.0001
Step 5100 Loss: 0.7366, lr 0.0001
Step 5200 Loss: 0.7350, lr 0.0001
Step 5300 Loss: 0.7339, lr 0.0001
Step 5400 Loss: 0.7331, lr 0.0001
Step 5500 Loss: 0.7326, lr 0.0001
Step 5600 Loss: 0.7315, lr 0.0001
Step 5700 Loss: 0.7314, lr 0.0001
Step 5800 Loss: 0.7311, lr 0.0001
Step 5900 Loss: 0.7310, lr 0.0001
Step 6000 Loss: 0.7322, lr 0.0001
Step 6100 Loss: 0.7332, lr 0.0001
Step 6200 Loss: 0.7342, lr 0.0001
Step 6300 Loss: 0.7363, lr 0.0001
Step 6400 Loss: 0.7392, lr 0.0001
Step 6500 Loss: 0.7408, lr 0.0001
Step 6600 Loss: 0.7405, lr 0.0001
Step 6700 Loss: 0.7399, lr 0.0001
Train Epoch: [1/100] Loss: 0.7400,lr 0.000100
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Step 6800 Loss: 1.2603, lr 9.997532801828658e-05
Step 6900 Loss: 0.9356, lr 9.997532801828658e-05
Step 7000 Loss: 0.8573, lr 9.997532801828658e-05
Step 7100 Loss: 0.8115, lr 9.997532801828658e-05
Step 7200 Loss: 0.7787, lr 9.997532801828658e-05
Step 7300 Loss: 0.7536, lr 9.997532801828658e-05
Step 7400 Loss: 0.7287, lr 9.997532801828658e-05
Step 7500 Loss: 0.7097, lr 9.997532801828658e-05
Step 7600 Loss: 0.6914, lr 9.997532801828658e-05
Step 7700 Loss: 0.6757, lr 9.997532801828658e-05
Step 7800 Loss: 0.6627, lr 9.997532801828658e-05
Step 7900 Loss: 0.6516, lr 9.997532801828658e-05
Step 8000 Loss: 0.6412, lr 9.997532801828658e-05
Step 8100 Loss: 0.6321, lr 9.997532801828658e-05
Step 8200 Loss: 0.6238, lr 9.997532801828658e-05
Step 8300 Loss: 0.6153, lr 9.997532801828658e-05
Step 8400 Loss: 0.6063, lr 9.997532801828658e-05
Step 8500 Loss: 0.5990, lr 9.997532801828658e-05
Step 8600 Loss: 0.5917, lr 9.997532801828658e-05
Step 8700 Loss: 0.5855, lr 9.997532801828658e-05
Step 8800 Loss: 0.5785, lr 9.997532801828658e-05
Step 8900 Loss: 0.5728, lr 9.997532801828658e-05
Step 9000 Loss: 0.5669, lr 9.997532801828658e-05
Step 9100 Loss: 0.5612, lr 9.997532801828658e-05
Step 9200 Loss: 0.5570, lr 9.997532801828658e-05
Step 9300 Loss: 0.5523, lr 9.997532801828658e-05
Step 9400 Loss: 0.5472, lr 9.997532801828658e-05
Step 9500 Loss: 0.5433, lr 9.997532801828658e-05
Step 9600 Loss: 0.5391, lr 9.997532801828658e-05
Step 9700 Loss: 0.5362, lr 9.997532801828658e-05
Step 9800 Loss: 0.5330, lr 9.997532801828658e-05
Step 9900 Loss: 0.5291, lr 9.997532801828658e-05
Step 10000 Loss: 0.5259, lr 9.997532801828658e-05
Step 10100 Loss: 0.5228, lr 9.997532801828658e-05
Step 10200 Loss: 0.5193, lr 9.997532801828658e-05
Step 10300 Loss: 0.5164, lr 9.997532801828658e-05
Step 10400 Loss: 0.5139, lr 9.997532801828658e-05
Step 10500 Loss: 0.5114, lr 9.997532801828658e-05
Step 10600 Loss: 0.5095, lr 9.997532801828658e-05
Step 10700 Loss: 0.5067, lr 9.997532801828658e-05
Step 10800 Loss: 0.5053, lr 9.997532801828658e-05
Step 10900 Loss: 0.5039, lr 9.997532801828658e-05
Step 11000 Loss: 0.5028, lr 9.997532801828658e-05
Step 11100 Loss: 0.5013, lr 9.997532801828658e-05
Step 11200 Loss: 0.5000, lr 9.997532801828658e-05
Step 11300 Loss: 0.4985, lr 9.997532801828658e-05
Step 11400 Loss: 0.4972, lr 9.997532801828658e-05
Step 11500 Loss: 0.4960, lr 9.997532801828658e-05
Step 11600 Loss: 0.4956, lr 9.997532801828658e-05
Step 11700 Loss: 0.4957, lr 9.997532801828658e-05
Step 11800 Loss: 0.4954, lr 9.997532801828658e-05
Step 11900 Loss: 0.4958, lr 9.997532801828658e-05
Step 12000 Loss: 0.4961, lr 9.997532801828658e-05
Step 12100 Loss: 0.4967, lr 9.997532801828658e-05
Step 12200 Loss: 0.4972, lr 9.997532801828658e-05
Step 12300 Loss: 0.4981, lr 9.997532801828658e-05
Step 12400 Loss: 0.4992, lr 9.997532801828658e-05
Step 12500 Loss: 0.5009, lr 9.997532801828658e-05
Step 12600 Loss: 0.5023, lr 9.997532801828658e-05
Step 12700 Loss: 0.5040, lr 9.997532801828658e-05
Step 12800 Loss: 0.5062, lr 9.997532801828658e-05
Step 12900 Loss: 0.5085, lr 9.997532801828658e-05
Step 13000 Loss: 0.5111, lr 9.997532801828658e-05
Step 13100 Loss: 0.5139, lr 9.997532801828658e-05
Step 13200 Loss: 0.5169, lr 9.997532801828658e-05
Step 13300 Loss: 0.5190, lr 9.997532801828658e-05
Step 13400 Loss: 0.5191, lr 9.997532801828658e-05
Step 13500 Loss: 0.5190, lr 9.997532801828658e-05
Train Epoch: [2/100] Loss: 0.5195,lr 0.000100
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 6756
Step 13600 Loss: 1.0832, lr 9.990133642141359e-05
Step 13700 Loss: 0.8905, lr 9.990133642141359e-05
Step 13800 Loss: 0.8127, lr 9.990133642141359e-05
Step 13900 Loss: 0.7610, lr 9.990133642141359e-05
Step 14000 Loss: 0.7256, lr 9.990133642141359e-05
Step 14100 Loss: 0.6952, lr 9.990133642141359e-05
Step 14200 Loss: 0.6719, lr 9.990133642141359e-05
Step 14300 Loss: 0.6507, lr 9.990133642141359e-05
Step 14400 Loss: 0.6333, lr 9.990133642141359e-05
Step 14500 Loss: 0.6185, lr 9.990133642141359e-05
Step 14600 Loss: 0.6052, lr 9.990133642141359e-05
Step 14700 Loss: 0.5931, lr 9.990133642141359e-05
Step 14800 Loss: 0.5827, lr 9.990133642141359e-05
Step 14900 Loss: 0.5728, lr 9.990133642141359e-05
Step 15000 Loss: 0.5640, lr 9.990133642141359e-05
Step 15100 Loss: 0.5545, lr 9.990133642141359e-05
Step 15200 Loss: 0.5461, lr 9.990133642141359e-05
Step 15300 Loss: 0.5394, lr 9.990133642141359e-05
Step 15400 Loss: 0.5329, lr 9.990133642141359e-05
Step 15500 Loss: 0.5262, lr 9.990133642141359e-05
Step 15600 Loss: 0.5203, lr 9.990133642141359e-05
Step 15700 Loss: 0.5146, lr 9.990133642141359e-05
Step 15800 Loss: 0.5094, lr 9.990133642141359e-05
Step 15900 Loss: 0.5048, lr 9.990133642141359e-05
Step 16000 Loss: 0.5002, lr 9.990133642141359e-05
Step 16100 Loss: 0.4951, lr 9.990133642141359e-05
Step 16200 Loss: 0.4905, lr 9.990133642141359e-05
Step 16300 Loss: 0.4874, lr 9.990133642141359e-05
Step 16400 Loss: 0.4838, lr 9.990133642141359e-05
Step 16500 Loss: 0.4807, lr 9.990133642141359e-05
Step 16600 Loss: 0.4777, lr 9.990133642141359e-05
Step 16700 Loss: 0.4745, lr 9.990133642141359e-05
Step 16800 Loss: 0.4714, lr 9.990133642141359e-05
Step 16900 Loss: 0.4682, lr 9.990133642141359e-05
Step 17000 Loss: 0.4649, lr 9.990133642141359e-05
Step 17100 Loss: 0.4621, lr 9.990133642141359e-05
Step 17200 Loss: 0.4599, lr 9.990133642141359e-05
Step 17300 Loss: 0.4580, lr 9.990133642141359e-05
Step 17400 Loss: 0.4553, lr 9.990133642141359e-05
Step 17500 Loss: 0.4537, lr 9.990133642141359e-05
Step 17600 Loss: 0.4520, lr 9.990133642141359e-05
Step 17700 Loss: 0.4513, lr 9.990133642141359e-05
Step 17800 Loss: 0.4498, lr 9.990133642141359e-05
Step 17900 Loss: 0.4483, lr 9.990133642141359e-05
Step 18000 Loss: 0.4469, lr 9.990133642141359e-05
Step 18100 Loss: 0.4452, lr 9.990133642141359e-05
Step 18200 Loss: 0.4442, lr 9.990133642141359e-05
Step 18300 Loss: 0.4437, lr 9.990133642141359e-05
Step 18400 Loss: 0.4432, lr 9.990133642141359e-05
Step 18500 Loss: 0.4431, lr 9.990133642141359e-05
Step 18600 Loss: 0.4428, lr 9.990133642141359e-05
Step 18700 Loss: 0.4430, lr 9.990133642141359e-05
Step 18800 Loss: 0.4431, lr 9.990133642141359e-05
Step 18900 Loss: 0.4433, lr 9.990133642141359e-05
Step 19000 Loss: 0.4437, lr 9.990133642141359e-05
Step 19100 Loss: 0.4443, lr 9.990133642141359e-05
Step 19200 Loss: 0.4456, lr 9.990133642141359e-05
Step 19300 Loss: 0.4468, lr 9.990133642141359e-05
Step 19400 Loss: 0.4483, lr 9.990133642141359e-05
Step 19500 Loss: 0.4503, lr 9.990133642141359e-05
Step 19600 Loss: 0.4528, lr 9.990133642141359e-05
Step 19700 Loss: 0.4551, lr 9.990133642141359e-05
Step 19800 Loss: 0.4575, lr 9.990133642141359e-05
Step 19900 Loss: 0.4602, lr 9.990133642141359e-05
Step 20000 Loss: 0.4625, lr 9.990133642141359e-05
Step 20100 Loss: 0.4632, lr 9.990133642141359e-05
Step 20200 Loss: 0.4631, lr 9.990133642141359e-05
Train Epoch: [3/100] Loss: 0.4636,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6755
Step 20300 Loss: 1.4317, lr 9.977809823015401e-05
Step 20400 Loss: 0.9482, lr 9.977809823015401e-05
Step 20500 Loss: 0.8115, lr 9.977809823015401e-05
Step 20600 Loss: 0.7406, lr 9.977809823015401e-05
Step 20700 Loss: 0.6949, lr 9.977809823015401e-05
Step 20800 Loss: 0.6640, lr 9.977809823015401e-05
Step 20900 Loss: 0.6362, lr 9.977809823015401e-05
Step 21000 Loss: 0.6159, lr 9.977809823015401e-05
Step 21100 Loss: 0.5977, lr 9.977809823015401e-05
Step 21200 Loss: 0.5823, lr 9.977809823015401e-05
Step 21300 Loss: 0.5691, lr 9.977809823015401e-05
Step 21400 Loss: 0.5577, lr 9.977809823015401e-05
Step 21500 Loss: 0.5472, lr 9.977809823015401e-05
Step 21600 Loss: 0.5382, lr 9.977809823015401e-05
Step 21700 Loss: 0.5293, lr 9.977809823015401e-05
Step 21800 Loss: 0.5212, lr 9.977809823015401e-05
Step 21900 Loss: 0.5124, lr 9.977809823015401e-05
Step 22000 Loss: 0.5051, lr 9.977809823015401e-05
Step 22100 Loss: 0.4988, lr 9.977809823015401e-05
Step 22200 Loss: 0.4928, lr 9.977809823015401e-05
Step 22300 Loss: 0.4862, lr 9.977809823015401e-05
Step 22400 Loss: 0.4810, lr 9.977809823015401e-05
Step 22500 Loss: 0.4755, lr 9.977809823015401e-05
Step 22600 Loss: 0.4705, lr 9.977809823015401e-05
Step 22700 Loss: 0.4665, lr 9.977809823015401e-05
Step 22800 Loss: 0.4619, lr 9.977809823015401e-05
Step 22900 Loss: 0.4573, lr 9.977809823015401e-05
Step 23000 Loss: 0.4536, lr 9.977809823015401e-05
Step 23100 Loss: 0.4500, lr 9.977809823015401e-05
Step 23200 Loss: 0.4467, lr 9.977809823015401e-05
Step 23300 Loss: 0.4438, lr 9.977809823015401e-05
Step 23400 Loss: 0.4406, lr 9.977809823015401e-05
Step 23500 Loss: 0.4374, lr 9.977809823015401e-05
Step 23600 Loss: 0.4341, lr 9.977809823015401e-05
Step 23700 Loss: 0.4311, lr 9.977809823015401e-05
Step 23800 Loss: 0.4282, lr 9.977809823015401e-05
Step 23900 Loss: 0.4257, lr 9.977809823015401e-05
Step 24000 Loss: 0.4231, lr 9.977809823015401e-05
Step 24100 Loss: 0.4211, lr 9.977809823015401e-05
Step 24200 Loss: 0.4189, lr 9.977809823015401e-05
Step 24300 Loss: 0.4177, lr 9.977809823015401e-05
Step 24400 Loss: 0.4162, lr 9.977809823015401e-05
Step 24500 Loss: 0.4149, lr 9.977809823015401e-05
Step 24600 Loss: 0.4137, lr 9.977809823015401e-05
Step 24700 Loss: 0.4124, lr 9.977809823015401e-05
Step 24800 Loss: 0.4108, lr 9.977809823015401e-05
Step 24900 Loss: 0.4098, lr 9.977809823015401e-05
Step 25000 Loss: 0.4090, lr 9.977809823015401e-05
Step 25100 Loss: 0.4084, lr 9.977809823015401e-05
Step 25200 Loss: 0.4081, lr 9.977809823015401e-05
Step 25300 Loss: 0.4076, lr 9.977809823015401e-05
Step 25400 Loss: 0.4079, lr 9.977809823015401e-05
Step 25500 Loss: 0.4078, lr 9.977809823015401e-05
Step 25600 Loss: 0.4082, lr 9.977809823015401e-05
Step 25700 Loss: 0.4082, lr 9.977809823015401e-05
Step 25800 Loss: 0.4088, lr 9.977809823015401e-05
Step 25900 Loss: 0.4094, lr 9.977809823015401e-05
Step 26000 Loss: 0.4103, lr 9.977809823015401e-05
Step 26100 Loss: 0.4111, lr 9.977809823015401e-05
Step 26200 Loss: 0.4126, lr 9.977809823015401e-05
Step 26300 Loss: 0.4146, lr 9.977809823015401e-05
Step 26400 Loss: 0.4164, lr 9.977809823015401e-05
Step 26500 Loss: 0.4180, lr 9.977809823015401e-05
Step 26600 Loss: 0.4204, lr 9.977809823015401e-05
Step 26700 Loss: 0.4225, lr 9.977809823015401e-05
Step 26800 Loss: 0.4239, lr 9.977809823015401e-05
Step 26900 Loss: 0.4237, lr 9.977809823015401e-05
Step 27000 Loss: 0.4237, lr 9.977809823015401e-05
Train Epoch: [4/100] Loss: 0.4240,lr 0.000100
Calling G2SDataset.batch()
Done, time:  2.11 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 6755
Step 27100 Loss: 1.0372, lr 9.960573506572391e-05
Step 27200 Loss: 0.8128, lr 9.960573506572391e-05
Step 27300 Loss: 0.7198, lr 9.960573506572391e-05
Step 27400 Loss: 0.6641, lr 9.960573506572391e-05
Step 27500 Loss: 0.6303, lr 9.960573506572391e-05
Step 27600 Loss: 0.6038, lr 9.960573506572391e-05
Step 27700 Loss: 0.5805, lr 9.960573506572391e-05
Step 27800 Loss: 0.5637, lr 9.960573506572391e-05
Step 27900 Loss: 0.5474, lr 9.960573506572391e-05
Step 28000 Loss: 0.5336, lr 9.960573506572391e-05
Step 28100 Loss: 0.5213, lr 9.960573506572391e-05
Step 28200 Loss: 0.5103, lr 9.960573506572391e-05
Step 28300 Loss: 0.5009, lr 9.960573506572391e-05
Step 28400 Loss: 0.4912, lr 9.960573506572391e-05
Step 28500 Loss: 0.4831, lr 9.960573506572391e-05
Step 28600 Loss: 0.4757, lr 9.960573506572391e-05
Step 28700 Loss: 0.4685, lr 9.960573506572391e-05
Step 28800 Loss: 0.4625, lr 9.960573506572391e-05
Step 28900 Loss: 0.4566, lr 9.960573506572391e-05
Step 29000 Loss: 0.4506, lr 9.960573506572391e-05
Step 29100 Loss: 0.4453, lr 9.960573506572391e-05
Step 29200 Loss: 0.4404, lr 9.960573506572391e-05
Step 29300 Loss: 0.4355, lr 9.960573506572391e-05
Step 29400 Loss: 0.4311, lr 9.960573506572391e-05
Step 29500 Loss: 0.4264, lr 9.960573506572391e-05
Step 29600 Loss: 0.4224, lr 9.960573506572391e-05
Step 29700 Loss: 0.4182, lr 9.960573506572391e-05
Step 29800 Loss: 0.4148, lr 9.960573506572391e-05
Step 29900 Loss: 0.4113, lr 9.960573506572391e-05
Step 30000 Loss: 0.4090, lr 9.960573506572391e-05
Step 30100 Loss: 0.4058, lr 9.960573506572391e-05
Step 30200 Loss: 0.4024, lr 9.960573506572391e-05
Step 30300 Loss: 0.3997, lr 9.960573506572391e-05
Step 30400 Loss: 0.3966, lr 9.960573506572391e-05
Step 30500 Loss: 0.3938, lr 9.960573506572391e-05
Step 30600 Loss: 0.3910, lr 9.960573506572391e-05
Step 30700 Loss: 0.3887, lr 9.960573506572391e-05
Step 30800 Loss: 0.3869, lr 9.960573506572391e-05
Step 30900 Loss: 0.3846, lr 9.960573506572391e-05
Step 31000 Loss: 0.3831, lr 9.960573506572391e-05
Step 31100 Loss: 0.3818, lr 9.960573506572391e-05
Step 31200 Loss: 0.3808, lr 9.960573506572391e-05
Step 31300 Loss: 0.3795, lr 9.960573506572391e-05
Step 31400 Loss: 0.3784, lr 9.960573506572391e-05
Step 31500 Loss: 0.3773, lr 9.960573506572391e-05
Step 31600 Loss: 0.3757, lr 9.960573506572391e-05
Step 31700 Loss: 0.3749, lr 9.960573506572391e-05
Step 31800 Loss: 0.3742, lr 9.960573506572391e-05
Step 31900 Loss: 0.3738, lr 9.960573506572391e-05
Step 32000 Loss: 0.3735, lr 9.960573506572391e-05
Step 32100 Loss: 0.3732, lr 9.960573506572391e-05
Step 32200 Loss: 0.3735, lr 9.960573506572391e-05
Step 32300 Loss: 0.3734, lr 9.960573506572391e-05
Step 32400 Loss: 0.3740, lr 9.960573506572391e-05
Step 32500 Loss: 0.3744, lr 9.960573506572391e-05
Step 32600 Loss: 0.3748, lr 9.960573506572391e-05
Step 32700 Loss: 0.3757, lr 9.960573506572391e-05
Step 32800 Loss: 0.3767, lr 9.960573506572391e-05
Step 32900 Loss: 0.3780, lr 9.960573506572391e-05
Step 33000 Loss: 0.3792, lr 9.960573506572391e-05
Step 33100 Loss: 0.3811, lr 9.960573506572391e-05
Step 33200 Loss: 0.3830, lr 9.960573506572391e-05
Step 33300 Loss: 0.3849, lr 9.960573506572391e-05
Step 33400 Loss: 0.3872, lr 9.960573506572391e-05
Step 33500 Loss: 0.3890, lr 9.960573506572391e-05
Step 33600 Loss: 0.3900, lr 9.960573506572391e-05
Step 33700 Loss: 0.3894, lr 9.960573506572391e-05
Train Epoch: [5/100] Loss: 0.3903,lr 0.000100
Model Saving at epoch 5
Calling G2SDataset.batch()
Done, time:  2.53 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6755
Step 33800 Loss: 1.5361, lr 9.93844170297569e-05
Step 33900 Loss: 0.9372, lr 9.93844170297569e-05
Step 34000 Loss: 0.7767, lr 9.93844170297569e-05
Step 34100 Loss: 0.7009, lr 9.93844170297569e-05
Step 34200 Loss: 0.6524, lr 9.93844170297569e-05
Step 34300 Loss: 0.6201, lr 9.93844170297569e-05
Step 34400 Loss: 0.5907, lr 9.93844170297569e-05
Step 34500 Loss: 0.5683, lr 9.93844170297569e-05
Step 34600 Loss: 0.5497, lr 9.93844170297569e-05
Step 34700 Loss: 0.5317, lr 9.93844170297569e-05
Step 34800 Loss: 0.5176, lr 9.93844170297569e-05
Step 34900 Loss: 0.5053, lr 9.93844170297569e-05
Step 35000 Loss: 0.4933, lr 9.93844170297569e-05
Step 35100 Loss: 0.4830, lr 9.93844170297569e-05
Step 35200 Loss: 0.4740, lr 9.93844170297569e-05
Step 35300 Loss: 0.4667, lr 9.93844170297569e-05
Step 35400 Loss: 0.4583, lr 9.93844170297569e-05
Step 35500 Loss: 0.4516, lr 9.93844170297569e-05
Step 35600 Loss: 0.4453, lr 9.93844170297569e-05
Step 35700 Loss: 0.4398, lr 9.93844170297569e-05
Step 35800 Loss: 0.4335, lr 9.93844170297569e-05
Step 35900 Loss: 0.4283, lr 9.93844170297569e-05
Step 36000 Loss: 0.4235, lr 9.93844170297569e-05
Step 36100 Loss: 0.4188, lr 9.93844170297569e-05
Step 36200 Loss: 0.4150, lr 9.93844170297569e-05
Step 36300 Loss: 0.4106, lr 9.93844170297569e-05
Step 36400 Loss: 0.4061, lr 9.93844170297569e-05
Step 36500 Loss: 0.4027, lr 9.93844170297569e-05
Step 36600 Loss: 0.3992, lr 9.93844170297569e-05
Step 36700 Loss: 0.3960, lr 9.93844170297569e-05
Step 36800 Loss: 0.3934, lr 9.93844170297569e-05
Step 36900 Loss: 0.3901, lr 9.93844170297569e-05
Step 37000 Loss: 0.3868, lr 9.93844170297569e-05
Step 37100 Loss: 0.3841, lr 9.93844170297569e-05
Step 37200 Loss: 0.3811, lr 9.93844170297569e-05
Step 37300 Loss: 0.3784, lr 9.93844170297569e-05
Step 37400 Loss: 0.3760, lr 9.93844170297569e-05
Step 37500 Loss: 0.3733, lr 9.93844170297569e-05
Step 37600 Loss: 0.3715, lr 9.93844170297569e-05
Step 37700 Loss: 0.3694, lr 9.93844170297569e-05
Step 37800 Loss: 0.3679, lr 9.93844170297569e-05
Step 37900 Loss: 0.3664, lr 9.93844170297569e-05
Step 38000 Loss: 0.3653, lr 9.93844170297569e-05
Step 38100 Loss: 0.3642, lr 9.93844170297569e-05
Step 38200 Loss: 0.3629, lr 9.93844170297569e-05
Step 38300 Loss: 0.3615, lr 9.93844170297569e-05
Step 38400 Loss: 0.3601, lr 9.93844170297569e-05
Step 38500 Loss: 0.3591, lr 9.93844170297569e-05
Step 38600 Loss: 0.3587, lr 9.93844170297569e-05
Step 38700 Loss: 0.3581, lr 9.93844170297569e-05
Step 38800 Loss: 0.3576, lr 9.93844170297569e-05
Step 38900 Loss: 0.3575, lr 9.93844170297569e-05
Step 39000 Loss: 0.3574, lr 9.93844170297569e-05
Step 39100 Loss: 0.3578, lr 9.93844170297569e-05
Step 39200 Loss: 0.3578, lr 9.93844170297569e-05
Step 39300 Loss: 0.3581, lr 9.93844170297569e-05
Step 39400 Loss: 0.3587, lr 9.93844170297569e-05
Step 39500 Loss: 0.3594, lr 9.93844170297569e-05
Step 39600 Loss: 0.3605, lr 9.93844170297569e-05
Step 39700 Loss: 0.3618, lr 9.93844170297569e-05
Step 39800 Loss: 0.3633, lr 9.93844170297569e-05
Step 39900 Loss: 0.3651, lr 9.93844170297569e-05
Step 40000 Loss: 0.3668, lr 9.93844170297569e-05
Step 40100 Loss: 0.3686, lr 9.93844170297569e-05
Step 40200 Loss: 0.3707, lr 9.93844170297569e-05
Step 40300 Loss: 0.3718, lr 9.93844170297569e-05
Step 40400 Loss: 0.3716, lr 9.93844170297569e-05
Step 40500 Loss: 0.3713, lr 9.93844170297569e-05
Train Epoch: [6/100] Loss: 0.3719,lr 0.000099
Calling G2SDataset.batch()
Done, time:  2.37 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.47 s, total batches: 6756
Step 40600 Loss: 1.0933, lr 9.911436253643445e-05
Step 40700 Loss: 0.8333, lr 9.911436253643445e-05
Step 40800 Loss: 0.7415, lr 9.911436253643445e-05
Step 40900 Loss: 0.6813, lr 9.911436253643445e-05
Step 41000 Loss: 0.6397, lr 9.911436253643445e-05
Step 41100 Loss: 0.6060, lr 9.911436253643445e-05
Step 41200 Loss: 0.5760, lr 9.911436253643445e-05
Step 41300 Loss: 0.5539, lr 9.911436253643445e-05
Step 41400 Loss: 0.5356, lr 9.911436253643445e-05
Step 41500 Loss: 0.5190, lr 9.911436253643445e-05
Step 41600 Loss: 0.5051, lr 9.911436253643445e-05
Step 41700 Loss: 0.4926, lr 9.911436253643445e-05
Step 41800 Loss: 0.4819, lr 9.911436253643445e-05
Step 41900 Loss: 0.4710, lr 9.911436253643445e-05
Step 42000 Loss: 0.4619, lr 9.911436253643445e-05
Step 42100 Loss: 0.4541, lr 9.911436253643445e-05
Step 42200 Loss: 0.4461, lr 9.911436253643445e-05
Step 42300 Loss: 0.4389, lr 9.911436253643445e-05
Step 42400 Loss: 0.4328, lr 9.911436253643445e-05
Step 42500 Loss: 0.4265, lr 9.911436253643445e-05
Step 42600 Loss: 0.4205, lr 9.911436253643445e-05
Step 42700 Loss: 0.4151, lr 9.911436253643445e-05
Step 42800 Loss: 0.4105, lr 9.911436253643445e-05
Step 42900 Loss: 0.4055, lr 9.911436253643445e-05
Step 43000 Loss: 0.4009, lr 9.911436253643445e-05
Step 43100 Loss: 0.3969, lr 9.911436253643445e-05
Step 43200 Loss: 0.3927, lr 9.911436253643445e-05
Step 43300 Loss: 0.3893, lr 9.911436253643445e-05
Step 43400 Loss: 0.3853, lr 9.911436253643445e-05
Step 43500 Loss: 0.3827, lr 9.911436253643445e-05
Step 43600 Loss: 0.3800, lr 9.911436253643445e-05
Step 43700 Loss: 0.3764, lr 9.911436253643445e-05
Step 43800 Loss: 0.3735, lr 9.911436253643445e-05
Step 43900 Loss: 0.3704, lr 9.911436253643445e-05
Step 44000 Loss: 0.3678, lr 9.911436253643445e-05
Step 44100 Loss: 0.3650, lr 9.911436253643445e-05
Step 44200 Loss: 0.3629, lr 9.911436253643445e-05
Step 44300 Loss: 0.3607, lr 9.911436253643445e-05
Step 44400 Loss: 0.3585, lr 9.911436253643445e-05
Step 44500 Loss: 0.3567, lr 9.911436253643445e-05
Step 44600 Loss: 0.3554, lr 9.911436253643445e-05
Step 44700 Loss: 0.3539, lr 9.911436253643445e-05
Step 44800 Loss: 0.3525, lr 9.911436253643445e-05
Step 44900 Loss: 0.3511, lr 9.911436253643445e-05
Step 45000 Loss: 0.3501, lr 9.911436253643445e-05
Step 45100 Loss: 0.3485, lr 9.911436253643445e-05
Step 45200 Loss: 0.3472, lr 9.911436253643445e-05
Step 45300 Loss: 0.3464, lr 9.911436253643445e-05
Step 45400 Loss: 0.3461, lr 9.911436253643445e-05
Step 45500 Loss: 0.3455, lr 9.911436253643445e-05
Step 45600 Loss: 0.3450, lr 9.911436253643445e-05
Step 45700 Loss: 0.3450, lr 9.911436253643445e-05
Step 45800 Loss: 0.3450, lr 9.911436253643445e-05
Step 45900 Loss: 0.3454, lr 9.911436253643445e-05
Step 46000 Loss: 0.3455, lr 9.911436253643445e-05
Step 46100 Loss: 0.3455, lr 9.911436253643445e-05
Step 46200 Loss: 0.3461, lr 9.911436253643445e-05
Step 46300 Loss: 0.3468, lr 9.911436253643445e-05
Step 46400 Loss: 0.3479, lr 9.911436253643445e-05
Step 46500 Loss: 0.3493, lr 9.911436253643445e-05
Step 46600 Loss: 0.3508, lr 9.911436253643445e-05
Step 46700 Loss: 0.3524, lr 9.911436253643445e-05
Step 46800 Loss: 0.3544, lr 9.911436253643445e-05
Step 46900 Loss: 0.3561, lr 9.911436253643445e-05
Step 47000 Loss: 0.3579, lr 9.911436253643445e-05
Step 47100 Loss: 0.3586, lr 9.911436253643445e-05
Step 47200 Loss: 0.3581, lr 9.911436253643445e-05
Train Epoch: [7/100] Loss: 0.3584,lr 0.000099
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6756
Step 47300 Loss: 1.9552, lr 9.879583809693738e-05
Step 47400 Loss: 0.9235, lr 9.879583809693738e-05
Step 47500 Loss: 0.7450, lr 9.879583809693738e-05
Step 47600 Loss: 0.6563, lr 9.879583809693738e-05
Step 47700 Loss: 0.6041, lr 9.879583809693738e-05
Step 47800 Loss: 0.5703, lr 9.879583809693738e-05
Step 47900 Loss: 0.5403, lr 9.879583809693738e-05
Step 48000 Loss: 0.5192, lr 9.879583809693738e-05
Step 48100 Loss: 0.5015, lr 9.879583809693738e-05
Step 48200 Loss: 0.4851, lr 9.879583809693738e-05
Step 48300 Loss: 0.4707, lr 9.879583809693738e-05
Step 48400 Loss: 0.4594, lr 9.879583809693738e-05
Step 48500 Loss: 0.4482, lr 9.879583809693738e-05
Step 48600 Loss: 0.4392, lr 9.879583809693738e-05
Step 48700 Loss: 0.4302, lr 9.879583809693738e-05
Step 48800 Loss: 0.4234, lr 9.879583809693738e-05
Step 48900 Loss: 0.4156, lr 9.879583809693738e-05
Step 49000 Loss: 0.4098, lr 9.879583809693738e-05
Step 49100 Loss: 0.4043, lr 9.879583809693738e-05
Step 49200 Loss: 0.3989, lr 9.879583809693738e-05
Step 49300 Loss: 0.3933, lr 9.879583809693738e-05
Step 49400 Loss: 0.3882, lr 9.879583809693738e-05
Step 49500 Loss: 0.3841, lr 9.879583809693738e-05
Step 49600 Loss: 0.3800, lr 9.879583809693738e-05
Step 49700 Loss: 0.3763, lr 9.879583809693738e-05
Step 49800 Loss: 0.3723, lr 9.879583809693738e-05
Step 49900 Loss: 0.3686, lr 9.879583809693738e-05
Step 50000 Loss: 0.3653, lr 9.879583809693738e-05
Step 50100 Loss: 0.3624, lr 9.879583809693738e-05
Step 50200 Loss: 0.3596, lr 9.879583809693738e-05
Step 50300 Loss: 0.3575, lr 9.879583809693738e-05
Step 50400 Loss: 0.3547, lr 9.879583809693738e-05
Step 50500 Loss: 0.3517, lr 9.879583809693738e-05
Step 50600 Loss: 0.3493, lr 9.879583809693738e-05
Step 50700 Loss: 0.3468, lr 9.879583809693738e-05
Step 50800 Loss: 0.3445, lr 9.879583809693738e-05
Step 50900 Loss: 0.3422, lr 9.879583809693738e-05
Step 51000 Loss: 0.3402, lr 9.879583809693738e-05
Step 51100 Loss: 0.3386, lr 9.879583809693738e-05
Step 51200 Loss: 0.3371, lr 9.879583809693738e-05
Step 51300 Loss: 0.3358, lr 9.879583809693738e-05
Step 51400 Loss: 0.3344, lr 9.879583809693738e-05
Step 51500 Loss: 0.3335, lr 9.879583809693738e-05
Step 51600 Loss: 0.3324, lr 9.879583809693738e-05
Step 51700 Loss: 0.3311, lr 9.879583809693738e-05
Step 51800 Loss: 0.3300, lr 9.879583809693738e-05
Step 51900 Loss: 0.3289, lr 9.879583809693738e-05
Step 52000 Loss: 0.3278, lr 9.879583809693738e-05
Step 52100 Loss: 0.3276, lr 9.879583809693738e-05
Step 52200 Loss: 0.3270, lr 9.879583809693738e-05
Step 52300 Loss: 0.3267, lr 9.879583809693738e-05
Step 52400 Loss: 0.3265, lr 9.879583809693738e-05
Step 52500 Loss: 0.3267, lr 9.879583809693738e-05
Step 52600 Loss: 0.3271, lr 9.879583809693738e-05
Step 52700 Loss: 0.3273, lr 9.879583809693738e-05
Step 52800 Loss: 0.3276, lr 9.879583809693738e-05
Step 52900 Loss: 0.3280, lr 9.879583809693738e-05
Step 53000 Loss: 0.3288, lr 9.879583809693738e-05
Step 53100 Loss: 0.3297, lr 9.879583809693738e-05
Step 53200 Loss: 0.3308, lr 9.879583809693738e-05
Step 53300 Loss: 0.3322, lr 9.879583809693738e-05
Step 53400 Loss: 0.3340, lr 9.879583809693738e-05
Step 53500 Loss: 0.3358, lr 9.879583809693738e-05
Step 53600 Loss: 0.3378, lr 9.879583809693738e-05
Step 53700 Loss: 0.3396, lr 9.879583809693738e-05
Step 53800 Loss: 0.3409, lr 9.879583809693738e-05
Step 53900 Loss: 0.3411, lr 9.879583809693738e-05
Step 54000 Loss: 0.3407, lr 9.879583809693738e-05
Train Epoch: [8/100] Loss: 0.3412,lr 0.000099
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 54100 Loss: 1.0386, lr 9.842915805643157e-05
Step 54200 Loss: 0.7758, lr 9.842915805643157e-05
Step 54300 Loss: 0.6939, lr 9.842915805643157e-05
Step 54400 Loss: 0.6377, lr 9.842915805643157e-05
Step 54500 Loss: 0.5955, lr 9.842915805643157e-05
Step 54600 Loss: 0.5617, lr 9.842915805643157e-05
Step 54700 Loss: 0.5328, lr 9.842915805643157e-05
Step 54800 Loss: 0.5101, lr 9.842915805643157e-05
Step 54900 Loss: 0.4921, lr 9.842915805643157e-05
Step 55000 Loss: 0.4754, lr 9.842915805643157e-05
Step 55100 Loss: 0.4611, lr 9.842915805643157e-05
Step 55200 Loss: 0.4494, lr 9.842915805643157e-05
Step 55300 Loss: 0.4390, lr 9.842915805643157e-05
Step 55400 Loss: 0.4295, lr 9.842915805643157e-05
Step 55500 Loss: 0.4211, lr 9.842915805643157e-05
Step 55600 Loss: 0.4141, lr 9.842915805643157e-05
Step 55700 Loss: 0.4071, lr 9.842915805643157e-05
Step 55800 Loss: 0.4008, lr 9.842915805643157e-05
Step 55900 Loss: 0.3949, lr 9.842915805643157e-05
Step 56000 Loss: 0.3895, lr 9.842915805643157e-05
Step 56100 Loss: 0.3840, lr 9.842915805643157e-05
Step 56200 Loss: 0.3793, lr 9.842915805643157e-05
Step 56300 Loss: 0.3749, lr 9.842915805643157e-05
Step 56400 Loss: 0.3702, lr 9.842915805643157e-05
Step 56500 Loss: 0.3662, lr 9.842915805643157e-05
Step 56600 Loss: 0.3624, lr 9.842915805643157e-05
Step 56700 Loss: 0.3586, lr 9.842915805643157e-05
Step 56800 Loss: 0.3555, lr 9.842915805643157e-05
Step 56900 Loss: 0.3524, lr 9.842915805643157e-05
Step 57000 Loss: 0.3501, lr 9.842915805643157e-05
Step 57100 Loss: 0.3477, lr 9.842915805643157e-05
Step 57200 Loss: 0.3447, lr 9.842915805643157e-05
Step 57300 Loss: 0.3420, lr 9.842915805643157e-05
Step 57400 Loss: 0.3390, lr 9.842915805643157e-05
Step 57500 Loss: 0.3367, lr 9.842915805643157e-05
Step 57600 Loss: 0.3342, lr 9.842915805643157e-05
Step 57700 Loss: 0.3323, lr 9.842915805643157e-05
Step 57800 Loss: 0.3302, lr 9.842915805643157e-05
Step 57900 Loss: 0.3284, lr 9.842915805643157e-05
Step 58000 Loss: 0.3265, lr 9.842915805643157e-05
Step 58100 Loss: 0.3256, lr 9.842915805643157e-05
Step 58200 Loss: 0.3241, lr 9.842915805643157e-05
Step 58300 Loss: 0.3232, lr 9.842915805643157e-05
Step 58400 Loss: 0.3218, lr 9.842915805643157e-05
Step 58500 Loss: 0.3210, lr 9.842915805643157e-05
Step 58600 Loss: 0.3196, lr 9.842915805643157e-05
Step 58700 Loss: 0.3186, lr 9.842915805643157e-05
Step 58800 Loss: 0.3179, lr 9.842915805643157e-05
Step 58900 Loss: 0.3174, lr 9.842915805643157e-05
Step 59000 Loss: 0.3171, lr 9.842915805643157e-05
Step 59100 Loss: 0.3166, lr 9.842915805643157e-05
Step 59200 Loss: 0.3169, lr 9.842915805643157e-05
Step 59300 Loss: 0.3169, lr 9.842915805643157e-05
Step 59400 Loss: 0.3173, lr 9.842915805643157e-05
Step 59500 Loss: 0.3172, lr 9.842915805643157e-05
Step 59600 Loss: 0.3175, lr 9.842915805643157e-05
Step 59700 Loss: 0.3181, lr 9.842915805643157e-05
Step 59800 Loss: 0.3189, lr 9.842915805643157e-05
Step 59900 Loss: 0.3199, lr 9.842915805643157e-05
Step 60000 Loss: 0.3211, lr 9.842915805643157e-05
Step 60100 Loss: 0.3227, lr 9.842915805643157e-05
Step 60200 Loss: 0.3243, lr 9.842915805643157e-05
Step 60300 Loss: 0.3262, lr 9.842915805643157e-05
Step 60400 Loss: 0.3280, lr 9.842915805643157e-05
Step 60500 Loss: 0.3299, lr 9.842915805643157e-05
Step 60600 Loss: 0.3307, lr 9.842915805643157e-05
Step 60700 Loss: 0.3304, lr 9.842915805643157e-05
Step 60800 Loss: 0.3303, lr 9.842915805643157e-05
Train Epoch: [9/100] Loss: 0.3304,lr 0.000098
Calling G2SDataset.batch()
Done, time:  2.17 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 6756
Step 60900 Loss: 0.8618, lr 9.801468428384717e-05
Step 61000 Loss: 0.6905, lr 9.801468428384717e-05
Step 61100 Loss: 0.6084, lr 9.801468428384717e-05
Step 61200 Loss: 0.5577, lr 9.801468428384717e-05
Step 61300 Loss: 0.5276, lr 9.801468428384717e-05
Step 61400 Loss: 0.5009, lr 9.801468428384717e-05
Step 61500 Loss: 0.4814, lr 9.801468428384717e-05
Step 61600 Loss: 0.4645, lr 9.801468428384717e-05
Step 61700 Loss: 0.4494, lr 9.801468428384717e-05
Step 61800 Loss: 0.4356, lr 9.801468428384717e-05
Step 61900 Loss: 0.4244, lr 9.801468428384717e-05
Step 62000 Loss: 0.4142, lr 9.801468428384717e-05
Step 62100 Loss: 0.4064, lr 9.801468428384717e-05
Step 62200 Loss: 0.3974, lr 9.801468428384717e-05
Step 62300 Loss: 0.3913, lr 9.801468428384717e-05
Step 62400 Loss: 0.3845, lr 9.801468428384717e-05
Step 62500 Loss: 0.3792, lr 9.801468428384717e-05
Step 62600 Loss: 0.3743, lr 9.801468428384717e-05
Step 62700 Loss: 0.3693, lr 9.801468428384717e-05
Step 62800 Loss: 0.3641, lr 9.801468428384717e-05
Step 62900 Loss: 0.3596, lr 9.801468428384717e-05
Step 63000 Loss: 0.3556, lr 9.801468428384717e-05
Step 63100 Loss: 0.3522, lr 9.801468428384717e-05
Step 63200 Loss: 0.3485, lr 9.801468428384717e-05
Step 63300 Loss: 0.3448, lr 9.801468428384717e-05
Step 63400 Loss: 0.3414, lr 9.801468428384717e-05
Step 63500 Loss: 0.3384, lr 9.801468428384717e-05
Step 63600 Loss: 0.3358, lr 9.801468428384717e-05
Step 63700 Loss: 0.3332, lr 9.801468428384717e-05
Step 63800 Loss: 0.3311, lr 9.801468428384717e-05
Step 63900 Loss: 0.3288, lr 9.801468428384717e-05
Step 64000 Loss: 0.3260, lr 9.801468428384717e-05
Step 64100 Loss: 0.3237, lr 9.801468428384717e-05
Step 64200 Loss: 0.3213, lr 9.801468428384717e-05
Step 64300 Loss: 0.3197, lr 9.801468428384717e-05
Step 64400 Loss: 0.3175, lr 9.801468428384717e-05
Step 64500 Loss: 0.3158, lr 9.801468428384717e-05
Step 64600 Loss: 0.3142, lr 9.801468428384717e-05
Step 64700 Loss: 0.3127, lr 9.801468428384717e-05
Step 64800 Loss: 0.3116, lr 9.801468428384717e-05
Step 64900 Loss: 0.3104, lr 9.801468428384717e-05
Step 65000 Loss: 0.3098, lr 9.801468428384717e-05
Step 65100 Loss: 0.3089, lr 9.801468428384717e-05
Step 65200 Loss: 0.3081, lr 9.801468428384717e-05
Step 65300 Loss: 0.3070, lr 9.801468428384717e-05
Step 65400 Loss: 0.3060, lr 9.801468428384717e-05
Step 65500 Loss: 0.3051, lr 9.801468428384717e-05
Step 65600 Loss: 0.3047, lr 9.801468428384717e-05
Step 65700 Loss: 0.3045, lr 9.801468428384717e-05
Step 65800 Loss: 0.3043, lr 9.801468428384717e-05
Step 65900 Loss: 0.3042, lr 9.801468428384717e-05
Step 66000 Loss: 0.3043, lr 9.801468428384717e-05
Step 66100 Loss: 0.3045, lr 9.801468428384717e-05
Step 66200 Loss: 0.3048, lr 9.801468428384717e-05
Step 66300 Loss: 0.3050, lr 9.801468428384717e-05
Step 66400 Loss: 0.3054, lr 9.801468428384717e-05
Step 66500 Loss: 0.3059, lr 9.801468428384717e-05
Step 66600 Loss: 0.3066, lr 9.801468428384717e-05
Step 66700 Loss: 0.3079, lr 9.801468428384717e-05
Step 66800 Loss: 0.3094, lr 9.801468428384717e-05
Step 66900 Loss: 0.3111, lr 9.801468428384717e-05
Step 67000 Loss: 0.3127, lr 9.801468428384717e-05
Step 67100 Loss: 0.3146, lr 9.801468428384717e-05
Step 67200 Loss: 0.3164, lr 9.801468428384717e-05
Step 67300 Loss: 0.3178, lr 9.801468428384717e-05
Step 67400 Loss: 0.3179, lr 9.801468428384717e-05
Step 67500 Loss: 0.3177, lr 9.801468428384717e-05
Train Epoch: [10/100] Loss: 0.3181,lr 0.000098
Model Saving at epoch 10
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Step 67600 Loss: 1.1020, lr 9.75528258147577e-05
Step 67700 Loss: 0.7507, lr 9.75528258147577e-05
Step 67800 Loss: 0.6338, lr 9.75528258147577e-05
Step 67900 Loss: 0.5701, lr 9.75528258147577e-05
Step 68000 Loss: 0.5310, lr 9.75528258147577e-05
Step 68100 Loss: 0.5051, lr 9.75528258147577e-05
Step 68200 Loss: 0.4799, lr 9.75528258147577e-05
Step 68300 Loss: 0.4602, lr 9.75528258147577e-05
Step 68400 Loss: 0.4463, lr 9.75528258147577e-05
Step 68500 Loss: 0.4320, lr 9.75528258147577e-05
Step 68600 Loss: 0.4202, lr 9.75528258147577e-05
Step 68700 Loss: 0.4108, lr 9.75528258147577e-05
Step 68800 Loss: 0.4018, lr 9.75528258147577e-05
Step 68900 Loss: 0.3937, lr 9.75528258147577e-05
Step 69000 Loss: 0.3859, lr 9.75528258147577e-05
Step 69100 Loss: 0.3799, lr 9.75528258147577e-05
Step 69200 Loss: 0.3739, lr 9.75528258147577e-05
Step 69300 Loss: 0.3686, lr 9.75528258147577e-05
Step 69400 Loss: 0.3635, lr 9.75528258147577e-05
Step 69500 Loss: 0.3586, lr 9.75528258147577e-05
Step 69600 Loss: 0.3537, lr 9.75528258147577e-05
Step 69700 Loss: 0.3495, lr 9.75528258147577e-05
Step 69800 Loss: 0.3455, lr 9.75528258147577e-05
Step 69900 Loss: 0.3417, lr 9.75528258147577e-05
Step 70000 Loss: 0.3382, lr 9.75528258147577e-05
Step 70100 Loss: 0.3349, lr 9.75528258147577e-05
Step 70200 Loss: 0.3321, lr 9.75528258147577e-05
Step 70300 Loss: 0.3296, lr 9.75528258147577e-05
Step 70400 Loss: 0.3266, lr 9.75528258147577e-05
Step 70500 Loss: 0.3244, lr 9.75528258147577e-05
Step 70600 Loss: 0.3226, lr 9.75528258147577e-05
Step 70700 Loss: 0.3202, lr 9.75528258147577e-05
Step 70800 Loss: 0.3177, lr 9.75528258147577e-05
Step 70900 Loss: 0.3155, lr 9.75528258147577e-05
Step 71000 Loss: 0.3133, lr 9.75528258147577e-05
Step 71100 Loss: 0.3116, lr 9.75528258147577e-05
Step 71200 Loss: 0.3099, lr 9.75528258147577e-05
Step 71300 Loss: 0.3079, lr 9.75528258147577e-05
Step 71400 Loss: 0.3068, lr 9.75528258147577e-05
Step 71500 Loss: 0.3051, lr 9.75528258147577e-05
Step 71600 Loss: 0.3042, lr 9.75528258147577e-05
Step 71700 Loss: 0.3029, lr 9.75528258147577e-05
Step 71800 Loss: 0.3020, lr 9.75528258147577e-05
Step 71900 Loss: 0.3010, lr 9.75528258147577e-05
Step 72000 Loss: 0.3000, lr 9.75528258147577e-05
Step 72100 Loss: 0.2989, lr 9.75528258147577e-05
Step 72200 Loss: 0.2980, lr 9.75528258147577e-05
Step 72300 Loss: 0.2973, lr 9.75528258147577e-05
Step 72400 Loss: 0.2969, lr 9.75528258147577e-05
Step 72500 Loss: 0.2967, lr 9.75528258147577e-05
Step 72600 Loss: 0.2963, lr 9.75528258147577e-05
Step 72700 Loss: 0.2966, lr 9.75528258147577e-05
Step 72800 Loss: 0.2965, lr 9.75528258147577e-05
Step 72900 Loss: 0.2970, lr 9.75528258147577e-05
Step 73000 Loss: 0.2971, lr 9.75528258147577e-05
Step 73100 Loss: 0.2974, lr 9.75528258147577e-05
Step 73200 Loss: 0.2980, lr 9.75528258147577e-05
Step 73300 Loss: 0.2988, lr 9.75528258147577e-05
Step 73400 Loss: 0.2996, lr 9.75528258147577e-05
Step 73500 Loss: 0.3008, lr 9.75528258147577e-05
Step 73600 Loss: 0.3023, lr 9.75528258147577e-05
Step 73700 Loss: 0.3039, lr 9.75528258147577e-05
Step 73800 Loss: 0.3055, lr 9.75528258147577e-05
Step 73900 Loss: 0.3073, lr 9.75528258147577e-05
Step 74000 Loss: 0.3094, lr 9.75528258147577e-05
Step 74100 Loss: 0.3107, lr 9.75528258147577e-05
Step 74200 Loss: 0.3104, lr 9.75528258147577e-05
Step 74300 Loss: 0.3104, lr 9.75528258147577e-05
Train Epoch: [11/100] Loss: 0.3107,lr 0.000098
Calling G2SDataset.batch()
Done, time:  2.25 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.24 s, total batches: 6756
Step 74400 Loss: 0.8128, lr 9.70440384477113e-05
Step 74500 Loss: 0.6553, lr 9.70440384477113e-05
Step 74600 Loss: 0.5776, lr 9.70440384477113e-05
Step 74700 Loss: 0.5308, lr 9.70440384477113e-05
Step 74800 Loss: 0.5026, lr 9.70440384477113e-05
Step 74900 Loss: 0.4784, lr 9.70440384477113e-05
Step 75000 Loss: 0.4586, lr 9.70440384477113e-05
Step 75100 Loss: 0.4431, lr 9.70440384477113e-05
Step 75200 Loss: 0.4292, lr 9.70440384477113e-05
Step 75300 Loss: 0.4163, lr 9.70440384477113e-05
Step 75400 Loss: 0.4055, lr 9.70440384477113e-05
Step 75500 Loss: 0.3964, lr 9.70440384477113e-05
Step 75600 Loss: 0.3882, lr 9.70440384477113e-05
Step 75700 Loss: 0.3794, lr 9.70440384477113e-05
Step 75800 Loss: 0.3733, lr 9.70440384477113e-05
Step 75900 Loss: 0.3673, lr 9.70440384477113e-05
Step 76000 Loss: 0.3619, lr 9.70440384477113e-05
Step 76100 Loss: 0.3573, lr 9.70440384477113e-05
Step 76200 Loss: 0.3525, lr 9.70440384477113e-05
Step 76300 Loss: 0.3474, lr 9.70440384477113e-05
Step 76400 Loss: 0.3429, lr 9.70440384477113e-05
Step 76500 Loss: 0.3388, lr 9.70440384477113e-05
Step 76600 Loss: 0.3358, lr 9.70440384477113e-05
Step 76700 Loss: 0.3320, lr 9.70440384477113e-05
Step 76800 Loss: 0.3284, lr 9.70440384477113e-05
Step 76900 Loss: 0.3253, lr 9.70440384477113e-05
Step 77000 Loss: 0.3226, lr 9.70440384477113e-05
Step 77100 Loss: 0.3200, lr 9.70440384477113e-05
Step 77200 Loss: 0.3176, lr 9.70440384477113e-05
Step 77300 Loss: 0.3159, lr 9.70440384477113e-05
Step 77400 Loss: 0.3138, lr 9.70440384477113e-05
Step 77500 Loss: 0.3113, lr 9.70440384477113e-05
Step 77600 Loss: 0.3093, lr 9.70440384477113e-05
Step 77700 Loss: 0.3072, lr 9.70440384477113e-05
Step 77800 Loss: 0.3054, lr 9.70440384477113e-05
Step 77900 Loss: 0.3033, lr 9.70440384477113e-05
Step 78000 Loss: 0.3017, lr 9.70440384477113e-05
Step 78100 Loss: 0.3001, lr 9.70440384477113e-05
Step 78200 Loss: 0.2985, lr 9.70440384477113e-05
Step 78300 Loss: 0.2973, lr 9.70440384477113e-05
Step 78400 Loss: 0.2962, lr 9.70440384477113e-05
Step 78500 Loss: 0.2955, lr 9.70440384477113e-05
Step 78600 Loss: 0.2941, lr 9.70440384477113e-05
Step 78700 Loss: 0.2933, lr 9.70440384477113e-05
Step 78800 Loss: 0.2924, lr 9.70440384477113e-05
Step 78900 Loss: 0.2913, lr 9.70440384477113e-05
Step 79000 Loss: 0.2903, lr 9.70440384477113e-05
Step 79100 Loss: 0.2899, lr 9.70440384477113e-05
Step 79200 Loss: 0.2895, lr 9.70440384477113e-05
Step 79300 Loss: 0.2893, lr 9.70440384477113e-05
Step 79400 Loss: 0.2892, lr 9.70440384477113e-05
Step 79500 Loss: 0.2893, lr 9.70440384477113e-05
Step 79600 Loss: 0.2894, lr 9.70440384477113e-05
Step 79700 Loss: 0.2897, lr 9.70440384477113e-05
Step 79800 Loss: 0.2900, lr 9.70440384477113e-05
Step 79900 Loss: 0.2900, lr 9.70440384477113e-05
Step 80000 Loss: 0.2907, lr 9.70440384477113e-05
Step 80100 Loss: 0.2914, lr 9.70440384477113e-05
Step 80200 Loss: 0.2923, lr 9.70440384477113e-05
Step 80300 Loss: 0.2938, lr 9.70440384477113e-05
Step 80400 Loss: 0.2951, lr 9.70440384477113e-05
Step 80500 Loss: 0.2969, lr 9.70440384477113e-05
Step 80600 Loss: 0.2986, lr 9.70440384477113e-05
Step 80700 Loss: 0.3004, lr 9.70440384477113e-05
Step 80800 Loss: 0.3020, lr 9.70440384477113e-05
Step 80900 Loss: 0.3023, lr 9.70440384477113e-05
Step 81000 Loss: 0.3020, lr 9.70440384477113e-05
Train Epoch: [12/100] Loss: 0.3027,lr 0.000097
Calling G2SDataset.batch()
Done, time:  2.21 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Step 81100 Loss: 1.1244, lr 9.64888242944126e-05
Step 81200 Loss: 0.7103, lr 9.64888242944126e-05
Step 81300 Loss: 0.6091, lr 9.64888242944126e-05
Step 81400 Loss: 0.5475, lr 9.64888242944126e-05
Step 81500 Loss: 0.5096, lr 9.64888242944126e-05
Step 81600 Loss: 0.4849, lr 9.64888242944126e-05
Step 81700 Loss: 0.4618, lr 9.64888242944126e-05
Step 81800 Loss: 0.4435, lr 9.64888242944126e-05
Step 81900 Loss: 0.4295, lr 9.64888242944126e-05
Step 82000 Loss: 0.4159, lr 9.64888242944126e-05
Step 82100 Loss: 0.4043, lr 9.64888242944126e-05
Step 82200 Loss: 0.3946, lr 9.64888242944126e-05
Step 82300 Loss: 0.3856, lr 9.64888242944126e-05
Step 82400 Loss: 0.3781, lr 9.64888242944126e-05
Step 82500 Loss: 0.3709, lr 9.64888242944126e-05
Step 82600 Loss: 0.3655, lr 9.64888242944126e-05
Step 82700 Loss: 0.3599, lr 9.64888242944126e-05
Step 82800 Loss: 0.3547, lr 9.64888242944126e-05
Step 82900 Loss: 0.3502, lr 9.64888242944126e-05
Step 83000 Loss: 0.3458, lr 9.64888242944126e-05
Step 83100 Loss: 0.3408, lr 9.64888242944126e-05
Step 83200 Loss: 0.3365, lr 9.64888242944126e-05
Step 83300 Loss: 0.3329, lr 9.64888242944126e-05
Step 83400 Loss: 0.3292, lr 9.64888242944126e-05
Step 83500 Loss: 0.3256, lr 9.64888242944126e-05
Step 83600 Loss: 0.3221, lr 9.64888242944126e-05
Step 83700 Loss: 0.3191, lr 9.64888242944126e-05
Step 83800 Loss: 0.3168, lr 9.64888242944126e-05
Step 83900 Loss: 0.3139, lr 9.64888242944126e-05
Step 84000 Loss: 0.3118, lr 9.64888242944126e-05
Step 84100 Loss: 0.3096, lr 9.64888242944126e-05
Step 84200 Loss: 0.3074, lr 9.64888242944126e-05
Step 84300 Loss: 0.3048, lr 9.64888242944126e-05
Step 84400 Loss: 0.3027, lr 9.64888242944126e-05
Step 84500 Loss: 0.3007, lr 9.64888242944126e-05
Step 84600 Loss: 0.2988, lr 9.64888242944126e-05
Step 84700 Loss: 0.2970, lr 9.64888242944126e-05
Step 84800 Loss: 0.2949, lr 9.64888242944126e-05
Step 84900 Loss: 0.2937, lr 9.64888242944126e-05
Step 85000 Loss: 0.2924, lr 9.64888242944126e-05
Step 85100 Loss: 0.2915, lr 9.64888242944126e-05
Step 85200 Loss: 0.2903, lr 9.64888242944126e-05
Step 85300 Loss: 0.2895, lr 9.64888242944126e-05
Step 85400 Loss: 0.2886, lr 9.64888242944126e-05
Step 85500 Loss: 0.2878, lr 9.64888242944126e-05
Step 85600 Loss: 0.2868, lr 9.64888242944126e-05
Step 85700 Loss: 0.2858, lr 9.64888242944126e-05
Step 85800 Loss: 0.2850, lr 9.64888242944126e-05
Step 85900 Loss: 0.2846, lr 9.64888242944126e-05
Step 86000 Loss: 0.2846, lr 9.64888242944126e-05
Step 86100 Loss: 0.2840, lr 9.64888242944126e-05
Step 86200 Loss: 0.2842, lr 9.64888242944126e-05
Step 86300 Loss: 0.2841, lr 9.64888242944126e-05
Step 86400 Loss: 0.2846, lr 9.64888242944126e-05
Step 86500 Loss: 0.2844, lr 9.64888242944126e-05
Step 86600 Loss: 0.2847, lr 9.64888242944126e-05
Step 86700 Loss: 0.2851, lr 9.64888242944126e-05
Step 86800 Loss: 0.2856, lr 9.64888242944126e-05
Step 86900 Loss: 0.2863, lr 9.64888242944126e-05
Step 87000 Loss: 0.2875, lr 9.64888242944126e-05
Step 87100 Loss: 0.2889, lr 9.64888242944126e-05
Step 87200 Loss: 0.2903, lr 9.64888242944126e-05
Step 87300 Loss: 0.2919, lr 9.64888242944126e-05
Step 87400 Loss: 0.2937, lr 9.64888242944126e-05
Step 87500 Loss: 0.2959, lr 9.64888242944126e-05
Step 87600 Loss: 0.2971, lr 9.64888242944126e-05
Step 87700 Loss: 0.2968, lr 9.64888242944126e-05
Step 87800 Loss: 0.2965, lr 9.64888242944126e-05
Train Epoch: [13/100] Loss: 0.2972,lr 0.000096
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 6756
Step 87900 Loss: 0.8442, lr 9.588773128419907e-05
Step 88000 Loss: 0.6670, lr 9.588773128419907e-05
Step 88100 Loss: 0.5781, lr 9.588773128419907e-05
Step 88200 Loss: 0.5243, lr 9.588773128419907e-05
Step 88300 Loss: 0.4951, lr 9.588773128419907e-05
Step 88400 Loss: 0.4717, lr 9.588773128419907e-05
Step 88500 Loss: 0.4506, lr 9.588773128419907e-05
Step 88600 Loss: 0.4350, lr 9.588773128419907e-05
Step 88700 Loss: 0.4212, lr 9.588773128419907e-05
Step 88800 Loss: 0.4080, lr 9.588773128419907e-05
Step 88900 Loss: 0.3975, lr 9.588773128419907e-05
Step 89000 Loss: 0.3882, lr 9.588773128419907e-05
Step 89100 Loss: 0.3805, lr 9.588773128419907e-05
Step 89200 Loss: 0.3719, lr 9.588773128419907e-05
Step 89300 Loss: 0.3651, lr 9.588773128419907e-05
Step 89400 Loss: 0.3596, lr 9.588773128419907e-05
Step 89500 Loss: 0.3544, lr 9.588773128419907e-05
Step 89600 Loss: 0.3494, lr 9.588773128419907e-05
Step 89700 Loss: 0.3448, lr 9.588773128419907e-05
Step 89800 Loss: 0.3402, lr 9.588773128419907e-05
Step 89900 Loss: 0.3353, lr 9.588773128419907e-05
Step 90000 Loss: 0.3317, lr 9.588773128419907e-05
Step 90100 Loss: 0.3283, lr 9.588773128419907e-05
Step 90200 Loss: 0.3248, lr 9.588773128419907e-05
Step 90300 Loss: 0.3211, lr 9.588773128419907e-05
Step 90400 Loss: 0.3179, lr 9.588773128419907e-05
Step 90500 Loss: 0.3150, lr 9.588773128419907e-05
Step 90600 Loss: 0.3123, lr 9.588773128419907e-05
Step 90700 Loss: 0.3099, lr 9.588773128419907e-05
Step 90800 Loss: 0.3082, lr 9.588773128419907e-05
Step 90900 Loss: 0.3059, lr 9.588773128419907e-05
Step 91000 Loss: 0.3033, lr 9.588773128419907e-05
Step 91100 Loss: 0.3012, lr 9.588773128419907e-05
Step 91200 Loss: 0.2988, lr 9.588773128419907e-05
Step 91300 Loss: 0.2968, lr 9.588773128419907e-05
Step 91400 Loss: 0.2949, lr 9.588773128419907e-05
Step 91500 Loss: 0.2931, lr 9.588773128419907e-05
Step 91600 Loss: 0.2916, lr 9.588773128419907e-05
Step 91700 Loss: 0.2899, lr 9.588773128419907e-05
Step 91800 Loss: 0.2886, lr 9.588773128419907e-05
Step 91900 Loss: 0.2877, lr 9.588773128419907e-05
Step 92000 Loss: 0.2866, lr 9.588773128419907e-05
Step 92100 Loss: 0.2854, lr 9.588773128419907e-05
Step 92200 Loss: 0.2845, lr 9.588773128419907e-05
Step 92300 Loss: 0.2837, lr 9.588773128419907e-05
Step 92400 Loss: 0.2825, lr 9.588773128419907e-05
Step 92500 Loss: 0.2815, lr 9.588773128419907e-05
Step 92600 Loss: 0.2809, lr 9.588773128419907e-05
Step 92700 Loss: 0.2806, lr 9.588773128419907e-05
Step 92800 Loss: 0.2804, lr 9.588773128419907e-05
Step 92900 Loss: 0.2800, lr 9.588773128419907e-05
Step 93000 Loss: 0.2803, lr 9.588773128419907e-05
Step 93100 Loss: 0.2801, lr 9.588773128419907e-05
Step 93200 Loss: 0.2805, lr 9.588773128419907e-05
Step 93300 Loss: 0.2806, lr 9.588773128419907e-05
Step 93400 Loss: 0.2807, lr 9.588773128419907e-05
Step 93500 Loss: 0.2814, lr 9.588773128419907e-05
Step 93600 Loss: 0.2816, lr 9.588773128419907e-05
Step 93700 Loss: 0.2826, lr 9.588773128419907e-05
Step 93800 Loss: 0.2837, lr 9.588773128419907e-05
Step 93900 Loss: 0.2849, lr 9.588773128419907e-05
Step 94000 Loss: 0.2864, lr 9.588773128419907e-05
Step 94100 Loss: 0.2881, lr 9.588773128419907e-05
Step 94200 Loss: 0.2896, lr 9.588773128419907e-05
Step 94300 Loss: 0.2915, lr 9.588773128419907e-05
Step 94400 Loss: 0.2919, lr 9.588773128419907e-05
Step 94500 Loss: 0.2916, lr 9.588773128419907e-05
Train Epoch: [14/100] Loss: 0.2922,lr 0.000096
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Step 94600 Loss: 1.2510, lr 9.5241352623301e-05
Step 94700 Loss: 0.7314, lr 9.5241352623301e-05
Step 94800 Loss: 0.6268, lr 9.5241352623301e-05
Step 94900 Loss: 0.5584, lr 9.5241352623301e-05
Step 95000 Loss: 0.5174, lr 9.5241352623301e-05
Step 95100 Loss: 0.4902, lr 9.5241352623301e-05
Step 95200 Loss: 0.4644, lr 9.5241352623301e-05
Step 95300 Loss: 0.4437, lr 9.5241352623301e-05
Step 95400 Loss: 0.4293, lr 9.5241352623301e-05
Step 95500 Loss: 0.4145, lr 9.5241352623301e-05
Step 95600 Loss: 0.4021, lr 9.5241352623301e-05
Step 95700 Loss: 0.3921, lr 9.5241352623301e-05
Step 95800 Loss: 0.3826, lr 9.5241352623301e-05
Step 95900 Loss: 0.3746, lr 9.5241352623301e-05
Step 96000 Loss: 0.3670, lr 9.5241352623301e-05
Step 96100 Loss: 0.3610, lr 9.5241352623301e-05
Step 96200 Loss: 0.3548, lr 9.5241352623301e-05
Step 96300 Loss: 0.3500, lr 9.5241352623301e-05
Step 96400 Loss: 0.3453, lr 9.5241352623301e-05
Step 96500 Loss: 0.3408, lr 9.5241352623301e-05
Step 96600 Loss: 0.3357, lr 9.5241352623301e-05
Step 96700 Loss: 0.3313, lr 9.5241352623301e-05
Step 96800 Loss: 0.3277, lr 9.5241352623301e-05
Step 96900 Loss: 0.3241, lr 9.5241352623301e-05
Step 97000 Loss: 0.3207, lr 9.5241352623301e-05
Step 97100 Loss: 0.3172, lr 9.5241352623301e-05
Step 97200 Loss: 0.3140, lr 9.5241352623301e-05
Step 97300 Loss: 0.3114, lr 9.5241352623301e-05
Step 97400 Loss: 0.3087, lr 9.5241352623301e-05
Step 97500 Loss: 0.3064, lr 9.5241352623301e-05
Step 97600 Loss: 0.3045, lr 9.5241352623301e-05
Step 97700 Loss: 0.3022, lr 9.5241352623301e-05
Step 97800 Loss: 0.2997, lr 9.5241352623301e-05
Step 97900 Loss: 0.2975, lr 9.5241352623301e-05
Step 98000 Loss: 0.2954, lr 9.5241352623301e-05
Step 98100 Loss: 0.2934, lr 9.5241352623301e-05
Step 98200 Loss: 0.2913, lr 9.5241352623301e-05
Step 98300 Loss: 0.2895, lr 9.5241352623301e-05
Step 98400 Loss: 0.2882, lr 9.5241352623301e-05
Step 98500 Loss: 0.2866, lr 9.5241352623301e-05
Step 98600 Loss: 0.2856, lr 9.5241352623301e-05
Step 98700 Loss: 0.2844, lr 9.5241352623301e-05
Step 98800 Loss: 0.2836, lr 9.5241352623301e-05
Step 98900 Loss: 0.2827, lr 9.5241352623301e-05
Step 99000 Loss: 0.2814, lr 9.5241352623301e-05
Step 99100 Loss: 0.2804, lr 9.5241352623301e-05
Step 99200 Loss: 0.2794, lr 9.5241352623301e-05
Step 99300 Loss: 0.2784, lr 9.5241352623301e-05
Step 99400 Loss: 0.2780, lr 9.5241352623301e-05
Step 99500 Loss: 0.2777, lr 9.5241352623301e-05
Step 99600 Loss: 0.2773, lr 9.5241352623301e-05
Step 99700 Loss: 0.2771, lr 9.5241352623301e-05
Step 99800 Loss: 0.2770, lr 9.5241352623301e-05
Step 99900 Loss: 0.2772, lr 9.5241352623301e-05
Step 100000 Loss: 0.2773, lr 9.5241352623301e-05
Step 100100 Loss: 0.2775, lr 9.5241352623301e-05
Step 100200 Loss: 0.2780, lr 9.5241352623301e-05
Step 100300 Loss: 0.2786, lr 9.5241352623301e-05
Step 100400 Loss: 0.2793, lr 9.5241352623301e-05
Step 100500 Loss: 0.2803, lr 9.5241352623301e-05
Step 100600 Loss: 0.2814, lr 9.5241352623301e-05
Step 100700 Loss: 0.2824, lr 9.5241352623301e-05
Step 100800 Loss: 0.2838, lr 9.5241352623301e-05
Step 100900 Loss: 0.2854, lr 9.5241352623301e-05
Step 101000 Loss: 0.2873, lr 9.5241352623301e-05
Step 101100 Loss: 0.2885, lr 9.5241352623301e-05
Step 101200 Loss: 0.2883, lr 9.5241352623301e-05
Step 101300 Loss: 0.2881, lr 9.5241352623301e-05
Train Epoch: [15/100] Loss: 0.2888,lr 0.000095
Model Saving at epoch 15
Calling G2SDataset.batch()
Done, time:  2.29 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 6755
Step 101400 Loss: 0.8504, lr 9.455032620941843e-05
Step 101500 Loss: 0.6689, lr 9.455032620941843e-05
Step 101600 Loss: 0.5814, lr 9.455032620941843e-05
Step 101700 Loss: 0.5264, lr 9.455032620941843e-05
Step 101800 Loss: 0.4944, lr 9.455032620941843e-05
Step 101900 Loss: 0.4704, lr 9.455032620941843e-05
Step 102000 Loss: 0.4463, lr 9.455032620941843e-05
Step 102100 Loss: 0.4296, lr 9.455032620941843e-05
Step 102200 Loss: 0.4161, lr 9.455032620941843e-05
Step 102300 Loss: 0.4029, lr 9.455032620941843e-05
Step 102400 Loss: 0.3912, lr 9.455032620941843e-05
Step 102500 Loss: 0.3820, lr 9.455032620941843e-05
Step 102600 Loss: 0.3737, lr 9.455032620941843e-05
Step 102700 Loss: 0.3651, lr 9.455032620941843e-05
Step 102800 Loss: 0.3581, lr 9.455032620941843e-05
Step 102900 Loss: 0.3525, lr 9.455032620941843e-05
Step 103000 Loss: 0.3472, lr 9.455032620941843e-05
Step 103100 Loss: 0.3419, lr 9.455032620941843e-05
Step 103200 Loss: 0.3374, lr 9.455032620941843e-05
Step 103300 Loss: 0.3327, lr 9.455032620941843e-05
Step 103400 Loss: 0.3280, lr 9.455032620941843e-05
Step 103500 Loss: 0.3242, lr 9.455032620941843e-05
Step 103600 Loss: 0.3208, lr 9.455032620941843e-05
Step 103700 Loss: 0.3168, lr 9.455032620941843e-05
Step 103800 Loss: 0.3131, lr 9.455032620941843e-05
Step 103900 Loss: 0.3101, lr 9.455032620941843e-05
Step 104000 Loss: 0.3070, lr 9.455032620941843e-05
Step 104100 Loss: 0.3046, lr 9.455032620941843e-05
Step 104200 Loss: 0.3017, lr 9.455032620941843e-05
Step 104300 Loss: 0.2998, lr 9.455032620941843e-05
Step 104400 Loss: 0.2978, lr 9.455032620941843e-05
Step 104500 Loss: 0.2953, lr 9.455032620941843e-05
Step 104600 Loss: 0.2931, lr 9.455032620941843e-05
Step 104700 Loss: 0.2907, lr 9.455032620941843e-05
Step 104800 Loss: 0.2887, lr 9.455032620941843e-05
Step 104900 Loss: 0.2867, lr 9.455032620941843e-05
Step 105000 Loss: 0.2852, lr 9.455032620941843e-05
Step 105100 Loss: 0.2835, lr 9.455032620941843e-05
Step 105200 Loss: 0.2819, lr 9.455032620941843e-05
Step 105300 Loss: 0.2804, lr 9.455032620941843e-05
Step 105400 Loss: 0.2796, lr 9.455032620941843e-05
Step 105500 Loss: 0.2786, lr 9.455032620941843e-05
Step 105600 Loss: 0.2777, lr 9.455032620941843e-05
Step 105700 Loss: 0.2765, lr 9.455032620941843e-05
Step 105800 Loss: 0.2758, lr 9.455032620941843e-05
Step 105900 Loss: 0.2745, lr 9.455032620941843e-05
Step 106000 Loss: 0.2733, lr 9.455032620941843e-05
Step 106100 Loss: 0.2728, lr 9.455032620941843e-05
Step 106200 Loss: 0.2725, lr 9.455032620941843e-05
Step 106300 Loss: 0.2723, lr 9.455032620941843e-05
Step 106400 Loss: 0.2719, lr 9.455032620941843e-05
Step 106500 Loss: 0.2722, lr 9.455032620941843e-05
Step 106600 Loss: 0.2721, lr 9.455032620941843e-05
Step 106700 Loss: 0.2725, lr 9.455032620941843e-05
Step 106800 Loss: 0.2727, lr 9.455032620941843e-05
Step 106900 Loss: 0.2733, lr 9.455032620941843e-05
Step 107000 Loss: 0.2737, lr 9.455032620941843e-05
Step 107100 Loss: 0.2743, lr 9.455032620941843e-05
Step 107200 Loss: 0.2751, lr 9.455032620941843e-05
Step 107300 Loss: 0.2761, lr 9.455032620941843e-05
Step 107400 Loss: 0.2772, lr 9.455032620941843e-05
Step 107500 Loss: 0.2785, lr 9.455032620941843e-05
Step 107600 Loss: 0.2804, lr 9.455032620941843e-05
Step 107700 Loss: 0.2818, lr 9.455032620941843e-05
Step 107800 Loss: 0.2833, lr 9.455032620941843e-05
Step 107900 Loss: 0.2840, lr 9.455032620941843e-05
Step 108000 Loss: 0.2836, lr 9.455032620941843e-05
Train Epoch: [16/100] Loss: 0.2840,lr 0.000095
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.99 s, total batches: 6756
Step 108100 Loss: 1.5383, lr 9.381533400219321e-05
Step 108200 Loss: 0.6973, lr 9.381533400219321e-05
Step 108300 Loss: 0.5952, lr 9.381533400219321e-05
Step 108400 Loss: 0.5334, lr 9.381533400219321e-05
Step 108500 Loss: 0.4964, lr 9.381533400219321e-05
Step 108600 Loss: 0.4723, lr 9.381533400219321e-05
Step 108700 Loss: 0.4492, lr 9.381533400219321e-05
Step 108800 Loss: 0.4316, lr 9.381533400219321e-05
Step 108900 Loss: 0.4181, lr 9.381533400219321e-05
Step 109000 Loss: 0.4054, lr 9.381533400219321e-05
Step 109100 Loss: 0.3940, lr 9.381533400219321e-05
Step 109200 Loss: 0.3853, lr 9.381533400219321e-05
Step 109300 Loss: 0.3763, lr 9.381533400219321e-05
Step 109400 Loss: 0.3691, lr 9.381533400219321e-05
Step 109500 Loss: 0.3616, lr 9.381533400219321e-05
Step 109600 Loss: 0.3561, lr 9.381533400219321e-05
Step 109700 Loss: 0.3503, lr 9.381533400219321e-05
Step 109800 Loss: 0.3457, lr 9.381533400219321e-05
Step 109900 Loss: 0.3414, lr 9.381533400219321e-05
Step 110000 Loss: 0.3372, lr 9.381533400219321e-05
Step 110100 Loss: 0.3323, lr 9.381533400219321e-05
Step 110200 Loss: 0.3280, lr 9.381533400219321e-05
Step 110300 Loss: 0.3247, lr 9.381533400219321e-05
Step 110400 Loss: 0.3213, lr 9.381533400219321e-05
Step 110500 Loss: 0.3180, lr 9.381533400219321e-05
Step 110600 Loss: 0.3144, lr 9.381533400219321e-05
Step 110700 Loss: 0.3109, lr 9.381533400219321e-05
Step 110800 Loss: 0.3083, lr 9.381533400219321e-05
Step 110900 Loss: 0.3056, lr 9.381533400219321e-05
Step 111000 Loss: 0.3032, lr 9.381533400219321e-05
Step 111100 Loss: 0.3011, lr 9.381533400219321e-05
Step 111200 Loss: 0.2987, lr 9.381533400219321e-05
Step 111300 Loss: 0.2963, lr 9.381533400219321e-05
Step 111400 Loss: 0.2941, lr 9.381533400219321e-05
Step 111500 Loss: 0.2919, lr 9.381533400219321e-05
Step 111600 Loss: 0.2902, lr 9.381533400219321e-05
Step 111700 Loss: 0.2880, lr 9.381533400219321e-05
Step 111800 Loss: 0.2863, lr 9.381533400219321e-05
Step 111900 Loss: 0.2848, lr 9.381533400219321e-05
Step 112000 Loss: 0.2833, lr 9.381533400219321e-05
Step 112100 Loss: 0.2823, lr 9.381533400219321e-05
Step 112200 Loss: 0.2811, lr 9.381533400219321e-05
Step 112300 Loss: 0.2802, lr 9.381533400219321e-05
Step 112400 Loss: 0.2795, lr 9.381533400219321e-05
Step 112500 Loss: 0.2781, lr 9.381533400219321e-05
Step 112600 Loss: 0.2772, lr 9.381533400219321e-05
Step 112700 Loss: 0.2760, lr 9.381533400219321e-05
Step 112800 Loss: 0.2752, lr 9.381533400219321e-05
Step 112900 Loss: 0.2748, lr 9.381533400219321e-05
Step 113000 Loss: 0.2744, lr 9.381533400219321e-05
Step 113100 Loss: 0.2740, lr 9.381533400219321e-05
Step 113200 Loss: 0.2739, lr 9.381533400219321e-05
Step 113300 Loss: 0.2737, lr 9.381533400219321e-05
Step 113400 Loss: 0.2739, lr 9.381533400219321e-05
Step 113500 Loss: 0.2738, lr 9.381533400219321e-05
Step 113600 Loss: 0.2741, lr 9.381533400219321e-05
Step 113700 Loss: 0.2742, lr 9.381533400219321e-05
Step 113800 Loss: 0.2747, lr 9.381533400219321e-05
Step 113900 Loss: 0.2752, lr 9.381533400219321e-05
Step 114000 Loss: 0.2759, lr 9.381533400219321e-05
Step 114100 Loss: 0.2772, lr 9.381533400219321e-05
Step 114200 Loss: 0.2781, lr 9.381533400219321e-05
Step 114300 Loss: 0.2794, lr 9.381533400219321e-05
Step 114400 Loss: 0.2813, lr 9.381533400219321e-05
Step 114500 Loss: 0.2830, lr 9.381533400219321e-05
Step 114600 Loss: 0.2844, lr 9.381533400219321e-05
Step 114700 Loss: 0.2842, lr 9.381533400219321e-05
Step 114800 Loss: 0.2840, lr 9.381533400219321e-05
Train Epoch: [17/100] Loss: 0.2844,lr 0.000094
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6755
Step 114900 Loss: 0.8773, lr 9.303710135019722e-05
Step 115000 Loss: 0.6425, lr 9.303710135019722e-05
Step 115100 Loss: 0.5562, lr 9.303710135019722e-05
Step 115200 Loss: 0.5084, lr 9.303710135019722e-05
Step 115300 Loss: 0.4790, lr 9.303710135019722e-05
Step 115400 Loss: 0.4578, lr 9.303710135019722e-05
Step 115500 Loss: 0.4361, lr 9.303710135019722e-05
Step 115600 Loss: 0.4203, lr 9.303710135019722e-05
Step 115700 Loss: 0.4081, lr 9.303710135019722e-05
Step 115800 Loss: 0.3958, lr 9.303710135019722e-05
Step 115900 Loss: 0.3867, lr 9.303710135019722e-05
Step 116000 Loss: 0.3781, lr 9.303710135019722e-05
Step 116100 Loss: 0.3699, lr 9.303710135019722e-05
Step 116200 Loss: 0.3623, lr 9.303710135019722e-05
Step 116300 Loss: 0.3552, lr 9.303710135019722e-05
Step 116400 Loss: 0.3499, lr 9.303710135019722e-05
Step 116500 Loss: 0.3448, lr 9.303710135019722e-05
Step 116600 Loss: 0.3398, lr 9.303710135019722e-05
Step 116700 Loss: 0.3353, lr 9.303710135019722e-05
Step 116800 Loss: 0.3310, lr 9.303710135019722e-05
Step 116900 Loss: 0.3267, lr 9.303710135019722e-05
Step 117000 Loss: 0.3233, lr 9.303710135019722e-05
Step 117100 Loss: 0.3199, lr 9.303710135019722e-05
Step 117200 Loss: 0.3163, lr 9.303710135019722e-05
Step 117300 Loss: 0.3129, lr 9.303710135019722e-05
Step 117400 Loss: 0.3095, lr 9.303710135019722e-05
Step 117500 Loss: 0.3066, lr 9.303710135019722e-05
Step 117600 Loss: 0.3040, lr 9.303710135019722e-05
Step 117700 Loss: 0.3014, lr 9.303710135019722e-05
Step 117800 Loss: 0.2996, lr 9.303710135019722e-05
Step 117900 Loss: 0.2974, lr 9.303710135019722e-05
Step 118000 Loss: 0.2948, lr 9.303710135019722e-05
Step 118100 Loss: 0.2925, lr 9.303710135019722e-05
Step 118200 Loss: 0.2900, lr 9.303710135019722e-05
Step 118300 Loss: 0.2880, lr 9.303710135019722e-05
Step 118400 Loss: 0.2861, lr 9.303710135019722e-05
Step 118500 Loss: 0.2844, lr 9.303710135019722e-05
Step 118600 Loss: 0.2824, lr 9.303710135019722e-05
Step 118700 Loss: 0.2810, lr 9.303710135019722e-05
Step 118800 Loss: 0.2794, lr 9.303710135019722e-05
Step 118900 Loss: 0.2784, lr 9.303710135019722e-05
Step 119000 Loss: 0.2772, lr 9.303710135019722e-05
Step 119100 Loss: 0.2763, lr 9.303710135019722e-05
Step 119200 Loss: 0.2751, lr 9.303710135019722e-05
Step 119300 Loss: 0.2742, lr 9.303710135019722e-05
Step 119400 Loss: 0.2729, lr 9.303710135019722e-05
Step 119500 Loss: 0.2719, lr 9.303710135019722e-05
Step 119600 Loss: 0.2711, lr 9.303710135019722e-05
Step 119700 Loss: 0.2706, lr 9.303710135019722e-05
Step 119800 Loss: 0.2704, lr 9.303710135019722e-05
Step 119900 Loss: 0.2699, lr 9.303710135019722e-05
Step 120000 Loss: 0.2702, lr 9.303710135019722e-05
Step 120100 Loss: 0.2700, lr 9.303710135019722e-05
Step 120200 Loss: 0.2701, lr 9.303710135019722e-05
Step 120300 Loss: 0.2700, lr 9.303710135019722e-05
Step 120400 Loss: 0.2701, lr 9.303710135019722e-05
Step 120500 Loss: 0.2705, lr 9.303710135019722e-05
Step 120600 Loss: 0.2712, lr 9.303710135019722e-05
Step 120700 Loss: 0.2719, lr 9.303710135019722e-05
Step 120800 Loss: 0.2728, lr 9.303710135019722e-05
Step 120900 Loss: 0.2737, lr 9.303710135019722e-05
Step 121000 Loss: 0.2746, lr 9.303710135019722e-05
Step 121100 Loss: 0.2761, lr 9.303710135019722e-05
Step 121200 Loss: 0.2774, lr 9.303710135019722e-05
Step 121300 Loss: 0.2792, lr 9.303710135019722e-05
Step 121400 Loss: 0.2798, lr 9.303710135019722e-05
Step 121500 Loss: 0.2794, lr 9.303710135019722e-05
Step 121600 Loss: 0.2795, lr 9.303710135019722e-05
Train Epoch: [18/100] Loss: 0.2799,lr 0.000093
Calling G2SDataset.batch()
Done, time:  1.63 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 121700 Loss: 0.6766, lr 9.22163962751008e-05
Step 121800 Loss: 0.5592, lr 9.22163962751008e-05
Step 121900 Loss: 0.5017, lr 9.22163962751008e-05
Step 122000 Loss: 0.4686, lr 9.22163962751008e-05
Step 122100 Loss: 0.4489, lr 9.22163962751008e-05
Step 122200 Loss: 0.4286, lr 9.22163962751008e-05
Step 122300 Loss: 0.4130, lr 9.22163962751008e-05
Step 122400 Loss: 0.4004, lr 9.22163962751008e-05
Step 122500 Loss: 0.3895, lr 9.22163962751008e-05
Step 122600 Loss: 0.3790, lr 9.22163962751008e-05
Step 122700 Loss: 0.3703, lr 9.22163962751008e-05
Step 122800 Loss: 0.3621, lr 9.22163962751008e-05
Step 122900 Loss: 0.3556, lr 9.22163962751008e-05
Step 123000 Loss: 0.3478, lr 9.22163962751008e-05
Step 123100 Loss: 0.3430, lr 9.22163962751008e-05
Step 123200 Loss: 0.3376, lr 9.22163962751008e-05
Step 123300 Loss: 0.3328, lr 9.22163962751008e-05
Step 123400 Loss: 0.3286, lr 9.22163962751008e-05
Step 123500 Loss: 0.3241, lr 9.22163962751008e-05
Step 123600 Loss: 0.3197, lr 9.22163962751008e-05
Step 123700 Loss: 0.3154, lr 9.22163962751008e-05
Step 123800 Loss: 0.3117, lr 9.22163962751008e-05
Step 123900 Loss: 0.3086, lr 9.22163962751008e-05
Step 124000 Loss: 0.3054, lr 9.22163962751008e-05
Step 124100 Loss: 0.3019, lr 9.22163962751008e-05
Step 124200 Loss: 0.2989, lr 9.22163962751008e-05
Step 124300 Loss: 0.2962, lr 9.22163962751008e-05
Step 124400 Loss: 0.2939, lr 9.22163962751008e-05
Step 124500 Loss: 0.2917, lr 9.22163962751008e-05
Step 124600 Loss: 0.2897, lr 9.22163962751008e-05
Step 124700 Loss: 0.2875, lr 9.22163962751008e-05
Step 124800 Loss: 0.2850, lr 9.22163962751008e-05
Step 124900 Loss: 0.2827, lr 9.22163962751008e-05
Step 125000 Loss: 0.2806, lr 9.22163962751008e-05
Step 125100 Loss: 0.2788, lr 9.22163962751008e-05
Step 125200 Loss: 0.2766, lr 9.22163962751008e-05
Step 125300 Loss: 0.2749, lr 9.22163962751008e-05
Step 125400 Loss: 0.2736, lr 9.22163962751008e-05
Step 125500 Loss: 0.2721, lr 9.22163962751008e-05
Step 125600 Loss: 0.2711, lr 9.22163962751008e-05
Step 125700 Loss: 0.2699, lr 9.22163962751008e-05
Step 125800 Loss: 0.2690, lr 9.22163962751008e-05
Step 125900 Loss: 0.2679, lr 9.22163962751008e-05
Step 126000 Loss: 0.2668, lr 9.22163962751008e-05
Step 126100 Loss: 0.2657, lr 9.22163962751008e-05
Step 126200 Loss: 0.2646, lr 9.22163962751008e-05
Step 126300 Loss: 0.2637, lr 9.22163962751008e-05
Step 126400 Loss: 0.2633, lr 9.22163962751008e-05
Step 126500 Loss: 0.2629, lr 9.22163962751008e-05
Step 126600 Loss: 0.2626, lr 9.22163962751008e-05
Step 126700 Loss: 0.2623, lr 9.22163962751008e-05
Step 126800 Loss: 0.2624, lr 9.22163962751008e-05
Step 126900 Loss: 0.2624, lr 9.22163962751008e-05
Step 127000 Loss: 0.2626, lr 9.22163962751008e-05
Step 127100 Loss: 0.2628, lr 9.22163962751008e-05
Step 127200 Loss: 0.2630, lr 9.22163962751008e-05
Step 127300 Loss: 0.2635, lr 9.22163962751008e-05
Step 127400 Loss: 0.2640, lr 9.22163962751008e-05
Step 127500 Loss: 0.2648, lr 9.22163962751008e-05
Step 127600 Loss: 0.2658, lr 9.22163962751008e-05
Step 127700 Loss: 0.2669, lr 9.22163962751008e-05
Step 127800 Loss: 0.2680, lr 9.22163962751008e-05
Step 127900 Loss: 0.2695, lr 9.22163962751008e-05
Step 128000 Loss: 0.2710, lr 9.22163962751008e-05
Step 128100 Loss: 0.2723, lr 9.22163962751008e-05
Step 128200 Loss: 0.2727, lr 9.22163962751008e-05
Step 128300 Loss: 0.2723, lr 9.22163962751008e-05
Train Epoch: [19/100] Loss: 0.2735,lr 0.000092
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Step 128400 Loss: 0.9121, lr 9.135402871372814e-05
Step 128500 Loss: 0.6117, lr 9.135402871372814e-05
Step 128600 Loss: 0.5255, lr 9.135402871372814e-05
Step 128700 Loss: 0.4816, lr 9.135402871372814e-05
Step 128800 Loss: 0.4550, lr 9.135402871372814e-05
Step 128900 Loss: 0.4369, lr 9.135402871372814e-05
Step 129000 Loss: 0.4174, lr 9.135402871372814e-05
Step 129100 Loss: 0.4011, lr 9.135402871372814e-05
Step 129200 Loss: 0.3903, lr 9.135402871372814e-05
Step 129300 Loss: 0.3787, lr 9.135402871372814e-05
Step 129400 Loss: 0.3685, lr 9.135402871372814e-05
Step 129500 Loss: 0.3606, lr 9.135402871372814e-05
Step 129600 Loss: 0.3528, lr 9.135402871372814e-05
Step 129700 Loss: 0.3457, lr 9.135402871372814e-05
Step 129800 Loss: 0.3389, lr 9.135402871372814e-05
Step 129900 Loss: 0.3342, lr 9.135402871372814e-05
Step 130000 Loss: 0.3295, lr 9.135402871372814e-05
Step 130100 Loss: 0.3248, lr 9.135402871372814e-05
Step 130200 Loss: 0.3207, lr 9.135402871372814e-05
Step 130300 Loss: 0.3168, lr 9.135402871372814e-05
Step 130400 Loss: 0.3121, lr 9.135402871372814e-05
Step 130500 Loss: 0.3087, lr 9.135402871372814e-05
Step 130600 Loss: 0.3054, lr 9.135402871372814e-05
Step 130700 Loss: 0.3019, lr 9.135402871372814e-05
Step 130800 Loss: 0.2985, lr 9.135402871372814e-05
Step 130900 Loss: 0.2955, lr 9.135402871372814e-05
Step 131000 Loss: 0.2926, lr 9.135402871372814e-05
Step 131100 Loss: 0.2903, lr 9.135402871372814e-05
Step 131200 Loss: 0.2876, lr 9.135402871372814e-05
Step 131300 Loss: 0.2857, lr 9.135402871372814e-05
Step 131400 Loss: 0.2838, lr 9.135402871372814e-05
Step 131500 Loss: 0.2816, lr 9.135402871372814e-05
Step 131600 Loss: 0.2793, lr 9.135402871372814e-05
Step 131700 Loss: 0.2771, lr 9.135402871372814e-05
Step 131800 Loss: 0.2753, lr 9.135402871372814e-05
Step 131900 Loss: 0.2736, lr 9.135402871372814e-05
Step 132000 Loss: 0.2718, lr 9.135402871372814e-05
Step 132100 Loss: 0.2699, lr 9.135402871372814e-05
Step 132200 Loss: 0.2687, lr 9.135402871372814e-05
Step 132300 Loss: 0.2673, lr 9.135402871372814e-05
Step 132400 Loss: 0.2664, lr 9.135402871372814e-05
Step 132500 Loss: 0.2654, lr 9.135402871372814e-05
Step 132600 Loss: 0.2645, lr 9.135402871372814e-05
Step 132700 Loss: 0.2633, lr 9.135402871372814e-05
Step 132800 Loss: 0.2623, lr 9.135402871372814e-05
Step 132900 Loss: 0.2610, lr 9.135402871372814e-05
Step 133000 Loss: 0.2601, lr 9.135402871372814e-05
Step 133100 Loss: 0.2593, lr 9.135402871372814e-05
Step 133200 Loss: 0.2588, lr 9.135402871372814e-05
Step 133300 Loss: 0.2586, lr 9.135402871372814e-05
Step 133400 Loss: 0.2583, lr 9.135402871372814e-05
Step 133500 Loss: 0.2583, lr 9.135402871372814e-05
Step 133600 Loss: 0.2581, lr 9.135402871372814e-05
Step 133700 Loss: 0.2584, lr 9.135402871372814e-05
Step 133800 Loss: 0.2584, lr 9.135402871372814e-05
Step 133900 Loss: 0.2586, lr 9.135402871372814e-05
Step 134000 Loss: 0.2589, lr 9.135402871372814e-05
Step 134100 Loss: 0.2595, lr 9.135402871372814e-05
Step 134200 Loss: 0.2601, lr 9.135402871372814e-05
Step 134300 Loss: 0.2609, lr 9.135402871372814e-05
Step 134400 Loss: 0.2620, lr 9.135402871372814e-05
Step 134500 Loss: 0.2631, lr 9.135402871372814e-05
Step 134600 Loss: 0.2644, lr 9.135402871372814e-05
Step 134700 Loss: 0.2657, lr 9.135402871372814e-05
Step 134800 Loss: 0.2673, lr 9.135402871372814e-05
Step 134900 Loss: 0.2681, lr 9.135402871372814e-05
Step 135000 Loss: 0.2678, lr 9.135402871372814e-05
Step 135100 Loss: 0.2677, lr 9.135402871372814e-05
Train Epoch: [20/100] Loss: 0.2681,lr 0.000091
Model Saving at epoch 20
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 135200 Loss: 0.6666, lr 9.045084971874742e-05
Step 135300 Loss: 0.5436, lr 9.045084971874742e-05
Step 135400 Loss: 0.4857, lr 9.045084971874742e-05
Step 135500 Loss: 0.4519, lr 9.045084971874742e-05
Step 135600 Loss: 0.4324, lr 9.045084971874742e-05
Step 135700 Loss: 0.4152, lr 9.045084971874742e-05
Step 135800 Loss: 0.3986, lr 9.045084971874742e-05
Step 135900 Loss: 0.3861, lr 9.045084971874742e-05
Step 136000 Loss: 0.3753, lr 9.045084971874742e-05
Step 136100 Loss: 0.3654, lr 9.045084971874742e-05
Step 136200 Loss: 0.3567, lr 9.045084971874742e-05
Step 136300 Loss: 0.3490, lr 9.045084971874742e-05
Step 136400 Loss: 0.3426, lr 9.045084971874742e-05
Step 136500 Loss: 0.3350, lr 9.045084971874742e-05
Step 136600 Loss: 0.3302, lr 9.045084971874742e-05
Step 136700 Loss: 0.3253, lr 9.045084971874742e-05
Step 136800 Loss: 0.3211, lr 9.045084971874742e-05
Step 136900 Loss: 0.3167, lr 9.045084971874742e-05
Step 137000 Loss: 0.3128, lr 9.045084971874742e-05
Step 137100 Loss: 0.3085, lr 9.045084971874742e-05
Step 137200 Loss: 0.3044, lr 9.045084971874742e-05
Step 137300 Loss: 0.3011, lr 9.045084971874742e-05
Step 137400 Loss: 0.2983, lr 9.045084971874742e-05
Step 137500 Loss: 0.2950, lr 9.045084971874742e-05
Step 137600 Loss: 0.2917, lr 9.045084971874742e-05
Step 137700 Loss: 0.2889, lr 9.045084971874742e-05
Step 137800 Loss: 0.2864, lr 9.045084971874742e-05
Step 137900 Loss: 0.2840, lr 9.045084971874742e-05
Step 138000 Loss: 0.2818, lr 9.045084971874742e-05
Step 138100 Loss: 0.2800, lr 9.045084971874742e-05
Step 138200 Loss: 0.2779, lr 9.045084971874742e-05
Step 138300 Loss: 0.2758, lr 9.045084971874742e-05
Step 138400 Loss: 0.2739, lr 9.045084971874742e-05
Step 138500 Loss: 0.2717, lr 9.045084971874742e-05
Step 138600 Loss: 0.2700, lr 9.045084971874742e-05
Step 138700 Loss: 0.2681, lr 9.045084971874742e-05
Step 138800 Loss: 0.2665, lr 9.045084971874742e-05
Step 138900 Loss: 0.2653, lr 9.045084971874742e-05
Step 139000 Loss: 0.2637, lr 9.045084971874742e-05
Step 139100 Loss: 0.2627, lr 9.045084971874742e-05
Step 139200 Loss: 0.2617, lr 9.045084971874742e-05
Step 139300 Loss: 0.2610, lr 9.045084971874742e-05
Step 139400 Loss: 0.2597, lr 9.045084971874742e-05
Step 139500 Loss: 0.2588, lr 9.045084971874742e-05
Step 139600 Loss: 0.2577, lr 9.045084971874742e-05
Step 139700 Loss: 0.2566, lr 9.045084971874742e-05
Step 139800 Loss: 0.2556, lr 9.045084971874742e-05
Step 139900 Loss: 0.2552, lr 9.045084971874742e-05
Step 140000 Loss: 0.2547, lr 9.045084971874742e-05
Step 140100 Loss: 0.2545, lr 9.045084971874742e-05
Step 140200 Loss: 0.2543, lr 9.045084971874742e-05
Step 140300 Loss: 0.2543, lr 9.045084971874742e-05
Step 140400 Loss: 0.2543, lr 9.045084971874742e-05
Step 140500 Loss: 0.2545, lr 9.045084971874742e-05
Step 140600 Loss: 0.2547, lr 9.045084971874742e-05
Step 140700 Loss: 0.2549, lr 9.045084971874742e-05
Step 140800 Loss: 0.2555, lr 9.045084971874742e-05
Step 140900 Loss: 0.2559, lr 9.045084971874742e-05
Step 141000 Loss: 0.2567, lr 9.045084971874742e-05
Step 141100 Loss: 0.2577, lr 9.045084971874742e-05
Step 141200 Loss: 0.2588, lr 9.045084971874742e-05
Step 141300 Loss: 0.2602, lr 9.045084971874742e-05
Step 141400 Loss: 0.2615, lr 9.045084971874742e-05
Step 141500 Loss: 0.2628, lr 9.045084971874742e-05
Step 141600 Loss: 0.2642, lr 9.045084971874742e-05
Step 141700 Loss: 0.2646, lr 9.045084971874742e-05
Step 141800 Loss: 0.2643, lr 9.045084971874742e-05
Train Epoch: [21/100] Loss: 0.2647,lr 0.000090
Calling G2SDataset.batch()
Done, time:  1.97 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 141900 Loss: 0.9677, lr 8.950775061878456e-05
Step 142000 Loss: 0.5889, lr 8.950775061878456e-05
Step 142100 Loss: 0.5113, lr 8.950775061878456e-05
Step 142200 Loss: 0.4682, lr 8.950775061878456e-05
Step 142300 Loss: 0.4408, lr 8.950775061878456e-05
Step 142400 Loss: 0.4250, lr 8.950775061878456e-05
Step 142500 Loss: 0.4067, lr 8.950775061878456e-05
Step 142600 Loss: 0.3913, lr 8.950775061878456e-05
Step 142700 Loss: 0.3801, lr 8.950775061878456e-05
Step 142800 Loss: 0.3701, lr 8.950775061878456e-05
Step 142900 Loss: 0.3612, lr 8.950775061878456e-05
Step 143000 Loss: 0.3539, lr 8.950775061878456e-05
Step 143100 Loss: 0.3461, lr 8.950775061878456e-05
Step 143200 Loss: 0.3398, lr 8.950775061878456e-05
Step 143300 Loss: 0.3337, lr 8.950775061878456e-05
Step 143400 Loss: 0.3293, lr 8.950775061878456e-05
Step 143500 Loss: 0.3250, lr 8.950775061878456e-05
Step 143600 Loss: 0.3202, lr 8.950775061878456e-05
Step 143700 Loss: 0.3166, lr 8.950775061878456e-05
Step 143800 Loss: 0.3129, lr 8.950775061878456e-05
Step 143900 Loss: 0.3085, lr 8.950775061878456e-05
Step 144000 Loss: 0.3053, lr 8.950775061878456e-05
Step 144100 Loss: 0.3020, lr 8.950775061878456e-05
Step 144200 Loss: 0.2986, lr 8.950775061878456e-05
Step 144300 Loss: 0.2958, lr 8.950775061878456e-05
Step 144400 Loss: 0.2925, lr 8.950775061878456e-05
Step 144500 Loss: 0.2896, lr 8.950775061878456e-05
Step 144600 Loss: 0.2875, lr 8.950775061878456e-05
Step 144700 Loss: 0.2851, lr 8.950775061878456e-05
Step 144800 Loss: 0.2831, lr 8.950775061878456e-05
Step 144900 Loss: 0.2811, lr 8.950775061878456e-05
Step 145000 Loss: 0.2791, lr 8.950775061878456e-05
Step 145100 Loss: 0.2770, lr 8.950775061878456e-05
Step 145200 Loss: 0.2748, lr 8.950775061878456e-05
Step 145300 Loss: 0.2729, lr 8.950775061878456e-05
Step 145400 Loss: 0.2712, lr 8.950775061878456e-05
Step 145500 Loss: 0.2695, lr 8.950775061878456e-05
Step 145600 Loss: 0.2676, lr 8.950775061878456e-05
Step 145700 Loss: 0.2664, lr 8.950775061878456e-05
Step 145800 Loss: 0.2650, lr 8.950775061878456e-05
Step 145900 Loss: 0.2641, lr 8.950775061878456e-05
Step 146000 Loss: 0.2629, lr 8.950775061878456e-05
Step 146100 Loss: 0.2620, lr 8.950775061878456e-05
Step 146200 Loss: 0.2610, lr 8.950775061878456e-05
Step 146300 Loss: 0.2600, lr 8.950775061878456e-05
Step 146400 Loss: 0.2589, lr 8.950775061878456e-05
Step 146500 Loss: 0.2578, lr 8.950775061878456e-05
Step 146600 Loss: 0.2571, lr 8.950775061878456e-05
Step 146700 Loss: 0.2568, lr 8.950775061878456e-05
Step 146800 Loss: 0.2564, lr 8.950775061878456e-05
Step 146900 Loss: 0.2558, lr 8.950775061878456e-05
Step 147000 Loss: 0.2559, lr 8.950775061878456e-05
Step 147100 Loss: 0.2557, lr 8.950775061878456e-05
Step 147200 Loss: 0.2561, lr 8.950775061878456e-05
Step 147300 Loss: 0.2560, lr 8.950775061878456e-05
Step 147400 Loss: 0.2562, lr 8.950775061878456e-05
Step 147500 Loss: 0.2565, lr 8.950775061878456e-05
Step 147600 Loss: 0.2568, lr 8.950775061878456e-05
Step 147700 Loss: 0.2576, lr 8.950775061878456e-05
Step 147800 Loss: 0.2585, lr 8.950775061878456e-05
Step 147900 Loss: 0.2593, lr 8.950775061878456e-05
Step 148000 Loss: 0.2603, lr 8.950775061878456e-05
Step 148100 Loss: 0.2616, lr 8.950775061878456e-05
Step 148200 Loss: 0.2630, lr 8.950775061878456e-05
Step 148300 Loss: 0.2645, lr 8.950775061878456e-05
Step 148400 Loss: 0.2659, lr 8.950775061878456e-05
Step 148500 Loss: 0.2657, lr 8.950775061878456e-05
Step 148600 Loss: 0.2655, lr 8.950775061878456e-05
Train Epoch: [22/100] Loss: 0.2661,lr 0.000090
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Step 148700 Loss: 0.7043, lr 8.852566213878951e-05
Step 148800 Loss: 0.5506, lr 8.852566213878951e-05
Step 148900 Loss: 0.4891, lr 8.852566213878951e-05
Step 149000 Loss: 0.4526, lr 8.852566213878951e-05
Step 149100 Loss: 0.4338, lr 8.852566213878951e-05
Step 149200 Loss: 0.4171, lr 8.852566213878951e-05
Step 149300 Loss: 0.3986, lr 8.852566213878951e-05
Step 149400 Loss: 0.3863, lr 8.852566213878951e-05
Step 149500 Loss: 0.3751, lr 8.852566213878951e-05
Step 149600 Loss: 0.3651, lr 8.852566213878951e-05
Step 149700 Loss: 0.3565, lr 8.852566213878951e-05
Step 149800 Loss: 0.3492, lr 8.852566213878951e-05
Step 149900 Loss: 0.3427, lr 8.852566213878951e-05
Step 150000 Loss: 0.3355, lr 8.852566213878951e-05
Step 150100 Loss: 0.3298, lr 8.852566213878951e-05
Step 150200 Loss: 0.3255, lr 8.852566213878951e-05
Step 150300 Loss: 0.3211, lr 8.852566213878951e-05
Step 150400 Loss: 0.3168, lr 8.852566213878951e-05
Step 150500 Loss: 0.3129, lr 8.852566213878951e-05
Step 150600 Loss: 0.3087, lr 8.852566213878951e-05
Step 150700 Loss: 0.3047, lr 8.852566213878951e-05
Step 150800 Loss: 0.3017, lr 8.852566213878951e-05
Step 150900 Loss: 0.2986, lr 8.852566213878951e-05
Step 151000 Loss: 0.2953, lr 8.852566213878951e-05
Step 151100 Loss: 0.2920, lr 8.852566213878951e-05
Step 151200 Loss: 0.2891, lr 8.852566213878951e-05
Step 151300 Loss: 0.2861, lr 8.852566213878951e-05
Step 151400 Loss: 0.2837, lr 8.852566213878951e-05
Step 151500 Loss: 0.2814, lr 8.852566213878951e-05
Step 151600 Loss: 0.2798, lr 8.852566213878951e-05
Step 151700 Loss: 0.2776, lr 8.852566213878951e-05
Step 151800 Loss: 0.2752, lr 8.852566213878951e-05
Step 151900 Loss: 0.2732, lr 8.852566213878951e-05
Step 152000 Loss: 0.2709, lr 8.852566213878951e-05
Step 152100 Loss: 0.2691, lr 8.852566213878951e-05
Step 152200 Loss: 0.2670, lr 8.852566213878951e-05
Step 152300 Loss: 0.2656, lr 8.852566213878951e-05
Step 152400 Loss: 0.2642, lr 8.852566213878951e-05
Step 152500 Loss: 0.2626, lr 8.852566213878951e-05
Step 152600 Loss: 0.2614, lr 8.852566213878951e-05
Step 152700 Loss: 0.2604, lr 8.852566213878951e-05
Step 152800 Loss: 0.2596, lr 8.852566213878951e-05
Step 152900 Loss: 0.2586, lr 8.852566213878951e-05
Step 153000 Loss: 0.2575, lr 8.852566213878951e-05
Step 153100 Loss: 0.2567, lr 8.852566213878951e-05
Step 153200 Loss: 0.2555, lr 8.852566213878951e-05
Step 153300 Loss: 0.2545, lr 8.852566213878951e-05
Step 153400 Loss: 0.2537, lr 8.852566213878951e-05
Step 153500 Loss: 0.2536, lr 8.852566213878951e-05
Step 153600 Loss: 0.2532, lr 8.852566213878951e-05
Step 153700 Loss: 0.2527, lr 8.852566213878951e-05
Step 153800 Loss: 0.2528, lr 8.852566213878951e-05
Step 153900 Loss: 0.2527, lr 8.852566213878951e-05
Step 154000 Loss: 0.2529, lr 8.852566213878951e-05
Step 154100 Loss: 0.2531, lr 8.852566213878951e-05
Step 154200 Loss: 0.2534, lr 8.852566213878951e-05
Step 154300 Loss: 0.2540, lr 8.852566213878951e-05
Step 154400 Loss: 0.2547, lr 8.852566213878951e-05
Step 154500 Loss: 0.2556, lr 8.852566213878951e-05
Step 154600 Loss: 0.2566, lr 8.852566213878951e-05
Step 154700 Loss: 0.2575, lr 8.852566213878951e-05
Step 154800 Loss: 0.2584, lr 8.852566213878951e-05
Step 154900 Loss: 0.2599, lr 8.852566213878951e-05
Step 155000 Loss: 0.2611, lr 8.852566213878951e-05
Step 155100 Loss: 0.2626, lr 8.852566213878951e-05
Step 155200 Loss: 0.2634, lr 8.852566213878951e-05
Step 155300 Loss: 0.2630, lr 8.852566213878951e-05
Train Epoch: [23/100] Loss: 0.2632,lr 0.000089
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6756
Step 155400 Loss: 1.3064, lr 8.750555348152303e-05
Step 155500 Loss: 0.6216, lr 8.750555348152303e-05
Step 155600 Loss: 0.5223, lr 8.750555348152303e-05
Step 155700 Loss: 0.4741, lr 8.750555348152303e-05
Step 155800 Loss: 0.4457, lr 8.750555348152303e-05
Step 155900 Loss: 0.4287, lr 8.750555348152303e-05
Step 156000 Loss: 0.4094, lr 8.750555348152303e-05
Step 156100 Loss: 0.3938, lr 8.750555348152303e-05
Step 156200 Loss: 0.3827, lr 8.750555348152303e-05
Step 156300 Loss: 0.3715, lr 8.750555348152303e-05
Step 156400 Loss: 0.3625, lr 8.750555348152303e-05
Step 156500 Loss: 0.3553, lr 8.750555348152303e-05
Step 156600 Loss: 0.3473, lr 8.750555348152303e-05
Step 156700 Loss: 0.3410, lr 8.750555348152303e-05
Step 156800 Loss: 0.3345, lr 8.750555348152303e-05
Step 156900 Loss: 0.3299, lr 8.750555348152303e-05
Step 157000 Loss: 0.3243, lr 8.750555348152303e-05
Step 157100 Loss: 0.3202, lr 8.750555348152303e-05
Step 157200 Loss: 0.3160, lr 8.750555348152303e-05
Step 157300 Loss: 0.3123, lr 8.750555348152303e-05
Step 157400 Loss: 0.3079, lr 8.750555348152303e-05
Step 157500 Loss: 0.3042, lr 8.750555348152303e-05
Step 157600 Loss: 0.3008, lr 8.750555348152303e-05
Step 157700 Loss: 0.2975, lr 8.750555348152303e-05
Step 157800 Loss: 0.2946, lr 8.750555348152303e-05
Step 157900 Loss: 0.2912, lr 8.750555348152303e-05
Step 158000 Loss: 0.2879, lr 8.750555348152303e-05
Step 158100 Loss: 0.2854, lr 8.750555348152303e-05
Step 158200 Loss: 0.2830, lr 8.750555348152303e-05
Step 158300 Loss: 0.2807, lr 8.750555348152303e-05
Step 158400 Loss: 0.2787, lr 8.750555348152303e-05
Step 158500 Loss: 0.2764, lr 8.750555348152303e-05
Step 158600 Loss: 0.2741, lr 8.750555348152303e-05
Step 158700 Loss: 0.2720, lr 8.750555348152303e-05
Step 158800 Loss: 0.2700, lr 8.750555348152303e-05
Step 158900 Loss: 0.2684, lr 8.750555348152303e-05
Step 159000 Loss: 0.2665, lr 8.750555348152303e-05
Step 159100 Loss: 0.2647, lr 8.750555348152303e-05
Step 159200 Loss: 0.2633, lr 8.750555348152303e-05
Step 159300 Loss: 0.2619, lr 8.750555348152303e-05
Step 159400 Loss: 0.2609, lr 8.750555348152303e-05
Step 159500 Loss: 0.2597, lr 8.750555348152303e-05
Step 159600 Loss: 0.2588, lr 8.750555348152303e-05
Step 159700 Loss: 0.2578, lr 8.750555348152303e-05
Step 159800 Loss: 0.2565, lr 8.750555348152303e-05
Step 159900 Loss: 0.2556, lr 8.750555348152303e-05
Step 160000 Loss: 0.2544, lr 8.750555348152303e-05
Step 160100 Loss: 0.2535, lr 8.750555348152303e-05
Step 160200 Loss: 0.2531, lr 8.750555348152303e-05
Step 160300 Loss: 0.2528, lr 8.750555348152303e-05
Step 160400 Loss: 0.2522, lr 8.750555348152303e-05
Step 160500 Loss: 0.2521, lr 8.750555348152303e-05
Step 160600 Loss: 0.2518, lr 8.750555348152303e-05
Step 160700 Loss: 0.2520, lr 8.750555348152303e-05
Step 160800 Loss: 0.2521, lr 8.750555348152303e-05
Step 160900 Loss: 0.2522, lr 8.750555348152303e-05
Step 161000 Loss: 0.2523, lr 8.750555348152303e-05
Step 161100 Loss: 0.2527, lr 8.750555348152303e-05
Step 161200 Loss: 0.2534, lr 8.750555348152303e-05
Step 161300 Loss: 0.2542, lr 8.750555348152303e-05
Step 161400 Loss: 0.2550, lr 8.750555348152303e-05
Step 161500 Loss: 0.2561, lr 8.750555348152303e-05
Step 161600 Loss: 0.2571, lr 8.750555348152303e-05
Step 161700 Loss: 0.2584, lr 8.750555348152303e-05
Step 161800 Loss: 0.2598, lr 8.750555348152303e-05
Step 161900 Loss: 0.2610, lr 8.750555348152303e-05
Step 162000 Loss: 0.2609, lr 8.750555348152303e-05
Step 162100 Loss: 0.2607, lr 8.750555348152303e-05
Train Epoch: [24/100] Loss: 0.2609,lr 0.000088
Calling G2SDataset.batch()
Done, time:  2.39 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6755
Step 162200 Loss: 0.7256, lr 8.644843137107063e-05
Step 162300 Loss: 0.5427, lr 8.644843137107063e-05
Step 162400 Loss: 0.4797, lr 8.644843137107063e-05
Step 162500 Loss: 0.4433, lr 8.644843137107063e-05
Step 162600 Loss: 0.4248, lr 8.644843137107063e-05
Step 162700 Loss: 0.4091, lr 8.644843137107063e-05
Step 162800 Loss: 0.3917, lr 8.644843137107063e-05
Step 162900 Loss: 0.3799, lr 8.644843137107063e-05
Step 163000 Loss: 0.3699, lr 8.644843137107063e-05
Step 163100 Loss: 0.3601, lr 8.644843137107063e-05
Step 163200 Loss: 0.3522, lr 8.644843137107063e-05
Step 163300 Loss: 0.3447, lr 8.644843137107063e-05
Step 163400 Loss: 0.3381, lr 8.644843137107063e-05
Step 163500 Loss: 0.3312, lr 8.644843137107063e-05
Step 163600 Loss: 0.3256, lr 8.644843137107063e-05
Step 163700 Loss: 0.3212, lr 8.644843137107063e-05
Step 163800 Loss: 0.3166, lr 8.644843137107063e-05
Step 163900 Loss: 0.3120, lr 8.644843137107063e-05
Step 164000 Loss: 0.3079, lr 8.644843137107063e-05
Step 164100 Loss: 0.3037, lr 8.644843137107063e-05
Step 164200 Loss: 0.2994, lr 8.644843137107063e-05
Step 164300 Loss: 0.2960, lr 8.644843137107063e-05
Step 164400 Loss: 0.2930, lr 8.644843137107063e-05
Step 164500 Loss: 0.2896, lr 8.644843137107063e-05
Step 164600 Loss: 0.2864, lr 8.644843137107063e-05
Step 164700 Loss: 0.2837, lr 8.644843137107063e-05
Step 164800 Loss: 0.2806, lr 8.644843137107063e-05
Step 164900 Loss: 0.2784, lr 8.644843137107063e-05
Step 165000 Loss: 0.2757, lr 8.644843137107063e-05
Step 165100 Loss: 0.2739, lr 8.644843137107063e-05
Step 165200 Loss: 0.2720, lr 8.644843137107063e-05
Step 165300 Loss: 0.2698, lr 8.644843137107063e-05
Step 165400 Loss: 0.2677, lr 8.644843137107063e-05
Step 165500 Loss: 0.2652, lr 8.644843137107063e-05
Step 165600 Loss: 0.2635, lr 8.644843137107063e-05
Step 165700 Loss: 0.2615, lr 8.644843137107063e-05
Step 165800 Loss: 0.2599, lr 8.644843137107063e-05
Step 165900 Loss: 0.2584, lr 8.644843137107063e-05
Step 166000 Loss: 0.2569, lr 8.644843137107063e-05
Step 166100 Loss: 0.2554, lr 8.644843137107063e-05
Step 166200 Loss: 0.2547, lr 8.644843137107063e-05
Step 166300 Loss: 0.2536, lr 8.644843137107063e-05
Step 166400 Loss: 0.2525, lr 8.644843137107063e-05
Step 166500 Loss: 0.2513, lr 8.644843137107063e-05
Step 166600 Loss: 0.2505, lr 8.644843137107063e-05
Step 166700 Loss: 0.2491, lr 8.644843137107063e-05
Step 166800 Loss: 0.2482, lr 8.644843137107063e-05
Step 166900 Loss: 0.2476, lr 8.644843137107063e-05
Step 167000 Loss: 0.2471, lr 8.644843137107063e-05
Step 167100 Loss: 0.2469, lr 8.644843137107063e-05
Step 167200 Loss: 0.2464, lr 8.644843137107063e-05
Step 167300 Loss: 0.2465, lr 8.644843137107063e-05
Step 167400 Loss: 0.2463, lr 8.644843137107063e-05
Step 167500 Loss: 0.2465, lr 8.644843137107063e-05
Step 167600 Loss: 0.2465, lr 8.644843137107063e-05
Step 167700 Loss: 0.2467, lr 8.644843137107063e-05
Step 167800 Loss: 0.2470, lr 8.644843137107063e-05
Step 167900 Loss: 0.2475, lr 8.644843137107063e-05
Step 168000 Loss: 0.2482, lr 8.644843137107063e-05
Step 168100 Loss: 0.2489, lr 8.644843137107063e-05
Step 168200 Loss: 0.2498, lr 8.644843137107063e-05
Step 168300 Loss: 0.2508, lr 8.644843137107063e-05
Step 168400 Loss: 0.2524, lr 8.644843137107063e-05
Step 168500 Loss: 0.2537, lr 8.644843137107063e-05
Step 168600 Loss: 0.2550, lr 8.644843137107063e-05
Step 168700 Loss: 0.2560, lr 8.644843137107063e-05
Step 168800 Loss: 0.2558, lr 8.644843137107063e-05
Train Epoch: [25/100] Loss: 0.2561,lr 0.000086
Model Saving at epoch 25
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Step 168900 Loss: 1.5676, lr 8.535533905932742e-05
Step 169000 Loss: 0.6720, lr 8.535533905932742e-05
Step 169100 Loss: 0.5402, lr 8.535533905932742e-05
Step 169200 Loss: 0.4818, lr 8.535533905932742e-05
Step 169300 Loss: 0.4506, lr 8.535533905932742e-05
Step 169400 Loss: 0.4288, lr 8.535533905932742e-05
Step 169500 Loss: 0.4081, lr 8.535533905932742e-05
Step 169600 Loss: 0.3915, lr 8.535533905932742e-05
Step 169700 Loss: 0.3798, lr 8.535533905932742e-05
Step 169800 Loss: 0.3676, lr 8.535533905932742e-05
Step 169900 Loss: 0.3573, lr 8.535533905932742e-05
Step 170000 Loss: 0.3490, lr 8.535533905932742e-05
Step 170100 Loss: 0.3409, lr 8.535533905932742e-05
Step 170200 Loss: 0.3340, lr 8.535533905932742e-05
Step 170300 Loss: 0.3266, lr 8.535533905932742e-05
Step 170400 Loss: 0.3221, lr 8.535533905932742e-05
Step 170500 Loss: 0.3165, lr 8.535533905932742e-05
Step 170600 Loss: 0.3123, lr 8.535533905932742e-05
Step 170700 Loss: 0.3080, lr 8.535533905932742e-05
Step 170800 Loss: 0.3040, lr 8.535533905932742e-05
Step 170900 Loss: 0.2995, lr 8.535533905932742e-05
Step 171000 Loss: 0.2953, lr 8.535533905932742e-05
Step 171100 Loss: 0.2920, lr 8.535533905932742e-05
Step 171200 Loss: 0.2891, lr 8.535533905932742e-05
Step 171300 Loss: 0.2860, lr 8.535533905932742e-05
Step 171400 Loss: 0.2828, lr 8.535533905932742e-05
Step 171500 Loss: 0.2797, lr 8.535533905932742e-05
Step 171600 Loss: 0.2773, lr 8.535533905932742e-05
Step 171700 Loss: 0.2751, lr 8.535533905932742e-05
Step 171800 Loss: 0.2729, lr 8.535533905932742e-05
Step 171900 Loss: 0.2709, lr 8.535533905932742e-05
Step 172000 Loss: 0.2685, lr 8.535533905932742e-05
Step 172100 Loss: 0.2662, lr 8.535533905932742e-05
Step 172200 Loss: 0.2640, lr 8.535533905932742e-05
Step 172300 Loss: 0.2620, lr 8.535533905932742e-05
Step 172400 Loss: 0.2603, lr 8.535533905932742e-05
Step 172500 Loss: 0.2583, lr 8.535533905932742e-05
Step 172600 Loss: 0.2568, lr 8.535533905932742e-05
Step 172700 Loss: 0.2555, lr 8.535533905932742e-05
Step 172800 Loss: 0.2540, lr 8.535533905932742e-05
Step 172900 Loss: 0.2530, lr 8.535533905932742e-05
Step 173000 Loss: 0.2518, lr 8.535533905932742e-05
Step 173100 Loss: 0.2509, lr 8.535533905932742e-05
Step 173200 Loss: 0.2499, lr 8.535533905932742e-05
Step 173300 Loss: 0.2487, lr 8.535533905932742e-05
Step 173400 Loss: 0.2477, lr 8.535533905932742e-05
Step 173500 Loss: 0.2465, lr 8.535533905932742e-05
Step 173600 Loss: 0.2456, lr 8.535533905932742e-05
Step 173700 Loss: 0.2453, lr 8.535533905932742e-05
Step 173800 Loss: 0.2449, lr 8.535533905932742e-05
Step 173900 Loss: 0.2444, lr 8.535533905932742e-05
Step 174000 Loss: 0.2443, lr 8.535533905932742e-05
Step 174100 Loss: 0.2441, lr 8.535533905932742e-05
Step 174200 Loss: 0.2442, lr 8.535533905932742e-05
Step 174300 Loss: 0.2443, lr 8.535533905932742e-05
Step 174400 Loss: 0.2442, lr 8.535533905932742e-05
Step 174500 Loss: 0.2444, lr 8.535533905932742e-05
Step 174600 Loss: 0.2448, lr 8.535533905932742e-05
Step 174700 Loss: 0.2452, lr 8.535533905932742e-05
Step 174800 Loss: 0.2459, lr 8.535533905932742e-05
Step 174900 Loss: 0.2468, lr 8.535533905932742e-05
Step 175000 Loss: 0.2476, lr 8.535533905932742e-05
Step 175100 Loss: 0.2486, lr 8.535533905932742e-05
Step 175200 Loss: 0.2502, lr 8.535533905932742e-05
Step 175300 Loss: 0.2514, lr 8.535533905932742e-05
Step 175400 Loss: 0.2528, lr 8.535533905932742e-05
Step 175500 Loss: 0.2527, lr 8.535533905932742e-05
Step 175600 Loss: 0.2527, lr 8.535533905932742e-05
Train Epoch: [26/100] Loss: 0.2533,lr 0.000085
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Step 175700 Loss: 0.7599, lr 8.422735529643448e-05
Step 175800 Loss: 0.5477, lr 8.422735529643448e-05
Step 175900 Loss: 0.4771, lr 8.422735529643448e-05
Step 176000 Loss: 0.4387, lr 8.422735529643448e-05
Step 176100 Loss: 0.4173, lr 8.422735529643448e-05
Step 176200 Loss: 0.4021, lr 8.422735529643448e-05
Step 176300 Loss: 0.3839, lr 8.422735529643448e-05
Step 176400 Loss: 0.3706, lr 8.422735529643448e-05
Step 176500 Loss: 0.3606, lr 8.422735529643448e-05
Step 176600 Loss: 0.3506, lr 8.422735529643448e-05
Step 176700 Loss: 0.3415, lr 8.422735529643448e-05
Step 176800 Loss: 0.3344, lr 8.422735529643448e-05
Step 176900 Loss: 0.3277, lr 8.422735529643448e-05
Step 177000 Loss: 0.3212, lr 8.422735529643448e-05
Step 177100 Loss: 0.3152, lr 8.422735529643448e-05
Step 177200 Loss: 0.3109, lr 8.422735529643448e-05
Step 177300 Loss: 0.3062, lr 8.422735529643448e-05
Step 177400 Loss: 0.3021, lr 8.422735529643448e-05
Step 177500 Loss: 0.2983, lr 8.422735529643448e-05
Step 177600 Loss: 0.2946, lr 8.422735529643448e-05
Step 177700 Loss: 0.2906, lr 8.422735529643448e-05
Step 177800 Loss: 0.2872, lr 8.422735529643448e-05
Step 177900 Loss: 0.2842, lr 8.422735529643448e-05
Step 178000 Loss: 0.2810, lr 8.422735529643448e-05
Step 178100 Loss: 0.2784, lr 8.422735529643448e-05
Step 178200 Loss: 0.2756, lr 8.422735529643448e-05
Step 178300 Loss: 0.2729, lr 8.422735529643448e-05
Step 178400 Loss: 0.2706, lr 8.422735529643448e-05
Step 178500 Loss: 0.2681, lr 8.422735529643448e-05
Step 178600 Loss: 0.2661, lr 8.422735529643448e-05
Step 178700 Loss: 0.2644, lr 8.422735529643448e-05
Step 178800 Loss: 0.2620, lr 8.422735529643448e-05
Step 178900 Loss: 0.2598, lr 8.422735529643448e-05
Step 179000 Loss: 0.2575, lr 8.422735529643448e-05
Step 179100 Loss: 0.2558, lr 8.422735529643448e-05
Step 179200 Loss: 0.2539, lr 8.422735529643448e-05
Step 179300 Loss: 0.2524, lr 8.422735529643448e-05
Step 179400 Loss: 0.2506, lr 8.422735529643448e-05
Step 179500 Loss: 0.2495, lr 8.422735529643448e-05
Step 179600 Loss: 0.2481, lr 8.422735529643448e-05
Step 179700 Loss: 0.2473, lr 8.422735529643448e-05
Step 179800 Loss: 0.2462, lr 8.422735529643448e-05
Step 179900 Loss: 0.2454, lr 8.422735529643448e-05
Step 180000 Loss: 0.2442, lr 8.422735529643448e-05
Step 180100 Loss: 0.2433, lr 8.422735529643448e-05
Step 180200 Loss: 0.2421, lr 8.422735529643448e-05
Step 180300 Loss: 0.2412, lr 8.422735529643448e-05
Step 180400 Loss: 0.2406, lr 8.422735529643448e-05
Step 180500 Loss: 0.2402, lr 8.422735529643448e-05
Step 180600 Loss: 0.2400, lr 8.422735529643448e-05
Step 180700 Loss: 0.2395, lr 8.422735529643448e-05
Step 180800 Loss: 0.2397, lr 8.422735529643448e-05
Step 180900 Loss: 0.2393, lr 8.422735529643448e-05
Step 181000 Loss: 0.2395, lr 8.422735529643448e-05
Step 181100 Loss: 0.2393, lr 8.422735529643448e-05
Step 181200 Loss: 0.2397, lr 8.422735529643448e-05
Step 181300 Loss: 0.2399, lr 8.422735529643448e-05
Step 181400 Loss: 0.2404, lr 8.422735529643448e-05
Step 181500 Loss: 0.2410, lr 8.422735529643448e-05
Step 181600 Loss: 0.2418, lr 8.422735529643448e-05
Step 181700 Loss: 0.2425, lr 8.422735529643448e-05
Step 181800 Loss: 0.2435, lr 8.422735529643448e-05
Step 181900 Loss: 0.2448, lr 8.422735529643448e-05
Step 182000 Loss: 0.2459, lr 8.422735529643448e-05
Step 182100 Loss: 0.2475, lr 8.422735529643448e-05
Step 182200 Loss: 0.2484, lr 8.422735529643448e-05
Step 182300 Loss: 0.2480, lr 8.422735529643448e-05
Step 182400 Loss: 0.2481, lr 8.422735529643448e-05
Train Epoch: [27/100] Loss: 0.2488,lr 0.000084
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 182500 Loss: 0.6287, lr 8.306559326618263e-05
Step 182600 Loss: 0.5152, lr 8.306559326618263e-05
Step 182700 Loss: 0.4597, lr 8.306559326618263e-05
Step 182800 Loss: 0.4301, lr 8.306559326618263e-05
Step 182900 Loss: 0.4123, lr 8.306559326618263e-05
Step 183000 Loss: 0.3929, lr 8.306559326618263e-05
Step 183100 Loss: 0.3786, lr 8.306559326618263e-05
Step 183200 Loss: 0.3668, lr 8.306559326618263e-05
Step 183300 Loss: 0.3559, lr 8.306559326618263e-05
Step 183400 Loss: 0.3457, lr 8.306559326618263e-05
Step 183500 Loss: 0.3376, lr 8.306559326618263e-05
Step 183600 Loss: 0.3298, lr 8.306559326618263e-05
Step 183700 Loss: 0.3239, lr 8.306559326618263e-05
Step 183800 Loss: 0.3167, lr 8.306559326618263e-05
Step 183900 Loss: 0.3119, lr 8.306559326618263e-05
Step 184000 Loss: 0.3073, lr 8.306559326618263e-05
Step 184100 Loss: 0.3032, lr 8.306559326618263e-05
Step 184200 Loss: 0.2995, lr 8.306559326618263e-05
Step 184300 Loss: 0.2957, lr 8.306559326618263e-05
Step 184400 Loss: 0.2913, lr 8.306559326618263e-05
Step 184500 Loss: 0.2872, lr 8.306559326618263e-05
Step 184600 Loss: 0.2843, lr 8.306559326618263e-05
Step 184700 Loss: 0.2815, lr 8.306559326618263e-05
Step 184800 Loss: 0.2784, lr 8.306559326618263e-05
Step 184900 Loss: 0.2754, lr 8.306559326618263e-05
Step 185000 Loss: 0.2724, lr 8.306559326618263e-05
Step 185100 Loss: 0.2697, lr 8.306559326618263e-05
Step 185200 Loss: 0.2674, lr 8.306559326618263e-05
Step 185300 Loss: 0.2652, lr 8.306559326618263e-05
Step 185400 Loss: 0.2634, lr 8.306559326618263e-05
Step 185500 Loss: 0.2614, lr 8.306559326618263e-05
Step 185600 Loss: 0.2592, lr 8.306559326618263e-05
Step 185700 Loss: 0.2572, lr 8.306559326618263e-05
Step 185800 Loss: 0.2553, lr 8.306559326618263e-05
Step 185900 Loss: 0.2536, lr 8.306559326618263e-05
Step 186000 Loss: 0.2516, lr 8.306559326618263e-05
Step 186100 Loss: 0.2500, lr 8.306559326618263e-05
Step 186200 Loss: 0.2487, lr 8.306559326618263e-05
Step 186300 Loss: 0.2472, lr 8.306559326618263e-05
Step 186400 Loss: 0.2464, lr 8.306559326618263e-05
Step 186500 Loss: 0.2452, lr 8.306559326618263e-05
Step 186600 Loss: 0.2443, lr 8.306559326618263e-05
Step 186700 Loss: 0.2432, lr 8.306559326618263e-05
Step 186800 Loss: 0.2421, lr 8.306559326618263e-05
Step 186900 Loss: 0.2414, lr 8.306559326618263e-05
Step 187000 Loss: 0.2402, lr 8.306559326618263e-05
Step 187100 Loss: 0.2394, lr 8.306559326618263e-05
Step 187200 Loss: 0.2389, lr 8.306559326618263e-05
Step 187300 Loss: 0.2386, lr 8.306559326618263e-05
Step 187400 Loss: 0.2382, lr 8.306559326618263e-05
Step 187500 Loss: 0.2380, lr 8.306559326618263e-05
Step 187600 Loss: 0.2379, lr 8.306559326618263e-05
Step 187700 Loss: 0.2378, lr 8.306559326618263e-05
Step 187800 Loss: 0.2378, lr 8.306559326618263e-05
Step 187900 Loss: 0.2380, lr 8.306559326618263e-05
Step 188000 Loss: 0.2380, lr 8.306559326618263e-05
Step 188100 Loss: 0.2384, lr 8.306559326618263e-05
Step 188200 Loss: 0.2387, lr 8.306559326618263e-05
Step 188300 Loss: 0.2393, lr 8.306559326618263e-05
Step 188400 Loss: 0.2401, lr 8.306559326618263e-05
Step 188500 Loss: 0.2408, lr 8.306559326618263e-05
Step 188600 Loss: 0.2421, lr 8.306559326618263e-05
Step 188700 Loss: 0.2434, lr 8.306559326618263e-05
Step 188800 Loss: 0.2449, lr 8.306559326618263e-05
Step 188900 Loss: 0.2459, lr 8.306559326618263e-05
Step 189000 Loss: 0.2462, lr 8.306559326618263e-05
Step 189100 Loss: 0.2458, lr 8.306559326618263e-05
Train Epoch: [28/100] Loss: 0.2464,lr 0.000083
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6755
Step 189200 Loss: 0.8464, lr 8.187119948743452e-05
Step 189300 Loss: 0.5495, lr 8.187119948743452e-05
Step 189400 Loss: 0.4771, lr 8.187119948743452e-05
Step 189500 Loss: 0.4376, lr 8.187119948743452e-05
Step 189600 Loss: 0.4136, lr 8.187119948743452e-05
Step 189700 Loss: 0.3977, lr 8.187119948743452e-05
Step 189800 Loss: 0.3798, lr 8.187119948743452e-05
Step 189900 Loss: 0.3651, lr 8.187119948743452e-05
Step 190000 Loss: 0.3559, lr 8.187119948743452e-05
Step 190100 Loss: 0.3462, lr 8.187119948743452e-05
Step 190200 Loss: 0.3365, lr 8.187119948743452e-05
Step 190300 Loss: 0.3295, lr 8.187119948743452e-05
Step 190400 Loss: 0.3224, lr 8.187119948743452e-05
Step 190500 Loss: 0.3163, lr 8.187119948743452e-05
Step 190600 Loss: 0.3104, lr 8.187119948743452e-05
Step 190700 Loss: 0.3065, lr 8.187119948743452e-05
Step 190800 Loss: 0.3021, lr 8.187119948743452e-05
Step 190900 Loss: 0.2980, lr 8.187119948743452e-05
Step 191000 Loss: 0.2943, lr 8.187119948743452e-05
Step 191100 Loss: 0.2908, lr 8.187119948743452e-05
Step 191200 Loss: 0.2866, lr 8.187119948743452e-05
Step 191300 Loss: 0.2832, lr 8.187119948743452e-05
Step 191400 Loss: 0.2802, lr 8.187119948743452e-05
Step 191500 Loss: 0.2771, lr 8.187119948743452e-05
Step 191600 Loss: 0.2741, lr 8.187119948743452e-05
Step 191700 Loss: 0.2713, lr 8.187119948743452e-05
Step 191800 Loss: 0.2686, lr 8.187119948743452e-05
Step 191900 Loss: 0.2665, lr 8.187119948743452e-05
Step 192000 Loss: 0.2639, lr 8.187119948743452e-05
Step 192100 Loss: 0.2620, lr 8.187119948743452e-05
Step 192200 Loss: 0.2601, lr 8.187119948743452e-05
Step 192300 Loss: 0.2581, lr 8.187119948743452e-05
Step 192400 Loss: 0.2558, lr 8.187119948743452e-05
Step 192500 Loss: 0.2537, lr 8.187119948743452e-05
Step 192600 Loss: 0.2519, lr 8.187119948743452e-05
Step 192700 Loss: 0.2501, lr 8.187119948743452e-05
Step 192800 Loss: 0.2485, lr 8.187119948743452e-05
Step 192900 Loss: 0.2468, lr 8.187119948743452e-05
Step 193000 Loss: 0.2457, lr 8.187119948743452e-05
Step 193100 Loss: 0.2445, lr 8.187119948743452e-05
Step 193200 Loss: 0.2436, lr 8.187119948743452e-05
Step 193300 Loss: 0.2424, lr 8.187119948743452e-05
Step 193400 Loss: 0.2416, lr 8.187119948743452e-05
Step 193500 Loss: 0.2404, lr 8.187119948743452e-05
Step 193600 Loss: 0.2395, lr 8.187119948743452e-05
Step 193700 Loss: 0.2383, lr 8.187119948743452e-05
Step 193800 Loss: 0.2375, lr 8.187119948743452e-05
Step 193900 Loss: 0.2366, lr 8.187119948743452e-05
Step 194000 Loss: 0.2361, lr 8.187119948743452e-05
Step 194100 Loss: 0.2358, lr 8.187119948743452e-05
Step 194200 Loss: 0.2354, lr 8.187119948743452e-05
Step 194300 Loss: 0.2355, lr 8.187119948743452e-05
Step 194400 Loss: 0.2352, lr 8.187119948743452e-05
Step 194500 Loss: 0.2354, lr 8.187119948743452e-05
Step 194600 Loss: 0.2353, lr 8.187119948743452e-05
Step 194700 Loss: 0.2354, lr 8.187119948743452e-05
Step 194800 Loss: 0.2355, lr 8.187119948743452e-05
Step 194900 Loss: 0.2359, lr 8.187119948743452e-05
Step 195000 Loss: 0.2365, lr 8.187119948743452e-05
Step 195100 Loss: 0.2373, lr 8.187119948743452e-05
Step 195200 Loss: 0.2382, lr 8.187119948743452e-05
Step 195300 Loss: 0.2388, lr 8.187119948743452e-05
Step 195400 Loss: 0.2399, lr 8.187119948743452e-05
Step 195500 Loss: 0.2413, lr 8.187119948743452e-05
Step 195600 Loss: 0.2428, lr 8.187119948743452e-05
Step 195700 Loss: 0.2436, lr 8.187119948743452e-05
Step 195800 Loss: 0.2433, lr 8.187119948743452e-05
Step 195900 Loss: 0.2432, lr 8.187119948743452e-05
Train Epoch: [29/100] Loss: 0.2434,lr 0.000082
Calling G2SDataset.batch()
Done, time:  2.24 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.92 s, total batches: 6756
Step 196000 Loss: 0.6039, lr 8.064535268264887e-05
Step 196100 Loss: 0.4941, lr 8.064535268264887e-05
Step 196200 Loss: 0.4448, lr 8.064535268264887e-05
Step 196300 Loss: 0.4144, lr 8.064535268264887e-05
Step 196400 Loss: 0.3986, lr 8.064535268264887e-05
Step 196500 Loss: 0.3810, lr 8.064535268264887e-05
Step 196600 Loss: 0.3666, lr 8.064535268264887e-05
Step 196700 Loss: 0.3570, lr 8.064535268264887e-05
Step 196800 Loss: 0.3470, lr 8.064535268264887e-05
Step 196900 Loss: 0.3380, lr 8.064535268264887e-05
Step 197000 Loss: 0.3302, lr 8.064535268264887e-05
Step 197100 Loss: 0.3227, lr 8.064535268264887e-05
Step 197200 Loss: 0.3168, lr 8.064535268264887e-05
Step 197300 Loss: 0.3100, lr 8.064535268264887e-05
Step 197400 Loss: 0.3056, lr 8.064535268264887e-05
Step 197500 Loss: 0.3013, lr 8.064535268264887e-05
Step 197600 Loss: 0.2975, lr 8.064535268264887e-05
Step 197700 Loss: 0.2937, lr 8.064535268264887e-05
Step 197800 Loss: 0.2899, lr 8.064535268264887e-05
Step 197900 Loss: 0.2860, lr 8.064535268264887e-05
Step 198000 Loss: 0.2819, lr 8.064535268264887e-05
Step 198100 Loss: 0.2791, lr 8.064535268264887e-05
Step 198200 Loss: 0.2763, lr 8.064535268264887e-05
Step 198300 Loss: 0.2733, lr 8.064535268264887e-05
Step 198400 Loss: 0.2701, lr 8.064535268264887e-05
Step 198500 Loss: 0.2674, lr 8.064535268264887e-05
Step 198600 Loss: 0.2649, lr 8.064535268264887e-05
Step 198700 Loss: 0.2624, lr 8.064535268264887e-05
Step 198800 Loss: 0.2601, lr 8.064535268264887e-05
Step 198900 Loss: 0.2582, lr 8.064535268264887e-05
Step 199000 Loss: 0.2562, lr 8.064535268264887e-05
Step 199100 Loss: 0.2541, lr 8.064535268264887e-05
Step 199200 Loss: 0.2523, lr 8.064535268264887e-05
Step 199300 Loss: 0.2501, lr 8.064535268264887e-05
Step 199400 Loss: 0.2484, lr 8.064535268264887e-05
Step 199500 Loss: 0.2463, lr 8.064535268264887e-05
Step 199600 Loss: 0.2447, lr 8.064535268264887e-05
Step 199700 Loss: 0.2434, lr 8.064535268264887e-05
Step 199800 Loss: 0.2419, lr 8.064535268264887e-05
Step 199900 Loss: 0.2409, lr 8.064535268264887e-05
Step 200000 Loss: 0.2399, lr 8.064535268264887e-05
Step 200100 Loss: 0.2391, lr 8.064535268264887e-05
Step 200200 Loss: 0.2379, lr 8.064535268264887e-05
Step 200300 Loss: 0.2372, lr 8.064535268264887e-05
Step 200400 Loss: 0.2362, lr 8.064535268264887e-05
Step 200500 Loss: 0.2350, lr 8.064535268264887e-05
Step 200600 Loss: 0.2342, lr 8.064535268264887e-05
Step 200700 Loss: 0.2337, lr 8.064535268264887e-05
Step 200800 Loss: 0.2333, lr 8.064535268264887e-05
Step 200900 Loss: 0.2331, lr 8.064535268264887e-05
Step 201000 Loss: 0.2328, lr 8.064535268264887e-05
Step 201100 Loss: 0.2328, lr 8.064535268264887e-05
Step 201200 Loss: 0.2327, lr 8.064535268264887e-05
Step 201300 Loss: 0.2326, lr 8.064535268264887e-05
Step 201400 Loss: 0.2328, lr 8.064535268264887e-05
Step 201500 Loss: 0.2327, lr 8.064535268264887e-05
Step 201600 Loss: 0.2331, lr 8.064535268264887e-05
Step 201700 Loss: 0.2334, lr 8.064535268264887e-05
Step 201800 Loss: 0.2342, lr 8.064535268264887e-05
Step 201900 Loss: 0.2348, lr 8.064535268264887e-05
Step 202000 Loss: 0.2354, lr 8.064535268264887e-05
Step 202100 Loss: 0.2365, lr 8.064535268264887e-05
Step 202200 Loss: 0.2376, lr 8.064535268264887e-05
Step 202300 Loss: 0.2392, lr 8.064535268264887e-05
Step 202400 Loss: 0.2404, lr 8.064535268264887e-05
Step 202500 Loss: 0.2409, lr 8.064535268264887e-05
Step 202600 Loss: 0.2405, lr 8.064535268264887e-05
Train Epoch: [30/100] Loss: 0.2413,lr 0.000081
Model Saving at epoch 30
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 202700 Loss: 0.9887, lr 7.938926261462371e-05
Step 202800 Loss: 0.5785, lr 7.938926261462371e-05
Step 202900 Loss: 0.4899, lr 7.938926261462371e-05
Step 203000 Loss: 0.4438, lr 7.938926261462371e-05
Step 203100 Loss: 0.4158, lr 7.938926261462371e-05
Step 203200 Loss: 0.3980, lr 7.938926261462371e-05
Step 203300 Loss: 0.3795, lr 7.938926261462371e-05
Step 203400 Loss: 0.3656, lr 7.938926261462371e-05
Step 203500 Loss: 0.3551, lr 7.938926261462371e-05
Step 203600 Loss: 0.3451, lr 7.938926261462371e-05
Step 203700 Loss: 0.3357, lr 7.938926261462371e-05
Step 203800 Loss: 0.3286, lr 7.938926261462371e-05
Step 203900 Loss: 0.3213, lr 7.938926261462371e-05
Step 204000 Loss: 0.3152, lr 7.938926261462371e-05
Step 204100 Loss: 0.3090, lr 7.938926261462371e-05
Step 204200 Loss: 0.3049, lr 7.938926261462371e-05
Step 204300 Loss: 0.3001, lr 7.938926261462371e-05
Step 204400 Loss: 0.2958, lr 7.938926261462371e-05
Step 204500 Loss: 0.2922, lr 7.938926261462371e-05
Step 204600 Loss: 0.2887, lr 7.938926261462371e-05
Step 204700 Loss: 0.2845, lr 7.938926261462371e-05
Step 204800 Loss: 0.2810, lr 7.938926261462371e-05
Step 204900 Loss: 0.2780, lr 7.938926261462371e-05
Step 205000 Loss: 0.2749, lr 7.938926261462371e-05
Step 205100 Loss: 0.2719, lr 7.938926261462371e-05
Step 205200 Loss: 0.2689, lr 7.938926261462371e-05
Step 205300 Loss: 0.2661, lr 7.938926261462371e-05
Step 205400 Loss: 0.2640, lr 7.938926261462371e-05
Step 205500 Loss: 0.2617, lr 7.938926261462371e-05
Step 205600 Loss: 0.2594, lr 7.938926261462371e-05
Step 205700 Loss: 0.2574, lr 7.938926261462371e-05
Step 205800 Loss: 0.2553, lr 7.938926261462371e-05
Step 205900 Loss: 0.2530, lr 7.938926261462371e-05
Step 206000 Loss: 0.2509, lr 7.938926261462371e-05
Step 206100 Loss: 0.2490, lr 7.938926261462371e-05
Step 206200 Loss: 0.2472, lr 7.938926261462371e-05
Step 206300 Loss: 0.2454, lr 7.938926261462371e-05
Step 206400 Loss: 0.2435, lr 7.938926261462371e-05
Step 206500 Loss: 0.2422, lr 7.938926261462371e-05
Step 206600 Loss: 0.2409, lr 7.938926261462371e-05
Step 206700 Loss: 0.2400, lr 7.938926261462371e-05
Step 206800 Loss: 0.2387, lr 7.938926261462371e-05
Step 206900 Loss: 0.2378, lr 7.938926261462371e-05
Step 207000 Loss: 0.2366, lr 7.938926261462371e-05
Step 207100 Loss: 0.2356, lr 7.938926261462371e-05
Step 207200 Loss: 0.2345, lr 7.938926261462371e-05
Step 207300 Loss: 0.2333, lr 7.938926261462371e-05
Step 207400 Loss: 0.2327, lr 7.938926261462371e-05
Step 207500 Loss: 0.2322, lr 7.938926261462371e-05
Step 207600 Loss: 0.2320, lr 7.938926261462371e-05
Step 207700 Loss: 0.2314, lr 7.938926261462371e-05
Step 207800 Loss: 0.2314, lr 7.938926261462371e-05
Step 207900 Loss: 0.2311, lr 7.938926261462371e-05
Step 208000 Loss: 0.2313, lr 7.938926261462371e-05
Step 208100 Loss: 0.2312, lr 7.938926261462371e-05
Step 208200 Loss: 0.2313, lr 7.938926261462371e-05
Step 208300 Loss: 0.2314, lr 7.938926261462371e-05
Step 208400 Loss: 0.2319, lr 7.938926261462371e-05
Step 208500 Loss: 0.2323, lr 7.938926261462371e-05
Step 208600 Loss: 0.2331, lr 7.938926261462371e-05
Step 208700 Loss: 0.2338, lr 7.938926261462371e-05
Step 208800 Loss: 0.2343, lr 7.938926261462371e-05
Step 208900 Loss: 0.2353, lr 7.938926261462371e-05
Step 209000 Loss: 0.2367, lr 7.938926261462371e-05
Step 209100 Loss: 0.2380, lr 7.938926261462371e-05
Step 209200 Loss: 0.2390, lr 7.938926261462371e-05
Step 209300 Loss: 0.2388, lr 7.938926261462371e-05
Step 209400 Loss: 0.2384, lr 7.938926261462371e-05
Train Epoch: [31/100] Loss: 0.2395,lr 0.000079
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6756
Step 209500 Loss: 0.6649, lr 7.810416889260659e-05
Step 209600 Loss: 0.5147, lr 7.810416889260659e-05
Step 209700 Loss: 0.4547, lr 7.810416889260659e-05
Step 209800 Loss: 0.4200, lr 7.810416889260659e-05
Step 209900 Loss: 0.4016, lr 7.810416889260659e-05
Step 210000 Loss: 0.3846, lr 7.810416889260659e-05
Step 210100 Loss: 0.3682, lr 7.810416889260659e-05
Step 210200 Loss: 0.3572, lr 7.810416889260659e-05
Step 210300 Loss: 0.3473, lr 7.810416889260659e-05
Step 210400 Loss: 0.3379, lr 7.810416889260659e-05
Step 210500 Loss: 0.3296, lr 7.810416889260659e-05
Step 210600 Loss: 0.3233, lr 7.810416889260659e-05
Step 210700 Loss: 0.3169, lr 7.810416889260659e-05
Step 210800 Loss: 0.3101, lr 7.810416889260659e-05
Step 210900 Loss: 0.3051, lr 7.810416889260659e-05
Step 211000 Loss: 0.3007, lr 7.810416889260659e-05
Step 211100 Loss: 0.2966, lr 7.810416889260659e-05
Step 211200 Loss: 0.2927, lr 7.810416889260659e-05
Step 211300 Loss: 0.2890, lr 7.810416889260659e-05
Step 211400 Loss: 0.2852, lr 7.810416889260659e-05
Step 211500 Loss: 0.2812, lr 7.810416889260659e-05
Step 211600 Loss: 0.2781, lr 7.810416889260659e-05
Step 211700 Loss: 0.2752, lr 7.810416889260659e-05
Step 211800 Loss: 0.2721, lr 7.810416889260659e-05
Step 211900 Loss: 0.2690, lr 7.810416889260659e-05
Step 212000 Loss: 0.2664, lr 7.810416889260659e-05
Step 212100 Loss: 0.2636, lr 7.810416889260659e-05
Step 212200 Loss: 0.2614, lr 7.810416889260659e-05
Step 212300 Loss: 0.2589, lr 7.810416889260659e-05
Step 212400 Loss: 0.2572, lr 7.810416889260659e-05
Step 212500 Loss: 0.2551, lr 7.810416889260659e-05
Step 212600 Loss: 0.2527, lr 7.810416889260659e-05
Step 212700 Loss: 0.2508, lr 7.810416889260659e-05
Step 212800 Loss: 0.2485, lr 7.810416889260659e-05
Step 212900 Loss: 0.2467, lr 7.810416889260659e-05
Step 213000 Loss: 0.2448, lr 7.810416889260659e-05
Step 213100 Loss: 0.2432, lr 7.810416889260659e-05
Step 213200 Loss: 0.2417, lr 7.810416889260659e-05
Step 213300 Loss: 0.2402, lr 7.810416889260659e-05
Step 213400 Loss: 0.2390, lr 7.810416889260659e-05
Step 213500 Loss: 0.2381, lr 7.810416889260659e-05
Step 213600 Loss: 0.2371, lr 7.810416889260659e-05
Step 213700 Loss: 0.2359, lr 7.810416889260659e-05
Step 213800 Loss: 0.2349, lr 7.810416889260659e-05
Step 213900 Loss: 0.2340, lr 7.810416889260659e-05
Step 214000 Loss: 0.2327, lr 7.810416889260659e-05
Step 214100 Loss: 0.2319, lr 7.810416889260659e-05
Step 214200 Loss: 0.2311, lr 7.810416889260659e-05
Step 214300 Loss: 0.2307, lr 7.810416889260659e-05
Step 214400 Loss: 0.2304, lr 7.810416889260659e-05
Step 214500 Loss: 0.2301, lr 7.810416889260659e-05
Step 214600 Loss: 0.2302, lr 7.810416889260659e-05
Step 214700 Loss: 0.2298, lr 7.810416889260659e-05
Step 214800 Loss: 0.2299, lr 7.810416889260659e-05
Step 214900 Loss: 0.2298, lr 7.810416889260659e-05
Step 215000 Loss: 0.2298, lr 7.810416889260659e-05
Step 215100 Loss: 0.2300, lr 7.810416889260659e-05
Step 215200 Loss: 0.2302, lr 7.810416889260659e-05
Step 215300 Loss: 0.2308, lr 7.810416889260659e-05
Step 215400 Loss: 0.2313, lr 7.810416889260659e-05
Step 215500 Loss: 0.2319, lr 7.810416889260659e-05
Step 215600 Loss: 0.2328, lr 7.810416889260659e-05
Step 215700 Loss: 0.2339, lr 7.810416889260659e-05
Step 215800 Loss: 0.2354, lr 7.810416889260659e-05
Step 215900 Loss: 0.2371, lr 7.810416889260659e-05
Step 216000 Loss: 0.2377, lr 7.810416889260659e-05
Step 216100 Loss: 0.2373, lr 7.810416889260659e-05
Train Epoch: [32/100] Loss: 0.2377,lr 0.000078
Calling G2SDataset.batch()
Done, time:  2.02 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 216200 Loss: 1.2311, lr 7.679133974894988e-05
Step 216300 Loss: 0.5858, lr 7.679133974894988e-05
Step 216400 Loss: 0.4883, lr 7.679133974894988e-05
Step 216500 Loss: 0.4422, lr 7.679133974894988e-05
Step 216600 Loss: 0.4160, lr 7.679133974894988e-05
Step 216700 Loss: 0.3978, lr 7.679133974894988e-05
Step 216800 Loss: 0.3792, lr 7.679133974894988e-05
Step 216900 Loss: 0.3647, lr 7.679133974894988e-05
Step 217000 Loss: 0.3552, lr 7.679133974894988e-05
Step 217100 Loss: 0.3442, lr 7.679133974894988e-05
Step 217200 Loss: 0.3353, lr 7.679133974894988e-05
Step 217300 Loss: 0.3282, lr 7.679133974894988e-05
Step 217400 Loss: 0.3210, lr 7.679133974894988e-05
Step 217500 Loss: 0.3147, lr 7.679133974894988e-05
Step 217600 Loss: 0.3086, lr 7.679133974894988e-05
Step 217700 Loss: 0.3043, lr 7.679133974894988e-05
Step 217800 Loss: 0.2994, lr 7.679133974894988e-05
Step 217900 Loss: 0.2955, lr 7.679133974894988e-05
Step 218000 Loss: 0.2915, lr 7.679133974894988e-05
Step 218100 Loss: 0.2882, lr 7.679133974894988e-05
Step 218200 Loss: 0.2837, lr 7.679133974894988e-05
Step 218300 Loss: 0.2798, lr 7.679133974894988e-05
Step 218400 Loss: 0.2766, lr 7.679133974894988e-05
Step 218500 Loss: 0.2737, lr 7.679133974894988e-05
Step 218600 Loss: 0.2708, lr 7.679133974894988e-05
Step 218700 Loss: 0.2675, lr 7.679133974894988e-05
Step 218800 Loss: 0.2648, lr 7.679133974894988e-05
Step 218900 Loss: 0.2624, lr 7.679133974894988e-05
Step 219000 Loss: 0.2601, lr 7.679133974894988e-05
Step 219100 Loss: 0.2576, lr 7.679133974894988e-05
Step 219200 Loss: 0.2557, lr 7.679133974894988e-05
Step 219300 Loss: 0.2535, lr 7.679133974894988e-05
Step 219400 Loss: 0.2513, lr 7.679133974894988e-05
Step 219500 Loss: 0.2489, lr 7.679133974894988e-05
Step 219600 Loss: 0.2470, lr 7.679133974894988e-05
Step 219700 Loss: 0.2452, lr 7.679133974894988e-05
Step 219800 Loss: 0.2432, lr 7.679133974894988e-05
Step 219900 Loss: 0.2415, lr 7.679133974894988e-05
Step 220000 Loss: 0.2402, lr 7.679133974894988e-05
Step 220100 Loss: 0.2389, lr 7.679133974894988e-05
Step 220200 Loss: 0.2379, lr 7.679133974894988e-05
Step 220300 Loss: 0.2367, lr 7.679133974894988e-05
Step 220400 Loss: 0.2357, lr 7.679133974894988e-05
Step 220500 Loss: 0.2346, lr 7.679133974894988e-05
Step 220600 Loss: 0.2334, lr 7.679133974894988e-05
Step 220700 Loss: 0.2326, lr 7.679133974894988e-05
Step 220800 Loss: 0.2315, lr 7.679133974894988e-05
Step 220900 Loss: 0.2305, lr 7.679133974894988e-05
Step 221000 Loss: 0.2301, lr 7.679133974894988e-05
Step 221100 Loss: 0.2298, lr 7.679133974894988e-05
Step 221200 Loss: 0.2293, lr 7.679133974894988e-05
Step 221300 Loss: 0.2292, lr 7.679133974894988e-05
Step 221400 Loss: 0.2289, lr 7.679133974894988e-05
Step 221500 Loss: 0.2289, lr 7.679133974894988e-05
Step 221600 Loss: 0.2287, lr 7.679133974894988e-05
Step 221700 Loss: 0.2286, lr 7.679133974894988e-05
Step 221800 Loss: 0.2287, lr 7.679133974894988e-05
Step 221900 Loss: 0.2290, lr 7.679133974894988e-05
Step 222000 Loss: 0.2293, lr 7.679133974894988e-05
Step 222100 Loss: 0.2299, lr 7.679133974894988e-05
Step 222200 Loss: 0.2306, lr 7.679133974894988e-05
Step 222300 Loss: 0.2314, lr 7.679133974894988e-05
Step 222400 Loss: 0.2324, lr 7.679133974894988e-05
Step 222500 Loss: 0.2337, lr 7.679133974894988e-05
Step 222600 Loss: 0.2348, lr 7.679133974894988e-05
Step 222700 Loss: 0.2360, lr 7.679133974894988e-05
Step 222800 Loss: 0.2360, lr 7.679133974894988e-05
Step 222900 Loss: 0.2357, lr 7.679133974894988e-05
Train Epoch: [33/100] Loss: 0.2365,lr 0.000077
Calling G2SDataset.batch()
Done, time:  2.29 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6755
Step 223000 Loss: 0.7339, lr 7.545207078751862e-05
Step 223100 Loss: 0.5282, lr 7.545207078751862e-05
Step 223200 Loss: 0.4612, lr 7.545207078751862e-05
Step 223300 Loss: 0.4240, lr 7.545207078751862e-05
Step 223400 Loss: 0.4036, lr 7.545207078751862e-05
Step 223500 Loss: 0.3861, lr 7.545207078751862e-05
Step 223600 Loss: 0.3681, lr 7.545207078751862e-05
Step 223700 Loss: 0.3563, lr 7.545207078751862e-05
Step 223800 Loss: 0.3464, lr 7.545207078751862e-05
Step 223900 Loss: 0.3367, lr 7.545207078751862e-05
Step 224000 Loss: 0.3279, lr 7.545207078751862e-05
Step 224100 Loss: 0.3216, lr 7.545207078751862e-05
Step 224200 Loss: 0.3156, lr 7.545207078751862e-05
Step 224300 Loss: 0.3091, lr 7.545207078751862e-05
Step 224400 Loss: 0.3034, lr 7.545207078751862e-05
Step 224500 Loss: 0.2994, lr 7.545207078751862e-05
Step 224600 Loss: 0.2952, lr 7.545207078751862e-05
Step 224700 Loss: 0.2910, lr 7.545207078751862e-05
Step 224800 Loss: 0.2877, lr 7.545207078751862e-05
Step 224900 Loss: 0.2837, lr 7.545207078751862e-05
Step 225000 Loss: 0.2797, lr 7.545207078751862e-05
Step 225100 Loss: 0.2764, lr 7.545207078751862e-05
Step 225200 Loss: 0.2735, lr 7.545207078751862e-05
Step 225300 Loss: 0.2701, lr 7.545207078751862e-05
Step 225400 Loss: 0.2670, lr 7.545207078751862e-05
Step 225500 Loss: 0.2644, lr 7.545207078751862e-05
Step 225600 Loss: 0.2616, lr 7.545207078751862e-05
Step 225700 Loss: 0.2595, lr 7.545207078751862e-05
Step 225800 Loss: 0.2571, lr 7.545207078751862e-05
Step 225900 Loss: 0.2553, lr 7.545207078751862e-05
Step 226000 Loss: 0.2533, lr 7.545207078751862e-05
Step 226100 Loss: 0.2510, lr 7.545207078751862e-05
Step 226200 Loss: 0.2490, lr 7.545207078751862e-05
Step 226300 Loss: 0.2465, lr 7.545207078751862e-05
Step 226400 Loss: 0.2448, lr 7.545207078751862e-05
Step 226500 Loss: 0.2429, lr 7.545207078751862e-05
Step 226600 Loss: 0.2412, lr 7.545207078751862e-05
Step 226700 Loss: 0.2395, lr 7.545207078751862e-05
Step 226800 Loss: 0.2381, lr 7.545207078751862e-05
Step 226900 Loss: 0.2369, lr 7.545207078751862e-05
Step 227000 Loss: 0.2359, lr 7.545207078751862e-05
Step 227100 Loss: 0.2348, lr 7.545207078751862e-05
Step 227200 Loss: 0.2337, lr 7.545207078751862e-05
Step 227300 Loss: 0.2327, lr 7.545207078751862e-05
Step 227400 Loss: 0.2319, lr 7.545207078751862e-05
Step 227500 Loss: 0.2306, lr 7.545207078751862e-05
Step 227600 Loss: 0.2296, lr 7.545207078751862e-05
Step 227700 Loss: 0.2290, lr 7.545207078751862e-05
Step 227800 Loss: 0.2285, lr 7.545207078751862e-05
Step 227900 Loss: 0.2281, lr 7.545207078751862e-05
Step 228000 Loss: 0.2275, lr 7.545207078751862e-05
Step 228100 Loss: 0.2276, lr 7.545207078751862e-05
Step 228200 Loss: 0.2273, lr 7.545207078751862e-05
Step 228300 Loss: 0.2273, lr 7.545207078751862e-05
Step 228400 Loss: 0.2271, lr 7.545207078751862e-05
Step 228500 Loss: 0.2270, lr 7.545207078751862e-05
Step 228600 Loss: 0.2272, lr 7.545207078751862e-05
Step 228700 Loss: 0.2273, lr 7.545207078751862e-05
Step 228800 Loss: 0.2277, lr 7.545207078751862e-05
Step 228900 Loss: 0.2283, lr 7.545207078751862e-05
Step 229000 Loss: 0.2288, lr 7.545207078751862e-05
Step 229100 Loss: 0.2297, lr 7.545207078751862e-05
Step 229200 Loss: 0.2308, lr 7.545207078751862e-05
Step 229300 Loss: 0.2318, lr 7.545207078751862e-05
Step 229400 Loss: 0.2331, lr 7.545207078751862e-05
Step 229500 Loss: 0.2337, lr 7.545207078751862e-05
Step 229600 Loss: 0.2333, lr 7.545207078751862e-05
Train Epoch: [34/100] Loss: 0.2334,lr 0.000075
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6755
Step 229700 Loss: 1.5447, lr 7.408768370508582e-05
Step 229800 Loss: 0.5666, lr 7.408768370508582e-05
Step 229900 Loss: 0.4690, lr 7.408768370508582e-05
Step 230000 Loss: 0.4234, lr 7.408768370508582e-05
Step 230100 Loss: 0.3995, lr 7.408768370508582e-05
Step 230200 Loss: 0.3824, lr 7.408768370508582e-05
Step 230300 Loss: 0.3658, lr 7.408768370508582e-05
Step 230400 Loss: 0.3525, lr 7.408768370508582e-05
Step 230500 Loss: 0.3430, lr 7.408768370508582e-05
Step 230600 Loss: 0.3329, lr 7.408768370508582e-05
Step 230700 Loss: 0.3241, lr 7.408768370508582e-05
Step 230800 Loss: 0.3176, lr 7.408768370508582e-05
Step 230900 Loss: 0.3113, lr 7.408768370508582e-05
Step 231000 Loss: 0.3059, lr 7.408768370508582e-05
Step 231100 Loss: 0.3001, lr 7.408768370508582e-05
Step 231200 Loss: 0.2963, lr 7.408768370508582e-05
Step 231300 Loss: 0.2916, lr 7.408768370508582e-05
Step 231400 Loss: 0.2881, lr 7.408768370508582e-05
Step 231500 Loss: 0.2846, lr 7.408768370508582e-05
Step 231600 Loss: 0.2813, lr 7.408768370508582e-05
Step 231700 Loss: 0.2774, lr 7.408768370508582e-05
Step 231800 Loss: 0.2735, lr 7.408768370508582e-05
Step 231900 Loss: 0.2707, lr 7.408768370508582e-05
Step 232000 Loss: 0.2679, lr 7.408768370508582e-05
Step 232100 Loss: 0.2650, lr 7.408768370508582e-05
Step 232200 Loss: 0.2621, lr 7.408768370508582e-05
Step 232300 Loss: 0.2593, lr 7.408768370508582e-05
Step 232400 Loss: 0.2568, lr 7.408768370508582e-05
Step 232500 Loss: 0.2549, lr 7.408768370508582e-05
Step 232600 Loss: 0.2528, lr 7.408768370508582e-05
Step 232700 Loss: 0.2508, lr 7.408768370508582e-05
Step 232800 Loss: 0.2487, lr 7.408768370508582e-05
Step 232900 Loss: 0.2465, lr 7.408768370508582e-05
Step 233000 Loss: 0.2443, lr 7.408768370508582e-05
Step 233100 Loss: 0.2423, lr 7.408768370508582e-05
Step 233200 Loss: 0.2406, lr 7.408768370508582e-05
Step 233300 Loss: 0.2387, lr 7.408768370508582e-05
Step 233400 Loss: 0.2370, lr 7.408768370508582e-05
Step 233500 Loss: 0.2357, lr 7.408768370508582e-05
Step 233600 Loss: 0.2343, lr 7.408768370508582e-05
Step 233700 Loss: 0.2335, lr 7.408768370508582e-05
Step 233800 Loss: 0.2322, lr 7.408768370508582e-05
Step 233900 Loss: 0.2314, lr 7.408768370508582e-05
Step 234000 Loss: 0.2305, lr 7.408768370508582e-05
Step 234100 Loss: 0.2293, lr 7.408768370508582e-05
Step 234200 Loss: 0.2284, lr 7.408768370508582e-05
Step 234300 Loss: 0.2273, lr 7.408768370508582e-05
Step 234400 Loss: 0.2265, lr 7.408768370508582e-05
Step 234500 Loss: 0.2259, lr 7.408768370508582e-05
Step 234600 Loss: 0.2255, lr 7.408768370508582e-05
Step 234700 Loss: 0.2252, lr 7.408768370508582e-05
Step 234800 Loss: 0.2250, lr 7.408768370508582e-05
Step 234900 Loss: 0.2247, lr 7.408768370508582e-05
Step 235000 Loss: 0.2245, lr 7.408768370508582e-05
Step 235100 Loss: 0.2244, lr 7.408768370508582e-05
Step 235200 Loss: 0.2246, lr 7.408768370508582e-05
Step 235300 Loss: 0.2244, lr 7.408768370508582e-05
Step 235400 Loss: 0.2249, lr 7.408768370508582e-05
Step 235500 Loss: 0.2252, lr 7.408768370508582e-05
Step 235600 Loss: 0.2256, lr 7.408768370508582e-05
Step 235700 Loss: 0.2261, lr 7.408768370508582e-05
Step 235800 Loss: 0.2266, lr 7.408768370508582e-05
Step 235900 Loss: 0.2275, lr 7.408768370508582e-05
Step 236000 Loss: 0.2286, lr 7.408768370508582e-05
Step 236100 Loss: 0.2297, lr 7.408768370508582e-05
Step 236200 Loss: 0.2308, lr 7.408768370508582e-05
Step 236300 Loss: 0.2309, lr 7.408768370508582e-05
Step 236400 Loss: 0.2310, lr 7.408768370508582e-05
Train Epoch: [35/100] Loss: 0.2313,lr 0.000074
Model Saving at epoch 35
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.63 s, total batches: 6755
Step 236500 Loss: 0.7558, lr 7.26995249869774e-05
Step 236600 Loss: 0.5190, lr 7.26995249869774e-05
Step 236700 Loss: 0.4513, lr 7.26995249869774e-05
Step 236800 Loss: 0.4128, lr 7.26995249869774e-05
Step 236900 Loss: 0.3919, lr 7.26995249869774e-05
Step 237000 Loss: 0.3755, lr 7.26995249869774e-05
Step 237100 Loss: 0.3588, lr 7.26995249869774e-05
Step 237200 Loss: 0.3467, lr 7.26995249869774e-05
Step 237300 Loss: 0.3376, lr 7.26995249869774e-05
Step 237400 Loss: 0.3282, lr 7.26995249869774e-05
Step 237500 Loss: 0.3201, lr 7.26995249869774e-05
Step 237600 Loss: 0.3143, lr 7.26995249869774e-05
Step 237700 Loss: 0.3084, lr 7.26995249869774e-05
Step 237800 Loss: 0.3025, lr 7.26995249869774e-05
Step 237900 Loss: 0.2972, lr 7.26995249869774e-05
Step 238000 Loss: 0.2936, lr 7.26995249869774e-05
Step 238100 Loss: 0.2897, lr 7.26995249869774e-05
Step 238200 Loss: 0.2856, lr 7.26995249869774e-05
Step 238300 Loss: 0.2824, lr 7.26995249869774e-05
Step 238400 Loss: 0.2789, lr 7.26995249869774e-05
Step 238500 Loss: 0.2753, lr 7.26995249869774e-05
Step 238600 Loss: 0.2722, lr 7.26995249869774e-05
Step 238700 Loss: 0.2693, lr 7.26995249869774e-05
Step 238800 Loss: 0.2663, lr 7.26995249869774e-05
Step 238900 Loss: 0.2634, lr 7.26995249869774e-05
Step 239000 Loss: 0.2606, lr 7.26995249869774e-05
Step 239100 Loss: 0.2581, lr 7.26995249869774e-05
Step 239200 Loss: 0.2559, lr 7.26995249869774e-05
Step 239300 Loss: 0.2536, lr 7.26995249869774e-05
Step 239400 Loss: 0.2516, lr 7.26995249869774e-05
Step 239500 Loss: 0.2497, lr 7.26995249869774e-05
Step 239600 Loss: 0.2475, lr 7.26995249869774e-05
Step 239700 Loss: 0.2454, lr 7.26995249869774e-05
Step 239800 Loss: 0.2431, lr 7.26995249869774e-05
Step 239900 Loss: 0.2411, lr 7.26995249869774e-05
Step 240000 Loss: 0.2395, lr 7.26995249869774e-05
Step 240100 Loss: 0.2378, lr 7.26995249869774e-05
Step 240200 Loss: 0.2360, lr 7.26995249869774e-05
Step 240300 Loss: 0.2348, lr 7.26995249869774e-05
Step 240400 Loss: 0.2334, lr 7.26995249869774e-05
Step 240500 Loss: 0.2325, lr 7.26995249869774e-05
Step 240600 Loss: 0.2313, lr 7.26995249869774e-05
Step 240700 Loss: 0.2302, lr 7.26995249869774e-05
Step 240800 Loss: 0.2292, lr 7.26995249869774e-05
Step 240900 Loss: 0.2283, lr 7.26995249869774e-05
Step 241000 Loss: 0.2271, lr 7.26995249869774e-05
Step 241100 Loss: 0.2261, lr 7.26995249869774e-05
Step 241200 Loss: 0.2254, lr 7.26995249869774e-05
Step 241300 Loss: 0.2251, lr 7.26995249869774e-05
Step 241400 Loss: 0.2248, lr 7.26995249869774e-05
Step 241500 Loss: 0.2242, lr 7.26995249869774e-05
Step 241600 Loss: 0.2243, lr 7.26995249869774e-05
Step 241700 Loss: 0.2239, lr 7.26995249869774e-05
Step 241800 Loss: 0.2238, lr 7.26995249869774e-05
Step 241900 Loss: 0.2237, lr 7.26995249869774e-05
Step 242000 Loss: 0.2236, lr 7.26995249869774e-05
Step 242100 Loss: 0.2238, lr 7.26995249869774e-05
Step 242200 Loss: 0.2241, lr 7.26995249869774e-05
Step 242300 Loss: 0.2244, lr 7.26995249869774e-05
Step 242400 Loss: 0.2250, lr 7.26995249869774e-05
Step 242500 Loss: 0.2257, lr 7.26995249869774e-05
Step 242600 Loss: 0.2262, lr 7.26995249869774e-05
Step 242700 Loss: 0.2272, lr 7.26995249869774e-05
Step 242800 Loss: 0.2282, lr 7.26995249869774e-05
Step 242900 Loss: 0.2296, lr 7.26995249869774e-05
Step 243000 Loss: 0.2303, lr 7.26995249869774e-05
Step 243100 Loss: 0.2301, lr 7.26995249869774e-05
Step 243200 Loss: 0.2299, lr 7.26995249869774e-05
Train Epoch: [36/100] Loss: 0.2302,lr 0.000073
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6755
Step 243300 Loss: 0.5922, lr 7.12889645782537e-05
Step 243400 Loss: 0.4788, lr 7.12889645782537e-05
Step 243500 Loss: 0.4273, lr 7.12889645782537e-05
Step 243600 Loss: 0.4008, lr 7.12889645782537e-05
Step 243700 Loss: 0.3840, lr 7.12889645782537e-05
Step 243800 Loss: 0.3662, lr 7.12889645782537e-05
Step 243900 Loss: 0.3523, lr 7.12889645782537e-05
Step 244000 Loss: 0.3422, lr 7.12889645782537e-05
Step 244100 Loss: 0.3320, lr 7.12889645782537e-05
Step 244200 Loss: 0.3230, lr 7.12889645782537e-05
Step 244300 Loss: 0.3162, lr 7.12889645782537e-05
Step 244400 Loss: 0.3096, lr 7.12889645782537e-05
Step 244500 Loss: 0.3042, lr 7.12889645782537e-05
Step 244600 Loss: 0.2978, lr 7.12889645782537e-05
Step 244700 Loss: 0.2940, lr 7.12889645782537e-05
Step 244800 Loss: 0.2895, lr 7.12889645782537e-05
Step 244900 Loss: 0.2859, lr 7.12889645782537e-05
Step 245000 Loss: 0.2824, lr 7.12889645782537e-05
Step 245100 Loss: 0.2792, lr 7.12889645782537e-05
Step 245200 Loss: 0.2752, lr 7.12889645782537e-05
Step 245300 Loss: 0.2715, lr 7.12889645782537e-05
Step 245400 Loss: 0.2689, lr 7.12889645782537e-05
Step 245500 Loss: 0.2663, lr 7.12889645782537e-05
Step 245600 Loss: 0.2634, lr 7.12889645782537e-05
Step 245700 Loss: 0.2604, lr 7.12889645782537e-05
Step 245800 Loss: 0.2576, lr 7.12889645782537e-05
Step 245900 Loss: 0.2550, lr 7.12889645782537e-05
Step 246000 Loss: 0.2531, lr 7.12889645782537e-05
Step 246100 Loss: 0.2509, lr 7.12889645782537e-05
Step 246200 Loss: 0.2488, lr 7.12889645782537e-05
Step 246300 Loss: 0.2467, lr 7.12889645782537e-05
Step 246400 Loss: 0.2444, lr 7.12889645782537e-05
Step 246500 Loss: 0.2423, lr 7.12889645782537e-05
Step 246600 Loss: 0.2403, lr 7.12889645782537e-05
Step 246700 Loss: 0.2386, lr 7.12889645782537e-05
Step 246800 Loss: 0.2365, lr 7.12889645782537e-05
Step 246900 Loss: 0.2350, lr 7.12889645782537e-05
Step 247000 Loss: 0.2336, lr 7.12889645782537e-05
Step 247100 Loss: 0.2322, lr 7.12889645782537e-05
Step 247200 Loss: 0.2313, lr 7.12889645782537e-05
Step 247300 Loss: 0.2300, lr 7.12889645782537e-05
Step 247400 Loss: 0.2292, lr 7.12889645782537e-05
Step 247500 Loss: 0.2281, lr 7.12889645782537e-05
Step 247600 Loss: 0.2271, lr 7.12889645782537e-05
Step 247700 Loss: 0.2262, lr 7.12889645782537e-05
Step 247800 Loss: 0.2250, lr 7.12889645782537e-05
Step 247900 Loss: 0.2241, lr 7.12889645782537e-05
Step 248000 Loss: 0.2237, lr 7.12889645782537e-05
Step 248100 Loss: 0.2232, lr 7.12889645782537e-05
Step 248200 Loss: 0.2228, lr 7.12889645782537e-05
Step 248300 Loss: 0.2224, lr 7.12889645782537e-05
Step 248400 Loss: 0.2222, lr 7.12889645782537e-05
Step 248500 Loss: 0.2219, lr 7.12889645782537e-05
Step 248600 Loss: 0.2218, lr 7.12889645782537e-05
Step 248700 Loss: 0.2219, lr 7.12889645782537e-05
Step 248800 Loss: 0.2217, lr 7.12889645782537e-05
Step 248900 Loss: 0.2220, lr 7.12889645782537e-05
Step 249000 Loss: 0.2221, lr 7.12889645782537e-05
Step 249100 Loss: 0.2226, lr 7.12889645782537e-05
Step 249200 Loss: 0.2233, lr 7.12889645782537e-05
Step 249300 Loss: 0.2239, lr 7.12889645782537e-05
Step 249400 Loss: 0.2247, lr 7.12889645782537e-05
Step 249500 Loss: 0.2256, lr 7.12889645782537e-05
Step 249600 Loss: 0.2270, lr 7.12889645782537e-05
Step 249700 Loss: 0.2281, lr 7.12889645782537e-05
Step 249800 Loss: 0.2282, lr 7.12889645782537e-05
Step 249900 Loss: 0.2277, lr 7.12889645782537e-05
Train Epoch: [37/100] Loss: 0.2283,lr 0.000071
Calling G2SDataset.batch()
Done, time:  2.02 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Step 250000 Loss: 0.7908, lr 6.985739453173908e-05
Step 250100 Loss: 0.5139, lr 6.985739453173908e-05
Step 250200 Loss: 0.4438, lr 6.985739453173908e-05
Step 250300 Loss: 0.4078, lr 6.985739453173908e-05
Step 250400 Loss: 0.3857, lr 6.985739453173908e-05
Step 250500 Loss: 0.3712, lr 6.985739453173908e-05
Step 250600 Loss: 0.3551, lr 6.985739453173908e-05
Step 250700 Loss: 0.3423, lr 6.985739453173908e-05
Step 250800 Loss: 0.3339, lr 6.985739453173908e-05
Step 250900 Loss: 0.3246, lr 6.985739453173908e-05
Step 251000 Loss: 0.3166, lr 6.985739453173908e-05
Step 251100 Loss: 0.3106, lr 6.985739453173908e-05
Step 251200 Loss: 0.3048, lr 6.985739453173908e-05
Step 251300 Loss: 0.2995, lr 6.985739453173908e-05
Step 251400 Loss: 0.2940, lr 6.985739453173908e-05
Step 251500 Loss: 0.2905, lr 6.985739453173908e-05
Step 251600 Loss: 0.2864, lr 6.985739453173908e-05
Step 251700 Loss: 0.2827, lr 6.985739453173908e-05
Step 251800 Loss: 0.2792, lr 6.985739453173908e-05
Step 251900 Loss: 0.2760, lr 6.985739453173908e-05
Step 252000 Loss: 0.2721, lr 6.985739453173908e-05
Step 252100 Loss: 0.2691, lr 6.985739453173908e-05
Step 252200 Loss: 0.2661, lr 6.985739453173908e-05
Step 252300 Loss: 0.2631, lr 6.985739453173908e-05
Step 252400 Loss: 0.2601, lr 6.985739453173908e-05
Step 252500 Loss: 0.2575, lr 6.985739453173908e-05
Step 252600 Loss: 0.2549, lr 6.985739453173908e-05
Step 252700 Loss: 0.2527, lr 6.985739453173908e-05
Step 252800 Loss: 0.2502, lr 6.985739453173908e-05
Step 252900 Loss: 0.2483, lr 6.985739453173908e-05
Step 253000 Loss: 0.2470, lr 6.985739453173908e-05
Step 253100 Loss: 0.2451, lr 6.985739453173908e-05
Step 253200 Loss: 0.2428, lr 6.985739453173908e-05
Step 253300 Loss: 0.2407, lr 6.985739453173908e-05
Step 253400 Loss: 0.2388, lr 6.985739453173908e-05
Step 253500 Loss: 0.2372, lr 6.985739453173908e-05
Step 253600 Loss: 0.2354, lr 6.985739453173908e-05
Step 253700 Loss: 0.2335, lr 6.985739453173908e-05
Step 253800 Loss: 0.2324, lr 6.985739453173908e-05
Step 253900 Loss: 0.2312, lr 6.985739453173908e-05
Step 254000 Loss: 0.2302, lr 6.985739453173908e-05
Step 254100 Loss: 0.2291, lr 6.985739453173908e-05
Step 254200 Loss: 0.2282, lr 6.985739453173908e-05
Step 254300 Loss: 0.2272, lr 6.985739453173908e-05
Step 254400 Loss: 0.2261, lr 6.985739453173908e-05
Step 254500 Loss: 0.2250, lr 6.985739453173908e-05
Step 254600 Loss: 0.2240, lr 6.985739453173908e-05
Step 254700 Loss: 0.2232, lr 6.985739453173908e-05
Step 254800 Loss: 0.2228, lr 6.985739453173908e-05
Step 254900 Loss: 0.2224, lr 6.985739453173908e-05
Step 255000 Loss: 0.2219, lr 6.985739453173908e-05
Step 255100 Loss: 0.2218, lr 6.985739453173908e-05
Step 255200 Loss: 0.2213, lr 6.985739453173908e-05
Step 255300 Loss: 0.2214, lr 6.985739453173908e-05
Step 255400 Loss: 0.2211, lr 6.985739453173908e-05
Step 255500 Loss: 0.2211, lr 6.985739453173908e-05
Step 255600 Loss: 0.2211, lr 6.985739453173908e-05
Step 255700 Loss: 0.2213, lr 6.985739453173908e-05
Step 255800 Loss: 0.2217, lr 6.985739453173908e-05
Step 255900 Loss: 0.2222, lr 6.985739453173908e-05
Step 256000 Loss: 0.2227, lr 6.985739453173908e-05
Step 256100 Loss: 0.2232, lr 6.985739453173908e-05
Step 256200 Loss: 0.2245, lr 6.985739453173908e-05
Step 256300 Loss: 0.2254, lr 6.985739453173908e-05
Step 256400 Loss: 0.2265, lr 6.985739453173908e-05
Step 256500 Loss: 0.2272, lr 6.985739453173908e-05
Step 256600 Loss: 0.2270, lr 6.985739453173908e-05
Step 256700 Loss: 0.2268, lr 6.985739453173908e-05
Train Epoch: [38/100] Loss: 0.2272,lr 0.000070
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 256800 Loss: 0.5838, lr 6.840622763423397e-05
Step 256900 Loss: 0.4688, lr 6.840622763423397e-05
Step 257000 Loss: 0.4215, lr 6.840622763423397e-05
Step 257100 Loss: 0.3926, lr 6.840622763423397e-05
Step 257200 Loss: 0.3765, lr 6.840622763423397e-05
Step 257300 Loss: 0.3606, lr 6.840622763423397e-05
Step 257400 Loss: 0.3467, lr 6.840622763423397e-05
Step 257500 Loss: 0.3368, lr 6.840622763423397e-05
Step 257600 Loss: 0.3279, lr 6.840622763423397e-05
Step 257700 Loss: 0.3192, lr 6.840622763423397e-05
Step 257800 Loss: 0.3120, lr 6.840622763423397e-05
Step 257900 Loss: 0.3057, lr 6.840622763423397e-05
Step 258000 Loss: 0.3006, lr 6.840622763423397e-05
Step 258100 Loss: 0.2944, lr 6.840622763423397e-05
Step 258200 Loss: 0.2899, lr 6.840622763423397e-05
Step 258300 Loss: 0.2864, lr 6.840622763423397e-05
Step 258400 Loss: 0.2827, lr 6.840622763423397e-05
Step 258500 Loss: 0.2791, lr 6.840622763423397e-05
Step 258600 Loss: 0.2759, lr 6.840622763423397e-05
Step 258700 Loss: 0.2730, lr 6.840622763423397e-05
Step 258800 Loss: 0.2690, lr 6.840622763423397e-05
Step 258900 Loss: 0.2662, lr 6.840622763423397e-05
Step 259000 Loss: 0.2636, lr 6.840622763423397e-05
Step 259100 Loss: 0.2605, lr 6.840622763423397e-05
Step 259200 Loss: 0.2574, lr 6.840622763423397e-05
Step 259300 Loss: 0.2549, lr 6.840622763423397e-05
Step 259400 Loss: 0.2525, lr 6.840622763423397e-05
Step 259500 Loss: 0.2502, lr 6.840622763423397e-05
Step 259600 Loss: 0.2481, lr 6.840622763423397e-05
Step 259700 Loss: 0.2463, lr 6.840622763423397e-05
Step 259800 Loss: 0.2442, lr 6.840622763423397e-05
Step 259900 Loss: 0.2419, lr 6.840622763423397e-05
Step 260000 Loss: 0.2400, lr 6.840622763423397e-05
Step 260100 Loss: 0.2378, lr 6.840622763423397e-05
Step 260200 Loss: 0.2360, lr 6.840622763423397e-05
Step 260300 Loss: 0.2340, lr 6.840622763423397e-05
Step 260400 Loss: 0.2324, lr 6.840622763423397e-05
Step 260500 Loss: 0.2309, lr 6.840622763423397e-05
Step 260600 Loss: 0.2295, lr 6.840622763423397e-05
Step 260700 Loss: 0.2285, lr 6.840622763423397e-05
Step 260800 Loss: 0.2273, lr 6.840622763423397e-05
Step 260900 Loss: 0.2265, lr 6.840622763423397e-05
Step 261000 Loss: 0.2252, lr 6.840622763423397e-05
Step 261100 Loss: 0.2244, lr 6.840622763423397e-05
Step 261200 Loss: 0.2236, lr 6.840622763423397e-05
Step 261300 Loss: 0.2224, lr 6.840622763423397e-05
Step 261400 Loss: 0.2215, lr 6.840622763423397e-05
Step 261500 Loss: 0.2209, lr 6.840622763423397e-05
Step 261600 Loss: 0.2205, lr 6.840622763423397e-05
Step 261700 Loss: 0.2201, lr 6.840622763423397e-05
Step 261800 Loss: 0.2197, lr 6.840622763423397e-05
Step 261900 Loss: 0.2196, lr 6.840622763423397e-05
Step 262000 Loss: 0.2193, lr 6.840622763423397e-05
Step 262100 Loss: 0.2191, lr 6.840622763423397e-05
Step 262200 Loss: 0.2190, lr 6.840622763423397e-05
Step 262300 Loss: 0.2189, lr 6.840622763423397e-05
Step 262400 Loss: 0.2192, lr 6.840622763423397e-05
Step 262500 Loss: 0.2194, lr 6.840622763423397e-05
Step 262600 Loss: 0.2197, lr 6.840622763423397e-05
Step 262700 Loss: 0.2202, lr 6.840622763423397e-05
Step 262800 Loss: 0.2207, lr 6.840622763423397e-05
Step 262900 Loss: 0.2213, lr 6.840622763423397e-05
Step 263000 Loss: 0.2223, lr 6.840622763423397e-05
Step 263100 Loss: 0.2233, lr 6.840622763423397e-05
Step 263200 Loss: 0.2242, lr 6.840622763423397e-05
Step 263300 Loss: 0.2244, lr 6.840622763423397e-05
Step 263400 Loss: 0.2241, lr 6.840622763423397e-05
Train Epoch: [39/100] Loss: 0.2243,lr 0.000068
Calling G2SDataset.batch()
Done, time:  2.16 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.86 s, total batches: 6756
Step 263500 Loss: 0.9248, lr 6.693689601226464e-05
Step 263600 Loss: 0.5282, lr 6.693689601226464e-05
Step 263700 Loss: 0.4504, lr 6.693689601226464e-05
Step 263800 Loss: 0.4096, lr 6.693689601226464e-05
Step 263900 Loss: 0.3863, lr 6.693689601226464e-05
Step 264000 Loss: 0.3708, lr 6.693689601226464e-05
Step 264100 Loss: 0.3545, lr 6.693689601226464e-05
Step 264200 Loss: 0.3419, lr 6.693689601226464e-05
Step 264300 Loss: 0.3328, lr 6.693689601226464e-05
Step 264400 Loss: 0.3231, lr 6.693689601226464e-05
Step 264500 Loss: 0.3153, lr 6.693689601226464e-05
Step 264600 Loss: 0.3087, lr 6.693689601226464e-05
Step 264700 Loss: 0.3023, lr 6.693689601226464e-05
Step 264800 Loss: 0.2971, lr 6.693689601226464e-05
Step 264900 Loss: 0.2917, lr 6.693689601226464e-05
Step 265000 Loss: 0.2883, lr 6.693689601226464e-05
Step 265100 Loss: 0.2843, lr 6.693689601226464e-05
Step 265200 Loss: 0.2806, lr 6.693689601226464e-05
Step 265300 Loss: 0.2772, lr 6.693689601226464e-05
Step 265400 Loss: 0.2739, lr 6.693689601226464e-05
Step 265500 Loss: 0.2700, lr 6.693689601226464e-05
Step 265600 Loss: 0.2667, lr 6.693689601226464e-05
Step 265700 Loss: 0.2638, lr 6.693689601226464e-05
Step 265800 Loss: 0.2607, lr 6.693689601226464e-05
Step 265900 Loss: 0.2580, lr 6.693689601226464e-05
Step 266000 Loss: 0.2551, lr 6.693689601226464e-05
Step 266100 Loss: 0.2525, lr 6.693689601226464e-05
Step 266200 Loss: 0.2504, lr 6.693689601226464e-05
Step 266300 Loss: 0.2481, lr 6.693689601226464e-05
Step 266400 Loss: 0.2459, lr 6.693689601226464e-05
Step 266500 Loss: 0.2441, lr 6.693689601226464e-05
Step 266600 Loss: 0.2422, lr 6.693689601226464e-05
Step 266700 Loss: 0.2400, lr 6.693689601226464e-05
Step 266800 Loss: 0.2376, lr 6.693689601226464e-05
Step 266900 Loss: 0.2358, lr 6.693689601226464e-05
Step 267000 Loss: 0.2340, lr 6.693689601226464e-05
Step 267100 Loss: 0.2323, lr 6.693689601226464e-05
Step 267200 Loss: 0.2304, lr 6.693689601226464e-05
Step 267300 Loss: 0.2292, lr 6.693689601226464e-05
Step 267400 Loss: 0.2279, lr 6.693689601226464e-05
Step 267500 Loss: 0.2270, lr 6.693689601226464e-05
Step 267600 Loss: 0.2258, lr 6.693689601226464e-05
Step 267700 Loss: 0.2248, lr 6.693689601226464e-05
Step 267800 Loss: 0.2238, lr 6.693689601226464e-05
Step 267900 Loss: 0.2228, lr 6.693689601226464e-05
Step 268000 Loss: 0.2218, lr 6.693689601226464e-05
Step 268100 Loss: 0.2207, lr 6.693689601226464e-05
Step 268200 Loss: 0.2197, lr 6.693689601226464e-05
Step 268300 Loss: 0.2196, lr 6.693689601226464e-05
Step 268400 Loss: 0.2194, lr 6.693689601226464e-05
Step 268500 Loss: 0.2188, lr 6.693689601226464e-05
Step 268600 Loss: 0.2187, lr 6.693689601226464e-05
Step 268700 Loss: 0.2184, lr 6.693689601226464e-05
Step 268800 Loss: 0.2183, lr 6.693689601226464e-05
Step 268900 Loss: 0.2181, lr 6.693689601226464e-05
Step 269000 Loss: 0.2180, lr 6.693689601226464e-05
Step 269100 Loss: 0.2180, lr 6.693689601226464e-05
Step 269200 Loss: 0.2183, lr 6.693689601226464e-05
Step 269300 Loss: 0.2185, lr 6.693689601226464e-05
Step 269400 Loss: 0.2189, lr 6.693689601226464e-05
Step 269500 Loss: 0.2195, lr 6.693689601226464e-05
Step 269600 Loss: 0.2199, lr 6.693689601226464e-05
Step 269700 Loss: 0.2209, lr 6.693689601226464e-05
Step 269800 Loss: 0.2219, lr 6.693689601226464e-05
Step 269900 Loss: 0.2230, lr 6.693689601226464e-05
Step 270000 Loss: 0.2238, lr 6.693689601226464e-05
Step 270100 Loss: 0.2234, lr 6.693689601226464e-05
Step 270200 Loss: 0.2231, lr 6.693689601226464e-05
Train Epoch: [40/100] Loss: 0.2237,lr 0.000067
Model Saving at epoch 40
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 270300 Loss: 0.5876, lr 6.545084971874744e-05
Step 270400 Loss: 0.4631, lr 6.545084971874744e-05
Step 270500 Loss: 0.4163, lr 6.545084971874744e-05
Step 270600 Loss: 0.3858, lr 6.545084971874744e-05
Step 270700 Loss: 0.3695, lr 6.545084971874744e-05
Step 270800 Loss: 0.3547, lr 6.545084971874744e-05
Step 270900 Loss: 0.3399, lr 6.545084971874744e-05
Step 271000 Loss: 0.3303, lr 6.545084971874744e-05
Step 271100 Loss: 0.3217, lr 6.545084971874744e-05
Step 271200 Loss: 0.3132, lr 6.545084971874744e-05
Step 271300 Loss: 0.3063, lr 6.545084971874744e-05
Step 271400 Loss: 0.3004, lr 6.545084971874744e-05
Step 271500 Loss: 0.2958, lr 6.545084971874744e-05
Step 271600 Loss: 0.2896, lr 6.545084971874744e-05
Step 271700 Loss: 0.2856, lr 6.545084971874744e-05
Step 271800 Loss: 0.2821, lr 6.545084971874744e-05
Step 271900 Loss: 0.2788, lr 6.545084971874744e-05
Step 272000 Loss: 0.2750, lr 6.545084971874744e-05
Step 272100 Loss: 0.2722, lr 6.545084971874744e-05
Step 272200 Loss: 0.2688, lr 6.545084971874744e-05
Step 272300 Loss: 0.2649, lr 6.545084971874744e-05
Step 272400 Loss: 0.2619, lr 6.545084971874744e-05
Step 272500 Loss: 0.2595, lr 6.545084971874744e-05
Step 272600 Loss: 0.2565, lr 6.545084971874744e-05
Step 272700 Loss: 0.2535, lr 6.545084971874744e-05
Step 272800 Loss: 0.2512, lr 6.545084971874744e-05
Step 272900 Loss: 0.2486, lr 6.545084971874744e-05
Step 273000 Loss: 0.2466, lr 6.545084971874744e-05
Step 273100 Loss: 0.2443, lr 6.545084971874744e-05
Step 273200 Loss: 0.2426, lr 6.545084971874744e-05
Step 273300 Loss: 0.2404, lr 6.545084971874744e-05
Step 273400 Loss: 0.2382, lr 6.545084971874744e-05
Step 273500 Loss: 0.2365, lr 6.545084971874744e-05
Step 273600 Loss: 0.2341, lr 6.545084971874744e-05
Step 273700 Loss: 0.2324, lr 6.545084971874744e-05
Step 273800 Loss: 0.2305, lr 6.545084971874744e-05
Step 273900 Loss: 0.2289, lr 6.545084971874744e-05
Step 274000 Loss: 0.2273, lr 6.545084971874744e-05
Step 274100 Loss: 0.2259, lr 6.545084971874744e-05
Step 274200 Loss: 0.2247, lr 6.545084971874744e-05
Step 274300 Loss: 0.2238, lr 6.545084971874744e-05
Step 274400 Loss: 0.2228, lr 6.545084971874744e-05
Step 274500 Loss: 0.2217, lr 6.545084971874744e-05
Step 274600 Loss: 0.2206, lr 6.545084971874744e-05
Step 274700 Loss: 0.2199, lr 6.545084971874744e-05
Step 274800 Loss: 0.2187, lr 6.545084971874744e-05
Step 274900 Loss: 0.2177, lr 6.545084971874744e-05
Step 275000 Loss: 0.2172, lr 6.545084971874744e-05
Step 275100 Loss: 0.2168, lr 6.545084971874744e-05
Step 275200 Loss: 0.2164, lr 6.545084971874744e-05
Step 275300 Loss: 0.2159, lr 6.545084971874744e-05
Step 275400 Loss: 0.2159, lr 6.545084971874744e-05
Step 275500 Loss: 0.2155, lr 6.545084971874744e-05
Step 275600 Loss: 0.2154, lr 6.545084971874744e-05
Step 275700 Loss: 0.2153, lr 6.545084971874744e-05
Step 275800 Loss: 0.2152, lr 6.545084971874744e-05
Step 275900 Loss: 0.2153, lr 6.545084971874744e-05
Step 276000 Loss: 0.2155, lr 6.545084971874744e-05
Step 276100 Loss: 0.2159, lr 6.545084971874744e-05
Step 276200 Loss: 0.2164, lr 6.545084971874744e-05
Step 276300 Loss: 0.2168, lr 6.545084971874744e-05
Step 276400 Loss: 0.2175, lr 6.545084971874744e-05
Step 276500 Loss: 0.2186, lr 6.545084971874744e-05
Step 276600 Loss: 0.2194, lr 6.545084971874744e-05
Step 276700 Loss: 0.2206, lr 6.545084971874744e-05
Step 276800 Loss: 0.2210, lr 6.545084971874744e-05
Step 276900 Loss: 0.2205, lr 6.545084971874744e-05
Train Epoch: [41/100] Loss: 0.2209,lr 0.000065
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Step 277000 Loss: 1.1035, lr 6.394955530196154e-05
Step 277100 Loss: 0.5306, lr 6.394955530196154e-05
Step 277200 Loss: 0.4494, lr 6.394955530196154e-05
Step 277300 Loss: 0.4066, lr 6.394955530196154e-05
Step 277400 Loss: 0.3837, lr 6.394955530196154e-05
Step 277500 Loss: 0.3669, lr 6.394955530196154e-05
Step 277600 Loss: 0.3507, lr 6.394955530196154e-05
Step 277700 Loss: 0.3375, lr 6.394955530196154e-05
Step 277800 Loss: 0.3292, lr 6.394955530196154e-05
Step 277900 Loss: 0.3196, lr 6.394955530196154e-05
Step 278000 Loss: 0.3113, lr 6.394955530196154e-05
Step 278100 Loss: 0.3055, lr 6.394955530196154e-05
Step 278200 Loss: 0.2993, lr 6.394955530196154e-05
Step 278300 Loss: 0.2939, lr 6.394955530196154e-05
Step 278400 Loss: 0.2888, lr 6.394955530196154e-05
Step 278500 Loss: 0.2851, lr 6.394955530196154e-05
Step 278600 Loss: 0.2809, lr 6.394955530196154e-05
Step 278700 Loss: 0.2777, lr 6.394955530196154e-05
Step 278800 Loss: 0.2744, lr 6.394955530196154e-05
Step 278900 Loss: 0.2713, lr 6.394955530196154e-05
Step 279000 Loss: 0.2673, lr 6.394955530196154e-05
Step 279100 Loss: 0.2636, lr 6.394955530196154e-05
Step 279200 Loss: 0.2612, lr 6.394955530196154e-05
Step 279300 Loss: 0.2583, lr 6.394955530196154e-05
Step 279400 Loss: 0.2557, lr 6.394955530196154e-05
Step 279500 Loss: 0.2527, lr 6.394955530196154e-05
Step 279600 Loss: 0.2501, lr 6.394955530196154e-05
Step 279700 Loss: 0.2481, lr 6.394955530196154e-05
Step 279800 Loss: 0.2460, lr 6.394955530196154e-05
Step 279900 Loss: 0.2438, lr 6.394955530196154e-05
Step 280000 Loss: 0.2420, lr 6.394955530196154e-05
Step 280100 Loss: 0.2398, lr 6.394955530196154e-05
Step 280200 Loss: 0.2377, lr 6.394955530196154e-05
Step 280300 Loss: 0.2357, lr 6.394955530196154e-05
