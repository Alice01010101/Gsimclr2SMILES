Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = **
**** embed_size = *1024*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *1024*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *1024*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *1024*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *100*
**** max_steps = *300000*
**** warmup_steps = *100*
**** lr = *0.0001*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=1129, out_features=1024, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=1138, out_features=1024, bias=True)
        (W_r): Linear(in_features=114, out_features=1024, bias=False)
        (U_r): Linear(in_features=1024, out_features=1024, bias=True)
        (W_h): Linear(in_features=1138, out_features=1024, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=1024, bias=True)
        (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
        (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=1024, bias=True)
      (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
      (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (g): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=256, bias=False)
    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 9774736
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.19 s, total batches: 6756
Step 100 Loss: 1.1445, lr 0.0001
Step 200 Loss: 1.0073, lr 0.0001
Step 300 Loss: 0.9187, lr 0.0001
Step 400 Loss: 0.8735, lr 0.0001
Step 500 Loss: 0.8460, lr 0.0001
Step 600 Loss: 0.8024, lr 0.0001
Step 700 Loss: 0.7685, lr 0.0001
Step 800 Loss: 0.7428, lr 0.0001
Step 900 Loss: 0.7228, lr 0.0001
Step 1000 Loss: 0.7067, lr 0.0001
Step 1100 Loss: 0.6935, lr 0.0001
Step 1200 Loss: 0.6746, lr 0.0001
Step 1300 Loss: 0.6583, lr 0.0001
Step 1400 Loss: 0.6444, lr 0.0001
Step 1500 Loss: 0.6323, lr 0.0001
Step 1600 Loss: 0.6216, lr 0.0001
Step 1700 Loss: 0.6122, lr 0.0001
Step 1800 Loss: 0.6039, lr 0.0001
Step 1900 Loss: 0.5907, lr 0.0001
Step 2000 Loss: 0.5729, lr 0.0001
Step 2100 Loss: 0.5568, lr 0.0001
Step 2200 Loss: 0.5422, lr 0.0001
Step 2300 Loss: 0.5288, lr 0.0001
Step 2400 Loss: 0.5166, lr 0.0001
Step 2500 Loss: 0.5053, lr 0.0001
Step 2600 Loss: 0.4949, lr 0.0001
Step 2700 Loss: 0.4852, lr 0.0001
Step 2800 Loss: 0.4735, lr 0.0001
Step 2900 Loss: 0.4606, lr 0.0001
Step 3000 Loss: 0.4485, lr 0.0001
Step 3100 Loss: 0.4372, lr 0.0001
Step 3200 Loss: 0.4266, lr 0.0001
Step 3300 Loss: 0.4167, lr 0.0001
Step 3400 Loss: 0.4073, lr 0.0001
Step 3500 Loss: 0.3985, lr 0.0001
Step 3600 Loss: 0.3902, lr 0.0001
Step 3700 Loss: 0.3823, lr 0.0001
Step 3800 Loss: 0.3748, lr 0.0001
Step 3900 Loss: 0.3677, lr 0.0001
Step 4000 Loss: 0.3609, lr 0.0001
Step 4100 Loss: 0.3545, lr 0.0001
Step 4200 Loss: 0.3484, lr 0.0001
Step 4300 Loss: 0.3412, lr 0.0001
Step 4400 Loss: 0.3321, lr 0.0001
Step 4500 Loss: 0.3234, lr 0.0001
Step 4600 Loss: 0.3151, lr 0.0001
Step 4700 Loss: 0.3071, lr 0.0001
Step 4800 Loss: 0.2995, lr 0.0001
Step 4900 Loss: 0.2922, lr 0.0001
Step 5000 Loss: 0.2851, lr 0.0001
Step 5100 Loss: 0.2784, lr 0.0001
Step 5200 Loss: 0.2719, lr 0.0001
Step 5300 Loss: 0.2628, lr 0.0001
Step 5400 Loss: 0.2534, lr 0.0001
Step 5500 Loss: 0.2444, lr 0.0001
Step 5600 Loss: 0.2356, lr 0.0001
Step 5700 Loss: 0.2272, lr 0.0001
Step 5800 Loss: 0.2190, lr 0.0001
Step 5900 Loss: 0.2112, lr 0.0001
Step 6000 Loss: 0.2005, lr 0.0001
Step 6100 Loss: 0.1894, lr 0.0001
Step 6200 Loss: 0.1787, lr 0.0001
Step 6300 Loss: 0.1684, lr 0.0001
Step 6400 Loss: 0.1564, lr 0.0001
Step 6500 Loss: 0.1421, lr 0.0001
Step 6600 Loss: 0.1283, lr 0.0001
Step 6700 Loss: 0.1143, lr 0.0001
Train Epoch: [1/100] Loss: 0.1032,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6756
Step 6800 Loss: 1.0976, lr 9.997532801828658e-05
Step 6900 Loss: 0.9843, lr 9.997532801828658e-05
Step 7000 Loss: 0.8872, lr 9.997532801828658e-05
Step 7100 Loss: 0.8397, lr 9.997532801828658e-05
Step 7200 Loss: 0.8136, lr 9.997532801828658e-05
Step 7300 Loss: 0.7868, lr 9.997532801828658e-05
Step 7400 Loss: 0.7508, lr 9.997532801828658e-05
Step 7500 Loss: 0.7245, lr 9.997532801828658e-05
Step 7600 Loss: 0.7044, lr 9.997532801828658e-05
Step 7700 Loss: 0.6885, lr 9.997532801828658e-05
Step 7800 Loss: 0.6757, lr 9.997532801828658e-05
Step 7900 Loss: 0.6615, lr 9.997532801828658e-05
Step 8000 Loss: 0.6452, lr 9.997532801828658e-05
Step 8100 Loss: 0.6313, lr 9.997532801828658e-05
Step 8200 Loss: 0.6193, lr 9.997532801828658e-05
Step 8300 Loss: 0.6088, lr 9.997532801828658e-05
Step 8400 Loss: 0.5997, lr 9.997532801828658e-05
Step 8500 Loss: 0.5915, lr 9.997532801828658e-05
Step 8600 Loss: 0.5843, lr 9.997532801828658e-05
Step 8700 Loss: 0.5671, lr 9.997532801828658e-05
Step 8800 Loss: 0.5507, lr 9.997532801828658e-05
Step 8900 Loss: 0.5359, lr 9.997532801828658e-05
Step 9000 Loss: 0.5223, lr 9.997532801828658e-05
Step 9100 Loss: 0.5100, lr 9.997532801828658e-05
Step 9200 Loss: 0.4986, lr 9.997532801828658e-05
Step 9300 Loss: 0.4881, lr 9.997532801828658e-05
Step 9400 Loss: 0.4784, lr 9.997532801828658e-05
Step 9500 Loss: 0.4694, lr 9.997532801828658e-05
Step 9600 Loss: 0.4563, lr 9.997532801828658e-05
Step 9700 Loss: 0.4441, lr 9.997532801828658e-05
Step 9800 Loss: 0.4327, lr 9.997532801828658e-05
Step 9900 Loss: 0.4220, lr 9.997532801828658e-05
Step 10000 Loss: 0.4120, lr 9.997532801828658e-05
Step 10100 Loss: 0.4025, lr 9.997532801828658e-05
Step 10200 Loss: 0.3937, lr 9.997532801828658e-05
Step 10300 Loss: 0.3853, lr 9.997532801828658e-05
Step 10400 Loss: 0.3774, lr 9.997532801828658e-05
Step 10500 Loss: 0.3699, lr 9.997532801828658e-05
Step 10600 Loss: 0.3628, lr 9.997532801828658e-05
Step 10700 Loss: 0.3560, lr 9.997532801828658e-05
Step 10800 Loss: 0.3496, lr 9.997532801828658e-05
Step 10900 Loss: 0.3435, lr 9.997532801828658e-05
Step 11000 Loss: 0.3377, lr 9.997532801828658e-05
Step 11100 Loss: 0.3291, lr 9.997532801828658e-05
Step 11200 Loss: 0.3204, lr 9.997532801828658e-05
Step 11300 Loss: 0.3120, lr 9.997532801828658e-05
Step 11400 Loss: 0.3040, lr 9.997532801828658e-05
Step 11500 Loss: 0.2963, lr 9.997532801828658e-05
Step 11600 Loss: 0.2890, lr 9.997532801828658e-05
Step 11700 Loss: 0.2819, lr 9.997532801828658e-05
Step 11800 Loss: 0.2751, lr 9.997532801828658e-05
Step 11900 Loss: 0.2686, lr 9.997532801828658e-05
Step 12000 Loss: 0.2615, lr 9.997532801828658e-05
Step 12100 Loss: 0.2520, lr 9.997532801828658e-05
Step 12200 Loss: 0.2428, lr 9.997532801828658e-05
Step 12300 Loss: 0.2340, lr 9.997532801828658e-05
Step 12400 Loss: 0.2255, lr 9.997532801828658e-05
Step 12500 Loss: 0.2173, lr 9.997532801828658e-05
Step 12600 Loss: 0.2094, lr 9.997532801828658e-05
Step 12700 Loss: 0.2009, lr 9.997532801828658e-05
Step 12800 Loss: 0.1897, lr 9.997532801828658e-05
Step 12900 Loss: 0.1789, lr 9.997532801828658e-05
Step 13000 Loss: 0.1684, lr 9.997532801828658e-05
Step 13100 Loss: 0.1583, lr 9.997532801828658e-05
Step 13200 Loss: 0.1445, lr 9.997532801828658e-05
Step 13300 Loss: 0.1305, lr 9.997532801828658e-05
Step 13400 Loss: 0.1169, lr 9.997532801828658e-05
Step 13500 Loss: 0.1004, lr 9.997532801828658e-05
Train Epoch: [2/100] Loss: 0.0978,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 13600 Loss: 1.0144, lr 9.990133642141359e-05
Step 13700 Loss: 0.9355, lr 9.990133642141359e-05
Step 13800 Loss: 0.8618, lr 9.990133642141359e-05
Step 13900 Loss: 0.8260, lr 9.990133642141359e-05
Step 14000 Loss: 0.8048, lr 9.990133642141359e-05
Step 14100 Loss: 0.7688, lr 9.990133642141359e-05
Step 14200 Loss: 0.7376, lr 9.990133642141359e-05
Step 14300 Loss: 0.7143, lr 9.990133642141359e-05
Step 14400 Loss: 0.6962, lr 9.990133642141359e-05
Step 14500 Loss: 0.6818, lr 9.990133642141359e-05
Step 14600 Loss: 0.6700, lr 9.990133642141359e-05
Step 14700 Loss: 0.6532, lr 9.990133642141359e-05
Step 14800 Loss: 0.6380, lr 9.990133642141359e-05
Step 14900 Loss: 0.6250, lr 9.990133642141359e-05
Step 15000 Loss: 0.6138, lr 9.990133642141359e-05
Step 15100 Loss: 0.6039, lr 9.990133642141359e-05
Step 15200 Loss: 0.5952, lr 9.990133642141359e-05
Step 15300 Loss: 0.5875, lr 9.990133642141359e-05
Step 15400 Loss: 0.5763, lr 9.990133642141359e-05
Step 15500 Loss: 0.5590, lr 9.990133642141359e-05
Step 15600 Loss: 0.5433, lr 9.990133642141359e-05
Step 15700 Loss: 0.5291, lr 9.990133642141359e-05
Step 15800 Loss: 0.5161, lr 9.990133642141359e-05
Step 15900 Loss: 0.5041, lr 9.990133642141359e-05
Step 16000 Loss: 0.4932, lr 9.990133642141359e-05
Step 16100 Loss: 0.4831, lr 9.990133642141359e-05
Step 16200 Loss: 0.4737, lr 9.990133642141359e-05
Step 16300 Loss: 0.4629, lr 9.990133642141359e-05
Step 16400 Loss: 0.4502, lr 9.990133642141359e-05
Step 16500 Loss: 0.4383, lr 9.990133642141359e-05
Step 16600 Loss: 0.4272, lr 9.990133642141359e-05
Step 16700 Loss: 0.4169, lr 9.990133642141359e-05
Step 16800 Loss: 0.4071, lr 9.990133642141359e-05
Step 16900 Loss: 0.3979, lr 9.990133642141359e-05
Step 17000 Loss: 0.3893, lr 9.990133642141359e-05
Step 17100 Loss: 0.3811, lr 9.990133642141359e-05
Step 17200 Loss: 0.3734, lr 9.990133642141359e-05
Step 17300 Loss: 0.3661, lr 9.990133642141359e-05
Step 17400 Loss: 0.3591, lr 9.990133642141359e-05
Step 17500 Loss: 0.3525, lr 9.990133642141359e-05
Step 17600 Loss: 0.3463, lr 9.990133642141359e-05
Step 17700 Loss: 0.3403, lr 9.990133642141359e-05
Step 17800 Loss: 0.3336, lr 9.990133642141359e-05
Step 17900 Loss: 0.3246, lr 9.990133642141359e-05
Step 18000 Loss: 0.3161, lr 9.990133642141359e-05
Step 18100 Loss: 0.3078, lr 9.990133642141359e-05
Step 18200 Loss: 0.3000, lr 9.990133642141359e-05
Step 18300 Loss: 0.2925, lr 9.990133642141359e-05
Step 18400 Loss: 0.2852, lr 9.990133642141359e-05
Step 18500 Loss: 0.2783, lr 9.990133642141359e-05
Step 18600 Loss: 0.2716, lr 9.990133642141359e-05
Step 18700 Loss: 0.2652, lr 9.990133642141359e-05
Step 18800 Loss: 0.2567, lr 9.990133642141359e-05
Step 18900 Loss: 0.2473, lr 9.990133642141359e-05
Step 19000 Loss: 0.2383, lr 9.990133642141359e-05
Step 19100 Loss: 0.2297, lr 9.990133642141359e-05
Step 19200 Loss: 0.2213, lr 9.990133642141359e-05
Step 19300 Loss: 0.2132, lr 9.990133642141359e-05
Step 19400 Loss: 0.2054, lr 9.990133642141359e-05
Step 19500 Loss: 0.1953, lr 9.990133642141359e-05
Step 19600 Loss: 0.1843, lr 9.990133642141359e-05
Step 19700 Loss: 0.1736, lr 9.990133642141359e-05
Step 19800 Loss: 0.1633, lr 9.990133642141359e-05
Step 19900 Loss: 0.1519, lr 9.990133642141359e-05
Step 20000 Loss: 0.1376, lr 9.990133642141359e-05
Step 20100 Loss: 0.1238, lr 9.990133642141359e-05
Step 20200 Loss: 0.1105, lr 9.990133642141359e-05
Train Epoch: [3/100] Loss: 0.0974,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6755
Step 20300 Loss: 1.1544, lr 9.977809823015401e-05
Step 20400 Loss: 0.9864, lr 9.977809823015401e-05
Step 20500 Loss: 0.8944, lr 9.977809823015401e-05
Step 20600 Loss: 0.8426, lr 9.977809823015401e-05
Step 20700 Loss: 0.8147, lr 9.977809823015401e-05
Step 20800 Loss: 0.7907, lr 9.977809823015401e-05
Step 20900 Loss: 0.7531, lr 9.977809823015401e-05
Step 21000 Loss: 0.7259, lr 9.977809823015401e-05
Step 21100 Loss: 0.7051, lr 9.977809823015401e-05
Step 21200 Loss: 0.6888, lr 9.977809823015401e-05
Step 21300 Loss: 0.6757, lr 9.977809823015401e-05
Step 21400 Loss: 0.6623, lr 9.977809823015401e-05
Step 21500 Loss: 0.6457, lr 9.977809823015401e-05
Step 21600 Loss: 0.6315, lr 9.977809823015401e-05
Step 21700 Loss: 0.6193, lr 9.977809823015401e-05
Step 21800 Loss: 0.6087, lr 9.977809823015401e-05
Step 21900 Loss: 0.5994, lr 9.977809823015401e-05
Step 22000 Loss: 0.5912, lr 9.977809823015401e-05
Step 22100 Loss: 0.5839, lr 9.977809823015401e-05
Step 22200 Loss: 0.5680, lr 9.977809823015401e-05
Step 22300 Loss: 0.5514, lr 9.977809823015401e-05
Step 22400 Loss: 0.5364, lr 9.977809823015401e-05
Step 22500 Loss: 0.5227, lr 9.977809823015401e-05
Step 22600 Loss: 0.5102, lr 9.977809823015401e-05
Step 22700 Loss: 0.4987, lr 9.977809823015401e-05
Step 22800 Loss: 0.4882, lr 9.977809823015401e-05
Step 22900 Loss: 0.4784, lr 9.977809823015401e-05
Step 23000 Loss: 0.4694, lr 9.977809823015401e-05
Step 23100 Loss: 0.4567, lr 9.977809823015401e-05
Step 23200 Loss: 0.4444, lr 9.977809823015401e-05
Step 23300 Loss: 0.4329, lr 9.977809823015401e-05
Step 23400 Loss: 0.4222, lr 9.977809823015401e-05
Step 23500 Loss: 0.4121, lr 9.977809823015401e-05
Step 23600 Loss: 0.4026, lr 9.977809823015401e-05
Step 23700 Loss: 0.3937, lr 9.977809823015401e-05
Step 23800 Loss: 0.3852, lr 9.977809823015401e-05
Step 23900 Loss: 0.3773, lr 9.977809823015401e-05
Step 24000 Loss: 0.3697, lr 9.977809823015401e-05
Step 24100 Loss: 0.3626, lr 9.977809823015401e-05
Step 24200 Loss: 0.3558, lr 9.977809823015401e-05
Step 24300 Loss: 0.3493, lr 9.977809823015401e-05
Step 24400 Loss: 0.3432, lr 9.977809823015401e-05
Step 24500 Loss: 0.3374, lr 9.977809823015401e-05
Step 24600 Loss: 0.3292, lr 9.977809823015401e-05
Step 24700 Loss: 0.3204, lr 9.977809823015401e-05
Step 24800 Loss: 0.3120, lr 9.977809823015401e-05
Step 24900 Loss: 0.3040, lr 9.977809823015401e-05
Step 25000 Loss: 0.2963, lr 9.977809823015401e-05
Step 25100 Loss: 0.2889, lr 9.977809823015401e-05
Step 25200 Loss: 0.2818, lr 9.977809823015401e-05
Step 25300 Loss: 0.2750, lr 9.977809823015401e-05
Step 25400 Loss: 0.2684, lr 9.977809823015401e-05
Step 25500 Loss: 0.2617, lr 9.977809823015401e-05
Step 25600 Loss: 0.2522, lr 9.977809823015401e-05
Step 25700 Loss: 0.2430, lr 9.977809823015401e-05
Step 25800 Loss: 0.2341, lr 9.977809823015401e-05
Step 25900 Loss: 0.2256, lr 9.977809823015401e-05
Step 26000 Loss: 0.2174, lr 9.977809823015401e-05
Step 26100 Loss: 0.2094, lr 9.977809823015401e-05
Step 26200 Loss: 0.2013, lr 9.977809823015401e-05
Step 26300 Loss: 0.1900, lr 9.977809823015401e-05
Step 26400 Loss: 0.1792, lr 9.977809823015401e-05
Step 26500 Loss: 0.1687, lr 9.977809823015401e-05
Step 26600 Loss: 0.1585, lr 9.977809823015401e-05
Step 26700 Loss: 0.1452, lr 9.977809823015401e-05
Step 26800 Loss: 0.1312, lr 9.977809823015401e-05
Step 26900 Loss: 0.1176, lr 9.977809823015401e-05
Step 27000 Loss: 0.1018, lr 9.977809823015401e-05
Train Epoch: [4/100] Loss: 0.0973,lr 0.000100
Calling G2SDataset.batch()
Done, time:  1.77 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6755
Step 27100 Loss: 1.0246, lr 9.960573506572391e-05
Step 27200 Loss: 0.9466, lr 9.960573506572391e-05
Step 27300 Loss: 0.8656, lr 9.960573506572391e-05
Step 27400 Loss: 0.8275, lr 9.960573506572391e-05
Step 27500 Loss: 0.8054, lr 9.960573506572391e-05
Step 27600 Loss: 0.7716, lr 9.960573506572391e-05
Step 27700 Loss: 0.7394, lr 9.960573506572391e-05
Step 27800 Loss: 0.7154, lr 9.960573506572391e-05
Step 27900 Loss: 0.6969, lr 9.960573506572391e-05
Step 28000 Loss: 0.6822, lr 9.960573506572391e-05
Step 28100 Loss: 0.6702, lr 9.960573506572391e-05
Step 28200 Loss: 0.6541, lr 9.960573506572391e-05
Step 28300 Loss: 0.6387, lr 9.960573506572391e-05
Step 28400 Loss: 0.6255, lr 9.960573506572391e-05
Step 28500 Loss: 0.6140, lr 9.960573506572391e-05
Step 28600 Loss: 0.6041, lr 9.960573506572391e-05
Step 28700 Loss: 0.5953, lr 9.960573506572391e-05
Step 28800 Loss: 0.5875, lr 9.960573506572391e-05
Step 28900 Loss: 0.5776, lr 9.960573506572391e-05
Step 29000 Loss: 0.5601, lr 9.960573506572391e-05
Step 29100 Loss: 0.5442, lr 9.960573506572391e-05
Step 29200 Loss: 0.5298, lr 9.960573506572391e-05
Step 29300 Loss: 0.5167, lr 9.960573506572391e-05
Step 29400 Loss: 0.5047, lr 9.960573506572391e-05
Step 29500 Loss: 0.4937, lr 9.960573506572391e-05
Step 29600 Loss: 0.4835, lr 9.960573506572391e-05
Step 29700 Loss: 0.4740, lr 9.960573506572391e-05
Step 29800 Loss: 0.4636, lr 9.960573506572391e-05
Step 29900 Loss: 0.4509, lr 9.960573506572391e-05
Step 30000 Loss: 0.4389, lr 9.960573506572391e-05
Step 30100 Loss: 0.4278, lr 9.960573506572391e-05
Step 30200 Loss: 0.4173, lr 9.960573506572391e-05
Step 30300 Loss: 0.4075, lr 9.960573506572391e-05
Step 30400 Loss: 0.3983, lr 9.960573506572391e-05
Step 30500 Loss: 0.3896, lr 9.960573506572391e-05
Step 30600 Loss: 0.3814, lr 9.960573506572391e-05
Step 30700 Loss: 0.3736, lr 9.960573506572391e-05
Step 30800 Loss: 0.3663, lr 9.960573506572391e-05
Step 30900 Loss: 0.3593, lr 9.960573506572391e-05
Step 31000 Loss: 0.3527, lr 9.960573506572391e-05
Step 31100 Loss: 0.3463, lr 9.960573506572391e-05
Step 31200 Loss: 0.3404, lr 9.960573506572391e-05
Step 31300 Loss: 0.3340, lr 9.960573506572391e-05
Step 31400 Loss: 0.3250, lr 9.960573506572391e-05
Step 31500 Loss: 0.3164, lr 9.960573506572391e-05
Step 31600 Loss: 0.3082, lr 9.960573506572391e-05
Step 31700 Loss: 0.3003, lr 9.960573506572391e-05
Step 31800 Loss: 0.2927, lr 9.960573506572391e-05
Step 31900 Loss: 0.2855, lr 9.960573506572391e-05
Step 32000 Loss: 0.2785, lr 9.960573506572391e-05
Step 32100 Loss: 0.2718, lr 9.960573506572391e-05
Step 32200 Loss: 0.2654, lr 9.960573506572391e-05
Step 32300 Loss: 0.2572, lr 9.960573506572391e-05
Step 32400 Loss: 0.2478, lr 9.960573506572391e-05
Step 32500 Loss: 0.2388, lr 9.960573506572391e-05
Step 32600 Loss: 0.2301, lr 9.960573506572391e-05
Step 32700 Loss: 0.2217, lr 9.960573506572391e-05
Step 32800 Loss: 0.2136, lr 9.960573506572391e-05
Step 32900 Loss: 0.2057, lr 9.960573506572391e-05
Step 33000 Loss: 0.1960, lr 9.960573506572391e-05
Step 33100 Loss: 0.1849, lr 9.960573506572391e-05
Step 33200 Loss: 0.1742, lr 9.960573506572391e-05
Step 33300 Loss: 0.1639, lr 9.960573506572391e-05
Step 33400 Loss: 0.1529, lr 9.960573506572391e-05
Step 33500 Loss: 0.1386, lr 9.960573506572391e-05
Step 33600 Loss: 0.1248, lr 9.960573506572391e-05
Step 33700 Loss: 0.1114, lr 9.960573506572391e-05
Train Epoch: [5/100] Loss: 0.0970,lr 0.000100
Model Saving at epoch 5
Calling G2SDataset.batch()
Done, time:  1.92 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6755
Step 33800 Loss: 1.2545, lr 9.93844170297569e-05
Step 33900 Loss: 0.9908, lr 9.93844170297569e-05
Step 34000 Loss: 0.9012, lr 9.93844170297569e-05
Step 34100 Loss: 0.8455, lr 9.93844170297569e-05
Step 34200 Loss: 0.8162, lr 9.93844170297569e-05
Step 34300 Loss: 0.7945, lr 9.93844170297569e-05
Step 34400 Loss: 0.7557, lr 9.93844170297569e-05
Step 34500 Loss: 0.7276, lr 9.93844170297569e-05
Step 34600 Loss: 0.7064, lr 9.93844170297569e-05
Step 34700 Loss: 0.6898, lr 9.93844170297569e-05
Step 34800 Loss: 0.6764, lr 9.93844170297569e-05
Step 34900 Loss: 0.6637, lr 9.93844170297569e-05
Step 35000 Loss: 0.6467, lr 9.93844170297569e-05
Step 35100 Loss: 0.6323, lr 9.93844170297569e-05
Step 35200 Loss: 0.6200, lr 9.93844170297569e-05
Step 35300 Loss: 0.6092, lr 9.93844170297569e-05
Step 35400 Loss: 0.5998, lr 9.93844170297569e-05
Step 35500 Loss: 0.5915, lr 9.93844170297569e-05
Step 35600 Loss: 0.5841, lr 9.93844170297569e-05
Step 35700 Loss: 0.5693, lr 9.93844170297569e-05
Step 35800 Loss: 0.5525, lr 9.93844170297569e-05
Step 35900 Loss: 0.5374, lr 9.93844170297569e-05
Step 36000 Loss: 0.5236, lr 9.93844170297569e-05
Step 36100 Loss: 0.5110, lr 9.93844170297569e-05
Step 36200 Loss: 0.4994, lr 9.93844170297569e-05
Step 36300 Loss: 0.4888, lr 9.93844170297569e-05
Step 36400 Loss: 0.4790, lr 9.93844170297569e-05
Step 36500 Loss: 0.4699, lr 9.93844170297569e-05
Step 36600 Loss: 0.4576, lr 9.93844170297569e-05
Step 36700 Loss: 0.4452, lr 9.93844170297569e-05
Step 36800 Loss: 0.4337, lr 9.93844170297569e-05
Step 36900 Loss: 0.4228, lr 9.93844170297569e-05
Step 37000 Loss: 0.4127, lr 9.93844170297569e-05
Step 37100 Loss: 0.4031, lr 9.93844170297569e-05
Step 37200 Loss: 0.3942, lr 9.93844170297569e-05
Step 37300 Loss: 0.3857, lr 9.93844170297569e-05
Step 37400 Loss: 0.3777, lr 9.93844170297569e-05
Step 37500 Loss: 0.3701, lr 9.93844170297569e-05
Step 37600 Loss: 0.3629, lr 9.93844170297569e-05
Step 37700 Loss: 0.3561, lr 9.93844170297569e-05
Step 37800 Loss: 0.3496, lr 9.93844170297569e-05
Step 37900 Loss: 0.3435, lr 9.93844170297569e-05
Step 38000 Loss: 0.3376, lr 9.93844170297569e-05
Step 38100 Loss: 0.3298, lr 9.93844170297569e-05
Step 38200 Loss: 0.3210, lr 9.93844170297569e-05
Step 38300 Loss: 0.3125, lr 9.93844170297569e-05
Step 38400 Loss: 0.3044, lr 9.93844170297569e-05
Step 38500 Loss: 0.2967, lr 9.93844170297569e-05
Step 38600 Loss: 0.2893, lr 9.93844170297569e-05
Step 38700 Loss: 0.2822, lr 9.93844170297569e-05
Step 38800 Loss: 0.2753, lr 9.93844170297569e-05
Step 38900 Loss: 0.2688, lr 9.93844170297569e-05
Step 39000 Loss: 0.2623, lr 9.93844170297569e-05
Step 39100 Loss: 0.2528, lr 9.93844170297569e-05
Step 39200 Loss: 0.2436, lr 9.93844170297569e-05
Step 39300 Loss: 0.2347, lr 9.93844170297569e-05
Step 39400 Loss: 0.2261, lr 9.93844170297569e-05
Step 39500 Loss: 0.2178, lr 9.93844170297569e-05
Step 39600 Loss: 0.2099, lr 9.93844170297569e-05
Step 39700 Loss: 0.2021, lr 9.93844170297569e-05
Step 39800 Loss: 0.1908, lr 9.93844170297569e-05
Step 39900 Loss: 0.1799, lr 9.93844170297569e-05
Step 40000 Loss: 0.1694, lr 9.93844170297569e-05
Step 40100 Loss: 0.1592, lr 9.93844170297569e-05
Step 40200 Loss: 0.1463, lr 9.93844170297569e-05
Step 40300 Loss: 0.1322, lr 9.93844170297569e-05
Step 40400 Loss: 0.1186, lr 9.93844170297569e-05
Step 40500 Loss: 0.1035, lr 9.93844170297569e-05
Train Epoch: [6/100] Loss: 0.0971,lr 0.000099
Calling G2SDataset.batch()
Done, time:  1.97 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Step 40600 Loss: 1.0381, lr 9.911436253643445e-05
Step 40700 Loss: 0.9605, lr 9.911436253643445e-05
Step 40800 Loss: 0.8711, lr 9.911436253643445e-05
Step 40900 Loss: 0.8304, lr 9.911436253643445e-05
Step 41000 Loss: 0.8071, lr 9.911436253643445e-05
Step 41100 Loss: 0.7754, lr 9.911436253643445e-05
Step 41200 Loss: 0.7420, lr 9.911436253643445e-05
Step 41300 Loss: 0.7174, lr 9.911436253643445e-05
Step 41400 Loss: 0.6984, lr 9.911436253643445e-05
Step 41500 Loss: 0.6834, lr 9.911436253643445e-05
Step 41600 Loss: 0.6711, lr 9.911436253643445e-05
Step 41700 Loss: 0.6556, lr 9.911436253643445e-05
Step 41800 Loss: 0.6399, lr 9.911436253643445e-05
Step 41900 Loss: 0.6265, lr 9.911436253643445e-05
Step 42000 Loss: 0.6149, lr 9.911436253643445e-05
Step 42100 Loss: 0.6048, lr 9.911436253643445e-05
Step 42200 Loss: 0.5959, lr 9.911436253643445e-05
Step 42300 Loss: 0.5880, lr 9.911436253643445e-05
Step 42400 Loss: 0.5792, lr 9.911436253643445e-05
Step 42500 Loss: 0.5615, lr 9.911436253643445e-05
Step 42600 Loss: 0.5455, lr 9.911436253643445e-05
Step 42700 Loss: 0.5310, lr 9.911436253643445e-05
Step 42800 Loss: 0.5177, lr 9.911436253643445e-05
Step 42900 Loss: 0.5056, lr 9.911436253643445e-05
Step 43000 Loss: 0.4945, lr 9.911436253643445e-05
Step 43100 Loss: 0.4842, lr 9.911436253643445e-05
Step 43200 Loss: 0.4747, lr 9.911436253643445e-05
Step 43300 Loss: 0.4647, lr 9.911436253643445e-05
Step 43400 Loss: 0.4519, lr 9.911436253643445e-05
Step 43500 Loss: 0.4398, lr 9.911436253643445e-05
Step 43600 Loss: 0.4286, lr 9.911436253643445e-05
Step 43700 Loss: 0.4181, lr 9.911436253643445e-05
Step 43800 Loss: 0.4082, lr 9.911436253643445e-05
Step 43900 Loss: 0.3990, lr 9.911436253643445e-05
Step 44000 Loss: 0.3902, lr 9.911436253643445e-05
Step 44100 Loss: 0.3819, lr 9.911436253643445e-05
Step 44200 Loss: 0.3741, lr 9.911436253643445e-05
Step 44300 Loss: 0.3667, lr 9.911436253643445e-05
Step 44400 Loss: 0.3597, lr 9.911436253643445e-05
Step 44500 Loss: 0.3531, lr 9.911436253643445e-05
Step 44600 Loss: 0.3467, lr 9.911436253643445e-05
Step 44700 Loss: 0.3407, lr 9.911436253643445e-05
Step 44800 Loss: 0.3347, lr 9.911436253643445e-05
Step 44900 Loss: 0.3257, lr 9.911436253643445e-05
Step 45000 Loss: 0.3170, lr 9.911436253643445e-05
Step 45100 Loss: 0.3087, lr 9.911436253643445e-05
Step 45200 Loss: 0.3008, lr 9.911436253643445e-05
Step 45300 Loss: 0.2932, lr 9.911436253643445e-05
Step 45400 Loss: 0.2860, lr 9.911436253643445e-05
Step 45500 Loss: 0.2790, lr 9.911436253643445e-05
Step 45600 Loss: 0.2723, lr 9.911436253643445e-05
Step 45700 Loss: 0.2658, lr 9.911436253643445e-05
Step 45800 Loss: 0.2579, lr 9.911436253643445e-05
Step 45900 Loss: 0.2485, lr 9.911436253643445e-05
Step 46000 Loss: 0.2394, lr 9.911436253643445e-05
Step 46100 Loss: 0.2307, lr 9.911436253643445e-05
Step 46200 Loss: 0.2223, lr 9.911436253643445e-05
Step 46300 Loss: 0.2141, lr 9.911436253643445e-05
Step 46400 Loss: 0.2063, lr 9.911436253643445e-05
Step 46500 Loss: 0.1968, lr 9.911436253643445e-05
Step 46600 Loss: 0.1858, lr 9.911436253643445e-05
Step 46700 Loss: 0.1750, lr 9.911436253643445e-05
Step 46800 Loss: 0.1647, lr 9.911436253643445e-05
Step 46900 Loss: 0.1541, lr 9.911436253643445e-05
Step 47000 Loss: 0.1398, lr 9.911436253643445e-05
Step 47100 Loss: 0.1259, lr 9.911436253643445e-05
Step 47200 Loss: 0.1124, lr 9.911436253643445e-05
Train Epoch: [7/100] Loss: 0.0966,lr 0.000099
Calling G2SDataset.batch()
Done, time:  2.31 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Step 47300 Loss: 1.2545, lr 9.879583809693738e-05
Step 47400 Loss: 0.9962, lr 9.879583809693738e-05
Step 47500 Loss: 0.9102, lr 9.879583809693738e-05
Step 47600 Loss: 0.8496, lr 9.879583809693738e-05
Step 47700 Loss: 0.8184, lr 9.879583809693738e-05
Step 47800 Loss: 0.7995, lr 9.879583809693738e-05
Step 47900 Loss: 0.7591, lr 9.879583809693738e-05
Step 48000 Loss: 0.7301, lr 9.879583809693738e-05
Step 48100 Loss: 0.7082, lr 9.879583809693738e-05
Step 48200 Loss: 0.6912, lr 9.879583809693738e-05
Step 48300 Loss: 0.6775, lr 9.879583809693738e-05
Step 48400 Loss: 0.6655, lr 9.879583809693738e-05
Step 48500 Loss: 0.6482, lr 9.879583809693738e-05
Step 48600 Loss: 0.6336, lr 9.879583809693738e-05
Step 48700 Loss: 0.6210, lr 9.879583809693738e-05
Step 48800 Loss: 0.6102, lr 9.879583809693738e-05
Step 48900 Loss: 0.6006, lr 9.879583809693738e-05
Step 49000 Loss: 0.5922, lr 9.879583809693738e-05
Step 49100 Loss: 0.5847, lr 9.879583809693738e-05
Step 49200 Loss: 0.5710, lr 9.879583809693738e-05
Step 49300 Loss: 0.5541, lr 9.879583809693738e-05
Step 49400 Loss: 0.5388, lr 9.879583809693738e-05
Step 49500 Loss: 0.5249, lr 9.879583809693738e-05
Step 49600 Loss: 0.5121, lr 9.879583809693738e-05
Step 49700 Loss: 0.5005, lr 9.879583809693738e-05
Step 49800 Loss: 0.4897, lr 9.879583809693738e-05
Step 49900 Loss: 0.4798, lr 9.879583809693738e-05
Step 50000 Loss: 0.4706, lr 9.879583809693738e-05
Step 50100 Loss: 0.4588, lr 9.879583809693738e-05
Step 50200 Loss: 0.4464, lr 9.879583809693738e-05
Step 50300 Loss: 0.4347, lr 9.879583809693738e-05
Step 50400 Loss: 0.4238, lr 9.879583809693738e-05
Step 50500 Loss: 0.4136, lr 9.879583809693738e-05
Step 50600 Loss: 0.4040, lr 9.879583809693738e-05
Step 50700 Loss: 0.3949, lr 9.879583809693738e-05
Step 50800 Loss: 0.3864, lr 9.879583809693738e-05
Step 50900 Loss: 0.3784, lr 9.879583809693738e-05
Step 51000 Loss: 0.3707, lr 9.879583809693738e-05
Step 51100 Loss: 0.3635, lr 9.879583809693738e-05
Step 51200 Loss: 0.3567, lr 9.879583809693738e-05
Step 51300 Loss: 0.3502, lr 9.879583809693738e-05
Step 51400 Loss: 0.3440, lr 9.879583809693738e-05
Step 51500 Loss: 0.3381, lr 9.879583809693738e-05
Step 51600 Loss: 0.3306, lr 9.879583809693738e-05
Step 51700 Loss: 0.3217, lr 9.879583809693738e-05
Step 51800 Loss: 0.3133, lr 9.879583809693738e-05
Step 51900 Loss: 0.3051, lr 9.879583809693738e-05
Step 52000 Loss: 0.2974, lr 9.879583809693738e-05
Step 52100 Loss: 0.2899, lr 9.879583809693738e-05
Step 52200 Loss: 0.2828, lr 9.879583809693738e-05
Step 52300 Loss: 0.2759, lr 9.879583809693738e-05
Step 52400 Loss: 0.2693, lr 9.879583809693738e-05
Step 52500 Loss: 0.2630, lr 9.879583809693738e-05
Step 52600 Loss: 0.2537, lr 9.879583809693738e-05
Step 52700 Loss: 0.2444, lr 9.879583809693738e-05
Step 52800 Loss: 0.2355, lr 9.879583809693738e-05
Step 52900 Loss: 0.2269, lr 9.879583809693738e-05
Step 53000 Loss: 0.2186, lr 9.879583809693738e-05
Step 53100 Loss: 0.2106, lr 9.879583809693738e-05
Step 53200 Loss: 0.2028, lr 9.879583809693738e-05
Step 53300 Loss: 0.1919, lr 9.879583809693738e-05
Step 53400 Loss: 0.1810, lr 9.879583809693738e-05
Step 53500 Loss: 0.1704, lr 9.879583809693738e-05
Step 53600 Loss: 0.1602, lr 9.879583809693738e-05
Step 53700 Loss: 0.1477, lr 9.879583809693738e-05
Step 53800 Loss: 0.1336, lr 9.879583809693738e-05
Step 53900 Loss: 0.1199, lr 9.879583809693738e-05
Step 54000 Loss: 0.1053, lr 9.879583809693738e-05
Train Epoch: [8/100] Loss: 0.0966,lr 0.000099
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Step 54100 Loss: 1.0610, lr 9.842915805643157e-05
Step 54200 Loss: 0.9774, lr 9.842915805643157e-05
Step 54300 Loss: 0.8774, lr 9.842915805643157e-05
Step 54400 Loss: 0.8335, lr 9.842915805643157e-05
Step 54500 Loss: 0.8089, lr 9.842915805643157e-05
Step 54600 Loss: 0.7797, lr 9.842915805643157e-05
Step 54700 Loss: 0.7451, lr 9.842915805643157e-05
Step 54800 Loss: 0.7196, lr 9.842915805643157e-05
Step 54900 Loss: 0.7001, lr 9.842915805643157e-05
Step 55000 Loss: 0.6847, lr 9.842915805643157e-05
Step 55100 Loss: 0.6722, lr 9.842915805643157e-05
Step 55200 Loss: 0.6574, lr 9.842915805643157e-05
Step 55300 Loss: 0.6414, lr 9.842915805643157e-05
Step 55400 Loss: 0.6277, lr 9.842915805643157e-05
Step 55500 Loss: 0.6160, lr 9.842915805643157e-05
Step 55600 Loss: 0.6057, lr 9.842915805643157e-05
Step 55700 Loss: 0.5967, lr 9.842915805643157e-05
Step 55800 Loss: 0.5887, lr 9.842915805643157e-05
Step 55900 Loss: 0.5812, lr 9.842915805643157e-05
Step 56000 Loss: 0.5633, lr 9.842915805643157e-05
Step 56100 Loss: 0.5471, lr 9.842915805643157e-05
Step 56200 Loss: 0.5324, lr 9.842915805643157e-05
Step 56300 Loss: 0.5190, lr 9.842915805643157e-05
Step 56400 Loss: 0.5068, lr 9.842915805643157e-05
Step 56500 Loss: 0.4956, lr 9.842915805643157e-05
Step 56600 Loss: 0.4852, lr 9.842915805643157e-05
Step 56700 Loss: 0.4756, lr 9.842915805643157e-05
Step 56800 Loss: 0.4662, lr 9.842915805643157e-05
Step 56900 Loss: 0.4532, lr 9.842915805643157e-05
Step 57000 Loss: 0.4411, lr 9.842915805643157e-05
Step 57100 Loss: 0.4298, lr 9.842915805643157e-05
Step 57200 Loss: 0.4192, lr 9.842915805643157e-05
Step 57300 Loss: 0.4092, lr 9.842915805643157e-05
Step 57400 Loss: 0.3999, lr 9.842915805643157e-05
Step 57500 Loss: 0.3911, lr 9.842915805643157e-05
Step 57600 Loss: 0.3827, lr 9.842915805643157e-05
Step 57700 Loss: 0.3749, lr 9.842915805643157e-05
Step 57800 Loss: 0.3674, lr 9.842915805643157e-05
Step 57900 Loss: 0.3604, lr 9.842915805643157e-05
Step 58000 Loss: 0.3537, lr 9.842915805643157e-05
Step 58100 Loss: 0.3473, lr 9.842915805643157e-05
Step 58200 Loss: 0.3413, lr 9.842915805643157e-05
Step 58300 Loss: 0.3355, lr 9.842915805643157e-05
Step 58400 Loss: 0.3266, lr 9.842915805643157e-05
Step 58500 Loss: 0.3179, lr 9.842915805643157e-05
Step 58600 Loss: 0.3096, lr 9.842915805643157e-05
Step 58700 Loss: 0.3016, lr 9.842915805643157e-05
Step 58800 Loss: 0.2940, lr 9.842915805643157e-05
Step 58900 Loss: 0.2867, lr 9.842915805643157e-05
Step 59000 Loss: 0.2796, lr 9.842915805643157e-05
Step 59100 Loss: 0.2729, lr 9.842915805643157e-05
Step 59200 Loss: 0.2664, lr 9.842915805643157e-05
Step 59300 Loss: 0.2590, lr 9.842915805643157e-05
Step 59400 Loss: 0.2495, lr 9.842915805643157e-05
Step 59500 Loss: 0.2404, lr 9.842915805643157e-05
Step 59600 Loss: 0.2316, lr 9.842915805643157e-05
Step 59700 Loss: 0.2232, lr 9.842915805643157e-05
Step 59800 Loss: 0.2150, lr 9.842915805643157e-05
Step 59900 Loss: 0.2071, lr 9.842915805643157e-05
Step 60000 Loss: 0.1981, lr 9.842915805643157e-05
Step 60100 Loss: 0.1870, lr 9.842915805643157e-05
Step 60200 Loss: 0.1762, lr 9.842915805643157e-05
Step 60300 Loss: 0.1658, lr 9.842915805643157e-05
Step 60400 Loss: 0.1557, lr 9.842915805643157e-05
Step 60500 Loss: 0.1414, lr 9.842915805643157e-05
Step 60600 Loss: 0.1275, lr 9.842915805643157e-05
Step 60700 Loss: 0.1140, lr 9.842915805643157e-05
Step 60800 Loss: 0.0968, lr 9.842915805643157e-05
Train Epoch: [9/100] Loss: 0.0965,lr 0.000098
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.67 s, total batches: 6756
Step 60900 Loss: 1.0038, lr 9.801468428384717e-05
Step 61000 Loss: 0.9214, lr 9.801468428384717e-05
Step 61100 Loss: 0.8545, lr 9.801468428384717e-05
Step 61200 Loss: 0.8212, lr 9.801468428384717e-05
Step 61300 Loss: 0.8012, lr 9.801468428384717e-05
Step 61400 Loss: 0.7631, lr 9.801468428384717e-05
Step 61500 Loss: 0.7330, lr 9.801468428384717e-05
Step 61600 Loss: 0.7104, lr 9.801468428384717e-05
Step 61700 Loss: 0.6929, lr 9.801468428384717e-05
Step 61800 Loss: 0.6788, lr 9.801468428384717e-05
Step 61900 Loss: 0.6674, lr 9.801468428384717e-05
Step 62000 Loss: 0.6500, lr 9.801468428384717e-05
Step 62100 Loss: 0.6351, lr 9.801468428384717e-05
Step 62200 Loss: 0.6223, lr 9.801468428384717e-05
Step 62300 Loss: 0.6113, lr 9.801468428384717e-05
Step 62400 Loss: 0.6016, lr 9.801468428384717e-05
Step 62500 Loss: 0.5930, lr 9.801468428384717e-05
Step 62600 Loss: 0.5854, lr 9.801468428384717e-05
Step 62700 Loss: 0.5730, lr 9.801468428384717e-05
Step 62800 Loss: 0.5559, lr 9.801468428384717e-05
Step 62900 Loss: 0.5404, lr 9.801468428384717e-05
Step 63000 Loss: 0.5264, lr 9.801468428384717e-05
Step 63100 Loss: 0.5135, lr 9.801468428384717e-05
Step 63200 Loss: 0.5017, lr 9.801468428384717e-05
Step 63300 Loss: 0.4909, lr 9.801468428384717e-05
Step 63400 Loss: 0.4809, lr 9.801468428384717e-05
Step 63500 Loss: 0.4716, lr 9.801468428384717e-05
Step 63600 Loss: 0.4603, lr 9.801468428384717e-05
Step 63700 Loss: 0.4477, lr 9.801468428384717e-05
Step 63800 Loss: 0.4360, lr 9.801468428384717e-05
Step 63900 Loss: 0.4250, lr 9.801468428384717e-05
Step 64000 Loss: 0.4147, lr 9.801468428384717e-05
Step 64100 Loss: 0.4050, lr 9.801468428384717e-05
Step 64200 Loss: 0.3959, lr 9.801468428384717e-05
Step 64300 Loss: 0.3873, lr 9.801468428384717e-05
Step 64400 Loss: 0.3792, lr 9.801468428384717e-05
Step 64500 Loss: 0.3715, lr 9.801468428384717e-05
Step 64600 Loss: 0.3643, lr 9.801468428384717e-05
Step 64700 Loss: 0.3574, lr 9.801468428384717e-05
Step 64800 Loss: 0.3508, lr 9.801468428384717e-05
Step 64900 Loss: 0.3446, lr 9.801468428384717e-05
Step 65000 Loss: 0.3387, lr 9.801468428384717e-05
Step 65100 Loss: 0.3316, lr 9.801468428384717e-05
Step 65200 Loss: 0.3227, lr 9.801468428384717e-05
Step 65300 Loss: 0.3142, lr 9.801468428384717e-05
Step 65400 Loss: 0.3060, lr 9.801468428384717e-05
Step 65500 Loss: 0.2982, lr 9.801468428384717e-05
Step 65600 Loss: 0.2907, lr 9.801468428384717e-05
Step 65700 Loss: 0.2835, lr 9.801468428384717e-05
Step 65800 Loss: 0.2766, lr 9.801468428384717e-05
Step 65900 Loss: 0.2700, lr 9.801468428384717e-05
Step 66000 Loss: 0.2636, lr 9.801468428384717e-05
Step 66100 Loss: 0.2547, lr 9.801468428384717e-05
Step 66200 Loss: 0.2454, lr 9.801468428384717e-05
Step 66300 Loss: 0.2365, lr 9.801468428384717e-05
Step 66400 Loss: 0.2278, lr 9.801468428384717e-05
Step 66500 Loss: 0.2195, lr 9.801468428384717e-05
Step 66600 Loss: 0.2115, lr 9.801468428384717e-05
Step 66700 Loss: 0.2037, lr 9.801468428384717e-05
Step 66800 Loss: 0.1932, lr 9.801468428384717e-05
Step 66900 Loss: 0.1822, lr 9.801468428384717e-05
Step 67000 Loss: 0.1716, lr 9.801468428384717e-05
Step 67100 Loss: 0.1613, lr 9.801468428384717e-05
Step 67200 Loss: 0.1494, lr 9.801468428384717e-05
Step 67300 Loss: 0.1352, lr 9.801468428384717e-05
Step 67400 Loss: 0.1215, lr 9.801468428384717e-05
Step 67500 Loss: 0.1076, lr 9.801468428384717e-05
Train Epoch: [10/100] Loss: 0.0962,lr 0.000098
Model Saving at epoch 10
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6755
Step 67600 Loss: 1.0980, lr 9.75528258147577e-05
Step 67700 Loss: 0.9814, lr 9.75528258147577e-05
Step 67800 Loss: 0.8848, lr 9.75528258147577e-05
Step 67900 Loss: 0.8372, lr 9.75528258147577e-05
Step 68000 Loss: 0.8110, lr 9.75528258147577e-05
Step 68100 Loss: 0.7845, lr 9.75528258147577e-05
Step 68200 Loss: 0.7485, lr 9.75528258147577e-05
Step 68300 Loss: 0.7221, lr 9.75528258147577e-05
Step 68400 Loss: 0.7020, lr 9.75528258147577e-05
Step 68500 Loss: 0.6862, lr 9.75528258147577e-05
Step 68600 Loss: 0.6734, lr 9.75528258147577e-05
Step 68700 Loss: 0.6594, lr 9.75528258147577e-05
Step 68800 Loss: 0.6430, lr 9.75528258147577e-05
Step 68900 Loss: 0.6291, lr 9.75528258147577e-05
Step 69000 Loss: 0.6172, lr 9.75528258147577e-05
Step 69100 Loss: 0.6067, lr 9.75528258147577e-05
Step 69200 Loss: 0.5976, lr 9.75528258147577e-05
Step 69300 Loss: 0.5895, lr 9.75528258147577e-05
Step 69400 Loss: 0.5823, lr 9.75528258147577e-05
Step 69500 Loss: 0.5653, lr 9.75528258147577e-05
Step 69600 Loss: 0.5489, lr 9.75528258147577e-05
Step 69700 Loss: 0.5341, lr 9.75528258147577e-05
Step 69800 Loss: 0.5205, lr 9.75528258147577e-05
Step 69900 Loss: 0.5082, lr 9.75528258147577e-05
Step 70000 Loss: 0.4968, lr 9.75528258147577e-05
Step 70100 Loss: 0.4864, lr 9.75528258147577e-05
Step 70200 Loss: 0.4767, lr 9.75528258147577e-05
Step 70300 Loss: 0.4677, lr 9.75528258147577e-05
Step 70400 Loss: 0.4546, lr 9.75528258147577e-05
Step 70500 Loss: 0.4424, lr 9.75528258147577e-05
Step 70600 Loss: 0.4310, lr 9.75528258147577e-05
Step 70700 Loss: 0.4203, lr 9.75528258147577e-05
Step 70800 Loss: 0.4103, lr 9.75528258147577e-05
Step 70900 Loss: 0.4009, lr 9.75528258147577e-05
Step 71000 Loss: 0.3920, lr 9.75528258147577e-05
Step 71100 Loss: 0.3837, lr 9.75528258147577e-05
Step 71200 Loss: 0.3757, lr 9.75528258147577e-05
Step 71300 Loss: 0.3683, lr 9.75528258147577e-05
Step 71400 Loss: 0.3612, lr 9.75528258147577e-05
Step 71500 Loss: 0.3544, lr 9.75528258147577e-05
Step 71600 Loss: 0.3480, lr 9.75528258147577e-05
Step 71700 Loss: 0.3419, lr 9.75528258147577e-05
Step 71800 Loss: 0.3361, lr 9.75528258147577e-05
Step 71900 Loss: 0.3276, lr 9.75528258147577e-05
Step 72000 Loss: 0.3189, lr 9.75528258147577e-05
Step 72100 Loss: 0.3105, lr 9.75528258147577e-05
Step 72200 Loss: 0.3025, lr 9.75528258147577e-05
Step 72300 Loss: 0.2948, lr 9.75528258147577e-05
Step 72400 Loss: 0.2875, lr 9.75528258147577e-05
Step 72500 Loss: 0.2804, lr 9.75528258147577e-05
Step 72600 Loss: 0.2736, lr 9.75528258147577e-05
Step 72700 Loss: 0.2671, lr 9.75528258147577e-05
Step 72800 Loss: 0.2600, lr 9.75528258147577e-05
Step 72900 Loss: 0.2505, lr 9.75528258147577e-05
Step 73000 Loss: 0.2414, lr 9.75528258147577e-05
Step 73100 Loss: 0.2326, lr 9.75528258147577e-05
Step 73200 Loss: 0.2241, lr 9.75528258147577e-05
Step 73300 Loss: 0.2159, lr 9.75528258147577e-05
Step 73400 Loss: 0.2079, lr 9.75528258147577e-05
Step 73500 Loss: 0.1995, lr 9.75528258147577e-05
Step 73600 Loss: 0.1883, lr 9.75528258147577e-05
Step 73700 Loss: 0.1775, lr 9.75528258147577e-05
Step 73800 Loss: 0.1670, lr 9.75528258147577e-05
Step 73900 Loss: 0.1569, lr 9.75528258147577e-05
Step 74000 Loss: 0.1431, lr 9.75528258147577e-05
Step 74100 Loss: 0.1291, lr 9.75528258147577e-05
Step 74200 Loss: 0.1155, lr 9.75528258147577e-05
Step 74300 Loss: 0.0991, lr 9.75528258147577e-05
Train Epoch: [11/100] Loss: 0.0967,lr 0.000098
Calling G2SDataset.batch()
Done, time:  1.93 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Step 74400 Loss: 1.0128, lr 9.70440384477113e-05
Step 74500 Loss: 0.9337, lr 9.70440384477113e-05
Step 74600 Loss: 0.8600, lr 9.70440384477113e-05
Step 74700 Loss: 0.8242, lr 9.70440384477113e-05
Step 74800 Loss: 0.8031, lr 9.70440384477113e-05
Step 74900 Loss: 0.7671, lr 9.70440384477113e-05
Step 75000 Loss: 0.7359, lr 9.70440384477113e-05
Step 75100 Loss: 0.7127, lr 9.70440384477113e-05
Step 75200 Loss: 0.6946, lr 9.70440384477113e-05
Step 75300 Loss: 0.6803, lr 9.70440384477113e-05
Step 75400 Loss: 0.6685, lr 9.70440384477113e-05
Step 75500 Loss: 0.6518, lr 9.70440384477113e-05
Step 75600 Loss: 0.6366, lr 9.70440384477113e-05
Step 75700 Loss: 0.6236, lr 9.70440384477113e-05
Step 75800 Loss: 0.6124, lr 9.70440384477113e-05
Step 75900 Loss: 0.6026, lr 9.70440384477113e-05
Step 76000 Loss: 0.5939, lr 9.70440384477113e-05
Step 76100 Loss: 0.5862, lr 9.70440384477113e-05
Step 76200 Loss: 0.5750, lr 9.70440384477113e-05
Step 76300 Loss: 0.5577, lr 9.70440384477113e-05
Step 76400 Loss: 0.5420, lr 9.70440384477113e-05
Step 76500 Loss: 0.5278, lr 9.70440384477113e-05
Step 76600 Loss: 0.5148, lr 9.70440384477113e-05
Step 76700 Loss: 0.5029, lr 9.70440384477113e-05
Step 76800 Loss: 0.4920, lr 9.70440384477113e-05
Step 76900 Loss: 0.4819, lr 9.70440384477113e-05
Step 77000 Loss: 0.4726, lr 9.70440384477113e-05
Step 77100 Loss: 0.4617, lr 9.70440384477113e-05
Step 77200 Loss: 0.4490, lr 9.70440384477113e-05
Step 77300 Loss: 0.4372, lr 9.70440384477113e-05
Step 77400 Loss: 0.4261, lr 9.70440384477113e-05
Step 77500 Loss: 0.4157, lr 9.70440384477113e-05
Step 77600 Loss: 0.4060, lr 9.70440384477113e-05
Step 77700 Loss: 0.3968, lr 9.70440384477113e-05
Step 77800 Loss: 0.3882, lr 9.70440384477113e-05
Step 77900 Loss: 0.3800, lr 9.70440384477113e-05
Step 78000 Loss: 0.3723, lr 9.70440384477113e-05
Step 78100 Loss: 0.3650, lr 9.70440384477113e-05
Step 78200 Loss: 0.3581, lr 9.70440384477113e-05
Step 78300 Loss: 0.3515, lr 9.70440384477113e-05
Step 78400 Loss: 0.3452, lr 9.70440384477113e-05
Step 78500 Loss: 0.3393, lr 9.70440384477113e-05
Step 78600 Loss: 0.3326, lr 9.70440384477113e-05
Step 78700 Loss: 0.3236, lr 9.70440384477113e-05
Step 78800 Loss: 0.3150, lr 9.70440384477113e-05
Step 78900 Loss: 0.3068, lr 9.70440384477113e-05
Step 79000 Loss: 0.2990, lr 9.70440384477113e-05
Step 79100 Loss: 0.2915, lr 9.70440384477113e-05
Step 79200 Loss: 0.2842, lr 9.70440384477113e-05
Step 79300 Loss: 0.2773, lr 9.70440384477113e-05
Step 79400 Loss: 0.2707, lr 9.70440384477113e-05
Step 79500 Loss: 0.2643, lr 9.70440384477113e-05
Step 79600 Loss: 0.2557, lr 9.70440384477113e-05
Step 79700 Loss: 0.2463, lr 9.70440384477113e-05
Step 79800 Loss: 0.2374, lr 9.70440384477113e-05
Step 79900 Loss: 0.2287, lr 9.70440384477113e-05
Step 80000 Loss: 0.2203, lr 9.70440384477113e-05
Step 80100 Loss: 0.2122, lr 9.70440384477113e-05
Step 80200 Loss: 0.2044, lr 9.70440384477113e-05
Step 80300 Loss: 0.1943, lr 9.70440384477113e-05
Step 80400 Loss: 0.1833, lr 9.70440384477113e-05
Step 80500 Loss: 0.1726, lr 9.70440384477113e-05
Step 80600 Loss: 0.1623, lr 9.70440384477113e-05
Step 80700 Loss: 0.1509, lr 9.70440384477113e-05
Step 80800 Loss: 0.1367, lr 9.70440384477113e-05
Step 80900 Loss: 0.1229, lr 9.70440384477113e-05
Step 81000 Loss: 0.1095, lr 9.70440384477113e-05
Train Epoch: [12/100] Loss: 0.0961,lr 0.000097
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 81100 Loss: 1.1530, lr 9.64888242944126e-05
Step 81200 Loss: 0.9850, lr 9.64888242944126e-05
Step 81300 Loss: 0.8931, lr 9.64888242944126e-05
Step 81400 Loss: 0.8413, lr 9.64888242944126e-05
Step 81500 Loss: 0.8135, lr 9.64888242944126e-05
Step 81600 Loss: 0.7895, lr 9.64888242944126e-05
Step 81700 Loss: 0.7520, lr 9.64888242944126e-05
Step 81800 Loss: 0.7248, lr 9.64888242944126e-05
Step 81900 Loss: 0.7041, lr 9.64888242944126e-05
Step 82000 Loss: 0.6878, lr 9.64888242944126e-05
Step 82100 Loss: 0.6747, lr 9.64888242944126e-05
Step 82200 Loss: 0.6614, lr 9.64888242944126e-05
Step 82300 Loss: 0.6447, lr 9.64888242944126e-05
Step 82400 Loss: 0.6306, lr 9.64888242944126e-05
Step 82500 Loss: 0.6184, lr 9.64888242944126e-05
Step 82600 Loss: 0.6078, lr 9.64888242944126e-05
Step 82700 Loss: 0.5985, lr 9.64888242944126e-05
Step 82800 Loss: 0.5903, lr 9.64888242944126e-05
Step 82900 Loss: 0.5830, lr 9.64888242944126e-05
Step 83000 Loss: 0.5671, lr 9.64888242944126e-05
Step 83100 Loss: 0.5506, lr 9.64888242944126e-05
Step 83200 Loss: 0.5356, lr 9.64888242944126e-05
Step 83300 Loss: 0.5219, lr 9.64888242944126e-05
Step 83400 Loss: 0.5094, lr 9.64888242944126e-05
Step 83500 Loss: 0.4980, lr 9.64888242944126e-05
Step 83600 Loss: 0.4874, lr 9.64888242944126e-05
Step 83700 Loss: 0.4776, lr 9.64888242944126e-05
Step 83800 Loss: 0.4686, lr 9.64888242944126e-05
Step 83900 Loss: 0.4560, lr 9.64888242944126e-05
Step 84000 Loss: 0.4437, lr 9.64888242944126e-05
Step 84100 Loss: 0.4322, lr 9.64888242944126e-05
Step 84200 Loss: 0.4214, lr 9.64888242944126e-05
Step 84300 Loss: 0.4113, lr 9.64888242944126e-05
Step 84400 Loss: 0.4018, lr 9.64888242944126e-05
Step 84500 Loss: 0.3929, lr 9.64888242944126e-05
Step 84600 Loss: 0.3845, lr 9.64888242944126e-05
Step 84700 Loss: 0.3765, lr 9.64888242944126e-05
Step 84800 Loss: 0.3690, lr 9.64888242944126e-05
Step 84900 Loss: 0.3619, lr 9.64888242944126e-05
Step 85000 Loss: 0.3551, lr 9.64888242944126e-05
Step 85100 Loss: 0.3486, lr 9.64888242944126e-05
Step 85200 Loss: 0.3425, lr 9.64888242944126e-05
Step 85300 Loss: 0.3367, lr 9.64888242944126e-05
Step 85400 Loss: 0.3285, lr 9.64888242944126e-05
Step 85500 Loss: 0.3197, lr 9.64888242944126e-05
Step 85600 Loss: 0.3113, lr 9.64888242944126e-05
Step 85700 Loss: 0.3033, lr 9.64888242944126e-05
Step 85800 Loss: 0.2956, lr 9.64888242944126e-05
Step 85900 Loss: 0.2882, lr 9.64888242944126e-05
Step 86000 Loss: 0.2811, lr 9.64888242944126e-05
Step 86100 Loss: 0.2743, lr 9.64888242944126e-05
Step 86200 Loss: 0.2678, lr 9.64888242944126e-05
Step 86300 Loss: 0.2610, lr 9.64888242944126e-05
Step 86400 Loss: 0.2515, lr 9.64888242944126e-05
Step 86500 Loss: 0.2423, lr 9.64888242944126e-05
Step 86600 Loss: 0.2335, lr 9.64888242944126e-05
Step 86700 Loss: 0.2249, lr 9.64888242944126e-05
Step 86800 Loss: 0.2167, lr 9.64888242944126e-05
Step 86900 Loss: 0.2087, lr 9.64888242944126e-05
Step 87000 Loss: 0.2005, lr 9.64888242944126e-05
Step 87100 Loss: 0.1893, lr 9.64888242944126e-05
Step 87200 Loss: 0.1785, lr 9.64888242944126e-05
Step 87300 Loss: 0.1680, lr 9.64888242944126e-05
Step 87400 Loss: 0.1578, lr 9.64888242944126e-05
Step 87500 Loss: 0.1445, lr 9.64888242944126e-05
Step 87600 Loss: 0.1305, lr 9.64888242944126e-05
Step 87700 Loss: 0.1168, lr 9.64888242944126e-05
Step 87800 Loss: 0.1010, lr 9.64888242944126e-05
Train Epoch: [13/100] Loss: 0.0962,lr 0.000096
Calling G2SDataset.batch()
Done, time:  1.56 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 87900 Loss: 1.0254, lr 9.588773128419907e-05
Step 88000 Loss: 0.9467, lr 9.588773128419907e-05
Step 88100 Loss: 0.8651, lr 9.588773128419907e-05
Step 88200 Loss: 0.8268, lr 9.588773128419907e-05
Step 88300 Loss: 0.8046, lr 9.588773128419907e-05
Step 88400 Loss: 0.7711, lr 9.588773128419907e-05
Step 88500 Loss: 0.7388, lr 9.588773128419907e-05
Step 88600 Loss: 0.7148, lr 9.588773128419907e-05
Step 88700 Loss: 0.6962, lr 9.588773128419907e-05
Step 88800 Loss: 0.6815, lr 9.588773128419907e-05
Step 88900 Loss: 0.6695, lr 9.588773128419907e-05
Step 89000 Loss: 0.6535, lr 9.588773128419907e-05
Step 89100 Loss: 0.6381, lr 9.588773128419907e-05
Step 89200 Loss: 0.6249, lr 9.588773128419907e-05
Step 89300 Loss: 0.6134, lr 9.588773128419907e-05
Step 89400 Loss: 0.6035, lr 9.588773128419907e-05
Step 89500 Loss: 0.5947, lr 9.588773128419907e-05
Step 89600 Loss: 0.5869, lr 9.588773128419907e-05
Step 89700 Loss: 0.5770, lr 9.588773128419907e-05
Step 89800 Loss: 0.5595, lr 9.588773128419907e-05
Step 89900 Loss: 0.5437, lr 9.588773128419907e-05
Step 90000 Loss: 0.5293, lr 9.588773128419907e-05
Step 90100 Loss: 0.5162, lr 9.588773128419907e-05
Step 90200 Loss: 0.5042, lr 9.588773128419907e-05
Step 90300 Loss: 0.4931, lr 9.588773128419907e-05
Step 90400 Loss: 0.4829, lr 9.588773128419907e-05
Step 90500 Loss: 0.4735, lr 9.588773128419907e-05
Step 90600 Loss: 0.4631, lr 9.588773128419907e-05
Step 90700 Loss: 0.4504, lr 9.588773128419907e-05
Step 90800 Loss: 0.4384, lr 9.588773128419907e-05
Step 90900 Loss: 0.4273, lr 9.588773128419907e-05
Step 91000 Loss: 0.4168, lr 9.588773128419907e-05
Step 91100 Loss: 0.4070, lr 9.588773128419907e-05
Step 91200 Loss: 0.3978, lr 9.588773128419907e-05
Step 91300 Loss: 0.3891, lr 9.588773128419907e-05
Step 91400 Loss: 0.3809, lr 9.588773128419907e-05
Step 91500 Loss: 0.3731, lr 9.588773128419907e-05
Step 91600 Loss: 0.3657, lr 9.588773128419907e-05
Step 91700 Loss: 0.3588, lr 9.588773128419907e-05
Step 91800 Loss: 0.3521, lr 9.588773128419907e-05
Step 91900 Loss: 0.3458, lr 9.588773128419907e-05
Step 92000 Loss: 0.3398, lr 9.588773128419907e-05
Step 92100 Loss: 0.3336, lr 9.588773128419907e-05
Step 92200 Loss: 0.3246, lr 9.588773128419907e-05
Step 92300 Loss: 0.3159, lr 9.588773128419907e-05
Step 92400 Loss: 0.3077, lr 9.588773128419907e-05
Step 92500 Loss: 0.2998, lr 9.588773128419907e-05
Step 92600 Loss: 0.2922, lr 9.588773128419907e-05
Step 92700 Loss: 0.2850, lr 9.588773128419907e-05
Step 92800 Loss: 0.2780, lr 9.588773128419907e-05
Step 92900 Loss: 0.2713, lr 9.588773128419907e-05
Step 93000 Loss: 0.2649, lr 9.588773128419907e-05
Step 93100 Loss: 0.2568, lr 9.588773128419907e-05
Step 93200 Loss: 0.2474, lr 9.588773128419907e-05
Step 93300 Loss: 0.2384, lr 9.588773128419907e-05
Step 93400 Loss: 0.2296, lr 9.588773128419907e-05
Step 93500 Loss: 0.2212, lr 9.588773128419907e-05
Step 93600 Loss: 0.2131, lr 9.588773128419907e-05
Step 93700 Loss: 0.2053, lr 9.588773128419907e-05
Step 93800 Loss: 0.1956, lr 9.588773128419907e-05
Step 93900 Loss: 0.1846, lr 9.588773128419907e-05
Step 94000 Loss: 0.1739, lr 9.588773128419907e-05
Step 94100 Loss: 0.1635, lr 9.588773128419907e-05
Step 94200 Loss: 0.1526, lr 9.588773128419907e-05
Step 94300 Loss: 0.1383, lr 9.588773128419907e-05
Step 94400 Loss: 0.1245, lr 9.588773128419907e-05
Step 94500 Loss: 0.1110, lr 9.588773128419907e-05
Train Epoch: [14/100] Loss: 0.0962,lr 0.000096
Calling G2SDataset.batch()
Done, time:  2.12 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 94600 Loss: 1.2532, lr 9.5241352623301e-05
Step 94700 Loss: 0.9907, lr 9.5241352623301e-05
Step 94800 Loss: 0.9019, lr 9.5241352623301e-05
Step 94900 Loss: 0.8455, lr 9.5241352623301e-05
Step 95000 Loss: 0.8159, lr 9.5241352623301e-05
Step 95100 Loss: 0.7947, lr 9.5241352623301e-05
Step 95200 Loss: 0.7557, lr 9.5241352623301e-05
Step 95300 Loss: 0.7275, lr 9.5241352623301e-05
Step 95400 Loss: 0.7061, lr 9.5241352623301e-05
Step 95500 Loss: 0.6894, lr 9.5241352623301e-05
Step 95600 Loss: 0.6760, lr 9.5241352623301e-05
Step 95700 Loss: 0.6635, lr 9.5241352623301e-05
Step 95800 Loss: 0.6465, lr 9.5241352623301e-05
Step 95900 Loss: 0.6321, lr 9.5241352623301e-05
Step 96000 Loss: 0.6197, lr 9.5241352623301e-05
Step 96100 Loss: 0.6090, lr 9.5241352623301e-05
Step 96200 Loss: 0.5995, lr 9.5241352623301e-05
Step 96300 Loss: 0.5912, lr 9.5241352623301e-05
Step 96400 Loss: 0.5838, lr 9.5241352623301e-05
Step 96500 Loss: 0.5692, lr 9.5241352623301e-05
Step 96600 Loss: 0.5524, lr 9.5241352623301e-05
Step 96700 Loss: 0.5372, lr 9.5241352623301e-05
Step 96800 Loss: 0.5234, lr 9.5241352623301e-05
Step 96900 Loss: 0.5108, lr 9.5241352623301e-05
Step 97000 Loss: 0.4992, lr 9.5241352623301e-05
Step 97100 Loss: 0.4886, lr 9.5241352623301e-05
Step 97200 Loss: 0.4787, lr 9.5241352623301e-05
Step 97300 Loss: 0.4696, lr 9.5241352623301e-05
Step 97400 Loss: 0.4574, lr 9.5241352623301e-05
Step 97500 Loss: 0.4450, lr 9.5241352623301e-05
Step 97600 Loss: 0.4335, lr 9.5241352623301e-05
Step 97700 Loss: 0.4226, lr 9.5241352623301e-05
Step 97800 Loss: 0.4125, lr 9.5241352623301e-05
Step 97900 Loss: 0.4029, lr 9.5241352623301e-05
Step 98000 Loss: 0.3939, lr 9.5241352623301e-05
Step 98100 Loss: 0.3854, lr 9.5241352623301e-05
Step 98200 Loss: 0.3774, lr 9.5241352623301e-05
Step 98300 Loss: 0.3698, lr 9.5241352623301e-05
Step 98400 Loss: 0.3626, lr 9.5241352623301e-05
Step 98500 Loss: 0.3558, lr 9.5241352623301e-05
Step 98600 Loss: 0.3493, lr 9.5241352623301e-05
Step 98700 Loss: 0.3432, lr 9.5241352623301e-05
Step 98800 Loss: 0.3373, lr 9.5241352623301e-05
Step 98900 Loss: 0.3296, lr 9.5241352623301e-05
Step 99000 Loss: 0.3207, lr 9.5241352623301e-05
Step 99100 Loss: 0.3123, lr 9.5241352623301e-05
Step 99200 Loss: 0.3042, lr 9.5241352623301e-05
Step 99300 Loss: 0.2964, lr 9.5241352623301e-05
Step 99400 Loss: 0.2890, lr 9.5241352623301e-05
Step 99500 Loss: 0.2819, lr 9.5241352623301e-05
Step 99600 Loss: 0.2751, lr 9.5241352623301e-05
Step 99700 Loss: 0.2685, lr 9.5241352623301e-05
Step 99800 Loss: 0.2621, lr 9.5241352623301e-05
Step 99900 Loss: 0.2526, lr 9.5241352623301e-05
Step 100000 Loss: 0.2433, lr 9.5241352623301e-05
Step 100100 Loss: 0.2345, lr 9.5241352623301e-05
Step 100200 Loss: 0.2259, lr 9.5241352623301e-05
Step 100300 Loss: 0.2176, lr 9.5241352623301e-05
Step 100400 Loss: 0.2096, lr 9.5241352623301e-05
Step 100500 Loss: 0.2019, lr 9.5241352623301e-05
Step 100600 Loss: 0.1906, lr 9.5241352623301e-05
Step 100700 Loss: 0.1798, lr 9.5241352623301e-05
Step 100800 Loss: 0.1692, lr 9.5241352623301e-05
Step 100900 Loss: 0.1590, lr 9.5241352623301e-05
Step 101000 Loss: 0.1462, lr 9.5241352623301e-05
Step 101100 Loss: 0.1321, lr 9.5241352623301e-05
Step 101200 Loss: 0.1184, lr 9.5241352623301e-05
Step 101300 Loss: 0.1033, lr 9.5241352623301e-05
Train Epoch: [15/100] Loss: 0.0961,lr 0.000095
Model Saving at epoch 15
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6755
Step 101400 Loss: 1.0422, lr 9.455032620941843e-05
Step 101500 Loss: 0.9635, lr 9.455032620941843e-05
Step 101600 Loss: 0.8718, lr 9.455032620941843e-05
Step 101700 Loss: 0.8305, lr 9.455032620941843e-05
Step 101800 Loss: 0.8069, lr 9.455032620941843e-05
Step 101900 Loss: 0.7759, lr 9.455032620941843e-05
Step 102000 Loss: 0.7422, lr 9.455032620941843e-05
Step 102100 Loss: 0.7174, lr 9.455032620941843e-05
Step 102200 Loss: 0.6983, lr 9.455032620941843e-05
Step 102300 Loss: 0.6832, lr 9.455032620941843e-05
Step 102400 Loss: 0.6709, lr 9.455032620941843e-05
Step 102500 Loss: 0.6556, lr 9.455032620941843e-05
Step 102600 Loss: 0.6398, lr 9.455032620941843e-05
Step 102700 Loss: 0.6264, lr 9.455032620941843e-05
Step 102800 Loss: 0.6148, lr 9.455032620941843e-05
Step 102900 Loss: 0.6046, lr 9.455032620941843e-05
Step 103000 Loss: 0.5957, lr 9.455032620941843e-05
Step 103100 Loss: 0.5878, lr 9.455032620941843e-05
Step 103200 Loss: 0.5793, lr 9.455032620941843e-05
Step 103300 Loss: 0.5615, lr 9.455032620941843e-05
Step 103400 Loss: 0.5455, lr 9.455032620941843e-05
Step 103500 Loss: 0.5310, lr 9.455032620941843e-05
Step 103600 Loss: 0.5177, lr 9.455032620941843e-05
Step 103700 Loss: 0.5056, lr 9.455032620941843e-05
Step 103800 Loss: 0.4944, lr 9.455032620941843e-05
Step 103900 Loss: 0.4841, lr 9.455032620941843e-05
Step 104000 Loss: 0.4746, lr 9.455032620941843e-05
Step 104100 Loss: 0.4647, lr 9.455032620941843e-05
Step 104200 Loss: 0.4518, lr 9.455032620941843e-05
Step 104300 Loss: 0.4398, lr 9.455032620941843e-05
Step 104400 Loss: 0.4286, lr 9.455032620941843e-05
Step 104500 Loss: 0.4180, lr 9.455032620941843e-05
Step 104600 Loss: 0.4082, lr 9.455032620941843e-05
Step 104700 Loss: 0.3989, lr 9.455032620941843e-05
Step 104800 Loss: 0.3901, lr 9.455032620941843e-05
Step 104900 Loss: 0.3818, lr 9.455032620941843e-05
Step 105000 Loss: 0.3740, lr 9.455032620941843e-05
Step 105100 Loss: 0.3666, lr 9.455032620941843e-05
Step 105200 Loss: 0.3596, lr 9.455032620941843e-05
Step 105300 Loss: 0.3529, lr 9.455032620941843e-05
Step 105400 Loss: 0.3466, lr 9.455032620941843e-05
Step 105500 Loss: 0.3405, lr 9.455032620941843e-05
Step 105600 Loss: 0.3347, lr 9.455032620941843e-05
Step 105700 Loss: 0.3256, lr 9.455032620941843e-05
Step 105800 Loss: 0.3169, lr 9.455032620941843e-05
Step 105900 Loss: 0.3086, lr 9.455032620941843e-05
Step 106000 Loss: 0.3007, lr 9.455032620941843e-05
Step 106100 Loss: 0.2931, lr 9.455032620941843e-05
Step 106200 Loss: 0.2858, lr 9.455032620941843e-05
Step 106300 Loss: 0.2788, lr 9.455032620941843e-05
Step 106400 Loss: 0.2721, lr 9.455032620941843e-05
Step 106500 Loss: 0.2657, lr 9.455032620941843e-05
Step 106600 Loss: 0.2579, lr 9.455032620941843e-05
Step 106700 Loss: 0.2485, lr 9.455032620941843e-05
Step 106800 Loss: 0.2394, lr 9.455032620941843e-05
Step 106900 Loss: 0.2307, lr 9.455032620941843e-05
Step 107000 Loss: 0.2222, lr 9.455032620941843e-05
Step 107100 Loss: 0.2141, lr 9.455032620941843e-05
Step 107200 Loss: 0.2062, lr 9.455032620941843e-05
Step 107300 Loss: 0.1970, lr 9.455032620941843e-05
Step 107400 Loss: 0.1859, lr 9.455032620941843e-05
Step 107500 Loss: 0.1751, lr 9.455032620941843e-05
Step 107600 Loss: 0.1648, lr 9.455032620941843e-05
Step 107700 Loss: 0.1544, lr 9.455032620941843e-05
Step 107800 Loss: 0.1400, lr 9.455032620941843e-05
Step 107900 Loss: 0.1261, lr 9.455032620941843e-05
Step 108000 Loss: 0.1126, lr 9.455032620941843e-05
Train Epoch: [16/100] Loss: 0.0966,lr 0.000095
Calling G2SDataset.batch()
Done, time:  2.20 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.78 s, total batches: 6756
Step 108100 Loss: 1.2533, lr 9.381533400219321e-05
Step 108200 Loss: 0.9966, lr 9.381533400219321e-05
Step 108300 Loss: 0.9120, lr 9.381533400219321e-05
Step 108400 Loss: 0.8502, lr 9.381533400219321e-05
Step 108500 Loss: 0.8186, lr 9.381533400219321e-05
Step 108600 Loss: 0.7995, lr 9.381533400219321e-05
Step 108700 Loss: 0.7595, lr 9.381533400219321e-05
Step 108800 Loss: 0.7303, lr 9.381533400219321e-05
Step 108900 Loss: 0.7083, lr 9.381533400219321e-05
Step 109000 Loss: 0.6911, lr 9.381533400219321e-05
Step 109100 Loss: 0.6774, lr 9.381533400219321e-05
Step 109200 Loss: 0.6655, lr 9.381533400219321e-05
Step 109300 Loss: 0.6482, lr 9.381533400219321e-05
Step 109400 Loss: 0.6335, lr 9.381533400219321e-05
Step 109500 Loss: 0.6210, lr 9.381533400219321e-05
Step 109600 Loss: 0.6100, lr 9.381533400219321e-05
Step 109700 Loss: 0.6005, lr 9.381533400219321e-05
Step 109800 Loss: 0.5920, lr 9.381533400219321e-05
Step 109900 Loss: 0.5845, lr 9.381533400219321e-05
Step 110000 Loss: 0.5711, lr 9.381533400219321e-05
Step 110100 Loss: 0.5541, lr 9.381533400219321e-05
Step 110200 Loss: 0.5388, lr 9.381533400219321e-05
Step 110300 Loss: 0.5248, lr 9.381533400219321e-05
Step 110400 Loss: 0.5121, lr 9.381533400219321e-05
Step 110500 Loss: 0.5004, lr 9.381533400219321e-05
Step 110600 Loss: 0.4897, lr 9.381533400219321e-05
Step 110700 Loss: 0.4797, lr 9.381533400219321e-05
Step 110800 Loss: 0.4705, lr 9.381533400219321e-05
Step 110900 Loss: 0.4588, lr 9.381533400219321e-05
Step 111000 Loss: 0.4463, lr 9.381533400219321e-05
Step 111100 Loss: 0.4347, lr 9.381533400219321e-05
Step 111200 Loss: 0.4237, lr 9.381533400219321e-05
Step 111300 Loss: 0.4135, lr 9.381533400219321e-05
Step 111400 Loss: 0.4039, lr 9.381533400219321e-05
Step 111500 Loss: 0.3948, lr 9.381533400219321e-05
Step 111600 Loss: 0.3863, lr 9.381533400219321e-05
Step 111700 Loss: 0.3782, lr 9.381533400219321e-05
Step 111800 Loss: 0.3706, lr 9.381533400219321e-05
Step 111900 Loss: 0.3634, lr 9.381533400219321e-05
Step 112000 Loss: 0.3565, lr 9.381533400219321e-05
Step 112100 Loss: 0.3500, lr 9.381533400219321e-05
Step 112200 Loss: 0.3438, lr 9.381533400219321e-05
Step 112300 Loss: 0.3379, lr 9.381533400219321e-05
Step 112400 Loss: 0.3305, lr 9.381533400219321e-05
Step 112500 Loss: 0.3216, lr 9.381533400219321e-05
Step 112600 Loss: 0.3131, lr 9.381533400219321e-05
Step 112700 Loss: 0.3050, lr 9.381533400219321e-05
Step 112800 Loss: 0.2972, lr 9.381533400219321e-05
Step 112900 Loss: 0.2898, lr 9.381533400219321e-05
Step 113000 Loss: 0.2826, lr 9.381533400219321e-05
Step 113100 Loss: 0.2758, lr 9.381533400219321e-05
Step 113200 Loss: 0.2692, lr 9.381533400219321e-05
Step 113300 Loss: 0.2628, lr 9.381533400219321e-05
Step 113400 Loss: 0.2536, lr 9.381533400219321e-05
Step 113500 Loss: 0.2443, lr 9.381533400219321e-05
Step 113600 Loss: 0.2354, lr 9.381533400219321e-05
Step 113700 Loss: 0.2268, lr 9.381533400219321e-05
Step 113800 Loss: 0.2185, lr 9.381533400219321e-05
Step 113900 Loss: 0.2104, lr 9.381533400219321e-05
Step 114000 Loss: 0.2027, lr 9.381533400219321e-05
Step 114100 Loss: 0.1918, lr 9.381533400219321e-05
Step 114200 Loss: 0.1809, lr 9.381533400219321e-05
Step 114300 Loss: 0.1703, lr 9.381533400219321e-05
Step 114400 Loss: 0.1600, lr 9.381533400219321e-05
Step 114500 Loss: 0.1477, lr 9.381533400219321e-05
Step 114600 Loss: 0.1335, lr 9.381533400219321e-05
Step 114700 Loss: 0.1198, lr 9.381533400219321e-05
Step 114800 Loss: 0.1054, lr 9.381533400219321e-05
Train Epoch: [17/100] Loss: 0.0960,lr 0.000094
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Step 114900 Loss: 1.0660, lr 9.303710135019722e-05
Step 115000 Loss: 0.9776, lr 9.303710135019722e-05
Step 115100 Loss: 0.8788, lr 9.303710135019722e-05
Step 115200 Loss: 0.8341, lr 9.303710135019722e-05
Step 115300 Loss: 0.8091, lr 9.303710135019722e-05
Step 115400 Loss: 0.7804, lr 9.303710135019722e-05
Step 115500 Loss: 0.7455, lr 9.303710135019722e-05
Step 115600 Loss: 0.7199, lr 9.303710135019722e-05
Step 115700 Loss: 0.7002, lr 9.303710135019722e-05
Step 115800 Loss: 0.6847, lr 9.303710135019722e-05
Step 115900 Loss: 0.6722, lr 9.303710135019722e-05
Step 116000 Loss: 0.6575, lr 9.303710135019722e-05
Step 116100 Loss: 0.6415, lr 9.303710135019722e-05
Step 116200 Loss: 0.6278, lr 9.303710135019722e-05
Step 116300 Loss: 0.6160, lr 9.303710135019722e-05
Step 116400 Loss: 0.6057, lr 9.303710135019722e-05
Step 116500 Loss: 0.5966, lr 9.303710135019722e-05
Step 116600 Loss: 0.5886, lr 9.303710135019722e-05
Step 116700 Loss: 0.5813, lr 9.303710135019722e-05
Step 116800 Loss: 0.5634, lr 9.303710135019722e-05
Step 116900 Loss: 0.5472, lr 9.303710135019722e-05
Step 117000 Loss: 0.5325, lr 9.303710135019722e-05
Step 117100 Loss: 0.5191, lr 9.303710135019722e-05
Step 117200 Loss: 0.5068, lr 9.303710135019722e-05
Step 117300 Loss: 0.4956, lr 9.303710135019722e-05
Step 117400 Loss: 0.4852, lr 9.303710135019722e-05
Step 117500 Loss: 0.4756, lr 9.303710135019722e-05
Step 117600 Loss: 0.4662, lr 9.303710135019722e-05
Step 117700 Loss: 0.4532, lr 9.303710135019722e-05
Step 117800 Loss: 0.4411, lr 9.303710135019722e-05
Step 117900 Loss: 0.4297, lr 9.303710135019722e-05
Step 118000 Loss: 0.4191, lr 9.303710135019722e-05
Step 118100 Loss: 0.4092, lr 9.303710135019722e-05
Step 118200 Loss: 0.3998, lr 9.303710135019722e-05
Step 118300 Loss: 0.3910, lr 9.303710135019722e-05
Step 118400 Loss: 0.3827, lr 9.303710135019722e-05
Step 118500 Loss: 0.3748, lr 9.303710135019722e-05
Step 118600 Loss: 0.3674, lr 9.303710135019722e-05
Step 118700 Loss: 0.3603, lr 9.303710135019722e-05
Step 118800 Loss: 0.3536, lr 9.303710135019722e-05
Step 118900 Loss: 0.3472, lr 9.303710135019722e-05
Step 119000 Loss: 0.3412, lr 9.303710135019722e-05
Step 119100 Loss: 0.3354, lr 9.303710135019722e-05
Step 119200 Loss: 0.3265, lr 9.303710135019722e-05
Step 119300 Loss: 0.3178, lr 9.303710135019722e-05
Step 119400 Loss: 0.3095, lr 9.303710135019722e-05
Step 119500 Loss: 0.3015, lr 9.303710135019722e-05
Step 119600 Loss: 0.2939, lr 9.303710135019722e-05
Step 119700 Loss: 0.2866, lr 9.303710135019722e-05
Step 119800 Loss: 0.2796, lr 9.303710135019722e-05
Step 119900 Loss: 0.2728, lr 9.303710135019722e-05
Step 120000 Loss: 0.2663, lr 9.303710135019722e-05
Step 120100 Loss: 0.2589, lr 9.303710135019722e-05
Step 120200 Loss: 0.2494, lr 9.303710135019722e-05
Step 120300 Loss: 0.2403, lr 9.303710135019722e-05
Step 120400 Loss: 0.2315, lr 9.303710135019722e-05
Step 120500 Loss: 0.2231, lr 9.303710135019722e-05
Step 120600 Loss: 0.2149, lr 9.303710135019722e-05
Step 120700 Loss: 0.2070, lr 9.303710135019722e-05
Step 120800 Loss: 0.1981, lr 9.303710135019722e-05
Step 120900 Loss: 0.1869, lr 9.303710135019722e-05
Step 121000 Loss: 0.1761, lr 9.303710135019722e-05
Step 121100 Loss: 0.1657, lr 9.303710135019722e-05
Step 121200 Loss: 0.1556, lr 9.303710135019722e-05
Step 121300 Loss: 0.1414, lr 9.303710135019722e-05
Step 121400 Loss: 0.1274, lr 9.303710135019722e-05
Step 121500 Loss: 0.1139, lr 9.303710135019722e-05
Step 121600 Loss: 0.0970, lr 9.303710135019722e-05
Train Epoch: [18/100] Loss: 0.0963,lr 0.000093
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6756
Step 121700 Loss: 1.0040, lr 9.22163962751008e-05
Step 121800 Loss: 0.9215, lr 9.22163962751008e-05
Step 121900 Loss: 0.8542, lr 9.22163962751008e-05
Step 122000 Loss: 0.8208, lr 9.22163962751008e-05
Step 122100 Loss: 0.8007, lr 9.22163962751008e-05
Step 122200 Loss: 0.7629, lr 9.22163962751008e-05
Step 122300 Loss: 0.7327, lr 9.22163962751008e-05
Step 122400 Loss: 0.7101, lr 9.22163962751008e-05
Step 122500 Loss: 0.6926, lr 9.22163962751008e-05
Step 122600 Loss: 0.6785, lr 9.22163962751008e-05
Step 122700 Loss: 0.6670, lr 9.22163962751008e-05
Step 122800 Loss: 0.6498, lr 9.22163962751008e-05
Step 122900 Loss: 0.6349, lr 9.22163962751008e-05
Step 123000 Loss: 0.6221, lr 9.22163962751008e-05
Step 123100 Loss: 0.6111, lr 9.22163962751008e-05
Step 123200 Loss: 0.6014, lr 9.22163962751008e-05
Step 123300 Loss: 0.5928, lr 9.22163962751008e-05
Step 123400 Loss: 0.5852, lr 9.22163962751008e-05
Step 123500 Loss: 0.5729, lr 9.22163962751008e-05
Step 123600 Loss: 0.5558, lr 9.22163962751008e-05
Step 123700 Loss: 0.5403, lr 9.22163962751008e-05
Step 123800 Loss: 0.5262, lr 9.22163962751008e-05
Step 123900 Loss: 0.5134, lr 9.22163962751008e-05
Step 124000 Loss: 0.5016, lr 9.22163962751008e-05
Step 124100 Loss: 0.4907, lr 9.22163962751008e-05
Step 124200 Loss: 0.4807, lr 9.22163962751008e-05
Step 124300 Loss: 0.4714, lr 9.22163962751008e-05
Step 124400 Loss: 0.4602, lr 9.22163962751008e-05
Step 124500 Loss: 0.4476, lr 9.22163962751008e-05
Step 124600 Loss: 0.4358, lr 9.22163962751008e-05
Step 124700 Loss: 0.4248, lr 9.22163962751008e-05
Step 124800 Loss: 0.4145, lr 9.22163962751008e-05
Step 124900 Loss: 0.4049, lr 9.22163962751008e-05
Step 125000 Loss: 0.3957, lr 9.22163962751008e-05
Step 125100 Loss: 0.3872, lr 9.22163962751008e-05
Step 125200 Loss: 0.3790, lr 9.22163962751008e-05
Step 125300 Loss: 0.3714, lr 9.22163962751008e-05
Step 125400 Loss: 0.3641, lr 9.22163962751008e-05
Step 125500 Loss: 0.3572, lr 9.22163962751008e-05
Step 125600 Loss: 0.3506, lr 9.22163962751008e-05
Step 125700 Loss: 0.3444, lr 9.22163962751008e-05
Step 125800 Loss: 0.3385, lr 9.22163962751008e-05
Step 125900 Loss: 0.3315, lr 9.22163962751008e-05
Step 126000 Loss: 0.3225, lr 9.22163962751008e-05
Step 126100 Loss: 0.3140, lr 9.22163962751008e-05
Step 126200 Loss: 0.3058, lr 9.22163962751008e-05
Step 126300 Loss: 0.2980, lr 9.22163962751008e-05
Step 126400 Loss: 0.2905, lr 9.22163962751008e-05
Step 126500 Loss: 0.2833, lr 9.22163962751008e-05
Step 126600 Loss: 0.2764, lr 9.22163962751008e-05
Step 126700 Loss: 0.2698, lr 9.22163962751008e-05
Step 126800 Loss: 0.2635, lr 9.22163962751008e-05
Step 126900 Loss: 0.2546, lr 9.22163962751008e-05
Step 127000 Loss: 0.2453, lr 9.22163962751008e-05
Step 127100 Loss: 0.2363, lr 9.22163962751008e-05
Step 127200 Loss: 0.2277, lr 9.22163962751008e-05
Step 127300 Loss: 0.2193, lr 9.22163962751008e-05
Step 127400 Loss: 0.2113, lr 9.22163962751008e-05
Step 127500 Loss: 0.2035, lr 9.22163962751008e-05
Step 127600 Loss: 0.1930, lr 9.22163962751008e-05
Step 127700 Loss: 0.1821, lr 9.22163962751008e-05
Step 127800 Loss: 0.1714, lr 9.22163962751008e-05
Step 127900 Loss: 0.1612, lr 9.22163962751008e-05
Step 128000 Loss: 0.1493, lr 9.22163962751008e-05
Step 128100 Loss: 0.1351, lr 9.22163962751008e-05
Step 128200 Loss: 0.1213, lr 9.22163962751008e-05
Step 128300 Loss: 0.1075, lr 9.22163962751008e-05
Train Epoch: [19/100] Loss: 0.0960,lr 0.000092
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 128400 Loss: 1.0996, lr 9.135402871372814e-05
Step 128500 Loss: 0.9808, lr 9.135402871372814e-05
Step 128600 Loss: 0.8856, lr 9.135402871372814e-05
Step 128700 Loss: 0.8375, lr 9.135402871372814e-05
Step 128800 Loss: 0.8111, lr 9.135402871372814e-05
Step 128900 Loss: 0.7849, lr 9.135402871372814e-05
Step 129000 Loss: 0.7487, lr 9.135402871372814e-05
Step 129100 Loss: 0.7222, lr 9.135402871372814e-05
Step 129200 Loss: 0.7020, lr 9.135402871372814e-05
Step 129300 Loss: 0.6862, lr 9.135402871372814e-05
Step 129400 Loss: 0.6733, lr 9.135402871372814e-05
Step 129500 Loss: 0.6594, lr 9.135402871372814e-05
Step 129600 Loss: 0.6430, lr 9.135402871372814e-05
Step 129700 Loss: 0.6291, lr 9.135402871372814e-05
Step 129800 Loss: 0.6171, lr 9.135402871372814e-05
Step 129900 Loss: 0.6066, lr 9.135402871372814e-05
Step 130000 Loss: 0.5975, lr 9.135402871372814e-05
Step 130100 Loss: 0.5894, lr 9.135402871372814e-05
Step 130200 Loss: 0.5821, lr 9.135402871372814e-05
Step 130300 Loss: 0.5652, lr 9.135402871372814e-05
Step 130400 Loss: 0.5488, lr 9.135402871372814e-05
Step 130500 Loss: 0.5340, lr 9.135402871372814e-05
Step 130600 Loss: 0.5204, lr 9.135402871372814e-05
Step 130700 Loss: 0.5080, lr 9.135402871372814e-05
Step 130800 Loss: 0.4967, lr 9.135402871372814e-05
Step 130900 Loss: 0.4862, lr 9.135402871372814e-05
Step 131000 Loss: 0.4765, lr 9.135402871372814e-05
Step 131100 Loss: 0.4676, lr 9.135402871372814e-05
Step 131200 Loss: 0.4545, lr 9.135402871372814e-05
Step 131300 Loss: 0.4423, lr 9.135402871372814e-05
Step 131400 Loss: 0.4309, lr 9.135402871372814e-05
Step 131500 Loss: 0.4202, lr 9.135402871372814e-05
Step 131600 Loss: 0.4102, lr 9.135402871372814e-05
Step 131700 Loss: 0.4008, lr 9.135402871372814e-05
Step 131800 Loss: 0.3919, lr 9.135402871372814e-05
Step 131900 Loss: 0.3835, lr 9.135402871372814e-05
Step 132000 Loss: 0.3756, lr 9.135402871372814e-05
Step 132100 Loss: 0.3681, lr 9.135402871372814e-05
Step 132200 Loss: 0.3610, lr 9.135402871372814e-05
Step 132300 Loss: 0.3543, lr 9.135402871372814e-05
Step 132400 Loss: 0.3479, lr 9.135402871372814e-05
Step 132500 Loss: 0.3418, lr 9.135402871372814e-05
Step 132600 Loss: 0.3359, lr 9.135402871372814e-05
Step 132700 Loss: 0.3275, lr 9.135402871372814e-05
Step 132800 Loss: 0.3187, lr 9.135402871372814e-05
Step 132900 Loss: 0.3104, lr 9.135402871372814e-05
Step 133000 Loss: 0.3023, lr 9.135402871372814e-05
Step 133100 Loss: 0.2947, lr 9.135402871372814e-05
Step 133200 Loss: 0.2873, lr 9.135402871372814e-05
Step 133300 Loss: 0.2803, lr 9.135402871372814e-05
Step 133400 Loss: 0.2735, lr 9.135402871372814e-05
Step 133500 Loss: 0.2670, lr 9.135402871372814e-05
Step 133600 Loss: 0.2599, lr 9.135402871372814e-05
Step 133700 Loss: 0.2504, lr 9.135402871372814e-05
Step 133800 Loss: 0.2413, lr 9.135402871372814e-05
Step 133900 Loss: 0.2324, lr 9.135402871372814e-05
Step 134000 Loss: 0.2239, lr 9.135402871372814e-05
Step 134100 Loss: 0.2157, lr 9.135402871372814e-05
Step 134200 Loss: 0.2078, lr 9.135402871372814e-05
Step 134300 Loss: 0.1993, lr 9.135402871372814e-05
Step 134400 Loss: 0.1881, lr 9.135402871372814e-05
Step 134500 Loss: 0.1773, lr 9.135402871372814e-05
Step 134600 Loss: 0.1668, lr 9.135402871372814e-05
Step 134700 Loss: 0.1567, lr 9.135402871372814e-05
Step 134800 Loss: 0.1430, lr 9.135402871372814e-05
Step 134900 Loss: 0.1289, lr 9.135402871372814e-05
Step 135000 Loss: 0.1154, lr 9.135402871372814e-05
Step 135100 Loss: 0.0990, lr 9.135402871372814e-05
Train Epoch: [20/100] Loss: 0.0960,lr 0.000091
Model Saving at epoch 20
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.92 s, total batches: 6756
Step 135200 Loss: 1.0133, lr 9.045084971874742e-05
Step 135300 Loss: 0.9347, lr 9.045084971874742e-05
Step 135400 Loss: 0.8600, lr 9.045084971874742e-05
Step 135500 Loss: 0.8240, lr 9.045084971874742e-05
Step 135600 Loss: 0.8028, lr 9.045084971874742e-05
Step 135700 Loss: 0.7673, lr 9.045084971874742e-05
Step 135800 Loss: 0.7359, lr 9.045084971874742e-05
Step 135900 Loss: 0.7126, lr 9.045084971874742e-05
Step 136000 Loss: 0.6945, lr 9.045084971874742e-05
Step 136100 Loss: 0.6801, lr 9.045084971874742e-05
Step 136200 Loss: 0.6683, lr 9.045084971874742e-05
Step 136300 Loss: 0.6517, lr 9.045084971874742e-05
Step 136400 Loss: 0.6365, lr 9.045084971874742e-05
Step 136500 Loss: 0.6235, lr 9.045084971874742e-05
Step 136600 Loss: 0.6122, lr 9.045084971874742e-05
Step 136700 Loss: 0.6024, lr 9.045084971874742e-05
Step 136800 Loss: 0.5937, lr 9.045084971874742e-05
Step 136900 Loss: 0.5860, lr 9.045084971874742e-05
Step 137000 Loss: 0.5751, lr 9.045084971874742e-05
Step 137100 Loss: 0.5577, lr 9.045084971874742e-05
Step 137200 Loss: 0.5420, lr 9.045084971874742e-05
Step 137300 Loss: 0.5278, lr 9.045084971874742e-05
Step 137400 Loss: 0.5148, lr 9.045084971874742e-05
Step 137500 Loss: 0.5029, lr 9.045084971874742e-05
Step 137600 Loss: 0.4919, lr 9.045084971874742e-05
Step 137700 Loss: 0.4818, lr 9.045084971874742e-05
Step 137800 Loss: 0.4725, lr 9.045084971874742e-05
Step 137900 Loss: 0.4617, lr 9.045084971874742e-05
Step 138000 Loss: 0.4490, lr 9.045084971874742e-05
Step 138100 Loss: 0.4371, lr 9.045084971874742e-05
Step 138200 Loss: 0.4261, lr 9.045084971874742e-05
Step 138300 Loss: 0.4157, lr 9.045084971874742e-05
Step 138400 Loss: 0.4059, lr 9.045084971874742e-05
Step 138500 Loss: 0.3968, lr 9.045084971874742e-05
Step 138600 Loss: 0.3881, lr 9.045084971874742e-05
Step 138700 Loss: 0.3799, lr 9.045084971874742e-05
Step 138800 Loss: 0.3722, lr 9.045084971874742e-05
Step 138900 Loss: 0.3649, lr 9.045084971874742e-05
Step 139000 Loss: 0.3580, lr 9.045084971874742e-05
Step 139100 Loss: 0.3514, lr 9.045084971874742e-05
Step 139200 Loss: 0.3451, lr 9.045084971874742e-05
Step 139300 Loss: 0.3391, lr 9.045084971874742e-05
Step 139400 Loss: 0.3325, lr 9.045084971874742e-05
Step 139500 Loss: 0.3235, lr 9.045084971874742e-05
Step 139600 Loss: 0.3150, lr 9.045084971874742e-05
Step 139700 Loss: 0.3068, lr 9.045084971874742e-05
Step 139800 Loss: 0.2989, lr 9.045084971874742e-05
Step 139900 Loss: 0.2914, lr 9.045084971874742e-05
Step 140000 Loss: 0.2841, lr 9.045084971874742e-05
Step 140100 Loss: 0.2772, lr 9.045084971874742e-05
Step 140200 Loss: 0.2706, lr 9.045084971874742e-05
Step 140300 Loss: 0.2642, lr 9.045084971874742e-05
Step 140400 Loss: 0.2556, lr 9.045084971874742e-05
Step 140500 Loss: 0.2463, lr 9.045084971874742e-05
Step 140600 Loss: 0.2373, lr 9.045084971874742e-05
Step 140700 Loss: 0.2286, lr 9.045084971874742e-05
Step 140800 Loss: 0.2202, lr 9.045084971874742e-05
Step 140900 Loss: 0.2122, lr 9.045084971874742e-05
Step 141000 Loss: 0.2044, lr 9.045084971874742e-05
Step 141100 Loss: 0.1943, lr 9.045084971874742e-05
Step 141200 Loss: 0.1833, lr 9.045084971874742e-05
Step 141300 Loss: 0.1726, lr 9.045084971874742e-05
Step 141400 Loss: 0.1623, lr 9.045084971874742e-05
Step 141500 Loss: 0.1510, lr 9.045084971874742e-05
Step 141600 Loss: 0.1367, lr 9.045084971874742e-05
Step 141700 Loss: 0.1229, lr 9.045084971874742e-05
Step 141800 Loss: 0.1095, lr 9.045084971874742e-05
Train Epoch: [21/100] Loss: 0.0962,lr 0.000090
Calling G2SDataset.batch()
Done, time:  1.96 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Step 141900 Loss: 1.1668, lr 8.950775061878456e-05
Step 142000 Loss: 0.9853, lr 8.950775061878456e-05
Step 142100 Loss: 0.8932, lr 8.950775061878456e-05
Step 142200 Loss: 0.8410, lr 8.950775061878456e-05
Step 142300 Loss: 0.8131, lr 8.950775061878456e-05
Step 142400 Loss: 0.7897, lr 8.950775061878456e-05
Step 142500 Loss: 0.7520, lr 8.950775061878456e-05
Step 142600 Loss: 0.7247, lr 8.950775061878456e-05
Step 142700 Loss: 0.7039, lr 8.950775061878456e-05
Step 142800 Loss: 0.6876, lr 8.950775061878456e-05
Step 142900 Loss: 0.6745, lr 8.950775061878456e-05
Step 143000 Loss: 0.6613, lr 8.950775061878456e-05
Step 143100 Loss: 0.6446, lr 8.950775061878456e-05
Step 143200 Loss: 0.6304, lr 8.950775061878456e-05
Step 143300 Loss: 0.6182, lr 8.950775061878456e-05
Step 143400 Loss: 0.6076, lr 8.950775061878456e-05
Step 143500 Loss: 0.5983, lr 8.950775061878456e-05
Step 143600 Loss: 0.5901, lr 8.950775061878456e-05
Step 143700 Loss: 0.5828, lr 8.950775061878456e-05
Step 143800 Loss: 0.5672, lr 8.950775061878456e-05
Step 143900 Loss: 0.5506, lr 8.950775061878456e-05
Step 144000 Loss: 0.5356, lr 8.950775061878456e-05
Step 144100 Loss: 0.5219, lr 8.950775061878456e-05
Step 144200 Loss: 0.5094, lr 8.950775061878456e-05
Step 144300 Loss: 0.4979, lr 8.950775061878456e-05
Step 144400 Loss: 0.4874, lr 8.950775061878456e-05
Step 144500 Loss: 0.4776, lr 8.950775061878456e-05
Step 144600 Loss: 0.4685, lr 8.950775061878456e-05
Step 144700 Loss: 0.4560, lr 8.950775061878456e-05
Step 144800 Loss: 0.4437, lr 8.950775061878456e-05
Step 144900 Loss: 0.4322, lr 8.950775061878456e-05
Step 145000 Loss: 0.4214, lr 8.950775061878456e-05
Step 145100 Loss: 0.4113, lr 8.950775061878456e-05
Step 145200 Loss: 0.4018, lr 8.950775061878456e-05
Step 145300 Loss: 0.3929, lr 8.950775061878456e-05
Step 145400 Loss: 0.3844, lr 8.950775061878456e-05
Step 145500 Loss: 0.3765, lr 8.950775061878456e-05
Step 145600 Loss: 0.3689, lr 8.950775061878456e-05
Step 145700 Loss: 0.3618, lr 8.950775061878456e-05
Step 145800 Loss: 0.3550, lr 8.950775061878456e-05
Step 145900 Loss: 0.3485, lr 8.950775061878456e-05
Step 146000 Loss: 0.3424, lr 8.950775061878456e-05
Step 146100 Loss: 0.3366, lr 8.950775061878456e-05
Step 146200 Loss: 0.3285, lr 8.950775061878456e-05
Step 146300 Loss: 0.3197, lr 8.950775061878456e-05
Step 146400 Loss: 0.3113, lr 8.950775061878456e-05
Step 146500 Loss: 0.3032, lr 8.950775061878456e-05
Step 146600 Loss: 0.2955, lr 8.950775061878456e-05
Step 146700 Loss: 0.2881, lr 8.950775061878456e-05
Step 146800 Loss: 0.2810, lr 8.950775061878456e-05
Step 146900 Loss: 0.2742, lr 8.950775061878456e-05
Step 147000 Loss: 0.2677, lr 8.950775061878456e-05
Step 147100 Loss: 0.2610, lr 8.950775061878456e-05
Step 147200 Loss: 0.2515, lr 8.950775061878456e-05
Step 147300 Loss: 0.2423, lr 8.950775061878456e-05
Step 147400 Loss: 0.2335, lr 8.950775061878456e-05
Step 147500 Loss: 0.2249, lr 8.950775061878456e-05
Step 147600 Loss: 0.2167, lr 8.950775061878456e-05
Step 147700 Loss: 0.2087, lr 8.950775061878456e-05
Step 147800 Loss: 0.2006, lr 8.950775061878456e-05
Step 147900 Loss: 0.1894, lr 8.950775061878456e-05
Step 148000 Loss: 0.1785, lr 8.950775061878456e-05
Step 148100 Loss: 0.1680, lr 8.950775061878456e-05
Step 148200 Loss: 0.1579, lr 8.950775061878456e-05
Step 148300 Loss: 0.1446, lr 8.950775061878456e-05
Step 148400 Loss: 0.1306, lr 8.950775061878456e-05
Step 148500 Loss: 0.1169, lr 8.950775061878456e-05
Step 148600 Loss: 0.1013, lr 8.950775061878456e-05
Train Epoch: [22/100] Loss: 0.0960,lr 0.000090
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Step 148700 Loss: 1.0274, lr 8.852566213878951e-05
Step 148800 Loss: 0.9493, lr 8.852566213878951e-05
Step 148900 Loss: 0.8660, lr 8.852566213878951e-05
Step 149000 Loss: 0.8272, lr 8.852566213878951e-05
Step 149100 Loss: 0.8048, lr 8.852566213878951e-05
Step 149200 Loss: 0.7717, lr 8.852566213878951e-05
Step 149300 Loss: 0.7392, lr 8.852566213878951e-05
Step 149400 Loss: 0.7150, lr 8.852566213878951e-05
Step 149500 Loss: 0.6964, lr 8.852566213878951e-05
Step 149600 Loss: 0.6816, lr 8.852566213878951e-05
Step 149700 Loss: 0.6696, lr 8.852566213878951e-05
Step 149800 Loss: 0.6537, lr 8.852566213878951e-05
Step 149900 Loss: 0.6382, lr 8.852566213878951e-05
Step 150000 Loss: 0.6249, lr 8.852566213878951e-05
Step 150100 Loss: 0.6135, lr 8.852566213878951e-05
Step 150200 Loss: 0.6035, lr 8.852566213878951e-05
Step 150300 Loss: 0.5947, lr 8.852566213878951e-05
Step 150400 Loss: 0.5869, lr 8.852566213878951e-05
Step 150500 Loss: 0.5772, lr 8.852566213878951e-05
Step 150600 Loss: 0.5597, lr 8.852566213878951e-05
Step 150700 Loss: 0.5438, lr 8.852566213878951e-05
Step 150800 Loss: 0.5294, lr 8.852566213878951e-05
Step 150900 Loss: 0.5163, lr 8.852566213878951e-05
Step 151000 Loss: 0.5042, lr 8.852566213878951e-05
Step 151100 Loss: 0.4932, lr 8.852566213878951e-05
Step 151200 Loss: 0.4830, lr 8.852566213878951e-05
Step 151300 Loss: 0.4735, lr 8.852566213878951e-05
Step 151400 Loss: 0.4632, lr 8.852566213878951e-05
Step 151500 Loss: 0.4504, lr 8.852566213878951e-05
Step 151600 Loss: 0.4385, lr 8.852566213878951e-05
Step 151700 Loss: 0.4273, lr 8.852566213878951e-05
Step 151800 Loss: 0.4169, lr 8.852566213878951e-05
Step 151900 Loss: 0.4070, lr 8.852566213878951e-05
Step 152000 Loss: 0.3978, lr 8.852566213878951e-05
Step 152100 Loss: 0.3891, lr 8.852566213878951e-05
Step 152200 Loss: 0.3809, lr 8.852566213878951e-05
Step 152300 Loss: 0.3731, lr 8.852566213878951e-05
Step 152400 Loss: 0.3657, lr 8.852566213878951e-05
Step 152500 Loss: 0.3587, lr 8.852566213878951e-05
Step 152600 Loss: 0.3521, lr 8.852566213878951e-05
Step 152700 Loss: 0.3458, lr 8.852566213878951e-05
Step 152800 Loss: 0.3398, lr 8.852566213878951e-05
Step 152900 Loss: 0.3336, lr 8.852566213878951e-05
Step 153000 Loss: 0.3246, lr 8.852566213878951e-05
Step 153100 Loss: 0.3159, lr 8.852566213878951e-05
Step 153200 Loss: 0.3077, lr 8.852566213878951e-05
Step 153300 Loss: 0.2998, lr 8.852566213878951e-05
Step 153400 Loss: 0.2922, lr 8.852566213878951e-05
Step 153500 Loss: 0.2850, lr 8.852566213878951e-05
Step 153600 Loss: 0.2780, lr 8.852566213878951e-05
Step 153700 Loss: 0.2713, lr 8.852566213878951e-05
Step 153800 Loss: 0.2649, lr 8.852566213878951e-05
Step 153900 Loss: 0.2568, lr 8.852566213878951e-05
Step 154000 Loss: 0.2474, lr 8.852566213878951e-05
Step 154100 Loss: 0.2384, lr 8.852566213878951e-05
Step 154200 Loss: 0.2296, lr 8.852566213878951e-05
Step 154300 Loss: 0.2212, lr 8.852566213878951e-05
Step 154400 Loss: 0.2131, lr 8.852566213878951e-05
Step 154500 Loss: 0.2053, lr 8.852566213878951e-05
Step 154600 Loss: 0.1956, lr 8.852566213878951e-05
Step 154700 Loss: 0.1846, lr 8.852566213878951e-05
Step 154800 Loss: 0.1739, lr 8.852566213878951e-05
Step 154900 Loss: 0.1635, lr 8.852566213878951e-05
Step 155000 Loss: 0.1527, lr 8.852566213878951e-05
Step 155100 Loss: 0.1384, lr 8.852566213878951e-05
Step 155200 Loss: 0.1245, lr 8.852566213878951e-05
Step 155300 Loss: 0.1110, lr 8.852566213878951e-05
Train Epoch: [23/100] Loss: 0.0959,lr 0.000089
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 155400 Loss: 1.2526, lr 8.750555348152303e-05
Step 155500 Loss: 0.9909, lr 8.750555348152303e-05
Step 155600 Loss: 0.9026, lr 8.750555348152303e-05
Step 155700 Loss: 0.8455, lr 8.750555348152303e-05
Step 155800 Loss: 0.8157, lr 8.750555348152303e-05
Step 155900 Loss: 0.7951, lr 8.750555348152303e-05
Step 156000 Loss: 0.7558, lr 8.750555348152303e-05
Step 156100 Loss: 0.7275, lr 8.750555348152303e-05
Step 156200 Loss: 0.7061, lr 8.750555348152303e-05
Step 156300 Loss: 0.6893, lr 8.750555348152303e-05
Step 156400 Loss: 0.6759, lr 8.750555348152303e-05
Step 156500 Loss: 0.6635, lr 8.750555348152303e-05
Step 156600 Loss: 0.6465, lr 8.750555348152303e-05
Step 156700 Loss: 0.6321, lr 8.750555348152303e-05
Step 156800 Loss: 0.6196, lr 8.750555348152303e-05
Step 156900 Loss: 0.6089, lr 8.750555348152303e-05
Step 157000 Loss: 0.5994, lr 8.750555348152303e-05
Step 157100 Loss: 0.5911, lr 8.750555348152303e-05
Step 157200 Loss: 0.5837, lr 8.750555348152303e-05
Step 157300 Loss: 0.5693, lr 8.750555348152303e-05
Step 157400 Loss: 0.5525, lr 8.750555348152303e-05
Step 157500 Loss: 0.5373, lr 8.750555348152303e-05
Step 157600 Loss: 0.5235, lr 8.750555348152303e-05
Step 157700 Loss: 0.5108, lr 8.750555348152303e-05
Step 157800 Loss: 0.4993, lr 8.750555348152303e-05
Step 157900 Loss: 0.4886, lr 8.750555348152303e-05
Step 158000 Loss: 0.4787, lr 8.750555348152303e-05
Step 158100 Loss: 0.4696, lr 8.750555348152303e-05
Step 158200 Loss: 0.4575, lr 8.750555348152303e-05
Step 158300 Loss: 0.4451, lr 8.750555348152303e-05
Step 158400 Loss: 0.4335, lr 8.750555348152303e-05
Step 158500 Loss: 0.4226, lr 8.750555348152303e-05
Step 158600 Loss: 0.4125, lr 8.750555348152303e-05
Step 158700 Loss: 0.4029, lr 8.750555348152303e-05
Step 158800 Loss: 0.3939, lr 8.750555348152303e-05
Step 158900 Loss: 0.3854, lr 8.750555348152303e-05
Step 159000 Loss: 0.3774, lr 8.750555348152303e-05
Step 159100 Loss: 0.3698, lr 8.750555348152303e-05
Step 159200 Loss: 0.3626, lr 8.750555348152303e-05
Step 159300 Loss: 0.3558, lr 8.750555348152303e-05
Step 159400 Loss: 0.3493, lr 8.750555348152303e-05
Step 159500 Loss: 0.3431, lr 8.750555348152303e-05
Step 159600 Loss: 0.3372, lr 8.750555348152303e-05
Step 159700 Loss: 0.3296, lr 8.750555348152303e-05
Step 159800 Loss: 0.3207, lr 8.750555348152303e-05
Step 159900 Loss: 0.3123, lr 8.750555348152303e-05
Step 160000 Loss: 0.3042, lr 8.750555348152303e-05
Step 160100 Loss: 0.2964, lr 8.750555348152303e-05
Step 160200 Loss: 0.2890, lr 8.750555348152303e-05
Step 160300 Loss: 0.2819, lr 8.750555348152303e-05
Step 160400 Loss: 0.2750, lr 8.750555348152303e-05
Step 160500 Loss: 0.2684, lr 8.750555348152303e-05
Step 160600 Loss: 0.2621, lr 8.750555348152303e-05
Step 160700 Loss: 0.2526, lr 8.750555348152303e-05
Step 160800 Loss: 0.2434, lr 8.750555348152303e-05
Step 160900 Loss: 0.2345, lr 8.750555348152303e-05
Step 161000 Loss: 0.2259, lr 8.750555348152303e-05
Step 161100 Loss: 0.2176, lr 8.750555348152303e-05
Step 161200 Loss: 0.2096, lr 8.750555348152303e-05
Step 161300 Loss: 0.2019, lr 8.750555348152303e-05
Step 161400 Loss: 0.1907, lr 8.750555348152303e-05
Step 161500 Loss: 0.1798, lr 8.750555348152303e-05
Step 161600 Loss: 0.1692, lr 8.750555348152303e-05
Step 161700 Loss: 0.1590, lr 8.750555348152303e-05
Step 161800 Loss: 0.1463, lr 8.750555348152303e-05
Step 161900 Loss: 0.1322, lr 8.750555348152303e-05
Step 162000 Loss: 0.1185, lr 8.750555348152303e-05
Step 162100 Loss: 0.1035, lr 8.750555348152303e-05
Train Epoch: [24/100] Loss: 0.0961,lr 0.000088
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Step 162200 Loss: 1.0450, lr 8.644843137107063e-05
Step 162300 Loss: 0.9666, lr 8.644843137107063e-05
Step 162400 Loss: 0.8729, lr 8.644843137107063e-05
Step 162500 Loss: 0.8309, lr 8.644843137107063e-05
Step 162600 Loss: 0.8071, lr 8.644843137107063e-05
Step 162700 Loss: 0.7765, lr 8.644843137107063e-05
Step 162800 Loss: 0.7426, lr 8.644843137107063e-05
Step 162900 Loss: 0.7176, lr 8.644843137107063e-05
Step 163000 Loss: 0.6985, lr 8.644843137107063e-05
Step 163100 Loss: 0.6833, lr 8.644843137107063e-05
Step 163200 Loss: 0.6709, lr 8.644843137107063e-05
Step 163300 Loss: 0.6557, lr 8.644843137107063e-05
Step 163400 Loss: 0.6399, lr 8.644843137107063e-05
Step 163500 Loss: 0.6264, lr 8.644843137107063e-05
Step 163600 Loss: 0.6148, lr 8.644843137107063e-05
Step 163700 Loss: 0.6046, lr 8.644843137107063e-05
Step 163800 Loss: 0.5957, lr 8.644843137107063e-05
Step 163900 Loss: 0.5877, lr 8.644843137107063e-05
Step 164000 Loss: 0.5795, lr 8.644843137107063e-05
Step 164100 Loss: 0.5617, lr 8.644843137107063e-05
Step 164200 Loss: 0.5456, lr 8.644843137107063e-05
Step 164300 Loss: 0.5310, lr 8.644843137107063e-05
Step 164400 Loss: 0.5178, lr 8.644843137107063e-05
Step 164500 Loss: 0.5056, lr 8.644843137107063e-05
Step 164600 Loss: 0.4944, lr 8.644843137107063e-05
Step 164700 Loss: 0.4841, lr 8.644843137107063e-05
Step 164800 Loss: 0.4746, lr 8.644843137107063e-05
Step 164900 Loss: 0.4648, lr 8.644843137107063e-05
Step 165000 Loss: 0.4519, lr 8.644843137107063e-05
Step 165100 Loss: 0.4399, lr 8.644843137107063e-05
Step 165200 Loss: 0.4286, lr 8.644843137107063e-05
Step 165300 Loss: 0.4181, lr 8.644843137107063e-05
Step 165400 Loss: 0.4082, lr 8.644843137107063e-05
Step 165500 Loss: 0.3989, lr 8.644843137107063e-05
Step 165600 Loss: 0.3901, lr 8.644843137107063e-05
Step 165700 Loss: 0.3818, lr 8.644843137107063e-05
Step 165800 Loss: 0.3740, lr 8.644843137107063e-05
Step 165900 Loss: 0.3666, lr 8.644843137107063e-05
Step 166000 Loss: 0.3595, lr 8.644843137107063e-05
Step 166100 Loss: 0.3529, lr 8.644843137107063e-05
Step 166200 Loss: 0.3465, lr 8.644843137107063e-05
Step 166300 Loss: 0.3405, lr 8.644843137107063e-05
Step 166400 Loss: 0.3347, lr 8.644843137107063e-05
Step 166500 Loss: 0.3256, lr 8.644843137107063e-05
Step 166600 Loss: 0.3169, lr 8.644843137107063e-05
Step 166700 Loss: 0.3086, lr 8.644843137107063e-05
Step 166800 Loss: 0.3007, lr 8.644843137107063e-05
Step 166900 Loss: 0.2931, lr 8.644843137107063e-05
Step 167000 Loss: 0.2858, lr 8.644843137107063e-05
Step 167100 Loss: 0.2788, lr 8.644843137107063e-05
Step 167200 Loss: 0.2721, lr 8.644843137107063e-05
Step 167300 Loss: 0.2656, lr 8.644843137107063e-05
Step 167400 Loss: 0.2579, lr 8.644843137107063e-05
Step 167500 Loss: 0.2485, lr 8.644843137107063e-05
Step 167600 Loss: 0.2394, lr 8.644843137107063e-05
Step 167700 Loss: 0.2306, lr 8.644843137107063e-05
Step 167800 Loss: 0.2222, lr 8.644843137107063e-05
Step 167900 Loss: 0.2140, lr 8.644843137107063e-05
Step 168000 Loss: 0.2062, lr 8.644843137107063e-05
Step 168100 Loss: 0.1969, lr 8.644843137107063e-05
Step 168200 Loss: 0.1858, lr 8.644843137107063e-05
Step 168300 Loss: 0.1751, lr 8.644843137107063e-05
Step 168400 Loss: 0.1647, lr 8.644843137107063e-05
Step 168500 Loss: 0.1544, lr 8.644843137107063e-05
Step 168600 Loss: 0.1400, lr 8.644843137107063e-05
Step 168700 Loss: 0.1261, lr 8.644843137107063e-05
Step 168800 Loss: 0.1126, lr 8.644843137107063e-05
Train Epoch: [25/100] Loss: 0.0964,lr 0.000086
Model Saving at epoch 25
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.22 s, total batches: 6756
Step 168900 Loss: 1.2528, lr 8.535533905932742e-05
Step 169000 Loss: 0.9975, lr 8.535533905932742e-05
Step 169100 Loss: 0.9125, lr 8.535533905932742e-05
Step 169200 Loss: 0.8501, lr 8.535533905932742e-05
Step 169300 Loss: 0.8184, lr 8.535533905932742e-05
Step 169400 Loss: 0.7991, lr 8.535533905932742e-05
Step 169500 Loss: 0.7596, lr 8.535533905932742e-05
Step 169600 Loss: 0.7303, lr 8.535533905932742e-05
Step 169700 Loss: 0.7082, lr 8.535533905932742e-05
Step 169800 Loss: 0.6910, lr 8.535533905932742e-05
Step 169900 Loss: 0.6772, lr 8.535533905932742e-05
Step 170000 Loss: 0.6656, lr 8.535533905932742e-05
Step 170100 Loss: 0.6482, lr 8.535533905932742e-05
Step 170200 Loss: 0.6335, lr 8.535533905932742e-05
Step 170300 Loss: 0.6209, lr 8.535533905932742e-05
Step 170400 Loss: 0.6100, lr 8.535533905932742e-05
Step 170500 Loss: 0.6004, lr 8.535533905932742e-05
Step 170600 Loss: 0.5919, lr 8.535533905932742e-05
Step 170700 Loss: 0.5844, lr 8.535533905932742e-05
Step 170800 Loss: 0.5712, lr 8.535533905932742e-05
Step 170900 Loss: 0.5542, lr 8.535533905932742e-05
Step 171000 Loss: 0.5388, lr 8.535533905932742e-05
Step 171100 Loss: 0.5249, lr 8.535533905932742e-05
Step 171200 Loss: 0.5121, lr 8.535533905932742e-05
Step 171300 Loss: 0.5004, lr 8.535533905932742e-05
Step 171400 Loss: 0.4896, lr 8.535533905932742e-05
Step 171500 Loss: 0.4797, lr 8.535533905932742e-05
Step 171600 Loss: 0.4705, lr 8.535533905932742e-05
Step 171700 Loss: 0.4589, lr 8.535533905932742e-05
Step 171800 Loss: 0.4464, lr 8.535533905932742e-05
Step 171900 Loss: 0.4347, lr 8.535533905932742e-05
Step 172000 Loss: 0.4237, lr 8.535533905932742e-05
Step 172100 Loss: 0.4135, lr 8.535533905932742e-05
Step 172200 Loss: 0.4039, lr 8.535533905932742e-05
Step 172300 Loss: 0.3948, lr 8.535533905932742e-05
Step 172400 Loss: 0.3863, lr 8.535533905932742e-05
Step 172500 Loss: 0.3782, lr 8.535533905932742e-05
Step 172600 Loss: 0.3706, lr 8.535533905932742e-05
Step 172700 Loss: 0.3633, lr 8.535533905932742e-05
Step 172800 Loss: 0.3565, lr 8.535533905932742e-05
Step 172900 Loss: 0.3499, lr 8.535533905932742e-05
Step 173000 Loss: 0.3437, lr 8.535533905932742e-05
Step 173100 Loss: 0.3378, lr 8.535533905932742e-05
Step 173200 Loss: 0.3305, lr 8.535533905932742e-05
Step 173300 Loss: 0.3216, lr 8.535533905932742e-05
Step 173400 Loss: 0.3131, lr 8.535533905932742e-05
Step 173500 Loss: 0.3050, lr 8.535533905932742e-05
Step 173600 Loss: 0.2972, lr 8.535533905932742e-05
Step 173700 Loss: 0.2897, lr 8.535533905932742e-05
Step 173800 Loss: 0.2826, lr 8.535533905932742e-05
Step 173900 Loss: 0.2757, lr 8.535533905932742e-05
Step 174000 Loss: 0.2691, lr 8.535533905932742e-05
Step 174100 Loss: 0.2628, lr 8.535533905932742e-05
Step 174200 Loss: 0.2536, lr 8.535533905932742e-05
Step 174300 Loss: 0.2443, lr 8.535533905932742e-05
Step 174400 Loss: 0.2354, lr 8.535533905932742e-05
Step 174500 Loss: 0.2268, lr 8.535533905932742e-05
Step 174600 Loss: 0.2185, lr 8.535533905932742e-05
Step 174700 Loss: 0.2104, lr 8.535533905932742e-05
Step 174800 Loss: 0.2027, lr 8.535533905932742e-05
Step 174900 Loss: 0.1919, lr 8.535533905932742e-05
Step 175000 Loss: 0.1809, lr 8.535533905932742e-05
Step 175100 Loss: 0.1704, lr 8.535533905932742e-05
Step 175200 Loss: 0.1601, lr 8.535533905932742e-05
Step 175300 Loss: 0.1479, lr 8.535533905932742e-05
Step 175400 Loss: 0.1337, lr 8.535533905932742e-05
Step 175500 Loss: 0.1200, lr 8.535533905932742e-05
Step 175600 Loss: 0.1056, lr 8.535533905932742e-05
Train Epoch: [26/100] Loss: 0.0960,lr 0.000085
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Step 175700 Loss: 1.0696, lr 8.422735529643448e-05
Step 175800 Loss: 0.9775, lr 8.422735529643448e-05
Step 175900 Loss: 0.8795, lr 8.422735529643448e-05
Step 176000 Loss: 0.8343, lr 8.422735529643448e-05
Step 176100 Loss: 0.8091, lr 8.422735529643448e-05
Step 176200 Loss: 0.7809, lr 8.422735529643448e-05
Step 176300 Loss: 0.7458, lr 8.422735529643448e-05
Step 176400 Loss: 0.7200, lr 8.422735529643448e-05
Step 176500 Loss: 0.7003, lr 8.422735529643448e-05
Step 176600 Loss: 0.6847, lr 8.422735529643448e-05
Step 176700 Loss: 0.6721, lr 8.422735529643448e-05
Step 176800 Loss: 0.6576, lr 8.422735529643448e-05
Step 176900 Loss: 0.6415, lr 8.422735529643448e-05
Step 177000 Loss: 0.6278, lr 8.422735529643448e-05
Step 177100 Loss: 0.6159, lr 8.422735529643448e-05
Step 177200 Loss: 0.6056, lr 8.422735529643448e-05
Step 177300 Loss: 0.5966, lr 8.422735529643448e-05
Step 177400 Loss: 0.5885, lr 8.422735529643448e-05
Step 177500 Loss: 0.5814, lr 8.422735529643448e-05
Step 177600 Loss: 0.5635, lr 8.422735529643448e-05
Step 177700 Loss: 0.5473, lr 8.422735529643448e-05
Step 177800 Loss: 0.5325, lr 8.422735529643448e-05
Step 177900 Loss: 0.5191, lr 8.422735529643448e-05
Step 178000 Loss: 0.5068, lr 8.422735529643448e-05
Step 178100 Loss: 0.4956, lr 8.422735529643448e-05
Step 178200 Loss: 0.4852, lr 8.422735529643448e-05
Step 178300 Loss: 0.4756, lr 8.422735529643448e-05
Step 178400 Loss: 0.4663, lr 8.422735529643448e-05
Step 178500 Loss: 0.4533, lr 8.422735529643448e-05
Step 178600 Loss: 0.4411, lr 8.422735529643448e-05
Step 178700 Loss: 0.4298, lr 8.422735529643448e-05
Step 178800 Loss: 0.4192, lr 8.422735529643448e-05
Step 178900 Loss: 0.4092, lr 8.422735529643448e-05
Step 179000 Loss: 0.3998, lr 8.422735529643448e-05
Step 179100 Loss: 0.3910, lr 8.422735529643448e-05
Step 179200 Loss: 0.3827, lr 8.422735529643448e-05
Step 179300 Loss: 0.3748, lr 8.422735529643448e-05
Step 179400 Loss: 0.3673, lr 8.422735529643448e-05
Step 179500 Loss: 0.3603, lr 8.422735529643448e-05
Step 179600 Loss: 0.3535, lr 8.422735529643448e-05
Step 179700 Loss: 0.3472, lr 8.422735529643448e-05
Step 179800 Loss: 0.3411, lr 8.422735529643448e-05
Step 179900 Loss: 0.3353, lr 8.422735529643448e-05
Step 180000 Loss: 0.3265, lr 8.422735529643448e-05
Step 180100 Loss: 0.3178, lr 8.422735529643448e-05
Step 180200 Loss: 0.3095, lr 8.422735529643448e-05
Step 180300 Loss: 0.3015, lr 8.422735529643448e-05
Step 180400 Loss: 0.2939, lr 8.422735529643448e-05
Step 180500 Loss: 0.2865, lr 8.422735529643448e-05
Step 180600 Loss: 0.2795, lr 8.422735529643448e-05
Step 180700 Loss: 0.2728, lr 8.422735529643448e-05
Step 180800 Loss: 0.2663, lr 8.422735529643448e-05
Step 180900 Loss: 0.2589, lr 8.422735529643448e-05
Step 181000 Loss: 0.2494, lr 8.422735529643448e-05
Step 181100 Loss: 0.2403, lr 8.422735529643448e-05
Step 181200 Loss: 0.2315, lr 8.422735529643448e-05
Step 181300 Loss: 0.2230, lr 8.422735529643448e-05
Step 181400 Loss: 0.2149, lr 8.422735529643448e-05
Step 181500 Loss: 0.2070, lr 8.422735529643448e-05
Step 181600 Loss: 0.1981, lr 8.422735529643448e-05
Step 181700 Loss: 0.1870, lr 8.422735529643448e-05
Step 181800 Loss: 0.1762, lr 8.422735529643448e-05
Step 181900 Loss: 0.1658, lr 8.422735529643448e-05
Step 182000 Loss: 0.1557, lr 8.422735529643448e-05
Step 182100 Loss: 0.1415, lr 8.422735529643448e-05
Step 182200 Loss: 0.1276, lr 8.422735529643448e-05
Step 182300 Loss: 0.1140, lr 8.422735529643448e-05
Step 182400 Loss: 0.0971, lr 8.422735529643448e-05
Train Epoch: [27/100] Loss: 0.0958,lr 0.000084
Calling G2SDataset.batch()
Done, time:  1.97 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6756
Step 182500 Loss: 1.0058, lr 8.306559326618263e-05
Step 182600 Loss: 0.9253, lr 8.306559326618263e-05
Step 182700 Loss: 0.8560, lr 8.306559326618263e-05
Step 182800 Loss: 0.8218, lr 8.306559326618263e-05
Step 182900 Loss: 0.8014, lr 8.306559326618263e-05
Step 183000 Loss: 0.7640, lr 8.306559326618263e-05
Step 183100 Loss: 0.7336, lr 8.306559326618263e-05
Step 183200 Loss: 0.7108, lr 8.306559326618263e-05
Step 183300 Loss: 0.6931, lr 8.306559326618263e-05
Step 183400 Loss: 0.6789, lr 8.306559326618263e-05
Step 183500 Loss: 0.6673, lr 8.306559326618263e-05
Step 183600 Loss: 0.6502, lr 8.306559326618263e-05
Step 183700 Loss: 0.6352, lr 8.306559326618263e-05
Step 183800 Loss: 0.6224, lr 8.306559326618263e-05
Step 183900 Loss: 0.6112, lr 8.306559326618263e-05
Step 184000 Loss: 0.6015, lr 8.306559326618263e-05
Step 184100 Loss: 0.5929, lr 8.306559326618263e-05
Step 184200 Loss: 0.5853, lr 8.306559326618263e-05
Step 184300 Loss: 0.5734, lr 8.306559326618263e-05
Step 184400 Loss: 0.5562, lr 8.306559326618263e-05
Step 184500 Loss: 0.5406, lr 8.306559326618263e-05
Step 184600 Loss: 0.5265, lr 8.306559326618263e-05
Step 184700 Loss: 0.5136, lr 8.306559326618263e-05
Step 184800 Loss: 0.5018, lr 8.306559326618263e-05
Step 184900 Loss: 0.4909, lr 8.306559326618263e-05
Step 185000 Loss: 0.4809, lr 8.306559326618263e-05
Step 185100 Loss: 0.4716, lr 8.306559326618263e-05
Step 185200 Loss: 0.4604, lr 8.306559326618263e-05
Step 185300 Loss: 0.4478, lr 8.306559326618263e-05
Step 185400 Loss: 0.4360, lr 8.306559326618263e-05
Step 185500 Loss: 0.4250, lr 8.306559326618263e-05
Step 185600 Loss: 0.4147, lr 8.306559326618263e-05
Step 185700 Loss: 0.4050, lr 8.306559326618263e-05
Step 185800 Loss: 0.3959, lr 8.306559326618263e-05
Step 185900 Loss: 0.3873, lr 8.306559326618263e-05
Step 186000 Loss: 0.3791, lr 8.306559326618263e-05
Step 186100 Loss: 0.3714, lr 8.306559326618263e-05
Step 186200 Loss: 0.3642, lr 8.306559326618263e-05
Step 186300 Loss: 0.3572, lr 8.306559326618263e-05
Step 186400 Loss: 0.3507, lr 8.306559326618263e-05
Step 186500 Loss: 0.3444, lr 8.306559326618263e-05
Step 186600 Loss: 0.3385, lr 8.306559326618263e-05
Step 186700 Loss: 0.3316, lr 8.306559326618263e-05
Step 186800 Loss: 0.3226, lr 8.306559326618263e-05
Step 186900 Loss: 0.3141, lr 8.306559326618263e-05
Step 187000 Loss: 0.3059, lr 8.306559326618263e-05
Step 187100 Loss: 0.2981, lr 8.306559326618263e-05
Step 187200 Loss: 0.2906, lr 8.306559326618263e-05
Step 187300 Loss: 0.2834, lr 8.306559326618263e-05
Step 187400 Loss: 0.2765, lr 8.306559326618263e-05
Step 187500 Loss: 0.2699, lr 8.306559326618263e-05
Step 187600 Loss: 0.2635, lr 8.306559326618263e-05
Step 187700 Loss: 0.2547, lr 8.306559326618263e-05
Step 187800 Loss: 0.2454, lr 8.306559326618263e-05
Step 187900 Loss: 0.2364, lr 8.306559326618263e-05
Step 188000 Loss: 0.2277, lr 8.306559326618263e-05
Step 188100 Loss: 0.2194, lr 8.306559326618263e-05
Step 188200 Loss: 0.2113, lr 8.306559326618263e-05
Step 188300 Loss: 0.2036, lr 8.306559326618263e-05
Step 188400 Loss: 0.1931, lr 8.306559326618263e-05
Step 188500 Loss: 0.1822, lr 8.306559326618263e-05
Step 188600 Loss: 0.1715, lr 8.306559326618263e-05
Step 188700 Loss: 0.1612, lr 8.306559326618263e-05
Step 188800 Loss: 0.1495, lr 8.306559326618263e-05
Step 188900 Loss: 0.1353, lr 8.306559326618263e-05
Step 189000 Loss: 0.1215, lr 8.306559326618263e-05
Step 189100 Loss: 0.1079, lr 8.306559326618263e-05
Train Epoch: [28/100] Loss: 0.0958,lr 0.000083
Calling G2SDataset.batch()
Done, time:  2.07 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6755
Step 189200 Loss: 1.1137, lr 8.187119948743452e-05
Step 189300 Loss: 0.9819, lr 8.187119948743452e-05
Step 189400 Loss: 0.8874, lr 8.187119948743452e-05
Step 189500 Loss: 0.8383, lr 8.187119948743452e-05
Step 189600 Loss: 0.8115, lr 8.187119948743452e-05
Step 189700 Loss: 0.7859, lr 8.187119948743452e-05
Step 189800 Loss: 0.7494, lr 8.187119948743452e-05
Step 189900 Loss: 0.7227, lr 8.187119948743452e-05
Step 190000 Loss: 0.7024, lr 8.187119948743452e-05
Step 190100 Loss: 0.6864, lr 8.187119948743452e-05
Step 190200 Loss: 0.6735, lr 8.187119948743452e-05
Step 190300 Loss: 0.6597, lr 8.187119948743452e-05
Step 190400 Loss: 0.6432, lr 8.187119948743452e-05
Step 190500 Loss: 0.6293, lr 8.187119948743452e-05
Step 190600 Loss: 0.6172, lr 8.187119948743452e-05
Step 190700 Loss: 0.6067, lr 8.187119948743452e-05
Step 190800 Loss: 0.5975, lr 8.187119948743452e-05
Step 190900 Loss: 0.5894, lr 8.187119948743452e-05
Step 191000 Loss: 0.5822, lr 8.187119948743452e-05
Step 191100 Loss: 0.5655, lr 8.187119948743452e-05
Step 191200 Loss: 0.5491, lr 8.187119948743452e-05
Step 191300 Loss: 0.5342, lr 8.187119948743452e-05
Step 191400 Loss: 0.5206, lr 8.187119948743452e-05
Step 191500 Loss: 0.5082, lr 8.187119948743452e-05
Step 191600 Loss: 0.4968, lr 8.187119948743452e-05
Step 191700 Loss: 0.4863, lr 8.187119948743452e-05
Step 191800 Loss: 0.4766, lr 8.187119948743452e-05
Step 191900 Loss: 0.4677, lr 8.187119948743452e-05
Step 192000 Loss: 0.4547, lr 8.187119948743452e-05
Step 192100 Loss: 0.4425, lr 8.187119948743452e-05
Step 192200 Loss: 0.4311, lr 8.187119948743452e-05
Step 192300 Loss: 0.4204, lr 8.187119948743452e-05
Step 192400 Loss: 0.4103, lr 8.187119948743452e-05
Step 192500 Loss: 0.4009, lr 8.187119948743452e-05
Step 192600 Loss: 0.3920, lr 8.187119948743452e-05
Step 192700 Loss: 0.3836, lr 8.187119948743452e-05
Step 192800 Loss: 0.3757, lr 8.187119948743452e-05
Step 192900 Loss: 0.3682, lr 8.187119948743452e-05
Step 193000 Loss: 0.3610, lr 8.187119948743452e-05
Step 193100 Loss: 0.3543, lr 8.187119948743452e-05
Step 193200 Loss: 0.3479, lr 8.187119948743452e-05
Step 193300 Loss: 0.3418, lr 8.187119948743452e-05
Step 193400 Loss: 0.3359, lr 8.187119948743452e-05
Step 193500 Loss: 0.3276, lr 8.187119948743452e-05
Step 193600 Loss: 0.3188, lr 8.187119948743452e-05
Step 193700 Loss: 0.3104, lr 8.187119948743452e-05
Step 193800 Loss: 0.3024, lr 8.187119948743452e-05
Step 193900 Loss: 0.2947, lr 8.187119948743452e-05
Step 194000 Loss: 0.2874, lr 8.187119948743452e-05
Step 194100 Loss: 0.2803, lr 8.187119948743452e-05
Step 194200 Loss: 0.2735, lr 8.187119948743452e-05
Step 194300 Loss: 0.2670, lr 8.187119948743452e-05
Step 194400 Loss: 0.2600, lr 8.187119948743452e-05
Step 194500 Loss: 0.2505, lr 8.187119948743452e-05
Step 194600 Loss: 0.2413, lr 8.187119948743452e-05
Step 194700 Loss: 0.2325, lr 8.187119948743452e-05
Step 194800 Loss: 0.2240, lr 8.187119948743452e-05
Step 194900 Loss: 0.2158, lr 8.187119948743452e-05
Step 195000 Loss: 0.2079, lr 8.187119948743452e-05
Step 195100 Loss: 0.1995, lr 8.187119948743452e-05
Step 195200 Loss: 0.1883, lr 8.187119948743452e-05
Step 195300 Loss: 0.1775, lr 8.187119948743452e-05
Step 195400 Loss: 0.1670, lr 8.187119948743452e-05
Step 195500 Loss: 0.1568, lr 8.187119948743452e-05
Step 195600 Loss: 0.1432, lr 8.187119948743452e-05
Step 195700 Loss: 0.1292, lr 8.187119948743452e-05
Step 195800 Loss: 0.1156, lr 8.187119948743452e-05
Step 195900 Loss: 0.0994, lr 8.187119948743452e-05
Train Epoch: [29/100] Loss: 0.0962,lr 0.000082
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Step 196000 Loss: 1.0156, lr 8.064535268264887e-05
Step 196100 Loss: 0.9367, lr 8.064535268264887e-05
Step 196200 Loss: 0.8607, lr 8.064535268264887e-05
Step 196300 Loss: 0.8243, lr 8.064535268264887e-05
Step 196400 Loss: 0.8029, lr 8.064535268264887e-05
Step 196500 Loss: 0.7678, lr 8.064535268264887e-05
Step 196600 Loss: 0.7362, lr 8.064535268264887e-05
Step 196700 Loss: 0.7128, lr 8.064535268264887e-05
Step 196800 Loss: 0.6946, lr 8.064535268264887e-05
Step 196900 Loss: 0.6801, lr 8.064535268264887e-05
Step 197000 Loss: 0.6683, lr 8.064535268264887e-05
Step 197100 Loss: 0.6520, lr 8.064535268264887e-05
Step 197200 Loss: 0.6368, lr 8.064535268264887e-05
Step 197300 Loss: 0.6237, lr 8.064535268264887e-05
Step 197400 Loss: 0.6124, lr 8.064535268264887e-05
Step 197500 Loss: 0.6025, lr 8.064535268264887e-05
Step 197600 Loss: 0.5938, lr 8.064535268264887e-05
Step 197700 Loss: 0.5861, lr 8.064535268264887e-05
Step 197800 Loss: 0.5754, lr 8.064535268264887e-05
Step 197900 Loss: 0.5580, lr 8.064535268264887e-05
Step 198000 Loss: 0.5423, lr 8.064535268264887e-05
Step 198100 Loss: 0.5280, lr 8.064535268264887e-05
Step 198200 Loss: 0.5149, lr 8.064535268264887e-05
Step 198300 Loss: 0.5030, lr 8.064535268264887e-05
Step 198400 Loss: 0.4920, lr 8.064535268264887e-05
Step 198500 Loss: 0.4819, lr 8.064535268264887e-05
Step 198600 Loss: 0.4725, lr 8.064535268264887e-05
Step 198700 Loss: 0.4618, lr 8.064535268264887e-05
Step 198800 Loss: 0.4491, lr 8.064535268264887e-05
Step 198900 Loss: 0.4373, lr 8.064535268264887e-05
Step 199000 Loss: 0.4262, lr 8.064535268264887e-05
Step 199100 Loss: 0.4158, lr 8.064535268264887e-05
Step 199200 Loss: 0.4060, lr 8.064535268264887e-05
Step 199300 Loss: 0.3968, lr 8.064535268264887e-05
Step 199400 Loss: 0.3882, lr 8.064535268264887e-05
Step 199500 Loss: 0.3800, lr 8.064535268264887e-05
Step 199600 Loss: 0.3722, lr 8.064535268264887e-05
Step 199700 Loss: 0.3649, lr 8.064535268264887e-05
Step 199800 Loss: 0.3580, lr 8.064535268264887e-05
Step 199900 Loss: 0.3514, lr 8.064535268264887e-05
Step 200000 Loss: 0.3451, lr 8.064535268264887e-05
Step 200100 Loss: 0.3391, lr 8.064535268264887e-05
Step 200200 Loss: 0.3326, lr 8.064535268264887e-05
Step 200300 Loss: 0.3236, lr 8.064535268264887e-05
Step 200400 Loss: 0.3150, lr 8.064535268264887e-05
Step 200500 Loss: 0.3068, lr 8.064535268264887e-05
Step 200600 Loss: 0.2989, lr 8.064535268264887e-05
Step 200700 Loss: 0.2914, lr 8.064535268264887e-05
Step 200800 Loss: 0.2842, lr 8.064535268264887e-05
Step 200900 Loss: 0.2772, lr 8.064535268264887e-05
Step 201000 Loss: 0.2705, lr 8.064535268264887e-05
Step 201100 Loss: 0.2641, lr 8.064535268264887e-05
Step 201200 Loss: 0.2557, lr 8.064535268264887e-05
Step 201300 Loss: 0.2463, lr 8.064535268264887e-05
Step 201400 Loss: 0.2373, lr 8.064535268264887e-05
Step 201500 Loss: 0.2287, lr 8.064535268264887e-05
Step 201600 Loss: 0.2203, lr 8.064535268264887e-05
Step 201700 Loss: 0.2122, lr 8.064535268264887e-05
Step 201800 Loss: 0.2044, lr 8.064535268264887e-05
Step 201900 Loss: 0.1944, lr 8.064535268264887e-05
Step 202000 Loss: 0.1833, lr 8.064535268264887e-05
Step 202100 Loss: 0.1727, lr 8.064535268264887e-05
Step 202200 Loss: 0.1623, lr 8.064535268264887e-05
Step 202300 Loss: 0.1511, lr 8.064535268264887e-05
Step 202400 Loss: 0.1368, lr 8.064535268264887e-05
Step 202500 Loss: 0.1230, lr 8.064535268264887e-05
Step 202600 Loss: 0.1096, lr 8.064535268264887e-05
Train Epoch: [30/100] Loss: 0.0957,lr 0.000081
Model Saving at epoch 30
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Step 202700 Loss: 1.1832, lr 7.938926261462371e-05
Step 202800 Loss: 0.9858, lr 7.938926261462371e-05
Step 202900 Loss: 0.8953, lr 7.938926261462371e-05
Step 203000 Loss: 0.8421, lr 7.938926261462371e-05
Step 203100 Loss: 0.8138, lr 7.938926261462371e-05
Step 203200 Loss: 0.7907, lr 7.938926261462371e-05
Step 203300 Loss: 0.7527, lr 7.938926261462371e-05
Step 203400 Loss: 0.7252, lr 7.938926261462371e-05
Step 203500 Loss: 0.7043, lr 7.938926261462371e-05
Step 203600 Loss: 0.6879, lr 7.938926261462371e-05
Step 203700 Loss: 0.6747, lr 7.938926261462371e-05
Step 203800 Loss: 0.6616, lr 7.938926261462371e-05
Step 203900 Loss: 0.6449, lr 7.938926261462371e-05
Step 204000 Loss: 0.6307, lr 7.938926261462371e-05
Step 204100 Loss: 0.6184, lr 7.938926261462371e-05
Step 204200 Loss: 0.6078, lr 7.938926261462371e-05
Step 204300 Loss: 0.5985, lr 7.938926261462371e-05
Step 204400 Loss: 0.5902, lr 7.938926261462371e-05
Step 204500 Loss: 0.5829, lr 7.938926261462371e-05
Step 204600 Loss: 0.5674, lr 7.938926261462371e-05
Step 204700 Loss: 0.5508, lr 7.938926261462371e-05
Step 204800 Loss: 0.5357, lr 7.938926261462371e-05
Step 204900 Loss: 0.5220, lr 7.938926261462371e-05
Step 205000 Loss: 0.5095, lr 7.938926261462371e-05
Step 205100 Loss: 0.4980, lr 7.938926261462371e-05
Step 205200 Loss: 0.4874, lr 7.938926261462371e-05
Step 205300 Loss: 0.4776, lr 7.938926261462371e-05
Step 205400 Loss: 0.4686, lr 7.938926261462371e-05
Step 205500 Loss: 0.4561, lr 7.938926261462371e-05
Step 205600 Loss: 0.4437, lr 7.938926261462371e-05
Step 205700 Loss: 0.4322, lr 7.938926261462371e-05
Step 205800 Loss: 0.4214, lr 7.938926261462371e-05
Step 205900 Loss: 0.4113, lr 7.938926261462371e-05
Step 206000 Loss: 0.4018, lr 7.938926261462371e-05
Step 206100 Loss: 0.3929, lr 7.938926261462371e-05
Step 206200 Loss: 0.3844, lr 7.938926261462371e-05
Step 206300 Loss: 0.3765, lr 7.938926261462371e-05
Step 206400 Loss: 0.3689, lr 7.938926261462371e-05
Step 206500 Loss: 0.3618, lr 7.938926261462371e-05
Step 206600 Loss: 0.3550, lr 7.938926261462371e-05
Step 206700 Loss: 0.3485, lr 7.938926261462371e-05
Step 206800 Loss: 0.3424, lr 7.938926261462371e-05
Step 206900 Loss: 0.3365, lr 7.938926261462371e-05
Step 207000 Loss: 0.3285, lr 7.938926261462371e-05
Step 207100 Loss: 0.3197, lr 7.938926261462371e-05
Step 207200 Loss: 0.3113, lr 7.938926261462371e-05
Step 207300 Loss: 0.3032, lr 7.938926261462371e-05
Step 207400 Loss: 0.2955, lr 7.938926261462371e-05
Step 207500 Loss: 0.2881, lr 7.938926261462371e-05
Step 207600 Loss: 0.2810, lr 7.938926261462371e-05
Step 207700 Loss: 0.2742, lr 7.938926261462371e-05
Step 207800 Loss: 0.2677, lr 7.938926261462371e-05
Step 207900 Loss: 0.2610, lr 7.938926261462371e-05
Step 208000 Loss: 0.2515, lr 7.938926261462371e-05
Step 208100 Loss: 0.2423, lr 7.938926261462371e-05
Step 208200 Loss: 0.2334, lr 7.938926261462371e-05
Step 208300 Loss: 0.2249, lr 7.938926261462371e-05
Step 208400 Loss: 0.2166, lr 7.938926261462371e-05
Step 208500 Loss: 0.2087, lr 7.938926261462371e-05
Step 208600 Loss: 0.2007, lr 7.938926261462371e-05
Step 208700 Loss: 0.1895, lr 7.938926261462371e-05
Step 208800 Loss: 0.1786, lr 7.938926261462371e-05
Step 208900 Loss: 0.1681, lr 7.938926261462371e-05
Step 209000 Loss: 0.1579, lr 7.938926261462371e-05
Step 209100 Loss: 0.1447, lr 7.938926261462371e-05
Step 209200 Loss: 0.1307, lr 7.938926261462371e-05
Step 209300 Loss: 0.1170, lr 7.938926261462371e-05
Step 209400 Loss: 0.1014, lr 7.938926261462371e-05
Train Epoch: [31/100] Loss: 0.0957,lr 0.000079
Calling G2SDataset.batch()
Done, time:  2.18 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 6756
Step 209500 Loss: 1.0296, lr 7.810416889260659e-05
Step 209600 Loss: 0.9516, lr 7.810416889260659e-05
Step 209700 Loss: 0.8668, lr 7.810416889260659e-05
Step 209800 Loss: 0.8275, lr 7.810416889260659e-05
Step 209900 Loss: 0.8049, lr 7.810416889260659e-05
Step 210000 Loss: 0.7722, lr 7.810416889260659e-05
Step 210100 Loss: 0.7395, lr 7.810416889260659e-05
Step 210200 Loss: 0.7152, lr 7.810416889260659e-05
Step 210300 Loss: 0.6965, lr 7.810416889260659e-05
Step 210400 Loss: 0.6817, lr 7.810416889260659e-05
Step 210500 Loss: 0.6696, lr 7.810416889260659e-05
Step 210600 Loss: 0.6539, lr 7.810416889260659e-05
Step 210700 Loss: 0.6383, lr 7.810416889260659e-05
Step 210800 Loss: 0.6250, lr 7.810416889260659e-05
Step 210900 Loss: 0.6136, lr 7.810416889260659e-05
Step 211000 Loss: 0.6035, lr 7.810416889260659e-05
Step 211100 Loss: 0.5947, lr 7.810416889260659e-05
Step 211200 Loss: 0.5869, lr 7.810416889260659e-05
Step 211300 Loss: 0.5775, lr 7.810416889260659e-05
Step 211400 Loss: 0.5599, lr 7.810416889260659e-05
Step 211500 Loss: 0.5440, lr 7.810416889260659e-05
Step 211600 Loss: 0.5295, lr 7.810416889260659e-05
Step 211700 Loss: 0.5164, lr 7.810416889260659e-05
Step 211800 Loss: 0.5043, lr 7.810416889260659e-05
Step 211900 Loss: 0.4932, lr 7.810416889260659e-05
Step 212000 Loss: 0.4830, lr 7.810416889260659e-05
Step 212100 Loss: 0.4735, lr 7.810416889260659e-05
Step 212200 Loss: 0.4634, lr 7.810416889260659e-05
Step 212300 Loss: 0.4505, lr 7.810416889260659e-05
Step 212400 Loss: 0.4386, lr 7.810416889260659e-05
Step 212500 Loss: 0.4274, lr 7.810416889260659e-05
Step 212600 Loss: 0.4169, lr 7.810416889260659e-05
Step 212700 Loss: 0.4071, lr 7.810416889260659e-05
Step 212800 Loss: 0.3978, lr 7.810416889260659e-05
Step 212900 Loss: 0.3891, lr 7.810416889260659e-05
Step 213000 Loss: 0.3809, lr 7.810416889260659e-05
Step 213100 Loss: 0.3731, lr 7.810416889260659e-05
Step 213200 Loss: 0.3657, lr 7.810416889260659e-05
Step 213300 Loss: 0.3587, lr 7.810416889260659e-05
Step 213400 Loss: 0.3521, lr 7.810416889260659e-05
Step 213500 Loss: 0.3458, lr 7.810416889260659e-05
Step 213600 Loss: 0.3397, lr 7.810416889260659e-05
Step 213700 Loss: 0.3336, lr 7.810416889260659e-05
Step 213800 Loss: 0.3246, lr 7.810416889260659e-05
Step 213900 Loss: 0.3159, lr 7.810416889260659e-05
Step 214000 Loss: 0.3077, lr 7.810416889260659e-05
Step 214100 Loss: 0.2998, lr 7.810416889260659e-05
Step 214200 Loss: 0.2922, lr 7.810416889260659e-05
Step 214300 Loss: 0.2849, lr 7.810416889260659e-05
Step 214400 Loss: 0.2780, lr 7.810416889260659e-05
Step 214500 Loss: 0.2713, lr 7.810416889260659e-05
Step 214600 Loss: 0.2648, lr 7.810416889260659e-05
Step 214700 Loss: 0.2568, lr 7.810416889260659e-05
Step 214800 Loss: 0.2474, lr 7.810416889260659e-05
Step 214900 Loss: 0.2383, lr 7.810416889260659e-05
Step 215000 Loss: 0.2296, lr 7.810416889260659e-05
Step 215100 Loss: 0.2212, lr 7.810416889260659e-05
Step 215200 Loss: 0.2131, lr 7.810416889260659e-05
Step 215300 Loss: 0.2052, lr 7.810416889260659e-05
Step 215400 Loss: 0.1957, lr 7.810416889260659e-05
Step 215500 Loss: 0.1846, lr 7.810416889260659e-05
Step 215600 Loss: 0.1739, lr 7.810416889260659e-05
Step 215700 Loss: 0.1635, lr 7.810416889260659e-05
Step 215800 Loss: 0.1528, lr 7.810416889260659e-05
Step 215900 Loss: 0.1385, lr 7.810416889260659e-05
Step 216000 Loss: 0.1246, lr 7.810416889260659e-05
Step 216100 Loss: 0.1111, lr 7.810416889260659e-05
Train Epoch: [32/100] Loss: 0.0957,lr 0.000078
Calling G2SDataset.batch()
Done, time:  2.11 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 6756
Step 216200 Loss: 1.2520, lr 7.679133974894988e-05
Step 216300 Loss: 0.9916, lr 7.679133974894988e-05
Step 216400 Loss: 0.9051, lr 7.679133974894988e-05
Step 216500 Loss: 0.8468, lr 7.679133974894988e-05
Step 216600 Loss: 0.8165, lr 7.679133974894988e-05
Step 216700 Loss: 0.7963, lr 7.679133974894988e-05
Step 216800 Loss: 0.7567, lr 7.679133974894988e-05
Step 216900 Loss: 0.7281, lr 7.679133974894988e-05
Step 217000 Loss: 0.7066, lr 7.679133974894988e-05
Step 217100 Loss: 0.6897, lr 7.679133974894988e-05
Step 217200 Loss: 0.6762, lr 7.679133974894988e-05
Step 217300 Loss: 0.6638, lr 7.679133974894988e-05
Step 217400 Loss: 0.6467, lr 7.679133974894988e-05
Step 217500 Loss: 0.6322, lr 7.679133974894988e-05
Step 217600 Loss: 0.6198, lr 7.679133974894988e-05
Step 217700 Loss: 0.6090, lr 7.679133974894988e-05
Step 217800 Loss: 0.5995, lr 7.679133974894988e-05
Step 217900 Loss: 0.5911, lr 7.679133974894988e-05
Step 218000 Loss: 0.5837, lr 7.679133974894988e-05
Step 218100 Loss: 0.5695, lr 7.679133974894988e-05
Step 218200 Loss: 0.5527, lr 7.679133974894988e-05
Step 218300 Loss: 0.5374, lr 7.679133974894988e-05
Step 218400 Loss: 0.5236, lr 7.679133974894988e-05
Step 218500 Loss: 0.5109, lr 7.679133974894988e-05
Step 218600 Loss: 0.4993, lr 7.679133974894988e-05
Step 218700 Loss: 0.4886, lr 7.679133974894988e-05
Step 218800 Loss: 0.4787, lr 7.679133974894988e-05
Step 218900 Loss: 0.4696, lr 7.679133974894988e-05
Step 219000 Loss: 0.4576, lr 7.679133974894988e-05
Step 219100 Loss: 0.4452, lr 7.679133974894988e-05
Step 219200 Loss: 0.4335, lr 7.679133974894988e-05
Step 219300 Loss: 0.4227, lr 7.679133974894988e-05
Step 219400 Loss: 0.4125, lr 7.679133974894988e-05
Step 219500 Loss: 0.4029, lr 7.679133974894988e-05
Step 219600 Loss: 0.3939, lr 7.679133974894988e-05
Step 219700 Loss: 0.3854, lr 7.679133974894988e-05
Step 219800 Loss: 0.3774, lr 7.679133974894988e-05
Step 219900 Loss: 0.3698, lr 7.679133974894988e-05
Step 220000 Loss: 0.3626, lr 7.679133974894988e-05
Step 220100 Loss: 0.3557, lr 7.679133974894988e-05
Step 220200 Loss: 0.3492, lr 7.679133974894988e-05
Step 220300 Loss: 0.3431, lr 7.679133974894988e-05
Step 220400 Loss: 0.3372, lr 7.679133974894988e-05
Step 220500 Loss: 0.3296, lr 7.679133974894988e-05
Step 220600 Loss: 0.3207, lr 7.679133974894988e-05
Step 220700 Loss: 0.3122, lr 7.679133974894988e-05
Step 220800 Loss: 0.3041, lr 7.679133974894988e-05
Step 220900 Loss: 0.2964, lr 7.679133974894988e-05
Step 221000 Loss: 0.2890, lr 7.679133974894988e-05
Step 221100 Loss: 0.2818, lr 7.679133974894988e-05
Step 221200 Loss: 0.2750, lr 7.679133974894988e-05
Step 221300 Loss: 0.2684, lr 7.679133974894988e-05
Step 221400 Loss: 0.2621, lr 7.679133974894988e-05
Step 221500 Loss: 0.2526, lr 7.679133974894988e-05
Step 221600 Loss: 0.2433, lr 7.679133974894988e-05
Step 221700 Loss: 0.2344, lr 7.679133974894988e-05
Step 221800 Loss: 0.2259, lr 7.679133974894988e-05
Step 221900 Loss: 0.2176, lr 7.679133974894988e-05
Step 222000 Loss: 0.2096, lr 7.679133974894988e-05
Step 222100 Loss: 0.2018, lr 7.679133974894988e-05
Step 222200 Loss: 0.1907, lr 7.679133974894988e-05
Step 222300 Loss: 0.1798, lr 7.679133974894988e-05
Step 222400 Loss: 0.1693, lr 7.679133974894988e-05
Step 222500 Loss: 0.1590, lr 7.679133974894988e-05
Step 222600 Loss: 0.1464, lr 7.679133974894988e-05
Step 222700 Loss: 0.1323, lr 7.679133974894988e-05
Step 222800 Loss: 0.1186, lr 7.679133974894988e-05
Step 222900 Loss: 0.1037, lr 7.679133974894988e-05
Train Epoch: [33/100] Loss: 0.0957,lr 0.000077
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6755
Step 223000 Loss: 1.0484, lr 7.545207078751862e-05
Step 223100 Loss: 0.9693, lr 7.545207078751862e-05
Step 223200 Loss: 0.8737, lr 7.545207078751862e-05
Step 223300 Loss: 0.8312, lr 7.545207078751862e-05
Step 223400 Loss: 0.8072, lr 7.545207078751862e-05
Step 223500 Loss: 0.7770, lr 7.545207078751862e-05
Step 223600 Loss: 0.7429, lr 7.545207078751862e-05
Step 223700 Loss: 0.7178, lr 7.545207078751862e-05
Step 223800 Loss: 0.6986, lr 7.545207078751862e-05
Step 223900 Loss: 0.6833, lr 7.545207078751862e-05
Step 224000 Loss: 0.6709, lr 7.545207078751862e-05
Step 224100 Loss: 0.6559, lr 7.545207078751862e-05
Step 224200 Loss: 0.6400, lr 7.545207078751862e-05
Step 224300 Loss: 0.6265, lr 7.545207078751862e-05
Step 224400 Loss: 0.6148, lr 7.545207078751862e-05
Step 224500 Loss: 0.6046, lr 7.545207078751862e-05
Step 224600 Loss: 0.5956, lr 7.545207078751862e-05
Step 224700 Loss: 0.5877, lr 7.545207078751862e-05
Step 224800 Loss: 0.5796, lr 7.545207078751862e-05
Step 224900 Loss: 0.5618, lr 7.545207078751862e-05
Step 225000 Loss: 0.5457, lr 7.545207078751862e-05
Step 225100 Loss: 0.5311, lr 7.545207078751862e-05
Step 225200 Loss: 0.5178, lr 7.545207078751862e-05
Step 225300 Loss: 0.5056, lr 7.545207078751862e-05
Step 225400 Loss: 0.4944, lr 7.545207078751862e-05
Step 225500 Loss: 0.4841, lr 7.545207078751862e-05
Step 225600 Loss: 0.4746, lr 7.545207078751862e-05
Step 225700 Loss: 0.4649, lr 7.545207078751862e-05
Step 225800 Loss: 0.4520, lr 7.545207078751862e-05
Step 225900 Loss: 0.4399, lr 7.545207078751862e-05
Step 226000 Loss: 0.4286, lr 7.545207078751862e-05
Step 226100 Loss: 0.4181, lr 7.545207078751862e-05
Step 226200 Loss: 0.4082, lr 7.545207078751862e-05
Step 226300 Loss: 0.3988, lr 7.545207078751862e-05
Step 226400 Loss: 0.3901, lr 7.545207078751862e-05
Step 226500 Loss: 0.3818, lr 7.545207078751862e-05
Step 226600 Loss: 0.3739, lr 7.545207078751862e-05
Step 226700 Loss: 0.3665, lr 7.545207078751862e-05
Step 226800 Loss: 0.3595, lr 7.545207078751862e-05
Step 226900 Loss: 0.3528, lr 7.545207078751862e-05
Step 227000 Loss: 0.3464, lr 7.545207078751862e-05
Step 227100 Loss: 0.3404, lr 7.545207078751862e-05
Step 227200 Loss: 0.3346, lr 7.545207078751862e-05
Step 227300 Loss: 0.3256, lr 7.545207078751862e-05
Step 227400 Loss: 0.3169, lr 7.545207078751862e-05
Step 227500 Loss: 0.3086, lr 7.545207078751862e-05
Step 227600 Loss: 0.3007, lr 7.545207078751862e-05
Step 227700 Loss: 0.2930, lr 7.545207078751862e-05
Step 227800 Loss: 0.2857, lr 7.545207078751862e-05
Step 227900 Loss: 0.2787, lr 7.545207078751862e-05
Step 228000 Loss: 0.2720, lr 7.545207078751862e-05
Step 228100 Loss: 0.2655, lr 7.545207078751862e-05
Step 228200 Loss: 0.2579, lr 7.545207078751862e-05
Step 228300 Loss: 0.2484, lr 7.545207078751862e-05
Step 228400 Loss: 0.2394, lr 7.545207078751862e-05
Step 228500 Loss: 0.2306, lr 7.545207078751862e-05
Step 228600 Loss: 0.2221, lr 7.545207078751862e-05
Step 228700 Loss: 0.2140, lr 7.545207078751862e-05
Step 228800 Loss: 0.2061, lr 7.545207078751862e-05
Step 228900 Loss: 0.1970, lr 7.545207078751862e-05
Step 229000 Loss: 0.1858, lr 7.545207078751862e-05
Step 229100 Loss: 0.1751, lr 7.545207078751862e-05
Step 229200 Loss: 0.1647, lr 7.545207078751862e-05
Step 229300 Loss: 0.1545, lr 7.545207078751862e-05
Step 229400 Loss: 0.1401, lr 7.545207078751862e-05
Step 229500 Loss: 0.1262, lr 7.545207078751862e-05
Step 229600 Loss: 0.1126, lr 7.545207078751862e-05
Train Epoch: [34/100] Loss: 0.0961,lr 0.000075
Calling G2SDataset.batch()
Done, time:  2.03 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6755
Step 229700 Loss: 1.2526, lr 7.408768370508582e-05
Step 229800 Loss: 0.9984, lr 7.408768370508582e-05
Step 229900 Loss: 0.9150, lr 7.408768370508582e-05
Step 230000 Loss: 0.8514, lr 7.408768370508582e-05
Step 230100 Loss: 0.8191, lr 7.408768370508582e-05
Step 230200 Loss: 0.7996, lr 7.408768370508582e-05
Step 230300 Loss: 0.7604, lr 7.408768370508582e-05
Step 230400 Loss: 0.7308, lr 7.408768370508582e-05
Step 230500 Loss: 0.7086, lr 7.408768370508582e-05
Step 230600 Loss: 0.6913, lr 7.408768370508582e-05
Step 230700 Loss: 0.6775, lr 7.408768370508582e-05
Step 230800 Loss: 0.6660, lr 7.408768370508582e-05
Step 230900 Loss: 0.6485, lr 7.408768370508582e-05
Step 231000 Loss: 0.6338, lr 7.408768370508582e-05
Step 231100 Loss: 0.6211, lr 7.408768370508582e-05
Step 231200 Loss: 0.6101, lr 7.408768370508582e-05
Step 231300 Loss: 0.6005, lr 7.408768370508582e-05
Step 231400 Loss: 0.5920, lr 7.408768370508582e-05
Step 231500 Loss: 0.5845, lr 7.408768370508582e-05
Step 231600 Loss: 0.5715, lr 7.408768370508582e-05
Step 231700 Loss: 0.5544, lr 7.408768370508582e-05
Step 231800 Loss: 0.5390, lr 7.408768370508582e-05
Step 231900 Loss: 0.5250, lr 7.408768370508582e-05
Step 232000 Loss: 0.5123, lr 7.408768370508582e-05
Step 232100 Loss: 0.5005, lr 7.408768370508582e-05
Step 232200 Loss: 0.4897, lr 7.408768370508582e-05
Step 232300 Loss: 0.4798, lr 7.408768370508582e-05
Step 232400 Loss: 0.4706, lr 7.408768370508582e-05
Step 232500 Loss: 0.4590, lr 7.408768370508582e-05
Step 232600 Loss: 0.4465, lr 7.408768370508582e-05
Step 232700 Loss: 0.4348, lr 7.408768370508582e-05
Step 232800 Loss: 0.4238, lr 7.408768370508582e-05
Step 232900 Loss: 0.4136, lr 7.408768370508582e-05
Step 233000 Loss: 0.4039, lr 7.408768370508582e-05
Step 233100 Loss: 0.3949, lr 7.408768370508582e-05
Step 233200 Loss: 0.3863, lr 7.408768370508582e-05
Step 233300 Loss: 0.3782, lr 7.408768370508582e-05
Step 233400 Loss: 0.3706, lr 7.408768370508582e-05
Step 233500 Loss: 0.3633, lr 7.408768370508582e-05
Step 233600 Loss: 0.3565, lr 7.408768370508582e-05
Step 233700 Loss: 0.3499, lr 7.408768370508582e-05
Step 233800 Loss: 0.3437, lr 7.408768370508582e-05
Step 233900 Loss: 0.3378, lr 7.408768370508582e-05
Step 234000 Loss: 0.3306, lr 7.408768370508582e-05
Step 234100 Loss: 0.3216, lr 7.408768370508582e-05
Step 234200 Loss: 0.3131, lr 7.408768370508582e-05
Step 234300 Loss: 0.3050, lr 7.408768370508582e-05
Step 234400 Loss: 0.2972, lr 7.408768370508582e-05
Step 234500 Loss: 0.2897, lr 7.408768370508582e-05
Step 234600 Loss: 0.2826, lr 7.408768370508582e-05
Step 234700 Loss: 0.2757, lr 7.408768370508582e-05
Step 234800 Loss: 0.2691, lr 7.408768370508582e-05
Step 234900 Loss: 0.2627, lr 7.408768370508582e-05
Step 235000 Loss: 0.2536, lr 7.408768370508582e-05
Step 235100 Loss: 0.2443, lr 7.408768370508582e-05
Step 235200 Loss: 0.2354, lr 7.408768370508582e-05
Step 235300 Loss: 0.2268, lr 7.408768370508582e-05
Step 235400 Loss: 0.2184, lr 7.408768370508582e-05
Step 235500 Loss: 0.2104, lr 7.408768370508582e-05
Step 235600 Loss: 0.2027, lr 7.408768370508582e-05
Step 235700 Loss: 0.1920, lr 7.408768370508582e-05
Step 235800 Loss: 0.1810, lr 7.408768370508582e-05
Step 235900 Loss: 0.1704, lr 7.408768370508582e-05
Step 236000 Loss: 0.1602, lr 7.408768370508582e-05
Step 236100 Loss: 0.1479, lr 7.408768370508582e-05
Step 236200 Loss: 0.1338, lr 7.408768370508582e-05
Step 236300 Loss: 0.1200, lr 7.408768370508582e-05
Step 236400 Loss: 0.1057, lr 7.408768370508582e-05
Train Epoch: [35/100] Loss: 0.0960,lr 0.000074
Model Saving at epoch 35
Calling G2SDataset.batch()
Done, time:  2.09 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.24 s, total batches: 6755
Step 236500 Loss: 1.0720, lr 7.26995249869774e-05
Step 236600 Loss: 0.9775, lr 7.26995249869774e-05
Step 236700 Loss: 0.8793, lr 7.26995249869774e-05
Step 236800 Loss: 0.8340, lr 7.26995249869774e-05
Step 236900 Loss: 0.8088, lr 7.26995249869774e-05
Step 237000 Loss: 0.7808, lr 7.26995249869774e-05
Step 237100 Loss: 0.7456, lr 7.26995249869774e-05
Step 237200 Loss: 0.7198, lr 7.26995249869774e-05
Step 237300 Loss: 0.7001, lr 7.26995249869774e-05
Step 237400 Loss: 0.6845, lr 7.26995249869774e-05
Step 237500 Loss: 0.6719, lr 7.26995249869774e-05
Step 237600 Loss: 0.6575, lr 7.26995249869774e-05
Step 237700 Loss: 0.6414, lr 7.26995249869774e-05
Step 237800 Loss: 0.6277, lr 7.26995249869774e-05
Step 237900 Loss: 0.6158, lr 7.26995249869774e-05
Step 238000 Loss: 0.6055, lr 7.26995249869774e-05
Step 238100 Loss: 0.5964, lr 7.26995249869774e-05
Step 238200 Loss: 0.5884, lr 7.26995249869774e-05
Step 238300 Loss: 0.5812, lr 7.26995249869774e-05
Step 238400 Loss: 0.5635, lr 7.26995249869774e-05
Step 238500 Loss: 0.5472, lr 7.26995249869774e-05
Step 238600 Loss: 0.5325, lr 7.26995249869774e-05
Step 238700 Loss: 0.5190, lr 7.26995249869774e-05
Step 238800 Loss: 0.5068, lr 7.26995249869774e-05
Step 238900 Loss: 0.4955, lr 7.26995249869774e-05
Step 239000 Loss: 0.4851, lr 7.26995249869774e-05
Step 239100 Loss: 0.4755, lr 7.26995249869774e-05
Step 239200 Loss: 0.4662, lr 7.26995249869774e-05
Step 239300 Loss: 0.4532, lr 7.26995249869774e-05
Step 239400 Loss: 0.4410, lr 7.26995249869774e-05
Step 239500 Loss: 0.4297, lr 7.26995249869774e-05
Step 239600 Loss: 0.4190, lr 7.26995249869774e-05
Step 239700 Loss: 0.4091, lr 7.26995249869774e-05
Step 239800 Loss: 0.3997, lr 7.26995249869774e-05
Step 239900 Loss: 0.3909, lr 7.26995249869774e-05
Step 240000 Loss: 0.3825, lr 7.26995249869774e-05
Step 240100 Loss: 0.3746, lr 7.26995249869774e-05
Step 240200 Loss: 0.3672, lr 7.26995249869774e-05
Step 240300 Loss: 0.3601, lr 7.26995249869774e-05
Step 240400 Loss: 0.3534, lr 7.26995249869774e-05
Step 240500 Loss: 0.3470, lr 7.26995249869774e-05
Step 240600 Loss: 0.3409, lr 7.26995249869774e-05
Step 240700 Loss: 0.3352, lr 7.26995249869774e-05
Step 240800 Loss: 0.3264, lr 7.26995249869774e-05
Step 240900 Loss: 0.3177, lr 7.26995249869774e-05
Step 241000 Loss: 0.3094, lr 7.26995249869774e-05
Step 241100 Loss: 0.3014, lr 7.26995249869774e-05
Step 241200 Loss: 0.2937, lr 7.26995249869774e-05
Step 241300 Loss: 0.2864, lr 7.26995249869774e-05
Step 241400 Loss: 0.2794, lr 7.26995249869774e-05
Step 241500 Loss: 0.2726, lr 7.26995249869774e-05
Step 241600 Loss: 0.2661, lr 7.26995249869774e-05
Step 241700 Loss: 0.2588, lr 7.26995249869774e-05
Step 241800 Loss: 0.2493, lr 7.26995249869774e-05
Step 241900 Loss: 0.2402, lr 7.26995249869774e-05
Step 242000 Loss: 0.2314, lr 7.26995249869774e-05
Step 242100 Loss: 0.2229, lr 7.26995249869774e-05
Step 242200 Loss: 0.2147, lr 7.26995249869774e-05
Step 242300 Loss: 0.2068, lr 7.26995249869774e-05
Step 242400 Loss: 0.1980, lr 7.26995249869774e-05
Step 242500 Loss: 0.1869, lr 7.26995249869774e-05
Step 242600 Loss: 0.1761, lr 7.26995249869774e-05
Step 242700 Loss: 0.1657, lr 7.26995249869774e-05
Step 242800 Loss: 0.1555, lr 7.26995249869774e-05
Step 242900 Loss: 0.1415, lr 7.26995249869774e-05
Step 243000 Loss: 0.1275, lr 7.26995249869774e-05
Step 243100 Loss: 0.1139, lr 7.26995249869774e-05
Step 243200 Loss: 0.0972, lr 7.26995249869774e-05
Train Epoch: [36/100] Loss: 0.0962,lr 0.000073
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6755
Step 243300 Loss: 1.0055, lr 7.12889645782537e-05
Step 243400 Loss: 0.9250, lr 7.12889645782537e-05
Step 243500 Loss: 0.8557, lr 7.12889645782537e-05
Step 243600 Loss: 0.8215, lr 7.12889645782537e-05
Step 243700 Loss: 0.8011, lr 7.12889645782537e-05
Step 243800 Loss: 0.7638, lr 7.12889645782537e-05
Step 243900 Loss: 0.7334, lr 7.12889645782537e-05
Step 244000 Loss: 0.7106, lr 7.12889645782537e-05
Step 244100 Loss: 0.6928, lr 7.12889645782537e-05
Step 244200 Loss: 0.6787, lr 7.12889645782537e-05
Step 244300 Loss: 0.6671, lr 7.12889645782537e-05
Step 244400 Loss: 0.6500, lr 7.12889645782537e-05
Step 244500 Loss: 0.6350, lr 7.12889645782537e-05
Step 244600 Loss: 0.6222, lr 7.12889645782537e-05
Step 244700 Loss: 0.6110, lr 7.12889645782537e-05
Step 244800 Loss: 0.6013, lr 7.12889645782537e-05
Step 244900 Loss: 0.5927, lr 7.12889645782537e-05
Step 245000 Loss: 0.5851, lr 7.12889645782537e-05
Step 245100 Loss: 0.5732, lr 7.12889645782537e-05
Step 245200 Loss: 0.5560, lr 7.12889645782537e-05
Step 245300 Loss: 0.5404, lr 7.12889645782537e-05
Step 245400 Loss: 0.5263, lr 7.12889645782537e-05
Step 245500 Loss: 0.5134, lr 7.12889645782537e-05
Step 245600 Loss: 0.5016, lr 7.12889645782537e-05
Step 245700 Loss: 0.4907, lr 7.12889645782537e-05
Step 245800 Loss: 0.4807, lr 7.12889645782537e-05
Step 245900 Loss: 0.4714, lr 7.12889645782537e-05
Step 246000 Loss: 0.4602, lr 7.12889645782537e-05
Step 246100 Loss: 0.4476, lr 7.12889645782537e-05
Step 246200 Loss: 0.4358, lr 7.12889645782537e-05
Step 246300 Loss: 0.4248, lr 7.12889645782537e-05
Step 246400 Loss: 0.4145, lr 7.12889645782537e-05
Step 246500 Loss: 0.4048, lr 7.12889645782537e-05
Step 246600 Loss: 0.3957, lr 7.12889645782537e-05
Step 246700 Loss: 0.3871, lr 7.12889645782537e-05
Step 246800 Loss: 0.3789, lr 7.12889645782537e-05
Step 246900 Loss: 0.3712, lr 7.12889645782537e-05
Step 247000 Loss: 0.3640, lr 7.12889645782537e-05
Step 247100 Loss: 0.3571, lr 7.12889645782537e-05
Step 247200 Loss: 0.3505, lr 7.12889645782537e-05
Step 247300 Loss: 0.3443, lr 7.12889645782537e-05
Step 247400 Loss: 0.3383, lr 7.12889645782537e-05
Step 247500 Loss: 0.3314, lr 7.12889645782537e-05
Step 247600 Loss: 0.3225, lr 7.12889645782537e-05
Step 247700 Loss: 0.3139, lr 7.12889645782537e-05
Step 247800 Loss: 0.3057, lr 7.12889645782537e-05
Step 247900 Loss: 0.2979, lr 7.12889645782537e-05
Step 248000 Loss: 0.2904, lr 7.12889645782537e-05
Step 248100 Loss: 0.2832, lr 7.12889645782537e-05
Step 248200 Loss: 0.2763, lr 7.12889645782537e-05
Step 248300 Loss: 0.2697, lr 7.12889645782537e-05
Step 248400 Loss: 0.2633, lr 7.12889645782537e-05
Step 248500 Loss: 0.2545, lr 7.12889645782537e-05
Step 248600 Loss: 0.2452, lr 7.12889645782537e-05
Step 248700 Loss: 0.2362, lr 7.12889645782537e-05
Step 248800 Loss: 0.2275, lr 7.12889645782537e-05
Step 248900 Loss: 0.2192, lr 7.12889645782537e-05
Step 249000 Loss: 0.2111, lr 7.12889645782537e-05
Step 249100 Loss: 0.2034, lr 7.12889645782537e-05
Step 249200 Loss: 0.1930, lr 7.12889645782537e-05
Step 249300 Loss: 0.1820, lr 7.12889645782537e-05
Step 249400 Loss: 0.1714, lr 7.12889645782537e-05
Step 249500 Loss: 0.1611, lr 7.12889645782537e-05
Step 249600 Loss: 0.1494, lr 7.12889645782537e-05
Step 249700 Loss: 0.1351, lr 7.12889645782537e-05
Step 249800 Loss: 0.1214, lr 7.12889645782537e-05
Step 249900 Loss: 0.1077, lr 7.12889645782537e-05
Train Epoch: [37/100] Loss: 0.0960,lr 0.000071
Calling G2SDataset.batch()
Done, time:  1.54 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.57 s, total batches: 6756
Step 250000 Loss: 1.1073, lr 6.985739453173908e-05
Step 250100 Loss: 0.9808, lr 6.985739453173908e-05
Step 250200 Loss: 0.8864, lr 6.985739453173908e-05
Step 250300 Loss: 0.8377, lr 6.985739453173908e-05
Step 250400 Loss: 0.8110, lr 6.985739453173908e-05
Step 250500 Loss: 0.7852, lr 6.985739453173908e-05
Step 250600 Loss: 0.7488, lr 6.985739453173908e-05
Step 250700 Loss: 0.7222, lr 6.985739453173908e-05
Step 250800 Loss: 0.7020, lr 6.985739453173908e-05
Step 250900 Loss: 0.6860, lr 6.985739453173908e-05
Step 251000 Loss: 0.6731, lr 6.985739453173908e-05
Step 251100 Loss: 0.6594, lr 6.985739453173908e-05
Step 251200 Loss: 0.6430, lr 6.985739453173908e-05
Step 251300 Loss: 0.6290, lr 6.985739453173908e-05
Step 251400 Loss: 0.6170, lr 6.985739453173908e-05
Step 251500 Loss: 0.6065, lr 6.985739453173908e-05
Step 251600 Loss: 0.5973, lr 6.985739453173908e-05
Step 251700 Loss: 0.5892, lr 6.985739453173908e-05
Step 251800 Loss: 0.5819, lr 6.985739453173908e-05
Step 251900 Loss: 0.5652, lr 6.985739453173908e-05
Step 252000 Loss: 0.5488, lr 6.985739453173908e-05
Step 252100 Loss: 0.5339, lr 6.985739453173908e-05
Step 252200 Loss: 0.5203, lr 6.985739453173908e-05
Step 252300 Loss: 0.5079, lr 6.985739453173908e-05
Step 252400 Loss: 0.4966, lr 6.985739453173908e-05
Step 252500 Loss: 0.4861, lr 6.985739453173908e-05
Step 252600 Loss: 0.4764, lr 6.985739453173908e-05
Step 252700 Loss: 0.4674, lr 6.985739453173908e-05
Step 252800 Loss: 0.4544, lr 6.985739453173908e-05
Step 252900 Loss: 0.4422, lr 6.985739453173908e-05
Step 253000 Loss: 0.4308, lr 6.985739453173908e-05
Step 253100 Loss: 0.4201, lr 6.985739453173908e-05
Step 253200 Loss: 0.4100, lr 6.985739453173908e-05
Step 253300 Loss: 0.4006, lr 6.985739453173908e-05
Step 253400 Loss: 0.3917, lr 6.985739453173908e-05
Step 253500 Loss: 0.3833, lr 6.985739453173908e-05
Step 253600 Loss: 0.3754, lr 6.985739453173908e-05
Step 253700 Loss: 0.3679, lr 6.985739453173908e-05
Step 253800 Loss: 0.3608, lr 6.985739453173908e-05
Step 253900 Loss: 0.3540, lr 6.985739453173908e-05
Step 254000 Loss: 0.3476, lr 6.985739453173908e-05
Step 254100 Loss: 0.3415, lr 6.985739453173908e-05
Step 254200 Loss: 0.3357, lr 6.985739453173908e-05
Step 254300 Loss: 0.3273, lr 6.985739453173908e-05
Step 254400 Loss: 0.3185, lr 6.985739453173908e-05
Step 254500 Loss: 0.3102, lr 6.985739453173908e-05
Step 254600 Loss: 0.3021, lr 6.985739453173908e-05
Step 254700 Loss: 0.2945, lr 6.985739453173908e-05
Step 254800 Loss: 0.2871, lr 6.985739453173908e-05
Step 254900 Loss: 0.2800, lr 6.985739453173908e-05
Step 255000 Loss: 0.2733, lr 6.985739453173908e-05
Step 255100 Loss: 0.2667, lr 6.985739453173908e-05
Step 255200 Loss: 0.2597, lr 6.985739453173908e-05
Step 255300 Loss: 0.2502, lr 6.985739453173908e-05
Step 255400 Loss: 0.2411, lr 6.985739453173908e-05
Step 255500 Loss: 0.2322, lr 6.985739453173908e-05
Step 255600 Loss: 0.2237, lr 6.985739453173908e-05
Step 255700 Loss: 0.2155, lr 6.985739453173908e-05
Step 255800 Loss: 0.2076, lr 6.985739453173908e-05
Step 255900 Loss: 0.1991, lr 6.985739453173908e-05
Step 256000 Loss: 0.1879, lr 6.985739453173908e-05
Step 256100 Loss: 0.1771, lr 6.985739453173908e-05
Step 256200 Loss: 0.1666, lr 6.985739453173908e-05
Step 256300 Loss: 0.1565, lr 6.985739453173908e-05
Step 256400 Loss: 0.1428, lr 6.985739453173908e-05
Step 256500 Loss: 0.1288, lr 6.985739453173908e-05
Step 256600 Loss: 0.1152, lr 6.985739453173908e-05
Step 256700 Loss: 0.0989, lr 6.985739453173908e-05
Train Epoch: [38/100] Loss: 0.0955,lr 0.000070
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Step 256800 Loss: 1.0153, lr 6.840622763423397e-05
Step 256900 Loss: 0.9360, lr 6.840622763423397e-05
Step 257000 Loss: 0.8602, lr 6.840622763423397e-05
Step 257100 Loss: 0.8238, lr 6.840622763423397e-05
Step 257200 Loss: 0.8025, lr 6.840622763423397e-05
Step 257300 Loss: 0.7674, lr 6.840622763423397e-05
Step 257400 Loss: 0.7359, lr 6.840622763423397e-05
Step 257500 Loss: 0.7125, lr 6.840622763423397e-05
Step 257600 Loss: 0.6943, lr 6.840622763423397e-05
Step 257700 Loss: 0.6799, lr 6.840622763423397e-05
Step 257800 Loss: 0.6681, lr 6.840622763423397e-05
Step 257900 Loss: 0.6516, lr 6.840622763423397e-05
Step 258000 Loss: 0.6363, lr 6.840622763423397e-05
Step 258100 Loss: 0.6233, lr 6.840622763423397e-05
Step 258200 Loss: 0.6120, lr 6.840622763423397e-05
Step 258300 Loss: 0.6021, lr 6.840622763423397e-05
Step 258400 Loss: 0.5935, lr 6.840622763423397e-05
Step 258500 Loss: 0.5857, lr 6.840622763423397e-05
Step 258600 Loss: 0.5751, lr 6.840622763423397e-05
Step 258700 Loss: 0.5577, lr 6.840622763423397e-05
Step 258800 Loss: 0.5420, lr 6.840622763423397e-05
Step 258900 Loss: 0.5277, lr 6.840622763423397e-05
Step 259000 Loss: 0.5147, lr 6.840622763423397e-05
Step 259100 Loss: 0.5027, lr 6.840622763423397e-05
Step 259200 Loss: 0.4918, lr 6.840622763423397e-05
Step 259300 Loss: 0.4817, lr 6.840622763423397e-05
Step 259400 Loss: 0.4723, lr 6.840622763423397e-05
Step 259500 Loss: 0.4616, lr 6.840622763423397e-05
Step 259600 Loss: 0.4489, lr 6.840622763423397e-05
Step 259700 Loss: 0.4370, lr 6.840622763423397e-05
Step 259800 Loss: 0.4259, lr 6.840622763423397e-05
Step 259900 Loss: 0.4155, lr 6.840622763423397e-05
Step 260000 Loss: 0.4058, lr 6.840622763423397e-05
Step 260100 Loss: 0.3966, lr 6.840622763423397e-05
Step 260200 Loss: 0.3879, lr 6.840622763423397e-05
Step 260300 Loss: 0.3797, lr 6.840622763423397e-05
Step 260400 Loss: 0.3720, lr 6.840622763423397e-05
Step 260500 Loss: 0.3647, lr 6.840622763423397e-05
Step 260600 Loss: 0.3577, lr 6.840622763423397e-05
Step 260700 Loss: 0.3511, lr 6.840622763423397e-05
Step 260800 Loss: 0.3449, lr 6.840622763423397e-05
Step 260900 Loss: 0.3389, lr 6.840622763423397e-05
Step 261000 Loss: 0.3323, lr 6.840622763423397e-05
Step 261100 Loss: 0.3233, lr 6.840622763423397e-05
Step 261200 Loss: 0.3148, lr 6.840622763423397e-05
Step 261300 Loss: 0.3065, lr 6.840622763423397e-05
Step 261400 Loss: 0.2987, lr 6.840622763423397e-05
Step 261500 Loss: 0.2911, lr 6.840622763423397e-05
Step 261600 Loss: 0.2839, lr 6.840622763423397e-05
Step 261700 Loss: 0.2770, lr 6.840622763423397e-05
Step 261800 Loss: 0.2703, lr 6.840622763423397e-05
Step 261900 Loss: 0.2639, lr 6.840622763423397e-05
Step 262000 Loss: 0.2555, lr 6.840622763423397e-05
Step 262100 Loss: 0.2461, lr 6.840622763423397e-05
Step 262200 Loss: 0.2371, lr 6.840622763423397e-05
Step 262300 Loss: 0.2284, lr 6.840622763423397e-05
Step 262400 Loss: 0.2201, lr 6.840622763423397e-05
Step 262500 Loss: 0.2120, lr 6.840622763423397e-05
Step 262600 Loss: 0.2042, lr 6.840622763423397e-05
Step 262700 Loss: 0.1941, lr 6.840622763423397e-05
Step 262800 Loss: 0.1831, lr 6.840622763423397e-05
Step 262900 Loss: 0.1724, lr 6.840622763423397e-05
Step 263000 Loss: 0.1621, lr 6.840622763423397e-05
Step 263100 Loss: 0.1508, lr 6.840622763423397e-05
Step 263200 Loss: 0.1366, lr 6.840622763423397e-05
Step 263300 Loss: 0.1227, lr 6.840622763423397e-05
Step 263400 Loss: 0.1093, lr 6.840622763423397e-05
Train Epoch: [39/100] Loss: 0.0958,lr 0.000068
Calling G2SDataset.batch()
Done, time:  1.81 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.74 s, total batches: 6756
Step 263500 Loss: 1.1851, lr 6.693689601226464e-05
Step 263600 Loss: 0.9860, lr 6.693689601226464e-05
Step 263700 Loss: 0.8953, lr 6.693689601226464e-05
Step 263800 Loss: 0.8421, lr 6.693689601226464e-05
Step 263900 Loss: 0.8137, lr 6.693689601226464e-05
Step 264000 Loss: 0.7906, lr 6.693689601226464e-05
Step 264100 Loss: 0.7526, lr 6.693689601226464e-05
Step 264200 Loss: 0.7251, lr 6.693689601226464e-05
Step 264300 Loss: 0.7042, lr 6.693689601226464e-05
Step 264400 Loss: 0.6878, lr 6.693689601226464e-05
Step 264500 Loss: 0.6746, lr 6.693689601226464e-05
Step 264600 Loss: 0.6616, lr 6.693689601226464e-05
Step 264700 Loss: 0.6448, lr 6.693689601226464e-05
Step 264800 Loss: 0.6306, lr 6.693689601226464e-05
Step 264900 Loss: 0.6183, lr 6.693689601226464e-05
Step 265000 Loss: 0.6077, lr 6.693689601226464e-05
Step 265100 Loss: 0.5984, lr 6.693689601226464e-05
Step 265200 Loss: 0.5901, lr 6.693689601226464e-05
Step 265300 Loss: 0.5828, lr 6.693689601226464e-05
Step 265400 Loss: 0.5673, lr 6.693689601226464e-05
Step 265500 Loss: 0.5507, lr 6.693689601226464e-05
Step 265600 Loss: 0.5356, lr 6.693689601226464e-05
Step 265700 Loss: 0.5219, lr 6.693689601226464e-05
Step 265800 Loss: 0.5094, lr 6.693689601226464e-05
Step 265900 Loss: 0.4979, lr 6.693689601226464e-05
Step 266000 Loss: 0.4873, lr 6.693689601226464e-05
Step 266100 Loss: 0.4775, lr 6.693689601226464e-05
Step 266200 Loss: 0.4684, lr 6.693689601226464e-05
Step 266300 Loss: 0.4559, lr 6.693689601226464e-05
Step 266400 Loss: 0.4436, lr 6.693689601226464e-05
Step 266500 Loss: 0.4321, lr 6.693689601226464e-05
Step 266600 Loss: 0.4213, lr 6.693689601226464e-05
Step 266700 Loss: 0.4112, lr 6.693689601226464e-05
Step 266800 Loss: 0.4017, lr 6.693689601226464e-05
Step 266900 Loss: 0.3927, lr 6.693689601226464e-05
Step 267000 Loss: 0.3843, lr 6.693689601226464e-05
Step 267100 Loss: 0.3763, lr 6.693689601226464e-05
Step 267200 Loss: 0.3688, lr 6.693689601226464e-05
Step 267300 Loss: 0.3616, lr 6.693689601226464e-05
Step 267400 Loss: 0.3548, lr 6.693689601226464e-05
Step 267500 Loss: 0.3484, lr 6.693689601226464e-05
Step 267600 Loss: 0.3422, lr 6.693689601226464e-05
Step 267700 Loss: 0.3364, lr 6.693689601226464e-05
Step 267800 Loss: 0.3284, lr 6.693689601226464e-05
Step 267900 Loss: 0.3195, lr 6.693689601226464e-05
Step 268000 Loss: 0.3111, lr 6.693689601226464e-05
Step 268100 Loss: 0.3031, lr 6.693689601226464e-05
Step 268200 Loss: 0.2953, lr 6.693689601226464e-05
Step 268300 Loss: 0.2879, lr 6.693689601226464e-05
Step 268400 Loss: 0.2808, lr 6.693689601226464e-05
Step 268500 Loss: 0.2740, lr 6.693689601226464e-05
Step 268600 Loss: 0.2675, lr 6.693689601226464e-05
Step 268700 Loss: 0.2609, lr 6.693689601226464e-05
Step 268800 Loss: 0.2513, lr 6.693689601226464e-05
Step 268900 Loss: 0.2421, lr 6.693689601226464e-05
Step 269000 Loss: 0.2333, lr 6.693689601226464e-05
Step 269100 Loss: 0.2247, lr 6.693689601226464e-05
Step 269200 Loss: 0.2165, lr 6.693689601226464e-05
Step 269300 Loss: 0.2085, lr 6.693689601226464e-05
Step 269400 Loss: 0.2004, lr 6.693689601226464e-05
Step 269500 Loss: 0.1892, lr 6.693689601226464e-05
Step 269600 Loss: 0.1783, lr 6.693689601226464e-05
Step 269700 Loss: 0.1678, lr 6.693689601226464e-05
Step 269800 Loss: 0.1576, lr 6.693689601226464e-05
Step 269900 Loss: 0.1445, lr 6.693689601226464e-05
Step 270000 Loss: 0.1304, lr 6.693689601226464e-05
Step 270100 Loss: 0.1168, lr 6.693689601226464e-05
Step 270200 Loss: 0.1011, lr 6.693689601226464e-05
Train Epoch: [40/100] Loss: 0.0954,lr 0.000067
Model Saving at epoch 40
Calling G2SDataset.batch()
Done, time:  2.08 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Step 270300 Loss: 1.0286, lr 6.545084971874744e-05
Step 270400 Loss: 0.9510, lr 6.545084971874744e-05
Step 270500 Loss: 0.8663, lr 6.545084971874744e-05
Step 270600 Loss: 0.8272, lr 6.545084971874744e-05
Step 270700 Loss: 0.8046, lr 6.545084971874744e-05
Step 270800 Loss: 0.7719, lr 6.545084971874744e-05
Step 270900 Loss: 0.7392, lr 6.545084971874744e-05
Step 271000 Loss: 0.7149, lr 6.545084971874744e-05
Step 271100 Loss: 0.6962, lr 6.545084971874744e-05
Step 271200 Loss: 0.6814, lr 6.545084971874744e-05
Step 271300 Loss: 0.6693, lr 6.545084971874744e-05
Step 271400 Loss: 0.6536, lr 6.545084971874744e-05
Step 271500 Loss: 0.6381, lr 6.545084971874744e-05
Step 271600 Loss: 0.6248, lr 6.545084971874744e-05
Step 271700 Loss: 0.6133, lr 6.545084971874744e-05
Step 271800 Loss: 0.6033, lr 6.545084971874744e-05
Step 271900 Loss: 0.5945, lr 6.545084971874744e-05
Step 272000 Loss: 0.5866, lr 6.545084971874744e-05
Step 272100 Loss: 0.5772, lr 6.545084971874744e-05
Step 272200 Loss: 0.5596, lr 6.545084971874744e-05
Step 272300 Loss: 0.5437, lr 6.545084971874744e-05
Step 272400 Loss: 0.5293, lr 6.545084971874744e-05
Step 272500 Loss: 0.5161, lr 6.545084971874744e-05
Step 272600 Loss: 0.5041, lr 6.545084971874744e-05
Step 272700 Loss: 0.4930, lr 6.545084971874744e-05
Step 272800 Loss: 0.4828, lr 6.545084971874744e-05
Step 272900 Loss: 0.4733, lr 6.545084971874744e-05
Step 273000 Loss: 0.4631, lr 6.545084971874744e-05
Step 273100 Loss: 0.4503, lr 6.545084971874744e-05
Step 273200 Loss: 0.4383, lr 6.545084971874744e-05
Step 273300 Loss: 0.4272, lr 6.545084971874744e-05
Step 273400 Loss: 0.4167, lr 6.545084971874744e-05
Step 273500 Loss: 0.4068, lr 6.545084971874744e-05
Step 273600 Loss: 0.3976, lr 6.545084971874744e-05
Step 273700 Loss: 0.3889, lr 6.545084971874744e-05
Step 273800 Loss: 0.3806, lr 6.545084971874744e-05
Step 273900 Loss: 0.3729, lr 6.545084971874744e-05
Step 274000 Loss: 0.3655, lr 6.545084971874744e-05
Step 274100 Loss: 0.3585, lr 6.545084971874744e-05
Step 274200 Loss: 0.3519, lr 6.545084971874744e-05
Step 274300 Loss: 0.3455, lr 6.545084971874744e-05
Step 274400 Loss: 0.3395, lr 6.545084971874744e-05
Step 274500 Loss: 0.3334, lr 6.545084971874744e-05
Step 274600 Loss: 0.3244, lr 6.545084971874744e-05
Step 274700 Loss: 0.3157, lr 6.545084971874744e-05
Step 274800 Loss: 0.3075, lr 6.545084971874744e-05
Step 274900 Loss: 0.2996, lr 6.545084971874744e-05
Step 275000 Loss: 0.2920, lr 6.545084971874744e-05
Step 275100 Loss: 0.2847, lr 6.545084971874744e-05
Step 275200 Loss: 0.2777, lr 6.545084971874744e-05
Step 275300 Loss: 0.2710, lr 6.545084971874744e-05
Step 275400 Loss: 0.2646, lr 6.545084971874744e-05
Step 275500 Loss: 0.2565, lr 6.545084971874744e-05
Step 275600 Loss: 0.2472, lr 6.545084971874744e-05
Step 275700 Loss: 0.2381, lr 6.545084971874744e-05
Step 275800 Loss: 0.2294, lr 6.545084971874744e-05
Step 275900 Loss: 0.2210, lr 6.545084971874744e-05
Step 276000 Loss: 0.2129, lr 6.545084971874744e-05
Step 276100 Loss: 0.2050, lr 6.545084971874744e-05
Step 276200 Loss: 0.1954, lr 6.545084971874744e-05
Step 276300 Loss: 0.1844, lr 6.545084971874744e-05
Step 276400 Loss: 0.1737, lr 6.545084971874744e-05
Step 276500 Loss: 0.1633, lr 6.545084971874744e-05
Step 276600 Loss: 0.1525, lr 6.545084971874744e-05
Step 276700 Loss: 0.1382, lr 6.545084971874744e-05
Step 276800 Loss: 0.1243, lr 6.545084971874744e-05
Step 276900 Loss: 0.1109, lr 6.545084971874744e-05
Train Epoch: [41/100] Loss: 0.0957,lr 0.000065
Calling G2SDataset.batch()
Done, time:  2.04 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Step 277000 Loss: 1.2517, lr 6.394955530196154e-05
Step 277100 Loss: 0.9917, lr 6.394955530196154e-05
Step 277200 Loss: 0.9040, lr 6.394955530196154e-05
Step 277300 Loss: 0.8460, lr 6.394955530196154e-05
Step 277400 Loss: 0.8158, lr 6.394955530196154e-05
Step 277500 Loss: 0.7957, lr 6.394955530196154e-05
Step 277600 Loss: 0.7562, lr 6.394955530196154e-05
Step 277700 Loss: 0.7276, lr 6.394955530196154e-05
Step 277800 Loss: 0.7061, lr 6.394955530196154e-05
Step 277900 Loss: 0.6893, lr 6.394955530196154e-05
Step 278000 Loss: 0.6758, lr 6.394955530196154e-05
Step 278100 Loss: 0.6635, lr 6.394955530196154e-05
Step 278200 Loss: 0.6464, lr 6.394955530196154e-05
Step 278300 Loss: 0.6319, lr 6.394955530196154e-05
Step 278400 Loss: 0.6195, lr 6.394955530196154e-05
Step 278500 Loss: 0.6087, lr 6.394955530196154e-05
Step 278600 Loss: 0.5992, lr 6.394955530196154e-05
Step 278700 Loss: 0.5908, lr 6.394955530196154e-05
Step 278800 Loss: 0.5834, lr 6.394955530196154e-05
Step 278900 Loss: 0.5693, lr 6.394955530196154e-05
Step 279000 Loss: 0.5524, lr 6.394955530196154e-05
Step 279100 Loss: 0.5372, lr 6.394955530196154e-05
Step 279200 Loss: 0.5233, lr 6.394955530196154e-05
Step 279300 Loss: 0.5107, lr 6.394955530196154e-05
Step 279400 Loss: 0.4991, lr 6.394955530196154e-05
Step 279500 Loss: 0.4884, lr 6.394955530196154e-05
Step 279600 Loss: 0.4785, lr 6.394955530196154e-05
Step 279700 Loss: 0.4694, lr 6.394955530196154e-05
Step 279800 Loss: 0.4574, lr 6.394955530196154e-05
Step 279900 Loss: 0.4449, lr 6.394955530196154e-05
Step 280000 Loss: 0.4333, lr 6.394955530196154e-05
Step 280100 Loss: 0.4225, lr 6.394955530196154e-05
Step 280200 Loss: 0.4123, lr 6.394955530196154e-05
Step 280300 Loss: 0.4027, lr 6.394955530196154e-05
Step 280400 Loss: 0.3937, lr 6.394955530196154e-05
Step 280500 Loss: 0.3852, lr 6.394955530196154e-05
Step 280600 Loss: 0.3771, lr 6.394955530196154e-05
Step 280700 Loss: 0.3695, lr 6.394955530196154e-05
Step 280800 Loss: 0.3623, lr 6.394955530196154e-05
Step 280900 Loss: 0.3555, lr 6.394955530196154e-05
Step 281000 Loss: 0.3490, lr 6.394955530196154e-05
Step 281100 Loss: 0.3428, lr 6.394955530196154e-05
Step 281200 Loss: 0.3370, lr 6.394955530196154e-05
Step 281300 Loss: 0.3293, lr 6.394955530196154e-05
Step 281400 Loss: 0.3205, lr 6.394955530196154e-05
Step 281500 Loss: 0.3120, lr 6.394955530196154e-05
Step 281600 Loss: 0.3039, lr 6.394955530196154e-05
Step 281700 Loss: 0.2962, lr 6.394955530196154e-05
Step 281800 Loss: 0.2887, lr 6.394955530196154e-05
Step 281900 Loss: 0.2816, lr 6.394955530196154e-05
Step 282000 Loss: 0.2747, lr 6.394955530196154e-05
Step 282100 Loss: 0.2682, lr 6.394955530196154e-05
Step 282200 Loss: 0.2618, lr 6.394955530196154e-05
Step 282300 Loss: 0.2523, lr 6.394955530196154e-05
Step 282400 Loss: 0.2431, lr 6.394955530196154e-05
Step 282500 Loss: 0.2342, lr 6.394955530196154e-05
Step 282600 Loss: 0.2256, lr 6.394955530196154e-05
Step 282700 Loss: 0.2173, lr 6.394955530196154e-05
Step 282800 Loss: 0.2093, lr 6.394955530196154e-05
Step 282900 Loss: 0.2016, lr 6.394955530196154e-05
Step 283000 Loss: 0.1905, lr 6.394955530196154e-05
Step 283100 Loss: 0.1796, lr 6.394955530196154e-05
Step 283200 Loss: 0.1690, lr 6.394955530196154e-05
Step 283300 Loss: 0.1588, lr 6.394955530196154e-05
Step 283400 Loss: 0.1462, lr 6.394955530196154e-05
Step 283500 Loss: 0.1320, lr 6.394955530196154e-05
Step 283600 Loss: 0.1183, lr 6.394955530196154e-05
Step 283700 Loss: 0.1035, lr 6.394955530196154e-05
Train Epoch: [42/100] Loss: 0.0957,lr 0.000064
Calling G2SDataset.batch()
Done, time:  2.13 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 6755
Step 283800 Loss: 1.0493, lr 6.24344943582428e-05
Step 283900 Loss: 0.9695, lr 6.24344943582428e-05
Step 284000 Loss: 0.8738, lr 6.24344943582428e-05
Step 284100 Loss: 0.8312, lr 6.24344943582428e-05
Step 284200 Loss: 0.8072, lr 6.24344943582428e-05
Step 284300 Loss: 0.7769, lr 6.24344943582428e-05
Step 284400 Loss: 0.7429, lr 6.24344943582428e-05
Step 284500 Loss: 0.7177, lr 6.24344943582428e-05
Step 284600 Loss: 0.6985, lr 6.24344943582428e-05
Step 284700 Loss: 0.6832, lr 6.24344943582428e-05
Step 284800 Loss: 0.6708, lr 6.24344943582428e-05
Step 284900 Loss: 0.6557, lr 6.24344943582428e-05
Step 285000 Loss: 0.6399, lr 6.24344943582428e-05
Step 285100 Loss: 0.6263, lr 6.24344943582428e-05
Step 285200 Loss: 0.6146, lr 6.24344943582428e-05
Step 285300 Loss: 0.6045, lr 6.24344943582428e-05
Step 285400 Loss: 0.5955, lr 6.24344943582428e-05
Step 285500 Loss: 0.5876, lr 6.24344943582428e-05
Step 285600 Loss: 0.5795, lr 6.24344943582428e-05
Step 285700 Loss: 0.5617, lr 6.24344943582428e-05
Step 285800 Loss: 0.5456, lr 6.24344943582428e-05
Step 285900 Loss: 0.5310, lr 6.24344943582428e-05
Step 286000 Loss: 0.5177, lr 6.24344943582428e-05
Step 286100 Loss: 0.5055, lr 6.24344943582428e-05
Step 286200 Loss: 0.4943, lr 6.24344943582428e-05
Step 286300 Loss: 0.4840, lr 6.24344943582428e-05
Step 286400 Loss: 0.4744, lr 6.24344943582428e-05
Step 286500 Loss: 0.4647, lr 6.24344943582428e-05
Step 286600 Loss: 0.4518, lr 6.24344943582428e-05
Step 286700 Loss: 0.4397, lr 6.24344943582428e-05
Step 286800 Loss: 0.4285, lr 6.24344943582428e-05
Step 286900 Loss: 0.4179, lr 6.24344943582428e-05
Step 287000 Loss: 0.4080, lr 6.24344943582428e-05
Step 287100 Loss: 0.3987, lr 6.24344943582428e-05
Step 287200 Loss: 0.3899, lr 6.24344943582428e-05
Step 287300 Loss: 0.3816, lr 6.24344943582428e-05
Step 287400 Loss: 0.3737, lr 6.24344943582428e-05
Step 287500 Loss: 0.3663, lr 6.24344943582428e-05
Step 287600 Loss: 0.3593, lr 6.24344943582428e-05
Step 287700 Loss: 0.3526, lr 6.24344943582428e-05
Step 287800 Loss: 0.3463, lr 6.24344943582428e-05
Step 287900 Loss: 0.3402, lr 6.24344943582428e-05
Step 288000 Loss: 0.3344, lr 6.24344943582428e-05
Step 288100 Loss: 0.3254, lr 6.24344943582428e-05
Step 288200 Loss: 0.3167, lr 6.24344943582428e-05
Step 288300 Loss: 0.3084, lr 6.24344943582428e-05
Step 288400 Loss: 0.3005, lr 6.24344943582428e-05
Step 288500 Loss: 0.2928, lr 6.24344943582428e-05
Step 288600 Loss: 0.2855, lr 6.24344943582428e-05
Step 288700 Loss: 0.2785, lr 6.24344943582428e-05
Step 288800 Loss: 0.2718, lr 6.24344943582428e-05
Step 288900 Loss: 0.2653, lr 6.24344943582428e-05
Step 289000 Loss: 0.2577, lr 6.24344943582428e-05
Step 289100 Loss: 0.2482, lr 6.24344943582428e-05
Step 289200 Loss: 0.2391, lr 6.24344943582428e-05
Step 289300 Loss: 0.2304, lr 6.24344943582428e-05
Step 289400 Loss: 0.2219, lr 6.24344943582428e-05
Step 289500 Loss: 0.2138, lr 6.24344943582428e-05
Step 289600 Loss: 0.2059, lr 6.24344943582428e-05
Step 289700 Loss: 0.1968, lr 6.24344943582428e-05
Step 289800 Loss: 0.1857, lr 6.24344943582428e-05
Step 289900 Loss: 0.1749, lr 6.24344943582428e-05
Step 290000 Loss: 0.1645, lr 6.24344943582428e-05
Step 290100 Loss: 0.1542, lr 6.24344943582428e-05
Step 290200 Loss: 0.1399, lr 6.24344943582428e-05
Step 290300 Loss: 0.1259, lr 6.24344943582428e-05
Step 290400 Loss: 0.1124, lr 6.24344943582428e-05
Train Epoch: [43/100] Loss: 0.0957,lr 0.000062
Calling G2SDataset.batch()
Done, time:  2.29 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.26 s, total batches: 6755
Step 290500 Loss: 1.2526, lr 6.0907162069827195e-05
Step 290600 Loss: 0.9976, lr 6.0907162069827195e-05
Step 290700 Loss: 0.9143, lr 6.0907162069827195e-05
Step 290800 Loss: 0.8508, lr 6.0907162069827195e-05
Step 290900 Loss: 0.8186, lr 6.0907162069827195e-05
Step 291000 Loss: 0.7992, lr 6.0907162069827195e-05
Step 291100 Loss: 0.7600, lr 6.0907162069827195e-05
Step 291200 Loss: 0.7305, lr 6.0907162069827195e-05
Step 291300 Loss: 0.7083, lr 6.0907162069827195e-05
Step 291400 Loss: 0.6910, lr 6.0907162069827195e-05
Step 291500 Loss: 0.6772, lr 6.0907162069827195e-05
Step 291600 Loss: 0.6657, lr 6.0907162069827195e-05
Step 291700 Loss: 0.6483, lr 6.0907162069827195e-05
Step 291800 Loss: 0.6335, lr 6.0907162069827195e-05
Step 291900 Loss: 0.6209, lr 6.0907162069827195e-05
Step 292000 Loss: 0.6099, lr 6.0907162069827195e-05
Step 292100 Loss: 0.6003, lr 6.0907162069827195e-05
Step 292200 Loss: 0.5918, lr 6.0907162069827195e-05
Step 292300 Loss: 0.5843, lr 6.0907162069827195e-05
Step 292400 Loss: 0.5712, lr 6.0907162069827195e-05
Step 292500 Loss: 0.5542, lr 6.0907162069827195e-05
Step 292600 Loss: 0.5388, lr 6.0907162069827195e-05
Step 292700 Loss: 0.5248, lr 6.0907162069827195e-05
Step 292800 Loss: 0.5120, lr 6.0907162069827195e-05
Step 292900 Loss: 0.5003, lr 6.0907162069827195e-05
Step 293000 Loss: 0.4895, lr 6.0907162069827195e-05
Step 293100 Loss: 0.4796, lr 6.0907162069827195e-05
Step 293200 Loss: 0.4703, lr 6.0907162069827195e-05
Step 293300 Loss: 0.4588, lr 6.0907162069827195e-05
Step 293400 Loss: 0.4463, lr 6.0907162069827195e-05
Step 293500 Loss: 0.4346, lr 6.0907162069827195e-05
Step 293600 Loss: 0.4236, lr 6.0907162069827195e-05
Step 293700 Loss: 0.4134, lr 6.0907162069827195e-05
Step 293800 Loss: 0.4037, lr 6.0907162069827195e-05
Step 293900 Loss: 0.3946, lr 6.0907162069827195e-05
Step 294000 Loss: 0.3861, lr 6.0907162069827195e-05
Step 294100 Loss: 0.3780, lr 6.0907162069827195e-05
Step 294200 Loss: 0.3703, lr 6.0907162069827195e-05
Step 294300 Loss: 0.3631, lr 6.0907162069827195e-05
Step 294400 Loss: 0.3562, lr 6.0907162069827195e-05
Step 294500 Loss: 0.3497, lr 6.0907162069827195e-05
Step 294600 Loss: 0.3435, lr 6.0907162069827195e-05
Step 294700 Loss: 0.3376, lr 6.0907162069827195e-05
Step 294800 Loss: 0.3303, lr 6.0907162069827195e-05
Step 294900 Loss: 0.3214, lr 6.0907162069827195e-05
Step 295000 Loss: 0.3129, lr 6.0907162069827195e-05
Step 295100 Loss: 0.3048, lr 6.0907162069827195e-05
Step 295200 Loss: 0.2970, lr 6.0907162069827195e-05
Step 295300 Loss: 0.2895, lr 6.0907162069827195e-05
Step 295400 Loss: 0.2823, lr 6.0907162069827195e-05
Step 295500 Loss: 0.2755, lr 6.0907162069827195e-05
Step 295600 Loss: 0.2689, lr 6.0907162069827195e-05
Step 295700 Loss: 0.2625, lr 6.0907162069827195e-05
Step 295800 Loss: 0.2534, lr 6.0907162069827195e-05
Step 295900 Loss: 0.2441, lr 6.0907162069827195e-05
Step 296000 Loss: 0.2352, lr 6.0907162069827195e-05
Step 296100 Loss: 0.2265, lr 6.0907162069827195e-05
Step 296200 Loss: 0.2182, lr 6.0907162069827195e-05
Step 296300 Loss: 0.2102, lr 6.0907162069827195e-05
Step 296400 Loss: 0.2024, lr 6.0907162069827195e-05
Step 296500 Loss: 0.1917, lr 6.0907162069827195e-05
Step 296600 Loss: 0.1808, lr 6.0907162069827195e-05
Step 296700 Loss: 0.1702, lr 6.0907162069827195e-05
Step 296800 Loss: 0.1599, lr 6.0907162069827195e-05
Step 296900 Loss: 0.1477, lr 6.0907162069827195e-05
Step 297000 Loss: 0.1336, lr 6.0907162069827195e-05
Step 297100 Loss: 0.1198, lr 6.0907162069827195e-05
Step 297200 Loss: 0.1055, lr 6.0907162069827195e-05
Train Epoch: [44/100] Loss: 0.0957,lr 0.000061
Calling G2SDataset.batch()
Done, time:  1.95 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 297300 Loss: 1.0717, lr 5.93690657292863e-05
Step 297400 Loss: 0.9773, lr 5.93690657292863e-05
Step 297500 Loss: 0.8791, lr 5.93690657292863e-05
Step 297600 Loss: 0.8338, lr 5.93690657292863e-05
Step 297700 Loss: 0.8086, lr 5.93690657292863e-05
Step 297800 Loss: 0.7806, lr 5.93690657292863e-05
Step 297900 Loss: 0.7454, lr 5.93690657292863e-05
Step 298000 Loss: 0.7196, lr 5.93690657292863e-05
Step 298100 Loss: 0.6999, lr 5.93690657292863e-05
Step 298200 Loss: 0.6843, lr 5.93690657292863e-05
Step 298300 Loss: 0.6717, lr 5.93690657292863e-05
Step 298400 Loss: 0.6573, lr 5.93690657292863e-05
Step 298500 Loss: 0.6411, lr 5.93690657292863e-05
Step 298600 Loss: 0.6274, lr 5.93690657292863e-05
Step 298700 Loss: 0.6156, lr 5.93690657292863e-05
Step 298800 Loss: 0.6052, lr 5.93690657292863e-05
Step 298900 Loss: 0.5962, lr 5.93690657292863e-05
Step 299000 Loss: 0.5881, lr 5.93690657292863e-05
Step 299100 Loss: 0.5810, lr 5.93690657292863e-05
Step 299200 Loss: 0.5632, lr 5.93690657292863e-05
Step 299300 Loss: 0.5470, lr 5.93690657292863e-05
Step 299400 Loss: 0.5322, lr 5.93690657292863e-05
Step 299500 Loss: 0.5188, lr 5.93690657292863e-05
Step 299600 Loss: 0.5065, lr 5.93690657292863e-05
Step 299700 Loss: 0.4952, lr 5.93690657292863e-05
Step 299800 Loss: 0.4848, lr 5.93690657292863e-05
Step 299900 Loss: 0.4752, lr 5.93690657292863e-05
Step 300000 Loss: 0.4660, lr 5.93690657292863e-05
Step 300100 Loss: 0.4529, lr 5.93690657292863e-05
Step 300200 Loss: 0.4408, lr 5.93690657292863e-05
Step 300300 Loss: 0.4294, lr 5.93690657292863e-05
Step 300400 Loss: 0.4188, lr 5.93690657292863e-05
Step 300500 Loss: 0.4088, lr 5.93690657292863e-05
Step 300600 Loss: 0.3995, lr 5.93690657292863e-05
Step 300700 Loss: 0.3906, lr 5.93690657292863e-05
Step 300800 Loss: 0.3823, lr 5.93690657292863e-05
Step 300900 Loss: 0.3744, lr 5.93690657292863e-05
Step 301000 Loss: 0.3669, lr 5.93690657292863e-05
Step 301100 Loss: 0.3599, lr 5.93690657292863e-05
Step 301200 Loss: 0.3532, lr 5.93690657292863e-05
Step 301300 Loss: 0.3468, lr 5.93690657292863e-05
Step 301400 Loss: 0.3407, lr 5.93690657292863e-05
Step 301500 Loss: 0.3349, lr 5.93690657292863e-05
Step 301600 Loss: 0.3262, lr 5.93690657292863e-05
Step 301700 Loss: 0.3175, lr 5.93690657292863e-05
Step 301800 Loss: 0.3091, lr 5.93690657292863e-05
Step 301900 Loss: 0.3011, lr 5.93690657292863e-05
Step 302000 Loss: 0.2935, lr 5.93690657292863e-05
Step 302100 Loss: 0.2862, lr 5.93690657292863e-05
Step 302200 Loss: 0.2791, lr 5.93690657292863e-05
Step 302300 Loss: 0.2724, lr 5.93690657292863e-05
Step 302400 Loss: 0.2659, lr 5.93690657292863e-05
Step 302500 Loss: 0.2586, lr 5.93690657292863e-05
Step 302600 Loss: 0.2491, lr 5.93690657292863e-05
Step 302700 Loss: 0.2400, lr 5.93690657292863e-05
Step 302800 Loss: 0.2312, lr 5.93690657292863e-05
Step 302900 Loss: 0.2227, lr 5.93690657292863e-05
Step 303000 Loss: 0.2145, lr 5.93690657292863e-05
Step 303100 Loss: 0.2066, lr 5.93690657292863e-05
Step 303200 Loss: 0.1978, lr 5.93690657292863e-05
Step 303300 Loss: 0.1867, lr 5.93690657292863e-05
Step 303400 Loss: 0.1759, lr 5.93690657292863e-05
Step 303500 Loss: 0.1654, lr 5.93690657292863e-05
Step 303600 Loss: 0.1553, lr 5.93690657292863e-05
Step 303700 Loss: 0.1412, lr 5.93690657292863e-05
Step 303800 Loss: 0.1273, lr 5.93690657292863e-05
Step 303900 Loss: 0.1137, lr 5.93690657292863e-05
Step 304000 Loss: 0.0968, lr 5.93690657292863e-05
Train Epoch: [45/100] Loss: 0.0956,lr 0.000059
Model Saving at epoch 45
Calling G2SDataset.batch()
Done, time:  2.14 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6755
Step 304100 Loss: 1.0055, lr 5.7821723252011606e-05
Step 304200 Loss: 0.9248, lr 5.7821723252011606e-05
Step 304300 Loss: 0.8553, lr 5.7821723252011606e-05
Step 304400 Loss: 0.8211, lr 5.7821723252011606e-05
Step 304500 Loss: 0.8007, lr 5.7821723252011606e-05
Step 304600 Loss: 0.7637, lr 5.7821723252011606e-05
Step 304700 Loss: 0.7331, lr 5.7821723252011606e-05
Step 304800 Loss: 0.7103, lr 5.7821723252011606e-05
Step 304900 Loss: 0.6926, lr 5.7821723252011606e-05
Step 305000 Loss: 0.6784, lr 5.7821723252011606e-05
Step 305100 Loss: 0.6669, lr 5.7821723252011606e-05
Step 305200 Loss: 0.6498, lr 5.7821723252011606e-05
Step 305300 Loss: 0.6348, lr 5.7821723252011606e-05
Step 305400 Loss: 0.6220, lr 5.7821723252011606e-05
Step 305500 Loss: 0.6108, lr 5.7821723252011606e-05
Step 305600 Loss: 0.6011, lr 5.7821723252011606e-05
Step 305700 Loss: 0.5925, lr 5.7821723252011606e-05
Step 305800 Loss: 0.5849, lr 5.7821723252011606e-05
Step 305900 Loss: 0.5730, lr 5.7821723252011606e-05
Step 306000 Loss: 0.5558, lr 5.7821723252011606e-05
Step 306100 Loss: 0.5403, lr 5.7821723252011606e-05
Step 306200 Loss: 0.5261, lr 5.7821723252011606e-05
Step 306300 Loss: 0.5132, lr 5.7821723252011606e-05
Step 306400 Loss: 0.5014, lr 5.7821723252011606e-05
Step 306500 Loss: 0.4905, lr 5.7821723252011606e-05
Step 306600 Loss: 0.4805, lr 5.7821723252011606e-05
Step 306700 Loss: 0.4712, lr 5.7821723252011606e-05
Step 306800 Loss: 0.4601, lr 5.7821723252011606e-05
Step 306900 Loss: 0.4475, lr 5.7821723252011606e-05
Step 307000 Loss: 0.4357, lr 5.7821723252011606e-05
Step 307100 Loss: 0.4246, lr 5.7821723252011606e-05
Step 307200 Loss: 0.4143, lr 5.7821723252011606e-05
Step 307300 Loss: 0.4046, lr 5.7821723252011606e-05
Step 307400 Loss: 0.3955, lr 5.7821723252011606e-05
Step 307500 Loss: 0.3869, lr 5.7821723252011606e-05
Step 307600 Loss: 0.3787, lr 5.7821723252011606e-05
Step 307700 Loss: 0.3710, lr 5.7821723252011606e-05
Step 307800 Loss: 0.3638, lr 5.7821723252011606e-05
Step 307900 Loss: 0.3569, lr 5.7821723252011606e-05
Step 308000 Loss: 0.3503, lr 5.7821723252011606e-05
Step 308100 Loss: 0.3440, lr 5.7821723252011606e-05
Step 308200 Loss: 0.3381, lr 5.7821723252011606e-05
Step 308300 Loss: 0.3312, lr 5.7821723252011606e-05
Step 308400 Loss: 0.3223, lr 5.7821723252011606e-05
Step 308500 Loss: 0.3137, lr 5.7821723252011606e-05
Step 308600 Loss: 0.3055, lr 5.7821723252011606e-05
Step 308700 Loss: 0.2977, lr 5.7821723252011606e-05
Step 308800 Loss: 0.2902, lr 5.7821723252011606e-05
Step 308900 Loss: 0.2830, lr 5.7821723252011606e-05
Step 309000 Loss: 0.2761, lr 5.7821723252011606e-05
Step 309100 Loss: 0.2695, lr 5.7821723252011606e-05
Step 309200 Loss: 0.2631, lr 5.7821723252011606e-05
Step 309300 Loss: 0.2543, lr 5.7821723252011606e-05
Step 309400 Loss: 0.2450, lr 5.7821723252011606e-05
Step 309500 Loss: 0.2361, lr 5.7821723252011606e-05
Step 309600 Loss: 0.2274, lr 5.7821723252011606e-05
Step 309700 Loss: 0.2190, lr 5.7821723252011606e-05
Step 309800 Loss: 0.2110, lr 5.7821723252011606e-05
Step 309900 Loss: 0.2032, lr 5.7821723252011606e-05
Step 310000 Loss: 0.1929, lr 5.7821723252011606e-05
Step 310100 Loss: 0.1819, lr 5.7821723252011606e-05
Step 310200 Loss: 0.1713, lr 5.7821723252011606e-05
Step 310300 Loss: 0.1610, lr 5.7821723252011606e-05
Step 310400 Loss: 0.1493, lr 5.7821723252011606e-05
Step 310500 Loss: 0.1350, lr 5.7821723252011606e-05
Step 310600 Loss: 0.1213, lr 5.7821723252011606e-05
Step 310700 Loss: 0.1077, lr 5.7821723252011606e-05
Train Epoch: [46/100] Loss: 0.0959,lr 0.000058
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6756
Step 310800 Loss: 1.1116, lr 5.6266661678215284e-05
Step 310900 Loss: 0.9810, lr 5.6266661678215284e-05
Step 311000 Loss: 0.8864, lr 5.6266661678215284e-05
Step 311100 Loss: 0.8374, lr 5.6266661678215284e-05
Step 311200 Loss: 0.8108, lr 5.6266661678215284e-05
Step 311300 Loss: 0.7852, lr 5.6266661678215284e-05
Step 311400 Loss: 0.7487, lr 5.6266661678215284e-05
Step 311500 Loss: 0.7221, lr 5.6266661678215284e-05
Step 311600 Loss: 0.7018, lr 5.6266661678215284e-05
Step 311700 Loss: 0.6858, lr 5.6266661678215284e-05
Step 311800 Loss: 0.6729, lr 5.6266661678215284e-05
Step 311900 Loss: 0.6593, lr 5.6266661678215284e-05
Step 312000 Loss: 0.6428, lr 5.6266661678215284e-05
Step 312100 Loss: 0.6289, lr 5.6266661678215284e-05
Step 312200 Loss: 0.6168, lr 5.6266661678215284e-05
Step 312300 Loss: 0.6063, lr 5.6266661678215284e-05
Step 312400 Loss: 0.5971, lr 5.6266661678215284e-05
Step 312500 Loss: 0.5890, lr 5.6266661678215284e-05
Step 312600 Loss: 0.5818, lr 5.6266661678215284e-05
Step 312700 Loss: 0.5651, lr 5.6266661678215284e-05
Step 312800 Loss: 0.5487, lr 5.6266661678215284e-05
Step 312900 Loss: 0.5338, lr 5.6266661678215284e-05
Step 313000 Loss: 0.5202, lr 5.6266661678215284e-05
Step 313100 Loss: 0.5078, lr 5.6266661678215284e-05
Step 313200 Loss: 0.4964, lr 5.6266661678215284e-05
Step 313300 Loss: 0.4859, lr 5.6266661678215284e-05
Step 313400 Loss: 0.4762, lr 5.6266661678215284e-05
Step 313500 Loss: 0.4672, lr 5.6266661678215284e-05
Step 313600 Loss: 0.4543, lr 5.6266661678215284e-05
Step 313700 Loss: 0.4421, lr 5.6266661678215284e-05
Step 313800 Loss: 0.4306, lr 5.6266661678215284e-05
Step 313900 Loss: 0.4199, lr 5.6266661678215284e-05
Step 314000 Loss: 0.4099, lr 5.6266661678215284e-05
Step 314100 Loss: 0.4004, lr 5.6266661678215284e-05
Step 314200 Loss: 0.3916, lr 5.6266661678215284e-05
Step 314300 Loss: 0.3832, lr 5.6266661678215284e-05
Step 314400 Loss: 0.3752, lr 5.6266661678215284e-05
Step 314500 Loss: 0.3677, lr 5.6266661678215284e-05
Step 314600 Loss: 0.3606, lr 5.6266661678215284e-05
Step 314700 Loss: 0.3539, lr 5.6266661678215284e-05
Step 314800 Loss: 0.3474, lr 5.6266661678215284e-05
Step 314900 Loss: 0.3413, lr 5.6266661678215284e-05
Step 315000 Loss: 0.3355, lr 5.6266661678215284e-05
Step 315100 Loss: 0.3271, lr 5.6266661678215284e-05
Step 315200 Loss: 0.3184, lr 5.6266661678215284e-05
Step 315300 Loss: 0.3100, lr 5.6266661678215284e-05
Step 315400 Loss: 0.3020, lr 5.6266661678215284e-05
Step 315500 Loss: 0.2943, lr 5.6266661678215284e-05
Step 315600 Loss: 0.2869, lr 5.6266661678215284e-05
Step 315700 Loss: 0.2799, lr 5.6266661678215284e-05
Step 315800 Loss: 0.2731, lr 5.6266661678215284e-05
Step 315900 Loss: 0.2666, lr 5.6266661678215284e-05
Step 316000 Loss: 0.2596, lr 5.6266661678215284e-05
Step 316100 Loss: 0.2501, lr 5.6266661678215284e-05
Step 316200 Loss: 0.2409, lr 5.6266661678215284e-05
Step 316300 Loss: 0.2321, lr 5.6266661678215284e-05
Step 316400 Loss: 0.2236, lr 5.6266661678215284e-05
Step 316500 Loss: 0.2153, lr 5.6266661678215284e-05
Step 316600 Loss: 0.2074, lr 5.6266661678215284e-05
Step 316700 Loss: 0.1990, lr 5.6266661678215284e-05
Step 316800 Loss: 0.1878, lr 5.6266661678215284e-05
Step 316900 Loss: 0.1770, lr 5.6266661678215284e-05
Step 317000 Loss: 0.1665, lr 5.6266661678215284e-05
Step 317100 Loss: 0.1564, lr 5.6266661678215284e-05
Step 317200 Loss: 0.1427, lr 5.6266661678215284e-05
Step 317300 Loss: 0.1287, lr 5.6266661678215284e-05
Step 317400 Loss: 0.1151, lr 5.6266661678215284e-05
Step 317500 Loss: 0.0989, lr 5.6266661678215284e-05
Train Epoch: [47/100] Loss: 0.0956,lr 0.000056
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.63 s, total batches: 6756
Step 317600 Loss: 1.0162, lr 5.470541566592577e-05
Step 317700 Loss: 0.9378, lr 5.470541566592577e-05
Step 317800 Loss: 0.8610, lr 5.470541566592577e-05
Step 317900 Loss: 0.8243, lr 5.470541566592577e-05
Step 318000 Loss: 0.8028, lr 5.470541566592577e-05
Step 318100 Loss: 0.7678, lr 5.470541566592577e-05
Step 318200 Loss: 0.7362, lr 5.470541566592577e-05
Step 318300 Loss: 0.7127, lr 5.470541566592577e-05
Step 318400 Loss: 0.6945, lr 5.470541566592577e-05
Step 318500 Loss: 0.6799, lr 5.470541566592577e-05
Step 318600 Loss: 0.6681, lr 5.470541566592577e-05
Step 318700 Loss: 0.6517, lr 5.470541566592577e-05
Step 318800 Loss: 0.6364, lr 5.470541566592577e-05
Step 318900 Loss: 0.6233, lr 5.470541566592577e-05
Step 319000 Loss: 0.6120, lr 5.470541566592577e-05
Step 319100 Loss: 0.6021, lr 5.470541566592577e-05
Step 319200 Loss: 0.5934, lr 5.470541566592577e-05
Step 319300 Loss: 0.5857, lr 5.470541566592577e-05
Step 319400 Loss: 0.5751, lr 5.470541566592577e-05
Step 319500 Loss: 0.5577, lr 5.470541566592577e-05
Step 319600 Loss: 0.5419, lr 5.470541566592577e-05
Step 319700 Loss: 0.5277, lr 5.470541566592577e-05
Step 319800 Loss: 0.5146, lr 5.470541566592577e-05
Step 319900 Loss: 0.5027, lr 5.470541566592577e-05
Step 320000 Loss: 0.4917, lr 5.470541566592577e-05
Step 320100 Loss: 0.4816, lr 5.470541566592577e-05
Step 320200 Loss: 0.4722, lr 5.470541566592577e-05
Step 320300 Loss: 0.4615, lr 5.470541566592577e-05
Step 320400 Loss: 0.4488, lr 5.470541566592577e-05
Step 320500 Loss: 0.4369, lr 5.470541566592577e-05
Step 320600 Loss: 0.4258, lr 5.470541566592577e-05
Step 320700 Loss: 0.4154, lr 5.470541566592577e-05
Step 320800 Loss: 0.4056, lr 5.470541566592577e-05
Step 320900 Loss: 0.3965, lr 5.470541566592577e-05
Step 321000 Loss: 0.3878, lr 5.470541566592577e-05
Step 321100 Loss: 0.3796, lr 5.470541566592577e-05
Step 321200 Loss: 0.3719, lr 5.470541566592577e-05
Step 321300 Loss: 0.3645, lr 5.470541566592577e-05
Step 321400 Loss: 0.3576, lr 5.470541566592577e-05
Step 321500 Loss: 0.3510, lr 5.470541566592577e-05
Step 321600 Loss: 0.3447, lr 5.470541566592577e-05
Step 321700 Loss: 0.3387, lr 5.470541566592577e-05
Step 321800 Loss: 0.3322, lr 5.470541566592577e-05
Step 321900 Loss: 0.3232, lr 5.470541566592577e-05
Step 322000 Loss: 0.3146, lr 5.470541566592577e-05
Step 322100 Loss: 0.3064, lr 5.470541566592577e-05
Step 322200 Loss: 0.2985, lr 5.470541566592577e-05
Step 322300 Loss: 0.2910, lr 5.470541566592577e-05
Step 322400 Loss: 0.2838, lr 5.470541566592577e-05
Step 322500 Loss: 0.2768, lr 5.470541566592577e-05
Step 322600 Loss: 0.2702, lr 5.470541566592577e-05
Step 322700 Loss: 0.2637, lr 5.470541566592577e-05
Step 322800 Loss: 0.2553, lr 5.470541566592577e-05
Step 322900 Loss: 0.2460, lr 5.470541566592577e-05
Step 323000 Loss: 0.2370, lr 5.470541566592577e-05
Step 323100 Loss: 0.2283, lr 5.470541566592577e-05
Step 323200 Loss: 0.2199, lr 5.470541566592577e-05
Step 323300 Loss: 0.2118, lr 5.470541566592577e-05
Step 323400 Loss: 0.2040, lr 5.470541566592577e-05
Step 323500 Loss: 0.1940, lr 5.470541566592577e-05
Step 323600 Loss: 0.1830, lr 5.470541566592577e-05
Step 323700 Loss: 0.1723, lr 5.470541566592577e-05
Step 323800 Loss: 0.1620, lr 5.470541566592577e-05
Step 323900 Loss: 0.1508, lr 5.470541566592577e-05
Step 324000 Loss: 0.1365, lr 5.470541566592577e-05
Step 324100 Loss: 0.1227, lr 5.470541566592577e-05
Step 324200 Loss: 0.1092, lr 5.470541566592577e-05
Train Epoch: [48/100] Loss: 0.0953,lr 0.000055
Calling G2SDataset.batch()
Done, time:  1.60 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6755
Step 324300 Loss: 1.1942, lr 5.313952597646573e-05
Step 324400 Loss: 0.9863, lr 5.313952597646573e-05
Step 324500 Loss: 0.8959, lr 5.313952597646573e-05
Step 324600 Loss: 0.8423, lr 5.313952597646573e-05
Step 324700 Loss: 0.8137, lr 5.313952597646573e-05
Step 324800 Loss: 0.7909, lr 5.313952597646573e-05
Step 324900 Loss: 0.7528, lr 5.313952597646573e-05
Step 325000 Loss: 0.7251, lr 5.313952597646573e-05
Step 325100 Loss: 0.7042, lr 5.313952597646573e-05
Step 325200 Loss: 0.6877, lr 5.313952597646573e-05
Step 325300 Loss: 0.6745, lr 5.313952597646573e-05
Step 325400 Loss: 0.6615, lr 5.313952597646573e-05
Step 325500 Loss: 0.6447, lr 5.313952597646573e-05
Step 325600 Loss: 0.6304, lr 5.313952597646573e-05
Step 325700 Loss: 0.6182, lr 5.313952597646573e-05
Step 325800 Loss: 0.6075, lr 5.313952597646573e-05
Step 325900 Loss: 0.5982, lr 5.313952597646573e-05
Step 326000 Loss: 0.5899, lr 5.313952597646573e-05
Step 326100 Loss: 0.5826, lr 5.313952597646573e-05
Step 326200 Loss: 0.5672, lr 5.313952597646573e-05
Step 326300 Loss: 0.5506, lr 5.313952597646573e-05
Step 326400 Loss: 0.5355, lr 5.313952597646573e-05
Step 326500 Loss: 0.5218, lr 5.313952597646573e-05
Step 326600 Loss: 0.5092, lr 5.313952597646573e-05
Step 326700 Loss: 0.4977, lr 5.313952597646573e-05
Step 326800 Loss: 0.4871, lr 5.313952597646573e-05
Step 326900 Loss: 0.4773, lr 5.313952597646573e-05
Step 327000 Loss: 0.4683, lr 5.313952597646573e-05
Step 327100 Loss: 0.4558, lr 5.313952597646573e-05
Step 327200 Loss: 0.4435, lr 5.313952597646573e-05
Step 327300 Loss: 0.4320, lr 5.313952597646573e-05
Step 327400 Loss: 0.4212, lr 5.313952597646573e-05
Step 327500 Loss: 0.4110, lr 5.313952597646573e-05
Step 327600 Loss: 0.4015, lr 5.313952597646573e-05
Step 327700 Loss: 0.3926, lr 5.313952597646573e-05
Step 327800 Loss: 0.3841, lr 5.313952597646573e-05
Step 327900 Loss: 0.3761, lr 5.313952597646573e-05
Step 328000 Loss: 0.3686, lr 5.313952597646573e-05
Step 328100 Loss: 0.3614, lr 5.313952597646573e-05
Step 328200 Loss: 0.3546, lr 5.313952597646573e-05
Step 328300 Loss: 0.3482, lr 5.313952597646573e-05
Step 328400 Loss: 0.3420, lr 5.313952597646573e-05
Step 328500 Loss: 0.3362, lr 5.313952597646573e-05
Step 328600 Loss: 0.3282, lr 5.313952597646573e-05
Step 328700 Loss: 0.3194, lr 5.313952597646573e-05
Step 328800 Loss: 0.3110, lr 5.313952597646573e-05
Step 328900 Loss: 0.3029, lr 5.313952597646573e-05
Step 329000 Loss: 0.2952, lr 5.313952597646573e-05
Step 329100 Loss: 0.2878, lr 5.313952597646573e-05
Step 329200 Loss: 0.2807, lr 5.313952597646573e-05
Step 329300 Loss: 0.2738, lr 5.313952597646573e-05
Step 329400 Loss: 0.2673, lr 5.313952597646573e-05
Step 329500 Loss: 0.2607, lr 5.313952597646573e-05
Step 329600 Loss: 0.2512, lr 5.313952597646573e-05
Step 329700 Loss: 0.2420, lr 5.313952597646573e-05
Step 329800 Loss: 0.2331, lr 5.313952597646573e-05
Step 329900 Loss: 0.2245, lr 5.313952597646573e-05
Step 330000 Loss: 0.2163, lr 5.313952597646573e-05
Step 330100 Loss: 0.2083, lr 5.313952597646573e-05
Step 330200 Loss: 0.2003, lr 5.313952597646573e-05
Step 330300 Loss: 0.1891, lr 5.313952597646573e-05
Step 330400 Loss: 0.1782, lr 5.313952597646573e-05
Step 330500 Loss: 0.1677, lr 5.313952597646573e-05
Step 330600 Loss: 0.1575, lr 5.313952597646573e-05
Step 330700 Loss: 0.1444, lr 5.313952597646573e-05
Step 330800 Loss: 0.1303, lr 5.313952597646573e-05
Step 330900 Loss: 0.1167, lr 5.313952597646573e-05
Step 331000 Loss: 0.1012, lr 5.313952597646573e-05
Train Epoch: [49/100] Loss: 0.0955,lr 0.000053
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 331100 Loss: 1.0293, lr 5.1570537953906474e-05
Step 331200 Loss: 0.9513, lr 5.1570537953906474e-05
Step 331300 Loss: 0.8664, lr 5.1570537953906474e-05
Step 331400 Loss: 0.8272, lr 5.1570537953906474e-05
Step 331500 Loss: 0.8045, lr 5.1570537953906474e-05
Step 331600 Loss: 0.7718, lr 5.1570537953906474e-05
Step 331700 Loss: 0.7391, lr 5.1570537953906474e-05
Step 331800 Loss: 0.7148, lr 5.1570537953906474e-05
Step 331900 Loss: 0.6961, lr 5.1570537953906474e-05
Step 332000 Loss: 0.6813, lr 5.1570537953906474e-05
Step 332100 Loss: 0.6692, lr 5.1570537953906474e-05
Step 332200 Loss: 0.6534, lr 5.1570537953906474e-05
Step 332300 Loss: 0.6379, lr 5.1570537953906474e-05
Step 332400 Loss: 0.6246, lr 5.1570537953906474e-05
Step 332500 Loss: 0.6131, lr 5.1570537953906474e-05
Step 332600 Loss: 0.6031, lr 5.1570537953906474e-05
Step 332700 Loss: 0.5943, lr 5.1570537953906474e-05
Step 332800 Loss: 0.5864, lr 5.1570537953906474e-05
Step 332900 Loss: 0.5770, lr 5.1570537953906474e-05
Step 333000 Loss: 0.5594, lr 5.1570537953906474e-05
Step 333100 Loss: 0.5435, lr 5.1570537953906474e-05
Step 333200 Loss: 0.5291, lr 5.1570537953906474e-05
Step 333300 Loss: 0.5159, lr 5.1570537953906474e-05
Step 333400 Loss: 0.5039, lr 5.1570537953906474e-05
Step 333500 Loss: 0.4928, lr 5.1570537953906474e-05
Step 333600 Loss: 0.4826, lr 5.1570537953906474e-05
Step 333700 Loss: 0.4731, lr 5.1570537953906474e-05
Step 333800 Loss: 0.4629, lr 5.1570537953906474e-05
Step 333900 Loss: 0.4501, lr 5.1570537953906474e-05
Step 334000 Loss: 0.4381, lr 5.1570537953906474e-05
Step 334100 Loss: 0.4269, lr 5.1570537953906474e-05
Step 334200 Loss: 0.4165, lr 5.1570537953906474e-05
Step 334300 Loss: 0.4066, lr 5.1570537953906474e-05
Step 334400 Loss: 0.3974, lr 5.1570537953906474e-05
Step 334500 Loss: 0.3886, lr 5.1570537953906474e-05
Step 334600 Loss: 0.3804, lr 5.1570537953906474e-05
Step 334700 Loss: 0.3726, lr 5.1570537953906474e-05
Step 334800 Loss: 0.3653, lr 5.1570537953906474e-05
Step 334900 Loss: 0.3583, lr 5.1570537953906474e-05
Step 335000 Loss: 0.3516, lr 5.1570537953906474e-05
Step 335100 Loss: 0.3453, lr 5.1570537953906474e-05
Step 335200 Loss: 0.3393, lr 5.1570537953906474e-05
Step 335300 Loss: 0.3332, lr 5.1570537953906474e-05
Step 335400 Loss: 0.3241, lr 5.1570537953906474e-05
Step 335500 Loss: 0.3155, lr 5.1570537953906474e-05
Step 335600 Loss: 0.3072, lr 5.1570537953906474e-05
Step 335700 Loss: 0.2993, lr 5.1570537953906474e-05
Step 335800 Loss: 0.2917, lr 5.1570537953906474e-05
Step 335900 Loss: 0.2845, lr 5.1570537953906474e-05
Step 336000 Loss: 0.2775, lr 5.1570537953906474e-05
Step 336100 Loss: 0.2708, lr 5.1570537953906474e-05
Step 336200 Loss: 0.2644, lr 5.1570537953906474e-05
Step 336300 Loss: 0.2563, lr 5.1570537953906474e-05
Step 336400 Loss: 0.2470, lr 5.1570537953906474e-05
Step 336500 Loss: 0.2379, lr 5.1570537953906474e-05
Step 336600 Loss: 0.2292, lr 5.1570537953906474e-05
Step 336700 Loss: 0.2208, lr 5.1570537953906474e-05
Step 336800 Loss: 0.2126, lr 5.1570537953906474e-05
Step 336900 Loss: 0.2048, lr 5.1570537953906474e-05
Step 337000 Loss: 0.1952, lr 5.1570537953906474e-05
Step 337100 Loss: 0.1841, lr 5.1570537953906474e-05
Step 337200 Loss: 0.1734, lr 5.1570537953906474e-05
Step 337300 Loss: 0.1630, lr 5.1570537953906474e-05
Step 337400 Loss: 0.1523, lr 5.1570537953906474e-05
Step 337500 Loss: 0.1380, lr 5.1570537953906474e-05
Step 337600 Loss: 0.1241, lr 5.1570537953906474e-05
Step 337700 Loss: 0.1106, lr 5.1570537953906474e-05
Train Epoch: [50/100] Loss: 0.0955,lr 0.000052
Model Saving at epoch 50
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.75 s, total batches: 6756
Step 337800 Loss: 1.2516, lr 5.0000000000000057e-05
Step 337900 Loss: 0.9916, lr 5.0000000000000057e-05
Step 338000 Loss: 0.9042, lr 5.0000000000000057e-05
Step 338100 Loss: 0.8461, lr 5.0000000000000057e-05
Step 338200 Loss: 0.8159, lr 5.0000000000000057e-05
Step 338300 Loss: 0.7957, lr 5.0000000000000057e-05
Step 338400 Loss: 0.7561, lr 5.0000000000000057e-05
Step 338500 Loss: 0.7276, lr 5.0000000000000057e-05
Step 338600 Loss: 0.7060, lr 5.0000000000000057e-05
Step 338700 Loss: 0.6892, lr 5.0000000000000057e-05
Step 338800 Loss: 0.6757, lr 5.0000000000000057e-05
Step 338900 Loss: 0.6634, lr 5.0000000000000057e-05
Step 339000 Loss: 0.6463, lr 5.0000000000000057e-05
Step 339100 Loss: 0.6318, lr 5.0000000000000057e-05
Step 339200 Loss: 0.6193, lr 5.0000000000000057e-05
Step 339300 Loss: 0.6085, lr 5.0000000000000057e-05
Step 339400 Loss: 0.5991, lr 5.0000000000000057e-05
Step 339500 Loss: 0.5907, lr 5.0000000000000057e-05
Step 339600 Loss: 0.5832, lr 5.0000000000000057e-05
Step 339700 Loss: 0.5690, lr 5.0000000000000057e-05
Step 339800 Loss: 0.5522, lr 5.0000000000000057e-05
Step 339900 Loss: 0.5370, lr 5.0000000000000057e-05
Step 340000 Loss: 0.5231, lr 5.0000000000000057e-05
Step 340100 Loss: 0.5105, lr 5.0000000000000057e-05
Step 340200 Loss: 0.4989, lr 5.0000000000000057e-05
Step 340300 Loss: 0.4882, lr 5.0000000000000057e-05
Step 340400 Loss: 0.4783, lr 5.0000000000000057e-05
Step 340500 Loss: 0.4691, lr 5.0000000000000057e-05
Step 340600 Loss: 0.4571, lr 5.0000000000000057e-05
Step 340700 Loss: 0.4447, lr 5.0000000000000057e-05
Step 340800 Loss: 0.4331, lr 5.0000000000000057e-05
Step 340900 Loss: 0.4222, lr 5.0000000000000057e-05
Step 341000 Loss: 0.4120, lr 5.0000000000000057e-05
Step 341100 Loss: 0.4025, lr 5.0000000000000057e-05
Step 341200 Loss: 0.3934, lr 5.0000000000000057e-05
Step 341300 Loss: 0.3849, lr 5.0000000000000057e-05
Step 341400 Loss: 0.3769, lr 5.0000000000000057e-05
Step 341500 Loss: 0.3693, lr 5.0000000000000057e-05
Step 341600 Loss: 0.3621, lr 5.0000000000000057e-05
Step 341700 Loss: 0.3553, lr 5.0000000000000057e-05
Step 341800 Loss: 0.3488, lr 5.0000000000000057e-05
Step 341900 Loss: 0.3426, lr 5.0000000000000057e-05
Step 342000 Loss: 0.3367, lr 5.0000000000000057e-05
Step 342100 Loss: 0.3291, lr 5.0000000000000057e-05
Step 342200 Loss: 0.3203, lr 5.0000000000000057e-05
Step 342300 Loss: 0.3118, lr 5.0000000000000057e-05
Step 342400 Loss: 0.3037, lr 5.0000000000000057e-05
Step 342500 Loss: 0.2959, lr 5.0000000000000057e-05
Step 342600 Loss: 0.2885, lr 5.0000000000000057e-05
Step 342700 Loss: 0.2814, lr 5.0000000000000057e-05
Step 342800 Loss: 0.2745, lr 5.0000000000000057e-05
Step 342900 Loss: 0.2679, lr 5.0000000000000057e-05
Step 343000 Loss: 0.2616, lr 5.0000000000000057e-05
Step 343100 Loss: 0.2521, lr 5.0000000000000057e-05
Step 343200 Loss: 0.2429, lr 5.0000000000000057e-05
Step 343300 Loss: 0.2340, lr 5.0000000000000057e-05
Step 343400 Loss: 0.2254, lr 5.0000000000000057e-05
Step 343500 Loss: 0.2171, lr 5.0000000000000057e-05
Step 343600 Loss: 0.2091, lr 5.0000000000000057e-05
Step 343700 Loss: 0.2014, lr 5.0000000000000057e-05
Step 343800 Loss: 0.1903, lr 5.0000000000000057e-05
Step 343900 Loss: 0.1793, lr 5.0000000000000057e-05
Step 344000 Loss: 0.1688, lr 5.0000000000000057e-05
Step 344100 Loss: 0.1586, lr 5.0000000000000057e-05
Step 344200 Loss: 0.1459, lr 5.0000000000000057e-05
Step 344300 Loss: 0.1318, lr 5.0000000000000057e-05
Step 344400 Loss: 0.1181, lr 5.0000000000000057e-05
Step 344500 Loss: 0.1032, lr 5.0000000000000057e-05
Train Epoch: [51/100] Loss: 0.0954,lr 0.000050
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Step 344600 Loss: 1.0480, lr 4.842946204609364e-05
Step 344700 Loss: 0.9685, lr 4.842946204609364e-05
Step 344800 Loss: 0.8731, lr 4.842946204609364e-05
Step 344900 Loss: 0.8307, lr 4.842946204609364e-05
Step 345000 Loss: 0.8067, lr 4.842946204609364e-05
Step 345100 Loss: 0.7765, lr 4.842946204609364e-05
Step 345200 Loss: 0.7425, lr 4.842946204609364e-05
Step 345300 Loss: 0.7174, lr 4.842946204609364e-05
Step 345400 Loss: 0.6981, lr 4.842946204609364e-05
Step 345500 Loss: 0.6829, lr 4.842946204609364e-05
Step 345600 Loss: 0.6705, lr 4.842946204609364e-05
Step 345700 Loss: 0.6555, lr 4.842946204609364e-05
Step 345800 Loss: 0.6397, lr 4.842946204609364e-05
Step 345900 Loss: 0.6261, lr 4.842946204609364e-05
Step 346000 Loss: 0.6144, lr 4.842946204609364e-05
Step 346100 Loss: 0.6042, lr 4.842946204609364e-05
Step 346200 Loss: 0.5953, lr 4.842946204609364e-05
Step 346300 Loss: 0.5873, lr 4.842946204609364e-05
Step 346400 Loss: 0.5793, lr 4.842946204609364e-05
Step 346500 Loss: 0.5615, lr 4.842946204609364e-05
Step 346600 Loss: 0.5454, lr 4.842946204609364e-05
Step 346700 Loss: 0.5308, lr 4.842946204609364e-05
Step 346800 Loss: 0.5174, lr 4.842946204609364e-05
Step 346900 Loss: 0.5053, lr 4.842946204609364e-05
Step 347000 Loss: 0.4941, lr 4.842946204609364e-05
Step 347100 Loss: 0.4837, lr 4.842946204609364e-05
Step 347200 Loss: 0.4742, lr 4.842946204609364e-05
Step 347300 Loss: 0.4645, lr 4.842946204609364e-05
Step 347400 Loss: 0.4516, lr 4.842946204609364e-05
Step 347500 Loss: 0.4395, lr 4.842946204609364e-05
Step 347600 Loss: 0.4282, lr 4.842946204609364e-05
Step 347700 Loss: 0.4177, lr 4.842946204609364e-05
Step 347800 Loss: 0.4078, lr 4.842946204609364e-05
Step 347900 Loss: 0.3984, lr 4.842946204609364e-05
Step 348000 Loss: 0.3897, lr 4.842946204609364e-05
Step 348100 Loss: 0.3814, lr 4.842946204609364e-05
Step 348200 Loss: 0.3735, lr 4.842946204609364e-05
Step 348300 Loss: 0.3661, lr 4.842946204609364e-05
Step 348400 Loss: 0.3591, lr 4.842946204609364e-05
Step 348500 Loss: 0.3524, lr 4.842946204609364e-05
Step 348600 Loss: 0.3460, lr 4.842946204609364e-05
Step 348700 Loss: 0.3400, lr 4.842946204609364e-05
Step 348800 Loss: 0.3342, lr 4.842946204609364e-05
Step 348900 Loss: 0.3252, lr 4.842946204609364e-05
Step 349000 Loss: 0.3165, lr 4.842946204609364e-05
Step 349100 Loss: 0.3082, lr 4.842946204609364e-05
Step 349200 Loss: 0.3002, lr 4.842946204609364e-05
Step 349300 Loss: 0.2926, lr 4.842946204609364e-05
Step 349400 Loss: 0.2853, lr 4.842946204609364e-05
Step 349500 Loss: 0.2783, lr 4.842946204609364e-05
Step 349600 Loss: 0.2716, lr 4.842946204609364e-05
Step 349700 Loss: 0.2651, lr 4.842946204609364e-05
Step 349800 Loss: 0.2574, lr 4.842946204609364e-05
Step 349900 Loss: 0.2480, lr 4.842946204609364e-05
Step 350000 Loss: 0.2389, lr 4.842946204609364e-05
Step 350100 Loss: 0.2302, lr 4.842946204609364e-05
Step 350200 Loss: 0.2217, lr 4.842946204609364e-05
Step 350300 Loss: 0.2135, lr 4.842946204609364e-05
Step 350400 Loss: 0.2057, lr 4.842946204609364e-05
Step 350500 Loss: 0.1965, lr 4.842946204609364e-05
Step 350600 Loss: 0.1854, lr 4.842946204609364e-05
Step 350700 Loss: 0.1746, lr 4.842946204609364e-05
Step 350800 Loss: 0.1642, lr 4.842946204609364e-05
Step 350900 Loss: 0.1540, lr 4.842946204609364e-05
Step 351000 Loss: 0.1396, lr 4.842946204609364e-05
Step 351100 Loss: 0.1257, lr 4.842946204609364e-05
Step 351200 Loss: 0.1122, lr 4.842946204609364e-05
Train Epoch: [52/100] Loss: 0.0952,lr 0.000048
Calling G2SDataset.batch()
Done, time:  2.13 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6755
Step 351300 Loss: 1.2522, lr 4.686047402353438e-05
Step 351400 Loss: 0.9981, lr 4.686047402353438e-05
Step 351500 Loss: 0.9154, lr 4.686047402353438e-05
Step 351600 Loss: 0.8512, lr 4.686047402353438e-05
Step 351700 Loss: 0.8188, lr 4.686047402353438e-05
Step 351800 Loss: 0.7993, lr 4.686047402353438e-05
Step 351900 Loss: 0.7603, lr 4.686047402353438e-05
Step 352000 Loss: 0.7306, lr 4.686047402353438e-05
Step 352100 Loss: 0.7084, lr 4.686047402353438e-05
Step 352200 Loss: 0.6911, lr 4.686047402353438e-05
Step 352300 Loss: 0.6772, lr 4.686047402353438e-05
Step 352400 Loss: 0.6656, lr 4.686047402353438e-05
Step 352500 Loss: 0.6482, lr 4.686047402353438e-05
Step 352600 Loss: 0.6334, lr 4.686047402353438e-05
Step 352700 Loss: 0.6207, lr 4.686047402353438e-05
Step 352800 Loss: 0.6098, lr 4.686047402353438e-05
Step 352900 Loss: 0.6001, lr 4.686047402353438e-05
Step 353000 Loss: 0.5916, lr 4.686047402353438e-05
Step 353100 Loss: 0.5841, lr 4.686047402353438e-05
Step 353200 Loss: 0.5712, lr 4.686047402353438e-05
Step 353300 Loss: 0.5541, lr 4.686047402353438e-05
Step 353400 Loss: 0.5387, lr 4.686047402353438e-05
Step 353500 Loss: 0.5247, lr 4.686047402353438e-05
Step 353600 Loss: 0.5119, lr 4.686047402353438e-05
Step 353700 Loss: 0.5002, lr 4.686047402353438e-05
Step 353800 Loss: 0.4894, lr 4.686047402353438e-05
Step 353900 Loss: 0.4794, lr 4.686047402353438e-05
Step 354000 Loss: 0.4702, lr 4.686047402353438e-05
Step 354100 Loss: 0.4587, lr 4.686047402353438e-05
Step 354200 Loss: 0.4461, lr 4.686047402353438e-05
Step 354300 Loss: 0.4344, lr 4.686047402353438e-05
Step 354400 Loss: 0.4235, lr 4.686047402353438e-05
Step 354500 Loss: 0.4132, lr 4.686047402353438e-05
Step 354600 Loss: 0.4036, lr 4.686047402353438e-05
Step 354700 Loss: 0.3945, lr 4.686047402353438e-05
Step 354800 Loss: 0.3859, lr 4.686047402353438e-05
Step 354900 Loss: 0.3778, lr 4.686047402353438e-05
Step 355000 Loss: 0.3702, lr 4.686047402353438e-05
Step 355100 Loss: 0.3629, lr 4.686047402353438e-05
Step 355200 Loss: 0.3561, lr 4.686047402353438e-05
Step 355300 Loss: 0.3495, lr 4.686047402353438e-05
Step 355400 Loss: 0.3433, lr 4.686047402353438e-05
Step 355500 Loss: 0.3374, lr 4.686047402353438e-05
Step 355600 Loss: 0.3302, lr 4.686047402353438e-05
Step 355700 Loss: 0.3213, lr 4.686047402353438e-05
Step 355800 Loss: 0.3128, lr 4.686047402353438e-05
Step 355900 Loss: 0.3046, lr 4.686047402353438e-05
Step 356000 Loss: 0.2968, lr 4.686047402353438e-05
Step 356100 Loss: 0.2893, lr 4.686047402353438e-05
Step 356200 Loss: 0.2822, lr 4.686047402353438e-05
Step 356300 Loss: 0.2753, lr 4.686047402353438e-05
Step 356400 Loss: 0.2687, lr 4.686047402353438e-05
Step 356500 Loss: 0.2623, lr 4.686047402353438e-05
Step 356600 Loss: 0.2532, lr 4.686047402353438e-05
Step 356700 Loss: 0.2439, lr 4.686047402353438e-05
Step 356800 Loss: 0.2350, lr 4.686047402353438e-05
Step 356900 Loss: 0.2264, lr 4.686047402353438e-05
Step 357000 Loss: 0.2181, lr 4.686047402353438e-05
Step 357100 Loss: 0.2100, lr 4.686047402353438e-05
Step 357200 Loss: 0.2023, lr 4.686047402353438e-05
Step 357300 Loss: 0.1916, lr 4.686047402353438e-05
Step 357400 Loss: 0.1806, lr 4.686047402353438e-05
Step 357500 Loss: 0.1700, lr 4.686047402353438e-05
Step 357600 Loss: 0.1597, lr 4.686047402353438e-05
Step 357700 Loss: 0.1476, lr 4.686047402353438e-05
Step 357800 Loss: 0.1334, lr 4.686047402353438e-05
Step 357900 Loss: 0.1197, lr 4.686047402353438e-05
Step 358000 Loss: 0.1055, lr 4.686047402353438e-05
Train Epoch: [53/100] Loss: 0.0957,lr 0.000047
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6755
Step 358100 Loss: 1.0745, lr 4.529458433407433e-05
Step 358200 Loss: 0.9774, lr 4.529458433407433e-05
Step 358300 Loss: 0.8801, lr 4.529458433407433e-05
Step 358400 Loss: 0.8343, lr 4.529458433407433e-05
Step 358500 Loss: 0.8089, lr 4.529458433407433e-05
Step 358600 Loss: 0.7811, lr 4.529458433407433e-05
Step 358700 Loss: 0.7458, lr 4.529458433407433e-05
Step 358800 Loss: 0.7199, lr 4.529458433407433e-05
Step 358900 Loss: 0.7001, lr 4.529458433407433e-05
Step 359000 Loss: 0.6844, lr 4.529458433407433e-05
Step 359100 Loss: 0.6718, lr 4.529458433407433e-05
Step 359200 Loss: 0.6574, lr 4.529458433407433e-05
Step 359300 Loss: 0.6412, lr 4.529458433407433e-05
Step 359400 Loss: 0.6274, lr 4.529458433407433e-05
Step 359500 Loss: 0.6156, lr 4.529458433407433e-05
Step 359600 Loss: 0.6052, lr 4.529458433407433e-05
Step 359700 Loss: 0.5962, lr 4.529458433407433e-05
Step 359800 Loss: 0.5881, lr 4.529458433407433e-05
Step 359900 Loss: 0.5809, lr 4.529458433407433e-05
Step 360000 Loss: 0.5633, lr 4.529458433407433e-05
Step 360100 Loss: 0.5470, lr 4.529458433407433e-05
Step 360200 Loss: 0.5322, lr 4.529458433407433e-05
Step 360300 Loss: 0.5188, lr 4.529458433407433e-05
Step 360400 Loss: 0.5065, lr 4.529458433407433e-05
Step 360500 Loss: 0.4952, lr 4.529458433407433e-05
Step 360600 Loss: 0.4848, lr 4.529458433407433e-05
Step 360700 Loss: 0.4752, lr 4.529458433407433e-05
Step 360800 Loss: 0.4659, lr 4.529458433407433e-05
Step 360900 Loss: 0.4529, lr 4.529458433407433e-05
Step 361000 Loss: 0.4407, lr 4.529458433407433e-05
Step 361100 Loss: 0.4294, lr 4.529458433407433e-05
Step 361200 Loss: 0.4188, lr 4.529458433407433e-05
Step 361300 Loss: 0.4088, lr 4.529458433407433e-05
Step 361400 Loss: 0.3994, lr 4.529458433407433e-05
Step 361500 Loss: 0.3905, lr 4.529458433407433e-05
Step 361600 Loss: 0.3822, lr 4.529458433407433e-05
Step 361700 Loss: 0.3743, lr 4.529458433407433e-05
Step 361800 Loss: 0.3668, lr 4.529458433407433e-05
Step 361900 Loss: 0.3598, lr 4.529458433407433e-05
Step 362000 Loss: 0.3531, lr 4.529458433407433e-05
Step 362100 Loss: 0.3467, lr 4.529458433407433e-05
Step 362200 Loss: 0.3406, lr 4.529458433407433e-05
Step 362300 Loss: 0.3348, lr 4.529458433407433e-05
Step 362400 Loss: 0.3261, lr 4.529458433407433e-05
Step 362500 Loss: 0.3174, lr 4.529458433407433e-05
Step 362600 Loss: 0.3090, lr 4.529458433407433e-05
Step 362700 Loss: 0.3010, lr 4.529458433407433e-05
Step 362800 Loss: 0.2934, lr 4.529458433407433e-05
Step 362900 Loss: 0.2861, lr 4.529458433407433e-05
Step 363000 Loss: 0.2790, lr 4.529458433407433e-05
Step 363100 Loss: 0.2723, lr 4.529458433407433e-05
Step 363200 Loss: 0.2658, lr 4.529458433407433e-05
Step 363300 Loss: 0.2584, lr 4.529458433407433e-05
Step 363400 Loss: 0.2490, lr 4.529458433407433e-05
Step 363500 Loss: 0.2398, lr 4.529458433407433e-05
Step 363600 Loss: 0.2311, lr 4.529458433407433e-05
Step 363700 Loss: 0.2226, lr 4.529458433407433e-05
Step 363800 Loss: 0.2144, lr 4.529458433407433e-05
Step 363900 Loss: 0.2065, lr 4.529458433407433e-05
Step 364000 Loss: 0.1977, lr 4.529458433407433e-05
Step 364100 Loss: 0.1866, lr 4.529458433407433e-05
Step 364200 Loss: 0.1758, lr 4.529458433407433e-05
Step 364300 Loss: 0.1653, lr 4.529458433407433e-05
Step 364400 Loss: 0.1552, lr 4.529458433407433e-05
Step 364500 Loss: 0.1412, lr 4.529458433407433e-05
Step 364600 Loss: 0.1272, lr 4.529458433407433e-05
Step 364700 Loss: 0.1136, lr 4.529458433407433e-05
Step 364800 Loss: 0.0969, lr 4.529458433407433e-05
Train Epoch: [54/100] Loss: 0.0956,lr 0.000045
Calling G2SDataset.batch()
Done, time:  1.80 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.84 s, total batches: 6756
Step 364900 Loss: 1.0060, lr 4.3733338321784836e-05
Step 365000 Loss: 0.9257, lr 4.3733338321784836e-05
Step 365100 Loss: 0.8559, lr 4.3733338321784836e-05
Step 365200 Loss: 0.8215, lr 4.3733338321784836e-05
Step 365300 Loss: 0.8010, lr 4.3733338321784836e-05
Step 365400 Loss: 0.7638, lr 4.3733338321784836e-05
Step 365500 Loss: 0.7333, lr 4.3733338321784836e-05
Step 365600 Loss: 0.7104, lr 4.3733338321784836e-05
Step 365700 Loss: 0.6927, lr 4.3733338321784836e-05
Step 365800 Loss: 0.6785, lr 4.3733338321784836e-05
Step 365900 Loss: 0.6669, lr 4.3733338321784836e-05
Step 366000 Loss: 0.6498, lr 4.3733338321784836e-05
Step 366100 Loss: 0.6348, lr 4.3733338321784836e-05
Step 366200 Loss: 0.6219, lr 4.3733338321784836e-05
Step 366300 Loss: 0.6108, lr 4.3733338321784836e-05
Step 366400 Loss: 0.6010, lr 4.3733338321784836e-05
Step 366500 Loss: 0.5924, lr 4.3733338321784836e-05
Step 366600 Loss: 0.5848, lr 4.3733338321784836e-05
Step 366700 Loss: 0.5730, lr 4.3733338321784836e-05
Step 366800 Loss: 0.5558, lr 4.3733338321784836e-05
Step 366900 Loss: 0.5402, lr 4.3733338321784836e-05
Step 367000 Loss: 0.5261, lr 4.3733338321784836e-05
Step 367100 Loss: 0.5131, lr 4.3733338321784836e-05
Step 367200 Loss: 0.5013, lr 4.3733338321784836e-05
Step 367300 Loss: 0.4904, lr 4.3733338321784836e-05
Step 367400 Loss: 0.4804, lr 4.3733338321784836e-05
Step 367500 Loss: 0.4711, lr 4.3733338321784836e-05
Step 367600 Loss: 0.4600, lr 4.3733338321784836e-05
Step 367700 Loss: 0.4473, lr 4.3733338321784836e-05
Step 367800 Loss: 0.4355, lr 4.3733338321784836e-05
Step 367900 Loss: 0.4245, lr 4.3733338321784836e-05
Step 368000 Loss: 0.4142, lr 4.3733338321784836e-05
Step 368100 Loss: 0.4045, lr 4.3733338321784836e-05
Step 368200 Loss: 0.3953, lr 4.3733338321784836e-05
Step 368300 Loss: 0.3867, lr 4.3733338321784836e-05
Step 368400 Loss: 0.3786, lr 4.3733338321784836e-05
Step 368500 Loss: 0.3709, lr 4.3733338321784836e-05
Step 368600 Loss: 0.3636, lr 4.3733338321784836e-05
Step 368700 Loss: 0.3567, lr 4.3733338321784836e-05
Step 368800 Loss: 0.3501, lr 4.3733338321784836e-05
Step 368900 Loss: 0.3439, lr 4.3733338321784836e-05
Step 369000 Loss: 0.3379, lr 4.3733338321784836e-05
Step 369100 Loss: 0.3311, lr 4.3733338321784836e-05
Step 369200 Loss: 0.3221, lr 4.3733338321784836e-05
Step 369300 Loss: 0.3136, lr 4.3733338321784836e-05
Step 369400 Loss: 0.3054, lr 4.3733338321784836e-05
Step 369500 Loss: 0.2975, lr 4.3733338321784836e-05
Step 369600 Loss: 0.2900, lr 4.3733338321784836e-05
Step 369700 Loss: 0.2828, lr 4.3733338321784836e-05
Step 369800 Loss: 0.2759, lr 4.3733338321784836e-05
Step 369900 Loss: 0.2693, lr 4.3733338321784836e-05
Step 370000 Loss: 0.2629, lr 4.3733338321784836e-05
Step 370100 Loss: 0.2541, lr 4.3733338321784836e-05
Step 370200 Loss: 0.2448, lr 4.3733338321784836e-05
Step 370300 Loss: 0.2358, lr 4.3733338321784836e-05
Step 370400 Loss: 0.2272, lr 4.3733338321784836e-05
Step 370500 Loss: 0.2188, lr 4.3733338321784836e-05
Step 370600 Loss: 0.2108, lr 4.3733338321784836e-05
Step 370700 Loss: 0.2030, lr 4.3733338321784836e-05
Step 370800 Loss: 0.1926, lr 4.3733338321784836e-05
Step 370900 Loss: 0.1816, lr 4.3733338321784836e-05
Step 371000 Loss: 0.1710, lr 4.3733338321784836e-05
Step 371100 Loss: 0.1607, lr 4.3733338321784836e-05
Step 371200 Loss: 0.1490, lr 4.3733338321784836e-05
Step 371300 Loss: 0.1348, lr 4.3733338321784836e-05
Step 371400 Loss: 0.1210, lr 4.3733338321784836e-05
Step 371500 Loss: 0.1074, lr 4.3733338321784836e-05
Train Epoch: [55/100] Loss: 0.0951,lr 0.000044
Model Saving at epoch 55
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.68 s, total batches: 6755
Step 371600 Loss: 1.1178, lr 4.21782767479885e-05
Step 371700 Loss: 0.9817, lr 4.21782767479885e-05
Step 371800 Loss: 0.8868, lr 4.21782767479885e-05
Step 371900 Loss: 0.8376, lr 4.21782767479885e-05
Step 372000 Loss: 0.8108, lr 4.21782767479885e-05
Step 372100 Loss: 0.7855, lr 4.21782767479885e-05
Step 372200 Loss: 0.7489, lr 4.21782767479885e-05
Step 372300 Loss: 0.7221, lr 4.21782767479885e-05
Step 372400 Loss: 0.7018, lr 4.21782767479885e-05
Step 372500 Loss: 0.6858, lr 4.21782767479885e-05
Step 372600 Loss: 0.6729, lr 4.21782767479885e-05
Step 372700 Loss: 0.6592, lr 4.21782767479885e-05
Step 372800 Loss: 0.6427, lr 4.21782767479885e-05
Step 372900 Loss: 0.6287, lr 4.21782767479885e-05
Step 373000 Loss: 0.6167, lr 4.21782767479885e-05
Step 373100 Loss: 0.6062, lr 4.21782767479885e-05
Step 373200 Loss: 0.5970, lr 4.21782767479885e-05
Step 373300 Loss: 0.5888, lr 4.21782767479885e-05
Step 373400 Loss: 0.5816, lr 4.21782767479885e-05
Step 373500 Loss: 0.5651, lr 4.21782767479885e-05
Step 373600 Loss: 0.5487, lr 4.21782767479885e-05
Step 373700 Loss: 0.5338, lr 4.21782767479885e-05
Step 373800 Loss: 0.5202, lr 4.21782767479885e-05
Step 373900 Loss: 0.5078, lr 4.21782767479885e-05
Step 374000 Loss: 0.4964, lr 4.21782767479885e-05
Step 374100 Loss: 0.4859, lr 4.21782767479885e-05
Step 374200 Loss: 0.4761, lr 4.21782767479885e-05
Step 374300 Loss: 0.4672, lr 4.21782767479885e-05
Step 374400 Loss: 0.4543, lr 4.21782767479885e-05
Step 374500 Loss: 0.4420, lr 4.21782767479885e-05
Step 374600 Loss: 0.4306, lr 4.21782767479885e-05
Step 374700 Loss: 0.4199, lr 4.21782767479885e-05
Step 374800 Loss: 0.4098, lr 4.21782767479885e-05
Step 374900 Loss: 0.4004, lr 4.21782767479885e-05
Step 375000 Loss: 0.3914, lr 4.21782767479885e-05
Step 375100 Loss: 0.3831, lr 4.21782767479885e-05
Step 375200 Loss: 0.3751, lr 4.21782767479885e-05
Step 375300 Loss: 0.3676, lr 4.21782767479885e-05
Step 375400 Loss: 0.3605, lr 4.21782767479885e-05
Step 375500 Loss: 0.3537, lr 4.21782767479885e-05
Step 375600 Loss: 0.3473, lr 4.21782767479885e-05
Step 375700 Loss: 0.3412, lr 4.21782767479885e-05
Step 375800 Loss: 0.3354, lr 4.21782767479885e-05
Step 375900 Loss: 0.3270, lr 4.21782767479885e-05
Step 376000 Loss: 0.3183, lr 4.21782767479885e-05
Step 376100 Loss: 0.3099, lr 4.21782767479885e-05
Step 376200 Loss: 0.3019, lr 4.21782767479885e-05
Step 376300 Loss: 0.2942, lr 4.21782767479885e-05
Step 376400 Loss: 0.2868, lr 4.21782767479885e-05
Step 376500 Loss: 0.2797, lr 4.21782767479885e-05
Step 376600 Loss: 0.2729, lr 4.21782767479885e-05
Step 376700 Loss: 0.2664, lr 4.21782767479885e-05
Step 376800 Loss: 0.2595, lr 4.21782767479885e-05
Step 376900 Loss: 0.2500, lr 4.21782767479885e-05
Step 377000 Loss: 0.2408, lr 4.21782767479885e-05
Step 377100 Loss: 0.2320, lr 4.21782767479885e-05
Step 377200 Loss: 0.2235, lr 4.21782767479885e-05
Step 377300 Loss: 0.2153, lr 4.21782767479885e-05
Step 377400 Loss: 0.2073, lr 4.21782767479885e-05
Step 377500 Loss: 0.1990, lr 4.21782767479885e-05
Step 377600 Loss: 0.1878, lr 4.21782767479885e-05
Step 377700 Loss: 0.1770, lr 4.21782767479885e-05
Step 377800 Loss: 0.1665, lr 4.21782767479885e-05
Step 377900 Loss: 0.1564, lr 4.21782767479885e-05
Step 378000 Loss: 0.1427, lr 4.21782767479885e-05
Step 378100 Loss: 0.1287, lr 4.21782767479885e-05
Step 378200 Loss: 0.1151, lr 4.21782767479885e-05
Step 378300 Loss: 0.0989, lr 4.21782767479885e-05
Train Epoch: [56/100] Loss: 0.0956,lr 0.000042
Calling G2SDataset.batch()
Done, time:  1.90 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Step 378400 Loss: 1.0160, lr 4.0630934270713814e-05
Step 378500 Loss: 0.9388, lr 4.0630934270713814e-05
Step 378600 Loss: 0.8616, lr 4.0630934270713814e-05
Step 378700 Loss: 0.8247, lr 4.0630934270713814e-05
Step 378800 Loss: 0.8031, lr 4.0630934270713814e-05
Step 378900 Loss: 0.7680, lr 4.0630934270713814e-05
Step 379000 Loss: 0.7364, lr 4.0630934270713814e-05
Step 379100 Loss: 0.7128, lr 4.0630934270713814e-05
Step 379200 Loss: 0.6945, lr 4.0630934270713814e-05
Step 379300 Loss: 0.6800, lr 4.0630934270713814e-05
Step 379400 Loss: 0.6681, lr 4.0630934270713814e-05
Step 379500 Loss: 0.6518, lr 4.0630934270713814e-05
Step 379600 Loss: 0.6365, lr 4.0630934270713814e-05
Step 379700 Loss: 0.6234, lr 4.0630934270713814e-05
Step 379800 Loss: 0.6121, lr 4.0630934270713814e-05
Step 379900 Loss: 0.6022, lr 4.0630934270713814e-05
Step 380000 Loss: 0.5935, lr 4.0630934270713814e-05
Step 380100 Loss: 0.5857, lr 4.0630934270713814e-05
Step 380200 Loss: 0.5751, lr 4.0630934270713814e-05
Step 380300 Loss: 0.5577, lr 4.0630934270713814e-05
Step 380400 Loss: 0.5419, lr 4.0630934270713814e-05
Step 380500 Loss: 0.5276, lr 4.0630934270713814e-05
Step 380600 Loss: 0.5146, lr 4.0630934270713814e-05
Step 380700 Loss: 0.5026, lr 4.0630934270713814e-05
Step 380800 Loss: 0.4916, lr 4.0630934270713814e-05
Step 380900 Loss: 0.4815, lr 4.0630934270713814e-05
Step 381000 Loss: 0.4721, lr 4.0630934270713814e-05
Step 381100 Loss: 0.4615, lr 4.0630934270713814e-05
Step 381200 Loss: 0.4487, lr 4.0630934270713814e-05
Step 381300 Loss: 0.4369, lr 4.0630934270713814e-05
Step 381400 Loss: 0.4257, lr 4.0630934270713814e-05
Step 381500 Loss: 0.4153, lr 4.0630934270713814e-05
Step 381600 Loss: 0.4056, lr 4.0630934270713814e-05
Step 381700 Loss: 0.3964, lr 4.0630934270713814e-05
Step 381800 Loss: 0.3877, lr 4.0630934270713814e-05
Step 381900 Loss: 0.3795, lr 4.0630934270713814e-05
Step 382000 Loss: 0.3718, lr 4.0630934270713814e-05
Step 382100 Loss: 0.3644, lr 4.0630934270713814e-05
Step 382200 Loss: 0.3575, lr 4.0630934270713814e-05
Step 382300 Loss: 0.3509, lr 4.0630934270713814e-05
Step 382400 Loss: 0.3446, lr 4.0630934270713814e-05
Step 382500 Loss: 0.3386, lr 4.0630934270713814e-05
Step 382600 Loss: 0.3321, lr 4.0630934270713814e-05
Step 382700 Loss: 0.3231, lr 4.0630934270713814e-05
Step 382800 Loss: 0.3145, lr 4.0630934270713814e-05
Step 382900 Loss: 0.3062, lr 4.0630934270713814e-05
Step 383000 Loss: 0.2984, lr 4.0630934270713814e-05
Step 383100 Loss: 0.2908, lr 4.0630934270713814e-05
Step 383200 Loss: 0.2836, lr 4.0630934270713814e-05
Step 383300 Loss: 0.2767, lr 4.0630934270713814e-05
Step 383400 Loss: 0.2700, lr 4.0630934270713814e-05
Step 383500 Loss: 0.2636, lr 4.0630934270713814e-05
Step 383600 Loss: 0.2552, lr 4.0630934270713814e-05
Step 383700 Loss: 0.2458, lr 4.0630934270713814e-05
Step 383800 Loss: 0.2368, lr 4.0630934270713814e-05
Step 383900 Loss: 0.2281, lr 4.0630934270713814e-05
Step 384000 Loss: 0.2197, lr 4.0630934270713814e-05
Step 384100 Loss: 0.2116, lr 4.0630934270713814e-05
Step 384200 Loss: 0.2038, lr 4.0630934270713814e-05
Step 384300 Loss: 0.1938, lr 4.0630934270713814e-05
Step 384400 Loss: 0.1828, lr 4.0630934270713814e-05
Step 384500 Loss: 0.1721, lr 4.0630934270713814e-05
Step 384600 Loss: 0.1618, lr 4.0630934270713814e-05
Step 384700 Loss: 0.1505, lr 4.0630934270713814e-05
Step 384800 Loss: 0.1362, lr 4.0630934270713814e-05
Step 384900 Loss: 0.1224, lr 4.0630934270713814e-05
Step 385000 Loss: 0.1090, lr 4.0630934270713814e-05
Train Epoch: [57/100] Loss: 0.0950,lr 0.000041
Calling G2SDataset.batch()
Done, time:  1.87 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.66 s, total batches: 6756
Step 385100 Loss: 1.1918, lr 3.909283793017293e-05
Step 385200 Loss: 0.9856, lr 3.909283793017293e-05
Step 385300 Loss: 0.8946, lr 3.909283793017293e-05
Step 385400 Loss: 0.8413, lr 3.909283793017293e-05
Step 385500 Loss: 0.8129, lr 3.909283793017293e-05
Step 385600 Loss: 0.7902, lr 3.909283793017293e-05
Step 385700 Loss: 0.7522, lr 3.909283793017293e-05
Step 385800 Loss: 0.7246, lr 3.909283793017293e-05
Step 385900 Loss: 0.7037, lr 3.909283793017293e-05
Step 386000 Loss: 0.6873, lr 3.909283793017293e-05
Step 386100 Loss: 0.6741, lr 3.909283793017293e-05
Step 386200 Loss: 0.6611, lr 3.909283793017293e-05
Step 386300 Loss: 0.6443, lr 3.909283793017293e-05
Step 386400 Loss: 0.6301, lr 3.909283793017293e-05
Step 386500 Loss: 0.6178, lr 3.909283793017293e-05
Step 386600 Loss: 0.6072, lr 3.909283793017293e-05
Step 386700 Loss: 0.5979, lr 3.909283793017293e-05
Step 386800 Loss: 0.5896, lr 3.909283793017293e-05
Step 386900 Loss: 0.5823, lr 3.909283793017293e-05
Step 387000 Loss: 0.5670, lr 3.909283793017293e-05
Step 387100 Loss: 0.5503, lr 3.909283793017293e-05
Step 387200 Loss: 0.5353, lr 3.909283793017293e-05
Step 387300 Loss: 0.5215, lr 3.909283793017293e-05
Step 387400 Loss: 0.5090, lr 3.909283793017293e-05
Step 387500 Loss: 0.4975, lr 3.909283793017293e-05
Step 387600 Loss: 0.4869, lr 3.909283793017293e-05
Step 387700 Loss: 0.4771, lr 3.909283793017293e-05
Step 387800 Loss: 0.4680, lr 3.909283793017293e-05
Step 387900 Loss: 0.4556, lr 3.909283793017293e-05
Step 388000 Loss: 0.4432, lr 3.909283793017293e-05
Step 388100 Loss: 0.4317, lr 3.909283793017293e-05
Step 388200 Loss: 0.4209, lr 3.909283793017293e-05
Step 388300 Loss: 0.4108, lr 3.909283793017293e-05
Step 388400 Loss: 0.4013, lr 3.909283793017293e-05
Step 388500 Loss: 0.3923, lr 3.909283793017293e-05
Step 388600 Loss: 0.3839, lr 3.909283793017293e-05
Step 388700 Loss: 0.3759, lr 3.909283793017293e-05
Step 388800 Loss: 0.3684, lr 3.909283793017293e-05
Step 388900 Loss: 0.3612, lr 3.909283793017293e-05
Step 389000 Loss: 0.3544, lr 3.909283793017293e-05
Step 389100 Loss: 0.3479, lr 3.909283793017293e-05
Step 389200 Loss: 0.3418, lr 3.909283793017293e-05
Step 389300 Loss: 0.3359, lr 3.909283793017293e-05
Step 389400 Loss: 0.3280, lr 3.909283793017293e-05
Step 389500 Loss: 0.3192, lr 3.909283793017293e-05
Step 389600 Loss: 0.3107, lr 3.909283793017293e-05
Step 389700 Loss: 0.3027, lr 3.909283793017293e-05
Step 389800 Loss: 0.2949, lr 3.909283793017293e-05
Step 389900 Loss: 0.2875, lr 3.909283793017293e-05
Step 390000 Loss: 0.2804, lr 3.909283793017293e-05
Step 390100 Loss: 0.2736, lr 3.909283793017293e-05
Step 390200 Loss: 0.2671, lr 3.909283793017293e-05
Step 390300 Loss: 0.2605, lr 3.909283793017293e-05
Step 390400 Loss: 0.2510, lr 3.909283793017293e-05
Step 390500 Loss: 0.2418, lr 3.909283793017293e-05
Step 390600 Loss: 0.2329, lr 3.909283793017293e-05
Step 390700 Loss: 0.2243, lr 3.909283793017293e-05
Step 390800 Loss: 0.2161, lr 3.909283793017293e-05
Step 390900 Loss: 0.2081, lr 3.909283793017293e-05
Step 391000 Loss: 0.2002, lr 3.909283793017293e-05
Step 391100 Loss: 0.1889, lr 3.909283793017293e-05
Step 391200 Loss: 0.1780, lr 3.909283793017293e-05
Step 391300 Loss: 0.1675, lr 3.909283793017293e-05
Step 391400 Loss: 0.1573, lr 3.909283793017293e-05
Step 391500 Loss: 0.1442, lr 3.909283793017293e-05
Step 391600 Loss: 0.1301, lr 3.909283793017293e-05
Step 391700 Loss: 0.1165, lr 3.909283793017293e-05
Step 391800 Loss: 0.1010, lr 3.909283793017293e-05
Train Epoch: [58/100] Loss: 0.0952,lr 0.000039
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6756
Step 391900 Loss: 1.0305, lr 3.75655056417573e-05
Step 392000 Loss: 0.9524, lr 3.75655056417573e-05
Step 392100 Loss: 0.8668, lr 3.75655056417573e-05
Step 392200 Loss: 0.8273, lr 3.75655056417573e-05
Step 392300 Loss: 0.8046, lr 3.75655056417573e-05
Step 392400 Loss: 0.7721, lr 3.75655056417573e-05
Step 392500 Loss: 0.7392, lr 3.75655056417573e-05
Step 392600 Loss: 0.7149, lr 3.75655056417573e-05
Step 392700 Loss: 0.6962, lr 3.75655056417573e-05
Step 392800 Loss: 0.6813, lr 3.75655056417573e-05
Step 392900 Loss: 0.6692, lr 3.75655056417573e-05
Step 393000 Loss: 0.6535, lr 3.75655056417573e-05
Step 393100 Loss: 0.6379, lr 3.75655056417573e-05
Step 393200 Loss: 0.6246, lr 3.75655056417573e-05
Step 393300 Loss: 0.6131, lr 3.75655056417573e-05
Step 393400 Loss: 0.6030, lr 3.75655056417573e-05
Step 393500 Loss: 0.5942, lr 3.75655056417573e-05
Step 393600 Loss: 0.5863, lr 3.75655056417573e-05
Step 393700 Loss: 0.5771, lr 3.75655056417573e-05
Step 393800 Loss: 0.5594, lr 3.75655056417573e-05
Step 393900 Loss: 0.5435, lr 3.75655056417573e-05
Step 394000 Loss: 0.5291, lr 3.75655056417573e-05
Step 394100 Loss: 0.5159, lr 3.75655056417573e-05
Step 394200 Loss: 0.5038, lr 3.75655056417573e-05
Step 394300 Loss: 0.4927, lr 3.75655056417573e-05
Step 394400 Loss: 0.4825, lr 3.75655056417573e-05
Step 394500 Loss: 0.4730, lr 3.75655056417573e-05
Step 394600 Loss: 0.4629, lr 3.75655056417573e-05
Step 394700 Loss: 0.4500, lr 3.75655056417573e-05
Step 394800 Loss: 0.4381, lr 3.75655056417573e-05
Step 394900 Loss: 0.4269, lr 3.75655056417573e-05
Step 395000 Loss: 0.4164, lr 3.75655056417573e-05
Step 395100 Loss: 0.4065, lr 3.75655056417573e-05
Step 395200 Loss: 0.3973, lr 3.75655056417573e-05
Step 395300 Loss: 0.3886, lr 3.75655056417573e-05
Step 395400 Loss: 0.3803, lr 3.75655056417573e-05
Step 395500 Loss: 0.3725, lr 3.75655056417573e-05
Step 395600 Loss: 0.3651, lr 3.75655056417573e-05
Step 395700 Loss: 0.3581, lr 3.75655056417573e-05
Step 395800 Loss: 0.3515, lr 3.75655056417573e-05
Step 395900 Loss: 0.3452, lr 3.75655056417573e-05
Step 396000 Loss: 0.3392, lr 3.75655056417573e-05
Step 396100 Loss: 0.3331, lr 3.75655056417573e-05
Step 396200 Loss: 0.3240, lr 3.75655056417573e-05
Step 396300 Loss: 0.3154, lr 3.75655056417573e-05
Step 396400 Loss: 0.3071, lr 3.75655056417573e-05
Step 396500 Loss: 0.2992, lr 3.75655056417573e-05
Step 396600 Loss: 0.2916, lr 3.75655056417573e-05
Step 396700 Loss: 0.2844, lr 3.75655056417573e-05
Step 396800 Loss: 0.2774, lr 3.75655056417573e-05
Step 396900 Loss: 0.2707, lr 3.75655056417573e-05
Step 397000 Loss: 0.2642, lr 3.75655056417573e-05
Step 397100 Loss: 0.2562, lr 3.75655056417573e-05
Step 397200 Loss: 0.2468, lr 3.75655056417573e-05
Step 397300 Loss: 0.2378, lr 3.75655056417573e-05
Step 397400 Loss: 0.2290, lr 3.75655056417573e-05
Step 397500 Loss: 0.2206, lr 3.75655056417573e-05
Step 397600 Loss: 0.2125, lr 3.75655056417573e-05
Step 397700 Loss: 0.2047, lr 3.75655056417573e-05
Step 397800 Loss: 0.1951, lr 3.75655056417573e-05
Step 397900 Loss: 0.1840, lr 3.75655056417573e-05
Step 398000 Loss: 0.1733, lr 3.75655056417573e-05
Step 398100 Loss: 0.1629, lr 3.75655056417573e-05
Step 398200 Loss: 0.1523, lr 3.75655056417573e-05
Step 398300 Loss: 0.1379, lr 3.75655056417573e-05
Step 398400 Loss: 0.1240, lr 3.75655056417573e-05
Step 398500 Loss: 0.1106, lr 3.75655056417573e-05
Train Epoch: [59/100] Loss: 0.0953,lr 0.000038
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.72 s, total batches: 6756
Step 398600 Loss: 1.2515, lr 3.605044469803859e-05
Step 398700 Loss: 0.9920, lr 3.605044469803859e-05
Step 398800 Loss: 0.9046, lr 3.605044469803859e-05
Step 398900 Loss: 0.8461, lr 3.605044469803859e-05
Step 399000 Loss: 0.8158, lr 3.605044469803859e-05
Step 399100 Loss: 0.7959, lr 3.605044469803859e-05
Step 399200 Loss: 0.7562, lr 3.605044469803859e-05
Step 399300 Loss: 0.7276, lr 3.605044469803859e-05
Step 399400 Loss: 0.7060, lr 3.605044469803859e-05
Step 399500 Loss: 0.6891, lr 3.605044469803859e-05
Step 399600 Loss: 0.6756, lr 3.605044469803859e-05
Step 399700 Loss: 0.6633, lr 3.605044469803859e-05
Step 399800 Loss: 0.6462, lr 3.605044469803859e-05
Step 399900 Loss: 0.6317, lr 3.605044469803859e-05
Step 400000 Loss: 0.6192, lr 3.605044469803859e-05
Step 400100 Loss: 0.6084, lr 3.605044469803859e-05
Step 400200 Loss: 0.5989, lr 3.605044469803859e-05
Step 400300 Loss: 0.5906, lr 3.605044469803859e-05
Step 400400 Loss: 0.5831, lr 3.605044469803859e-05
Step 400500 Loss: 0.5691, lr 3.605044469803859e-05
Step 400600 Loss: 0.5522, lr 3.605044469803859e-05
Step 400700 Loss: 0.5370, lr 3.605044469803859e-05
Step 400800 Loss: 0.5231, lr 3.605044469803859e-05
Step 400900 Loss: 0.5105, lr 3.605044469803859e-05
Step 401000 Loss: 0.4988, lr 3.605044469803859e-05
Step 401100 Loss: 0.4881, lr 3.605044469803859e-05
Step 401200 Loss: 0.4782, lr 3.605044469803859e-05
Step 401300 Loss: 0.4691, lr 3.605044469803859e-05
Step 401400 Loss: 0.4571, lr 3.605044469803859e-05
Step 401500 Loss: 0.4447, lr 3.605044469803859e-05
Step 401600 Loss: 0.4331, lr 3.605044469803859e-05
Step 401700 Loss: 0.4222, lr 3.605044469803859e-05
Step 401800 Loss: 0.4120, lr 3.605044469803859e-05
Step 401900 Loss: 0.4024, lr 3.605044469803859e-05
Step 402000 Loss: 0.3934, lr 3.605044469803859e-05
Step 402100 Loss: 0.3849, lr 3.605044469803859e-05
Step 402200 Loss: 0.3768, lr 3.605044469803859e-05
Step 402300 Loss: 0.3692, lr 3.605044469803859e-05
Step 402400 Loss: 0.3620, lr 3.605044469803859e-05
Step 402500 Loss: 0.3552, lr 3.605044469803859e-05
Step 402600 Loss: 0.3487, lr 3.605044469803859e-05
Step 402700 Loss: 0.3425, lr 3.605044469803859e-05
Step 402800 Loss: 0.3366, lr 3.605044469803859e-05
Step 402900 Loss: 0.3290, lr 3.605044469803859e-05
Step 403000 Loss: 0.3202, lr 3.605044469803859e-05
Step 403100 Loss: 0.3117, lr 3.605044469803859e-05
Step 403200 Loss: 0.3036, lr 3.605044469803859e-05
Step 403300 Loss: 0.2958, lr 3.605044469803859e-05
Step 403400 Loss: 0.2884, lr 3.605044469803859e-05
Step 403500 Loss: 0.2813, lr 3.605044469803859e-05
Step 403600 Loss: 0.2744, lr 3.605044469803859e-05
Step 403700 Loss: 0.2678, lr 3.605044469803859e-05
Step 403800 Loss: 0.2615, lr 3.605044469803859e-05
Step 403900 Loss: 0.2521, lr 3.605044469803859e-05
Step 404000 Loss: 0.2428, lr 3.605044469803859e-05
Step 404100 Loss: 0.2339, lr 3.605044469803859e-05
Step 404200 Loss: 0.2253, lr 3.605044469803859e-05
Step 404300 Loss: 0.2170, lr 3.605044469803859e-05
Step 404400 Loss: 0.2090, lr 3.605044469803859e-05
Step 404500 Loss: 0.2013, lr 3.605044469803859e-05
Step 404600 Loss: 0.1902, lr 3.605044469803859e-05
Step 404700 Loss: 0.1793, lr 3.605044469803859e-05
Step 404800 Loss: 0.1687, lr 3.605044469803859e-05
Step 404900 Loss: 0.1585, lr 3.605044469803859e-05
Step 405000 Loss: 0.1459, lr 3.605044469803859e-05
Step 405100 Loss: 0.1318, lr 3.605044469803859e-05
Step 405200 Loss: 0.1181, lr 3.605044469803859e-05
Step 405300 Loss: 0.1033, lr 3.605044469803859e-05
Train Epoch: [60/100] Loss: 0.0951,lr 0.000036
Model Saving at epoch 60
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.71 s, total batches: 6756
Step 405400 Loss: 1.0499, lr 3.454915028125269e-05
Step 405500 Loss: 0.9704, lr 3.454915028125269e-05
Step 405600 Loss: 0.8738, lr 3.454915028125269e-05
Step 405700 Loss: 0.8310, lr 3.454915028125269e-05
Step 405800 Loss: 0.8069, lr 3.454915028125269e-05
Step 405900 Loss: 0.7769, lr 3.454915028125269e-05
Step 406000 Loss: 0.7427, lr 3.454915028125269e-05
Step 406100 Loss: 0.7176, lr 3.454915028125269e-05
Step 406200 Loss: 0.6982, lr 3.454915028125269e-05
Step 406300 Loss: 0.6829, lr 3.454915028125269e-05
Step 406400 Loss: 0.6705, lr 3.454915028125269e-05
Step 406500 Loss: 0.6555, lr 3.454915028125269e-05
Step 406600 Loss: 0.6396, lr 3.454915028125269e-05
Step 406700 Loss: 0.6261, lr 3.454915028125269e-05
Step 406800 Loss: 0.6144, lr 3.454915028125269e-05
Step 406900 Loss: 0.6042, lr 3.454915028125269e-05
Step 407000 Loss: 0.5952, lr 3.454915028125269e-05
Step 407100 Loss: 0.5873, lr 3.454915028125269e-05
Step 407200 Loss: 0.5793, lr 3.454915028125269e-05
Step 407300 Loss: 0.5615, lr 3.454915028125269e-05
Step 407400 Loss: 0.5454, lr 3.454915028125269e-05
Step 407500 Loss: 0.5307, lr 3.454915028125269e-05
Step 407600 Loss: 0.5174, lr 3.454915028125269e-05
Step 407700 Loss: 0.5052, lr 3.454915028125269e-05
Step 407800 Loss: 0.4940, lr 3.454915028125269e-05
Step 407900 Loss: 0.4837, lr 3.454915028125269e-05
Step 408000 Loss: 0.4741, lr 3.454915028125269e-05
Step 408100 Loss: 0.4645, lr 3.454915028125269e-05
Step 408200 Loss: 0.4515, lr 3.454915028125269e-05
Step 408300 Loss: 0.4395, lr 3.454915028125269e-05
Step 408400 Loss: 0.4282, lr 3.454915028125269e-05
Step 408500 Loss: 0.4176, lr 3.454915028125269e-05
Step 408600 Loss: 0.4077, lr 3.454915028125269e-05
Step 408700 Loss: 0.3984, lr 3.454915028125269e-05
Step 408800 Loss: 0.3896, lr 3.454915028125269e-05
Step 408900 Loss: 0.3813, lr 3.454915028125269e-05
Step 409000 Loss: 0.3734, lr 3.454915028125269e-05
Step 409100 Loss: 0.3660, lr 3.454915028125269e-05
Step 409200 Loss: 0.3590, lr 3.454915028125269e-05
Step 409300 Loss: 0.3523, lr 3.454915028125269e-05
Step 409400 Loss: 0.3459, lr 3.454915028125269e-05
Step 409500 Loss: 0.3399, lr 3.454915028125269e-05
Step 409600 Loss: 0.3341, lr 3.454915028125269e-05
Step 409700 Loss: 0.3251, lr 3.454915028125269e-05
Step 409800 Loss: 0.3164, lr 3.454915028125269e-05
Step 409900 Loss: 0.3081, lr 3.454915028125269e-05
Step 410000 Loss: 0.3001, lr 3.454915028125269e-05
Step 410100 Loss: 0.2925, lr 3.454915028125269e-05
Step 410200 Loss: 0.2852, lr 3.454915028125269e-05
Step 410300 Loss: 0.2782, lr 3.454915028125269e-05
Step 410400 Loss: 0.2715, lr 3.454915028125269e-05
Step 410500 Loss: 0.2650, lr 3.454915028125269e-05
Step 410600 Loss: 0.2574, lr 3.454915028125269e-05
Step 410700 Loss: 0.2479, lr 3.454915028125269e-05
Step 410800 Loss: 0.2388, lr 3.454915028125269e-05
Step 410900 Loss: 0.2301, lr 3.454915028125269e-05
Step 411000 Loss: 0.2216, lr 3.454915028125269e-05
Step 411100 Loss: 0.2134, lr 3.454915028125269e-05
Step 411200 Loss: 0.2056, lr 3.454915028125269e-05
Step 411300 Loss: 0.1964, lr 3.454915028125269e-05
Step 411400 Loss: 0.1853, lr 3.454915028125269e-05
Step 411500 Loss: 0.1746, lr 3.454915028125269e-05
Step 411600 Loss: 0.1642, lr 3.454915028125269e-05
Step 411700 Loss: 0.1540, lr 3.454915028125269e-05
Step 411800 Loss: 0.1396, lr 3.454915028125269e-05
Step 411900 Loss: 0.1257, lr 3.454915028125269e-05
Step 412000 Loss: 0.1122, lr 3.454915028125269e-05
Train Epoch: [61/100] Loss: 0.0952,lr 0.000035
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.62 s, total batches: 6755
Step 412100 Loss: 1.2523, lr 3.3063103987735474e-05
Step 412200 Loss: 0.9986, lr 3.3063103987735474e-05
Step 412300 Loss: 0.9172, lr 3.3063103987735474e-05
Step 412400 Loss: 0.8522, lr 3.3063103987735474e-05
Step 412500 Loss: 0.8195, lr 3.3063103987735474e-05
Step 412600 Loss: 0.7997, lr 3.3063103987735474e-05
Step 412700 Loss: 0.7608, lr 3.3063103987735474e-05
Step 412800 Loss: 0.7311, lr 3.3063103987735474e-05
Step 412900 Loss: 0.7087, lr 3.3063103987735474e-05
Step 413000 Loss: 0.6913, lr 3.3063103987735474e-05
Step 413100 Loss: 0.6774, lr 3.3063103987735474e-05
Step 413200 Loss: 0.6659, lr 3.3063103987735474e-05
Step 413300 Loss: 0.6484, lr 3.3063103987735474e-05
Step 413400 Loss: 0.6336, lr 3.3063103987735474e-05
Step 413500 Loss: 0.6209, lr 3.3063103987735474e-05
Step 413600 Loss: 0.6099, lr 3.3063103987735474e-05
Step 413700 Loss: 0.6002, lr 3.3063103987735474e-05
Step 413800 Loss: 0.5917, lr 3.3063103987735474e-05
Step 413900 Loss: 0.5842, lr 3.3063103987735474e-05
Step 414000 Loss: 0.5713, lr 3.3063103987735474e-05
Step 414100 Loss: 0.5543, lr 3.3063103987735474e-05
Step 414200 Loss: 0.5388, lr 3.3063103987735474e-05
Step 414300 Loss: 0.5248, lr 3.3063103987735474e-05
Step 414400 Loss: 0.5120, lr 3.3063103987735474e-05
Step 414500 Loss: 0.5002, lr 3.3063103987735474e-05
Step 414600 Loss: 0.4894, lr 3.3063103987735474e-05
Step 414700 Loss: 0.4795, lr 3.3063103987735474e-05
Step 414800 Loss: 0.4702, lr 3.3063103987735474e-05
Step 414900 Loss: 0.4587, lr 3.3063103987735474e-05
Step 415000 Loss: 0.4462, lr 3.3063103987735474e-05
Step 415100 Loss: 0.4345, lr 3.3063103987735474e-05
Step 415200 Loss: 0.4235, lr 3.3063103987735474e-05
Step 415300 Loss: 0.4132, lr 3.3063103987735474e-05
Step 415400 Loss: 0.4036, lr 3.3063103987735474e-05
Step 415500 Loss: 0.3945, lr 3.3063103987735474e-05
Step 415600 Loss: 0.3859, lr 3.3063103987735474e-05
Step 415700 Loss: 0.3778, lr 3.3063103987735474e-05
Step 415800 Loss: 0.3702, lr 3.3063103987735474e-05
Step 415900 Loss: 0.3629, lr 3.3063103987735474e-05
Step 416000 Loss: 0.3560, lr 3.3063103987735474e-05
Step 416100 Loss: 0.3495, lr 3.3063103987735474e-05
Step 416200 Loss: 0.3433, lr 3.3063103987735474e-05
Step 416300 Loss: 0.3373, lr 3.3063103987735474e-05
Step 416400 Loss: 0.3301, lr 3.3063103987735474e-05
Step 416500 Loss: 0.3212, lr 3.3063103987735474e-05
Step 416600 Loss: 0.3127, lr 3.3063103987735474e-05
Step 416700 Loss: 0.3045, lr 3.3063103987735474e-05
Step 416800 Loss: 0.2967, lr 3.3063103987735474e-05
Step 416900 Loss: 0.2893, lr 3.3063103987735474e-05
Step 417000 Loss: 0.2821, lr 3.3063103987735474e-05
Step 417100 Loss: 0.2752, lr 3.3063103987735474e-05
Step 417200 Loss: 0.2686, lr 3.3063103987735474e-05
Step 417300 Loss: 0.2622, lr 3.3063103987735474e-05
Step 417400 Loss: 0.2532, lr 3.3063103987735474e-05
Step 417500 Loss: 0.2439, lr 3.3063103987735474e-05
Step 417600 Loss: 0.2349, lr 3.3063103987735474e-05
Step 417700 Loss: 0.2263, lr 3.3063103987735474e-05
Step 417800 Loss: 0.2180, lr 3.3063103987735474e-05
Step 417900 Loss: 0.2099, lr 3.3063103987735474e-05
Step 418000 Loss: 0.2022, lr 3.3063103987735474e-05
Step 418100 Loss: 0.1915, lr 3.3063103987735474e-05
Step 418200 Loss: 0.1805, lr 3.3063103987735474e-05
Step 418300 Loss: 0.1699, lr 3.3063103987735474e-05
Step 418400 Loss: 0.1597, lr 3.3063103987735474e-05
Step 418500 Loss: 0.1476, lr 3.3063103987735474e-05
Step 418600 Loss: 0.1334, lr 3.3063103987735474e-05
Step 418700 Loss: 0.1197, lr 3.3063103987735474e-05
Step 418800 Loss: 0.1055, lr 3.3063103987735474e-05
Train Epoch: [62/100] Loss: 0.0952,lr 0.000033
Calling G2SDataset.batch()
Done, time:  1.88 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.65 s, total batches: 6756
Step 418900 Loss: 1.0772, lr 3.159377236576616e-05
Step 419000 Loss: 0.9776, lr 3.159377236576616e-05
Step 419100 Loss: 0.8803, lr 3.159377236576616e-05
Step 419200 Loss: 0.8343, lr 3.159377236576616e-05
Step 419300 Loss: 0.8088, lr 3.159377236576616e-05
Step 419400 Loss: 0.7813, lr 3.159377236576616e-05
Step 419500 Loss: 0.7458, lr 3.159377236576616e-05
Step 419600 Loss: 0.7199, lr 3.159377236576616e-05
Step 419700 Loss: 0.7000, lr 3.159377236576616e-05
Step 419800 Loss: 0.6844, lr 3.159377236576616e-05
Step 419900 Loss: 0.6717, lr 3.159377236576616e-05
Step 420000 Loss: 0.6575, lr 3.159377236576616e-05
Step 420100 Loss: 0.6413, lr 3.159377236576616e-05
Step 420200 Loss: 0.6275, lr 3.159377236576616e-05
Step 420300 Loss: 0.6156, lr 3.159377236576616e-05
Step 420400 Loss: 0.6052, lr 3.159377236576616e-05
Step 420500 Loss: 0.5961, lr 3.159377236576616e-05
Step 420600 Loss: 0.5881, lr 3.159377236576616e-05
Step 420700 Loss: 0.5809, lr 3.159377236576616e-05
Step 420800 Loss: 0.5633, lr 3.159377236576616e-05
Step 420900 Loss: 0.5470, lr 3.159377236576616e-05
Step 421000 Loss: 0.5323, lr 3.159377236576616e-05
Step 421100 Loss: 0.5188, lr 3.159377236576616e-05
Step 421200 Loss: 0.5065, lr 3.159377236576616e-05
Step 421300 Loss: 0.4952, lr 3.159377236576616e-05
Step 421400 Loss: 0.4848, lr 3.159377236576616e-05
Step 421500 Loss: 0.4751, lr 3.159377236576616e-05
Step 421600 Loss: 0.4659, lr 3.159377236576616e-05
Step 421700 Loss: 0.4529, lr 3.159377236576616e-05
Step 421800 Loss: 0.4407, lr 3.159377236576616e-05
Step 421900 Loss: 0.4294, lr 3.159377236576616e-05
Step 422000 Loss: 0.4187, lr 3.159377236576616e-05
Step 422100 Loss: 0.4087, lr 3.159377236576616e-05
Step 422200 Loss: 0.3993, lr 3.159377236576616e-05
Step 422300 Loss: 0.3905, lr 3.159377236576616e-05
Step 422400 Loss: 0.3821, lr 3.159377236576616e-05
Step 422500 Loss: 0.3742, lr 3.159377236576616e-05
Step 422600 Loss: 0.3668, lr 3.159377236576616e-05
Step 422700 Loss: 0.3597, lr 3.159377236576616e-05
Step 422800 Loss: 0.3530, lr 3.159377236576616e-05
Step 422900 Loss: 0.3466, lr 3.159377236576616e-05
Step 423000 Loss: 0.3405, lr 3.159377236576616e-05
Step 423100 Loss: 0.3347, lr 3.159377236576616e-05
Step 423200 Loss: 0.3260, lr 3.159377236576616e-05
Step 423300 Loss: 0.3173, lr 3.159377236576616e-05
Step 423400 Loss: 0.3090, lr 3.159377236576616e-05
Step 423500 Loss: 0.3010, lr 3.159377236576616e-05
Step 423600 Loss: 0.2933, lr 3.159377236576616e-05
Step 423700 Loss: 0.2860, lr 3.159377236576616e-05
Step 423800 Loss: 0.2789, lr 3.159377236576616e-05
Step 423900 Loss: 0.2722, lr 3.159377236576616e-05
Step 424000 Loss: 0.2657, lr 3.159377236576616e-05
Step 424100 Loss: 0.2584, lr 3.159377236576616e-05
Step 424200 Loss: 0.2489, lr 3.159377236576616e-05
Step 424300 Loss: 0.2398, lr 3.159377236576616e-05
Step 424400 Loss: 0.2310, lr 3.159377236576616e-05
Step 424500 Loss: 0.2225, lr 3.159377236576616e-05
Step 424600 Loss: 0.2143, lr 3.159377236576616e-05
Step 424700 Loss: 0.2064, lr 3.159377236576616e-05
Step 424800 Loss: 0.1977, lr 3.159377236576616e-05
Step 424900 Loss: 0.1865, lr 3.159377236576616e-05
Step 425000 Loss: 0.1757, lr 3.159377236576616e-05
Step 425100 Loss: 0.1653, lr 3.159377236576616e-05
Step 425200 Loss: 0.1552, lr 3.159377236576616e-05
Step 425300 Loss: 0.1412, lr 3.159377236576616e-05
Step 425400 Loss: 0.1272, lr 3.159377236576616e-05
Step 425500 Loss: 0.1136, lr 3.159377236576616e-05
Step 425600 Loss: 0.0968, lr 3.159377236576616e-05
Train Epoch: [63/100] Loss: 0.0950,lr 0.000032
Calling G2SDataset.batch()
Done, time:  1.79 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.76 s, total batches: 6756
Step 425700 Loss: 1.0074, lr 3.0142605468261005e-05
Step 425800 Loss: 0.9266, lr 3.0142605468261005e-05
Step 425900 Loss: 0.8559, lr 3.0142605468261005e-05
Step 426000 Loss: 0.8213, lr 3.0142605468261005e-05
Step 426100 Loss: 0.8007, lr 3.0142605468261005e-05
Step 426200 Loss: 0.7641, lr 3.0142605468261005e-05
Step 426300 Loss: 0.7334, lr 3.0142605468261005e-05
Step 426400 Loss: 0.7104, lr 3.0142605468261005e-05
Step 426500 Loss: 0.6926, lr 3.0142605468261005e-05
Step 426600 Loss: 0.6784, lr 3.0142605468261005e-05
Step 426700 Loss: 0.6668, lr 3.0142605468261005e-05
Step 426800 Loss: 0.6498, lr 3.0142605468261005e-05
Step 426900 Loss: 0.6348, lr 3.0142605468261005e-05
Step 427000 Loss: 0.6219, lr 3.0142605468261005e-05
Step 427100 Loss: 0.6107, lr 3.0142605468261005e-05
Step 427200 Loss: 0.6010, lr 3.0142605468261005e-05
Step 427300 Loss: 0.5924, lr 3.0142605468261005e-05
Step 427400 Loss: 0.5847, lr 3.0142605468261005e-05
Step 427500 Loss: 0.5732, lr 3.0142605468261005e-05
Step 427600 Loss: 0.5559, lr 3.0142605468261005e-05
Step 427700 Loss: 0.5403, lr 3.0142605468261005e-05
Step 427800 Loss: 0.5261, lr 3.0142605468261005e-05
Step 427900 Loss: 0.5132, lr 3.0142605468261005e-05
Step 428000 Loss: 0.5014, lr 3.0142605468261005e-05
Step 428100 Loss: 0.4905, lr 3.0142605468261005e-05
Step 428200 Loss: 0.4804, lr 3.0142605468261005e-05
Step 428300 Loss: 0.4711, lr 3.0142605468261005e-05
Step 428400 Loss: 0.4601, lr 3.0142605468261005e-05
Step 428500 Loss: 0.4474, lr 3.0142605468261005e-05
Step 428600 Loss: 0.4356, lr 3.0142605468261005e-05
Step 428700 Loss: 0.4246, lr 3.0142605468261005e-05
Step 428800 Loss: 0.4142, lr 3.0142605468261005e-05
Step 428900 Loss: 0.4045, lr 3.0142605468261005e-05
Step 429000 Loss: 0.3953, lr 3.0142605468261005e-05
Step 429100 Loss: 0.3867, lr 3.0142605468261005e-05
Step 429200 Loss: 0.3786, lr 3.0142605468261005e-05
Step 429300 Loss: 0.3709, lr 3.0142605468261005e-05
Step 429400 Loss: 0.3636, lr 3.0142605468261005e-05
Step 429500 Loss: 0.3567, lr 3.0142605468261005e-05
Step 429600 Loss: 0.3501, lr 3.0142605468261005e-05
Step 429700 Loss: 0.3438, lr 3.0142605468261005e-05
Step 429800 Loss: 0.3379, lr 3.0142605468261005e-05
Step 429900 Loss: 0.3311, lr 3.0142605468261005e-05
Step 430000 Loss: 0.3221, lr 3.0142605468261005e-05
Step 430100 Loss: 0.3136, lr 3.0142605468261005e-05
Step 430200 Loss: 0.3054, lr 3.0142605468261005e-05
Step 430300 Loss: 0.2975, lr 3.0142605468261005e-05
Step 430400 Loss: 0.2900, lr 3.0142605468261005e-05
Step 430500 Loss: 0.2828, lr 3.0142605468261005e-05
Step 430600 Loss: 0.2759, lr 3.0142605468261005e-05
Step 430700 Loss: 0.2692, lr 3.0142605468261005e-05
Step 430800 Loss: 0.2629, lr 3.0142605468261005e-05
Step 430900 Loss: 0.2541, lr 3.0142605468261005e-05
Step 431000 Loss: 0.2448, lr 3.0142605468261005e-05
Step 431100 Loss: 0.2358, lr 3.0142605468261005e-05
Step 431200 Loss: 0.2272, lr 3.0142605468261005e-05
Step 431300 Loss: 0.2188, lr 3.0142605468261005e-05
Step 431400 Loss: 0.2108, lr 3.0142605468261005e-05
Step 431500 Loss: 0.2030, lr 3.0142605468261005e-05
Step 431600 Loss: 0.1927, lr 3.0142605468261005e-05
Step 431700 Loss: 0.1817, lr 3.0142605468261005e-05
Step 431800 Loss: 0.1710, lr 3.0142605468261005e-05
Step 431900 Loss: 0.1607, lr 3.0142605468261005e-05
Step 432000 Loss: 0.1491, lr 3.0142605468261005e-05
Step 432100 Loss: 0.1349, lr 3.0142605468261005e-05
Step 432200 Loss: 0.1211, lr 3.0142605468261005e-05
Step 432300 Loss: 0.1077, lr 3.0142605468261005e-05
Train Epoch: [64/100] Loss: 0.0952,lr 0.000030
Calling G2SDataset.batch()
Done, time:  2.15 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  2.17 s, total batches: 6755
Step 432400 Loss: 1.1263, lr 2.8711035421746404e-05
Step 432500 Loss: 0.9818, lr 2.8711035421746404e-05
Step 432600 Loss: 0.8884, lr 2.8711035421746404e-05
Step 432700 Loss: 0.8384, lr 2.8711035421746404e-05
Step 432800 Loss: 0.8113, lr 2.8711035421746404e-05
Step 432900 Loss: 0.7864, lr 2.8711035421746404e-05
Step 433000 Loss: 0.7495, lr 2.8711035421746404e-05
Step 433100 Loss: 0.7226, lr 2.8711035421746404e-05
Step 433200 Loss: 0.7021, lr 2.8711035421746404e-05
Step 433300 Loss: 0.6860, lr 2.8711035421746404e-05
Step 433400 Loss: 0.6731, lr 2.8711035421746404e-05
Step 433500 Loss: 0.6595, lr 2.8711035421746404e-05
Step 433600 Loss: 0.6430, lr 2.8711035421746404e-05
Step 433700 Loss: 0.6289, lr 2.8711035421746404e-05
Step 433800 Loss: 0.6168, lr 2.8711035421746404e-05
Step 433900 Loss: 0.6063, lr 2.8711035421746404e-05
Step 434000 Loss: 0.5971, lr 2.8711035421746404e-05
Step 434100 Loss: 0.5889, lr 2.8711035421746404e-05
Step 434200 Loss: 0.5816, lr 2.8711035421746404e-05
Step 434300 Loss: 0.5653, lr 2.8711035421746404e-05
Step 434400 Loss: 0.5488, lr 2.8711035421746404e-05
Step 434500 Loss: 0.5339, lr 2.8711035421746404e-05
Step 434600 Loss: 0.5203, lr 2.8711035421746404e-05
Step 434700 Loss: 0.5078, lr 2.8711035421746404e-05
Step 434800 Loss: 0.4964, lr 2.8711035421746404e-05
Step 434900 Loss: 0.4859, lr 2.8711035421746404e-05
Step 435000 Loss: 0.4762, lr 2.8711035421746404e-05
Step 435100 Loss: 0.4672, lr 2.8711035421746404e-05
Step 435200 Loss: 0.4544, lr 2.8711035421746404e-05
Step 435300 Loss: 0.4421, lr 2.8711035421746404e-05
Step 435400 Loss: 0.4306, lr 2.8711035421746404e-05
Step 435500 Loss: 0.4199, lr 2.8711035421746404e-05
Step 435600 Loss: 0.4098, lr 2.8711035421746404e-05
Step 435700 Loss: 0.4004, lr 2.8711035421746404e-05
Step 435800 Loss: 0.3915, lr 2.8711035421746404e-05
Step 435900 Loss: 0.3831, lr 2.8711035421746404e-05
Step 436000 Loss: 0.3751, lr 2.8711035421746404e-05
Step 436100 Loss: 0.3676, lr 2.8711035421746404e-05
Step 436200 Loss: 0.3605, lr 2.8711035421746404e-05
Step 436300 Loss: 0.3537, lr 2.8711035421746404e-05
Step 436400 Loss: 0.3473, lr 2.8711035421746404e-05
Step 436500 Loss: 0.3412, lr 2.8711035421746404e-05
Step 436600 Loss: 0.3353, lr 2.8711035421746404e-05
Step 436700 Loss: 0.3271, lr 2.8711035421746404e-05
Step 436800 Loss: 0.3183, lr 2.8711035421746404e-05
Step 436900 Loss: 0.3099, lr 2.8711035421746404e-05
Step 437000 Loss: 0.3019, lr 2.8711035421746404e-05
Step 437100 Loss: 0.2942, lr 2.8711035421746404e-05
Step 437200 Loss: 0.2868, lr 2.8711035421746404e-05
Step 437300 Loss: 0.2797, lr 2.8711035421746404e-05
Step 437400 Loss: 0.2729, lr 2.8711035421746404e-05
Step 437500 Loss: 0.2664, lr 2.8711035421746404e-05
Step 437600 Loss: 0.2595, lr 2.8711035421746404e-05
Step 437700 Loss: 0.2500, lr 2.8711035421746404e-05
Step 437800 Loss: 0.2408, lr 2.8711035421746404e-05
Step 437900 Loss: 0.2320, lr 2.8711035421746404e-05
Step 438000 Loss: 0.2235, lr 2.8711035421746404e-05
Step 438100 Loss: 0.2153, lr 2.8711035421746404e-05
Step 438200 Loss: 0.2073, lr 2.8711035421746404e-05
Step 438300 Loss: 0.1990, lr 2.8711035421746404e-05
Step 438400 Loss: 0.1878, lr 2.8711035421746404e-05
Step 438500 Loss: 0.1770, lr 2.8711035421746404e-05
Step 438600 Loss: 0.1665, lr 2.8711035421746404e-05
Step 438700 Loss: 0.1563, lr 2.8711035421746404e-05
Step 438800 Loss: 0.1428, lr 2.8711035421746404e-05
Step 438900 Loss: 0.1288, lr 2.8711035421746404e-05
Step 439000 Loss: 0.1152, lr 2.8711035421746404e-05
Step 439100 Loss: 0.0992, lr 2.8711035421746404e-05
Train Epoch: [65/100] Loss: 0.0955,lr 0.000029
Model Saving at epoch 65
Calling G2SDataset.batch()
Done, time:  1.89 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.97 s, total batches: 6756
Step 439200 Loss: 1.0172, lr 2.73004750130227e-05
Step 439300 Loss: 0.9399, lr 2.73004750130227e-05
Step 439400 Loss: 0.8617, lr 2.73004750130227e-05
Step 439500 Loss: 0.8246, lr 2.73004750130227e-05
Step 439600 Loss: 0.8029, lr 2.73004750130227e-05
Step 439700 Loss: 0.7683, lr 2.73004750130227e-05
Step 439800 Loss: 0.7365, lr 2.73004750130227e-05
Step 439900 Loss: 0.7128, lr 2.73004750130227e-05
Step 440000 Loss: 0.6945, lr 2.73004750130227e-05
Step 440100 Loss: 0.6799, lr 2.73004750130227e-05
Step 440200 Loss: 0.6681, lr 2.73004750130227e-05
Step 440300 Loss: 0.6518, lr 2.73004750130227e-05
Step 440400 Loss: 0.6364, lr 2.73004750130227e-05
Step 440500 Loss: 0.6233, lr 2.73004750130227e-05
Step 440600 Loss: 0.6120, lr 2.73004750130227e-05
Step 440700 Loss: 0.6021, lr 2.73004750130227e-05
Step 440800 Loss: 0.5933, lr 2.73004750130227e-05
Step 440900 Loss: 0.5856, lr 2.73004750130227e-05
Step 441000 Loss: 0.5752, lr 2.73004750130227e-05
Step 441100 Loss: 0.5577, lr 2.73004750130227e-05
Step 441200 Loss: 0.5420, lr 2.73004750130227e-05
Step 441300 Loss: 0.5276, lr 2.73004750130227e-05
Step 441400 Loss: 0.5146, lr 2.73004750130227e-05
Step 441500 Loss: 0.5026, lr 2.73004750130227e-05
Step 441600 Loss: 0.4916, lr 2.73004750130227e-05
Step 441700 Loss: 0.4814, lr 2.73004750130227e-05
Step 441800 Loss: 0.4721, lr 2.73004750130227e-05
Step 441900 Loss: 0.4615, lr 2.73004750130227e-05
Step 442000 Loss: 0.4487, lr 2.73004750130227e-05
Step 442100 Loss: 0.4368, lr 2.73004750130227e-05
Step 442200 Loss: 0.4257, lr 2.73004750130227e-05
Step 442300 Loss: 0.4153, lr 2.73004750130227e-05
Step 442400 Loss: 0.4055, lr 2.73004750130227e-05
Step 442500 Loss: 0.3963, lr 2.73004750130227e-05
Step 442600 Loss: 0.3876, lr 2.73004750130227e-05
Step 442700 Loss: 0.3794, lr 2.73004750130227e-05
Step 442800 Loss: 0.3717, lr 2.73004750130227e-05
Step 442900 Loss: 0.3644, lr 2.73004750130227e-05
Step 443000 Loss: 0.3574, lr 2.73004750130227e-05
Step 443100 Loss: 0.3508, lr 2.73004750130227e-05
Step 443200 Loss: 0.3445, lr 2.73004750130227e-05
Step 443300 Loss: 0.3385, lr 2.73004750130227e-05
Step 443400 Loss: 0.3321, lr 2.73004750130227e-05
Step 443500 Loss: 0.3231, lr 2.73004750130227e-05
Step 443600 Loss: 0.3145, lr 2.73004750130227e-05
Step 443700 Loss: 0.3062, lr 2.73004750130227e-05
Step 443800 Loss: 0.2984, lr 2.73004750130227e-05
Step 443900 Loss: 0.2908, lr 2.73004750130227e-05
Step 444000 Loss: 0.2836, lr 2.73004750130227e-05
Step 444100 Loss: 0.2766, lr 2.73004750130227e-05
Step 444200 Loss: 0.2699, lr 2.73004750130227e-05
Step 444300 Loss: 0.2635, lr 2.73004750130227e-05
Step 444400 Loss: 0.2552, lr 2.73004750130227e-05
Step 444500 Loss: 0.2458, lr 2.73004750130227e-05
Step 444600 Loss: 0.2368, lr 2.73004750130227e-05
Step 444700 Loss: 0.2281, lr 2.73004750130227e-05
Step 444800 Loss: 0.2197, lr 2.73004750130227e-05
Step 444900 Loss: 0.2116, lr 2.73004750130227e-05
Step 445000 Loss: 0.2038, lr 2.73004750130227e-05
Step 445100 Loss: 0.1939, lr 2.73004750130227e-05
Step 445200 Loss: 0.1829, lr 2.73004750130227e-05
Step 445300 Loss: 0.1722, lr 2.73004750130227e-05
Step 445400 Loss: 0.1618, lr 2.73004750130227e-05
Step 445500 Loss: 0.1507, lr 2.73004750130227e-05
Step 445600 Loss: 0.1364, lr 2.73004750130227e-05
Step 445700 Loss: 0.1226, lr 2.73004750130227e-05
Step 445800 Loss: 0.1092, lr 2.73004750130227e-05
Train Epoch: [66/100] Loss: 0.0950,lr 0.000027
Calling G2SDataset.batch()
Done, time:  2.01 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.64 s, total batches: 6755
Step 445900 Loss: 1.2152, lr 2.5912316294914263e-05
Step 446000 Loss: 0.9869, lr 2.5912316294914263e-05
Step 446100 Loss: 0.8963, lr 2.5912316294914263e-05
Step 446200 Loss: 0.8421, lr 2.5912316294914263e-05
Step 446300 Loss: 0.8134, lr 2.5912316294914263e-05
Step 446400 Loss: 0.7912, lr 2.5912316294914263e-05
Step 446500 Loss: 0.7528, lr 2.5912316294914263e-05
Step 446600 Loss: 0.7251, lr 2.5912316294914263e-05
Step 446700 Loss: 0.7040, lr 2.5912316294914263e-05
Step 446800 Loss: 0.6875, lr 2.5912316294914263e-05
Step 446900 Loss: 0.6743, lr 2.5912316294914263e-05
Step 447000 Loss: 0.6615, lr 2.5912316294914263e-05
Step 447100 Loss: 0.6446, lr 2.5912316294914263e-05
Step 447200 Loss: 0.6304, lr 2.5912316294914263e-05
Step 447300 Loss: 0.6181, lr 2.5912316294914263e-05
Step 447400 Loss: 0.6074, lr 2.5912316294914263e-05
Step 447500 Loss: 0.5980, lr 2.5912316294914263e-05
Step 447600 Loss: 0.5898, lr 2.5912316294914263e-05
Step 447700 Loss: 0.5824, lr 2.5912316294914263e-05
Step 447800 Loss: 0.5672, lr 2.5912316294914263e-05
Step 447900 Loss: 0.5506, lr 2.5912316294914263e-05
Step 448000 Loss: 0.5355, lr 2.5912316294914263e-05
Step 448100 Loss: 0.5217, lr 2.5912316294914263e-05
Step 448200 Loss: 0.5091, lr 2.5912316294914263e-05
Step 448300 Loss: 0.4976, lr 2.5912316294914263e-05
Step 448400 Loss: 0.4870, lr 2.5912316294914263e-05
Step 448500 Loss: 0.4772, lr 2.5912316294914263e-05
Step 448600 Loss: 0.4681, lr 2.5912316294914263e-05
Step 448700 Loss: 0.4557, lr 2.5912316294914263e-05
Step 448800 Loss: 0.4434, lr 2.5912316294914263e-05
Step 448900 Loss: 0.4318, lr 2.5912316294914263e-05
Step 449000 Loss: 0.4210, lr 2.5912316294914263e-05
Step 449100 Loss: 0.4109, lr 2.5912316294914263e-05
Step 449200 Loss: 0.4014, lr 2.5912316294914263e-05
Step 449300 Loss: 0.3924, lr 2.5912316294914263e-05
Step 449400 Loss: 0.3839, lr 2.5912316294914263e-05
Step 449500 Loss: 0.3759, lr 2.5912316294914263e-05
Step 449600 Loss: 0.3684, lr 2.5912316294914263e-05
Step 449700 Loss: 0.3612, lr 2.5912316294914263e-05
Step 449800 Loss: 0.3544, lr 2.5912316294914263e-05
Step 449900 Loss: 0.3480, lr 2.5912316294914263e-05
Step 450000 Loss: 0.3418, lr 2.5912316294914263e-05
Step 450100 Loss: 0.3359, lr 2.5912316294914263e-05
Step 450200 Loss: 0.3280, lr 2.5912316294914263e-05
Step 450300 Loss: 0.3192, lr 2.5912316294914263e-05
Step 450400 Loss: 0.3108, lr 2.5912316294914263e-05
Step 450500 Loss: 0.3027, lr 2.5912316294914263e-05
Step 450600 Loss: 0.2950, lr 2.5912316294914263e-05
Step 450700 Loss: 0.2876, lr 2.5912316294914263e-05
Step 450800 Loss: 0.2805, lr 2.5912316294914263e-05
Step 450900 Loss: 0.2736, lr 2.5912316294914263e-05
Step 451000 Loss: 0.2671, lr 2.5912316294914263e-05
Step 451100 Loss: 0.2606, lr 2.5912316294914263e-05
Step 451200 Loss: 0.2510, lr 2.5912316294914263e-05
Step 451300 Loss: 0.2418, lr 2.5912316294914263e-05
Step 451400 Loss: 0.2329, lr 2.5912316294914263e-05
Step 451500 Loss: 0.2244, lr 2.5912316294914263e-05
Step 451600 Loss: 0.2161, lr 2.5912316294914263e-05
Step 451700 Loss: 0.2081, lr 2.5912316294914263e-05
Step 451800 Loss: 0.2002, lr 2.5912316294914263e-05
Step 451900 Loss: 0.1891, lr 2.5912316294914263e-05
Step 452000 Loss: 0.1782, lr 2.5912316294914263e-05
Step 452100 Loss: 0.1677, lr 2.5912316294914263e-05
Step 452200 Loss: 0.1575, lr 2.5912316294914263e-05
Step 452300 Loss: 0.1444, lr 2.5912316294914263e-05
Step 452400 Loss: 0.1304, lr 2.5912316294914263e-05
Step 452500 Loss: 0.1167, lr 2.5912316294914263e-05
Step 452600 Loss: 0.1012, lr 2.5912316294914263e-05
Train Epoch: [67/100] Loss: 0.0954,lr 0.000026
Calling G2SDataset.batch()
Done, time:  2.05 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 452700 Loss: 1.0309, lr 2.4547929212481466e-05
Step 452800 Loss: 0.9537, lr 2.4547929212481466e-05
Step 452900 Loss: 0.8673, lr 2.4547929212481466e-05
Step 453000 Loss: 0.8275, lr 2.4547929212481466e-05
Step 453100 Loss: 0.8046, lr 2.4547929212481466e-05
Step 453200 Loss: 0.7724, lr 2.4547929212481466e-05
Step 453300 Loss: 0.7394, lr 2.4547929212481466e-05
Step 453400 Loss: 0.7150, lr 2.4547929212481466e-05
Step 453500 Loss: 0.6962, lr 2.4547929212481466e-05
Step 453600 Loss: 0.6813, lr 2.4547929212481466e-05
Step 453700 Loss: 0.6692, lr 2.4547929212481466e-05
Step 453800 Loss: 0.6535, lr 2.4547929212481466e-05
Step 453900 Loss: 0.6379, lr 2.4547929212481466e-05
Step 454000 Loss: 0.6246, lr 2.4547929212481466e-05
Step 454100 Loss: 0.6131, lr 2.4547929212481466e-05
Step 454200 Loss: 0.6030, lr 2.4547929212481466e-05
Step 454300 Loss: 0.5942, lr 2.4547929212481466e-05
Step 454400 Loss: 0.5863, lr 2.4547929212481466e-05
Step 454500 Loss: 0.5771, lr 2.4547929212481466e-05
Step 454600 Loss: 0.5595, lr 2.4547929212481466e-05
Step 454700 Loss: 0.5436, lr 2.4547929212481466e-05
Step 454800 Loss: 0.5291, lr 2.4547929212481466e-05
Step 454900 Loss: 0.5159, lr 2.4547929212481466e-05
Step 455000 Loss: 0.5038, lr 2.4547929212481466e-05
Step 455100 Loss: 0.4927, lr 2.4547929212481466e-05
Step 455200 Loss: 0.4825, lr 2.4547929212481466e-05
Step 455300 Loss: 0.4730, lr 2.4547929212481466e-05
Step 455400 Loss: 0.4629, lr 2.4547929212481466e-05
Step 455500 Loss: 0.4501, lr 2.4547929212481466e-05
Step 455600 Loss: 0.4381, lr 2.4547929212481466e-05
Step 455700 Loss: 0.4269, lr 2.4547929212481466e-05
Step 455800 Loss: 0.4164, lr 2.4547929212481466e-05
Step 455900 Loss: 0.4065, lr 2.4547929212481466e-05
Step 456000 Loss: 0.3973, lr 2.4547929212481466e-05
Step 456100 Loss: 0.3885, lr 2.4547929212481466e-05
Step 456200 Loss: 0.3803, lr 2.4547929212481466e-05
Step 456300 Loss: 0.3725, lr 2.4547929212481466e-05
Step 456400 Loss: 0.3651, lr 2.4547929212481466e-05
Step 456500 Loss: 0.3581, lr 2.4547929212481466e-05
Step 456600 Loss: 0.3515, lr 2.4547929212481466e-05
Step 456700 Loss: 0.3451, lr 2.4547929212481466e-05
Step 456800 Loss: 0.3391, lr 2.4547929212481466e-05
Step 456900 Loss: 0.3330, lr 2.4547929212481466e-05
Step 457000 Loss: 0.3240, lr 2.4547929212481466e-05
Step 457100 Loss: 0.3153, lr 2.4547929212481466e-05
Step 457200 Loss: 0.3071, lr 2.4547929212481466e-05
Step 457300 Loss: 0.2992, lr 2.4547929212481466e-05
Step 457400 Loss: 0.2916, lr 2.4547929212481466e-05
Step 457500 Loss: 0.2843, lr 2.4547929212481466e-05
Step 457600 Loss: 0.2773, lr 2.4547929212481466e-05
Step 457700 Loss: 0.2706, lr 2.4547929212481466e-05
Step 457800 Loss: 0.2642, lr 2.4547929212481466e-05
Step 457900 Loss: 0.2562, lr 2.4547929212481466e-05
Step 458000 Loss: 0.2468, lr 2.4547929212481466e-05
Step 458100 Loss: 0.2377, lr 2.4547929212481466e-05
Step 458200 Loss: 0.2290, lr 2.4547929212481466e-05
Step 458300 Loss: 0.2206, lr 2.4547929212481466e-05
Step 458400 Loss: 0.2125, lr 2.4547929212481466e-05
Step 458500 Loss: 0.2046, lr 2.4547929212481466e-05
Step 458600 Loss: 0.1951, lr 2.4547929212481466e-05
Step 458700 Loss: 0.1840, lr 2.4547929212481466e-05
Step 458800 Loss: 0.1733, lr 2.4547929212481466e-05
Step 458900 Loss: 0.1629, lr 2.4547929212481466e-05
Step 459000 Loss: 0.1523, lr 2.4547929212481466e-05
Step 459100 Loss: 0.1379, lr 2.4547929212481466e-05
Step 459200 Loss: 0.1240, lr 2.4547929212481466e-05
Step 459300 Loss: 0.1106, lr 2.4547929212481466e-05
Train Epoch: [68/100] Loss: 0.0950,lr 0.000025
Calling G2SDataset.batch()
Done, time:  1.85 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Step 459400 Loss: 1.2514, lr 2.320866025105019e-05
Step 459500 Loss: 0.9918, lr 2.320866025105019e-05
Step 459600 Loss: 0.9054, lr 2.320866025105019e-05
Step 459700 Loss: 0.8464, lr 2.320866025105019e-05
Step 459800 Loss: 0.8159, lr 2.320866025105019e-05
Step 459900 Loss: 0.7963, lr 2.320866025105019e-05
Step 460000 Loss: 0.7564, lr 2.320866025105019e-05
Step 460100 Loss: 0.7277, lr 2.320866025105019e-05
Step 460200 Loss: 0.7061, lr 2.320866025105019e-05
