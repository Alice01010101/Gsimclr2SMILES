Logging arguments
**** model = *g2s_series_rel*
**** data_name = *USPTO_480k*
**** task = *reaction_prediction*
**** representation_end = *smiles*
**** seed = *42*
**** max_src_len = *512*
**** max_tgt_len = *512*
**** num_workers = *0*
**** verbose = *False*
**** log_file = *USPTO_480k_g2s_series_rel_smiles_smiles.train.1.log*
**** vocab_file = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt*
**** preprocess_output_path = **
**** save_dir = *./checkpoints/USPTO_480k_g2s_series_rel_smiles_smiles.1*
**** train_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz*
**** valid_bin = *./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/val_0.npz*
**** load_from = **
**** embed_size = *1024*
**** share_embeddings = *False*
**** mpn_type = *dgat*
**** encoder_num_layers = *4*
**** encoder_hidden_size = *1024*
**** encoder_attn_heads = *8*
**** encoder_filter_size = *2048*
**** encoder_norm = **
**** encoder_skip_connection = **
**** encoder_positional_encoding = *none*
**** encoder_emb_scale = *sqrt*
**** compute_graph_distance = *True*
**** attn_enc_num_layers = *6*
**** attn_enc_hidden_size = *1024*
**** attn_enc_heads = *8*
**** attn_enc_filter_size = *2048*
**** rel_pos = *emb_only*
**** rel_pos_buckets = *11*
**** decoder_num_layers = *6*
**** decoder_hidden_size = *1024*
**** decoder_attn_heads = *8*
**** decoder_filter_size = *2048*
**** dropout = *0.3*
**** attn_dropout = *0.3*
**** max_relative_positions = *4*
**** enable_amp = *True*
**** epoch = *200*
**** max_steps = *300000*
**** warmup_steps = *100*
**** lr = *4.0*
**** beta1 = *0.9*
**** beta2 = *0.998*
**** eps = *1e-09*
**** weight_decay = *0.0*
**** clip_norm = *20.0*
**** batch_type = *tokens*
**** train_batch_size = *4096*
**** valid_batch_size = *4096*
**** accumulation_count = *4*
**** log_iter = *100*
**** eval_iter = *2000*
**** save_iter = *5000*
**** margin = *4.0*
**** do_profile = *False*
**** record_shapes = *False*
**** do_predict = *False*
**** do_score = *False*
**** checkpoint_step_start = *None*
**** checkpoint_step_end = *None*
**** predict_batch_size = *4096*
**** test_bin = **
**** result_file = **
**** beam_size = *5*
**** n_best = *10*
**** temperature = *1.0*
**** predict_min_len = *1*
**** predict_max_len = *512*
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Gsimclr(
  (encoder): GraphFeatEncoder(
    (mpn): DGATEncoder(
      (leaky_relu): LeakyReLU(negative_slope=0.01)
      (W_o): Sequential(
        (0): Linear(in_features=1129, out_features=1024, bias=True)
        (1): GELU()
      )
      (rnn): DGATGRU(
        (W_z): Linear(in_features=1138, out_features=1024, bias=True)
        (W_r): Linear(in_features=114, out_features=1024, bias=False)
        (U_r): Linear(in_features=1024, out_features=1024, bias=True)
        (W_h): Linear(in_features=1138, out_features=1024, bias=True)
        (leaky_relu): LeakyReLU(negative_slope=0.01)
        (attn_W_q): Linear(in_features=114, out_features=1024, bias=True)
        (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
        (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
        (softmax): Softmax(dim=1)
        (dropout): Dropout(p=0.3, inplace=False)
        (attn_dropout): Dropout(p=0.3, inplace=False)
      )
      (attn_W_q): Linear(in_features=105, out_features=1024, bias=True)
      (attn_W_k): Linear(in_features=1024, out_features=1024, bias=True)
      (attn_W_v): Linear(in_features=1024, out_features=1024, bias=True)
      (softmax): Softmax(dim=1)
      (dropout): Dropout(p=0.3, inplace=False)
      (attn_dropout): Dropout(p=0.3, inplace=False)
    )
  )
  (g): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=False)
    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=512, out_features=256, bias=False)
    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=256, out_features=128, bias=True)
  )
)
Number of parameters = 9774736
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Loading vocab from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/vocab_smiles.txt
Loading preprocessed features from ./my_preprocessed/USPTO_480k_g2s_series_rel_smiles_smiles/train_0.npz
Loaded and initialized G2SDataset, size: 409035
Start training
Calling G2SDataset.batch()
Done, time:  1.94 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6756
Step 100 Loss: 4.7617, lr 0.000125
Step 200 Loss: 4.5975, lr 0.000125
Step 300 Loss: 4.4854, lr 0.000125
Step 400 Loss: 4.4204, lr 0.000125
Step 500 Loss: 4.3774, lr 0.000125
Step 600 Loss: 4.3207, lr 0.000125
Step 700 Loss: 4.2770, lr 0.000125
Step 800 Loss: 4.2423, lr 0.000125
Step 900 Loss: 4.2151, lr 0.000125
Step 1000 Loss: 4.1930, lr 0.000125
Step 1100 Loss: 4.1737, lr 0.000125
Step 1200 Loss: 4.1495, lr 0.000125
Step 1300 Loss: 4.1292, lr 0.000125
Step 1400 Loss: 4.1112, lr 0.000125
Step 1500 Loss: 4.0958, lr 0.000125
Step 1600 Loss: 4.0817, lr 0.000125
Step 1700 Loss: 4.0694, lr 0.000125
Step 1800 Loss: 4.0582, lr 0.000125
Step 1900 Loss: 4.0425, lr 0.000125
Step 2000 Loss: 4.0221, lr 0.000125
Step 2100 Loss: 4.0037, lr 0.000125
Step 2200 Loss: 3.9870, lr 0.000125
Step 2300 Loss: 3.9716, lr 0.000125
Step 2400 Loss: 3.9575, lr 0.000125
Step 2500 Loss: 3.9445, lr 0.000125
Step 2600 Loss: 3.9323, lr 0.000125
Step 2700 Loss: 3.9210, lr 0.000125
Step 2800 Loss: 3.9079, lr 0.000125
Step 2900 Loss: 3.8935, lr 0.000125
Step 3000 Loss: 3.8802, lr 0.000125
Step 3100 Loss: 3.8677, lr 0.000125
Step 3200 Loss: 3.8558, lr 0.000125
Step 3300 Loss: 3.8446, lr 0.000125
Step 3400 Loss: 3.8342, lr 0.000125
Step 3500 Loss: 3.8242, lr 0.000125
Step 3600 Loss: 3.8148, lr 0.000125
Step 3700 Loss: 3.8059, lr 0.000125
Step 3800 Loss: 3.7975, lr 0.000125
Step 3900 Loss: 3.7894, lr 0.000125
Step 4000 Loss: 3.7819, lr 0.000125
Step 4100 Loss: 3.7746, lr 0.000125
Step 4200 Loss: 3.7679, lr 0.000125
Step 4300 Loss: 3.7599, lr 0.000125
Step 4400 Loss: 3.7500, lr 0.000125
Step 4500 Loss: 3.7405, lr 0.000125
Step 4600 Loss: 3.7315, lr 0.000125
Step 4700 Loss: 3.7229, lr 0.000125
Step 4800 Loss: 3.7148, lr 0.000125
Step 4900 Loss: 3.7069, lr 0.000125
Step 5000 Loss: 3.6995, lr 0.000125
Step 5100 Loss: 3.6923, lr 0.000125
Step 5200 Loss: 3.6853, lr 0.000125
Step 5300 Loss: 3.6758, lr 0.000125
Step 5400 Loss: 3.6661, lr 0.000125
Step 5500 Loss: 3.6567, lr 0.000125
Step 5600 Loss: 3.6476, lr 0.000125
Step 5700 Loss: 3.6390, lr 0.000125
Step 5800 Loss: 3.6307, lr 0.000125
Step 5900 Loss: 3.6226, lr 0.000125
Step 6000 Loss: 3.6119, lr 0.000125
Step 6100 Loss: 3.6009, lr 0.000125
Step 6200 Loss: 3.5903, lr 0.000125
Step 6300 Loss: 3.5800, lr 0.000125
Step 6400 Loss: 3.5681, lr 0.000125
Step 6500 Loss: 3.5538, lr 0.000125
Step 6600 Loss: 3.5396, lr 0.000125
Step 6700 Loss: 3.5252, lr 0.000125
Train Epoch: [1/200] Loss: 3.5140,lr 0.000250
Calling G2SDataset.batch()
Done, time:  1.59 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.58 s, total batches: 6756
Step 6800 Loss: 4.6209, lr 0.00025
Step 6900 Loss: 4.4527, lr 0.00025
Step 7000 Loss: 4.3345, lr 0.00025
Step 7100 Loss: 4.2756, lr 0.00025
Step 7200 Loss: 4.2424, lr 0.00025
Step 7300 Loss: 4.2101, lr 0.00025
Step 7400 Loss: 4.1689, lr 0.00025
Step 7500 Loss: 4.1387, lr 0.00025
Step 7600 Loss: 4.1155, lr 0.00025
Step 7700 Loss: 4.0970, lr 0.00025
Step 7800 Loss: 4.0821, lr 0.00025
Step 7900 Loss: 4.0658, lr 0.00025
Step 8000 Loss: 4.0475, lr 0.00025
Step 8100 Loss: 4.0318, lr 0.00025
Step 8200 Loss: 4.0183, lr 0.00025
Step 8300 Loss: 4.0066, lr 0.00025
Step 8400 Loss: 3.9959, lr 0.00025
Step 8500 Loss: 3.9865, lr 0.00025
Step 8600 Loss: 3.9781, lr 0.00025
Step 8700 Loss: 3.9599, lr 0.00025
Step 8800 Loss: 3.9423, lr 0.00025
Step 8900 Loss: 3.9266, lr 0.00025
Step 9000 Loss: 3.9121, lr 0.00025
Step 9100 Loss: 3.8987, lr 0.00025
Step 9200 Loss: 3.8866, lr 0.00025
Step 9300 Loss: 3.8753, lr 0.00025
Step 9400 Loss: 3.8648, lr 0.00025
Step 9500 Loss: 3.8553, lr 0.00025
Step 9600 Loss: 3.8414, lr 0.00025
Step 9700 Loss: 3.8287, lr 0.00025
Step 9800 Loss: 3.8167, lr 0.00025
Step 9900 Loss: 3.8055, lr 0.00025
Step 10000 Loss: 3.7949, lr 0.00025
Step 10100 Loss: 3.7849, lr 0.00025
Step 10200 Loss: 3.7754, lr 0.00025
Step 10300 Loss: 3.7664, lr 0.00025
Step 10400 Loss: 3.7580, lr 0.00025
Step 10500 Loss: 3.7500, lr 0.00025
Step 10600 Loss: 3.7425, lr 0.00025
Step 10700 Loss: 3.7353, lr 0.00025
Step 10800 Loss: 3.7286, lr 0.00025
Step 10900 Loss: 3.7222, lr 0.00025
Step 11000 Loss: 3.7162, lr 0.00025
Step 11100 Loss: 3.7073, lr 0.00025
Step 11200 Loss: 3.6981, lr 0.00025
Step 11300 Loss: 3.6895, lr 0.00025
Step 11400 Loss: 3.6812, lr 0.00025
Step 11500 Loss: 3.6732, lr 0.00025
Step 11600 Loss: 3.6658, lr 0.00025
Step 11700 Loss: 3.6587, lr 0.00025
Step 11800 Loss: 3.6517, lr 0.00025
Step 11900 Loss: 3.6452, lr 0.00025
Step 12000 Loss: 3.6378, lr 0.00025
Step 12100 Loss: 3.6283, lr 0.00025
Step 12200 Loss: 3.6191, lr 0.00025
Step 12300 Loss: 3.6102, lr 0.00025
Step 12400 Loss: 3.6017, lr 0.00025
Step 12500 Loss: 3.5935, lr 0.00025
Step 12600 Loss: 3.5856, lr 0.00025
Step 12700 Loss: 3.5772, lr 0.00025
Step 12800 Loss: 3.5661, lr 0.00025
Step 12900 Loss: 3.5555, lr 0.00025
Step 13000 Loss: 3.5452, lr 0.00025
Step 13100 Loss: 3.5351, lr 0.00025
Step 13200 Loss: 3.5216, lr 0.00025
Step 13300 Loss: 3.5077, lr 0.00025
Step 13400 Loss: 3.4938, lr 0.00025
Step 13500 Loss: 3.4771, lr 0.00025
Train Epoch: [2/200] Loss: 3.4746,lr 0.000375
Calling G2SDataset.batch()
Done, time:  1.91 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.69 s, total batches: 6756
Step 13600 Loss: 4.4679, lr 0.000375
Step 13700 Loss: 4.3612, lr 0.000375
Step 13800 Loss: 4.2735, lr 0.000375
Step 13900 Loss: 4.2295, lr 0.000375
Step 14000 Loss: 4.2043, lr 0.000375
Step 14100 Loss: 4.1639, lr 0.000375
Step 14200 Loss: 4.1292, lr 0.000375
Step 14300 Loss: 4.1031, lr 0.000375
Step 14400 Loss: 4.0828, lr 0.000375
Step 14500 Loss: 4.0666, lr 0.000375
Step 14600 Loss: 4.0529, lr 0.000375
Step 14700 Loss: 4.0345, lr 0.000375
Step 14800 Loss: 4.0179, lr 0.000375
Step 14900 Loss: 4.0035, lr 0.000375
Step 15000 Loss: 3.9912, lr 0.000375
Step 15100 Loss: 3.9803, lr 0.000375
Step 15200 Loss: 3.9707, lr 0.000375
Step 15300 Loss: 3.9621, lr 0.000375
Step 15400 Loss: 3.9500, lr 0.000375
Step 15500 Loss: 3.9318, lr 0.000375
Step 15600 Loss: 3.9152, lr 0.000375
Step 15700 Loss: 3.9003, lr 0.000375
Step 15800 Loss: 3.8868, lr 0.000375
Step 15900 Loss: 3.8742, lr 0.000375
Step 16000 Loss: 3.8626, lr 0.000375
Step 16100 Loss: 3.8519, lr 0.000375
Step 16200 Loss: 3.8420, lr 0.000375
Step 16300 Loss: 3.8308, lr 0.000375
Step 16400 Loss: 3.8176, lr 0.000375
Step 16500 Loss: 3.8054, lr 0.000375
Step 16600 Loss: 3.7939, lr 0.000375
Step 16700 Loss: 3.7831, lr 0.000375
Step 16800 Loss: 3.7730, lr 0.000375
Step 16900 Loss: 3.7633, lr 0.000375
Step 17000 Loss: 3.7542, lr 0.000375
Step 17100 Loss: 3.7458, lr 0.000375
Step 17200 Loss: 3.7377, lr 0.000375
Step 17300 Loss: 3.7301, lr 0.000375
Step 17400 Loss: 3.7228, lr 0.000375
Step 17500 Loss: 3.7160, lr 0.000375
Step 17600 Loss: 3.7096, lr 0.000375
Step 17700 Loss: 3.7035, lr 0.000375
Step 17800 Loss: 3.6967, lr 0.000375
Step 17900 Loss: 3.6875, lr 0.000375
Step 18000 Loss: 3.6787, lr 0.000375
Step 18100 Loss: 3.6703, lr 0.000375
Step 18200 Loss: 3.6623, lr 0.000375
Step 18300 Loss: 3.6547, lr 0.000375
Step 18400 Loss: 3.6475, lr 0.000375
Step 18500 Loss: 3.6406, lr 0.000375
Step 18600 Loss: 3.6339, lr 0.000375
Step 18700 Loss: 3.6275, lr 0.000375
Step 18800 Loss: 3.6188, lr 0.000375
Step 18900 Loss: 3.6095, lr 0.000375
Step 19000 Loss: 3.6005, lr 0.000375
Step 19100 Loss: 3.5919, lr 0.000375
Step 19200 Loss: 3.5836, lr 0.000375
Step 19300 Loss: 3.5755, lr 0.000375
Step 19400 Loss: 3.5678, lr 0.000375
Step 19500 Loss: 3.5578, lr 0.000375
Step 19600 Loss: 3.5469, lr 0.000375
Step 19700 Loss: 3.5366, lr 0.000375
Step 19800 Loss: 3.5265, lr 0.000375
Step 19900 Loss: 3.5152, lr 0.000375
Step 20000 Loss: 3.5012, lr 0.000375
Step 20100 Loss: 3.4874, lr 0.000375
Step 20200 Loss: 3.4738, lr 0.000375
Train Epoch: [3/200] Loss: 3.4607,lr 0.000500
Calling G2SDataset.batch()
Done, time:  2.06 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.73 s, total batches: 6755
Step 20300 Loss: 4.6597, lr 0.0005
Step 20400 Loss: 4.4159, lr 0.0005
Step 20500 Loss: 4.3039, lr 0.0005
Step 20600 Loss: 4.2415, lr 0.0005
Step 20700 Loss: 4.2074, lr 0.0005
Step 20800 Loss: 4.1796, lr 0.0005
Step 20900 Loss: 4.1381, lr 0.0005
Step 21000 Loss: 4.1080, lr 0.0005
Step 21100 Loss: 4.0849, lr 0.0005
Step 21200 Loss: 4.0667, lr 0.0005
Step 21300 Loss: 4.0518, lr 0.0005
Step 21400 Loss: 4.0368, lr 0.0005
Step 21500 Loss: 4.0185, lr 0.0005
Step 21600 Loss: 4.0031, lr 0.0005
Step 21700 Loss: 3.9898, lr 0.0005
Step 21800 Loss: 3.9781, lr 0.0005
Step 21900 Loss: 3.9678, lr 0.0005
Step 22000 Loss: 3.9587, lr 0.0005
Step 22100 Loss: 3.9505, lr 0.0005
Step 22200 Loss: 3.9339, lr 0.0005
Step 22300 Loss: 3.9166, lr 0.0005
Step 22400 Loss: 3.9010, lr 0.0005
Step 22500 Loss: 3.8867, lr 0.0005
Step 22600 Loss: 3.8735, lr 0.0005
Step 22700 Loss: 3.8615, lr 0.0005
Step 22800 Loss: 3.8505, lr 0.0005
Step 22900 Loss: 3.8402, lr 0.0005
Step 23000 Loss: 3.8307, lr 0.0005
Step 23100 Loss: 3.8176, lr 0.0005
Step 23200 Loss: 3.8049, lr 0.0005
Step 23300 Loss: 3.7931, lr 0.0005
Step 23400 Loss: 3.7820, lr 0.0005
Step 23500 Loss: 3.7716, lr 0.0005
Step 23600 Loss: 3.7616, lr 0.0005
Step 23700 Loss: 3.7523, lr 0.0005
Step 23800 Loss: 3.7436, lr 0.0005
Step 23900 Loss: 3.7353, lr 0.0005
Step 24000 Loss: 3.7274, lr 0.0005
Step 24100 Loss: 3.7201, lr 0.0005
Step 24200 Loss: 3.7131, lr 0.0005
Step 24300 Loss: 3.7065, lr 0.0005
Step 24400 Loss: 3.7002, lr 0.0005
Step 24500 Loss: 3.6943, lr 0.0005
Step 24600 Loss: 3.6860, lr 0.0005
Step 24700 Loss: 3.6770, lr 0.0005
Step 24800 Loss: 3.6684, lr 0.0005
Step 24900 Loss: 3.6602, lr 0.0005
Step 25000 Loss: 3.6524, lr 0.0005
Step 25100 Loss: 3.6449, lr 0.0005
Step 25200 Loss: 3.6379, lr 0.0005
Step 25300 Loss: 3.6311, lr 0.0005
Step 25400 Loss: 3.6245, lr 0.0005
Step 25500 Loss: 3.6177, lr 0.0005
Step 25600 Loss: 3.6082, lr 0.0005
Step 25700 Loss: 3.5989, lr 0.0005
Step 25800 Loss: 3.5901, lr 0.0005
Step 25900 Loss: 3.5816, lr 0.0005
Step 26000 Loss: 3.5734, lr 0.0005
Step 26100 Loss: 3.5655, lr 0.0005
Step 26200 Loss: 3.5574, lr 0.0005
Step 26300 Loss: 3.5463, lr 0.0005
Step 26400 Loss: 3.5356, lr 0.0005
Step 26500 Loss: 3.5253, lr 0.0005
Step 26600 Loss: 3.5152, lr 0.0005
Step 26700 Loss: 3.5022, lr 0.0005
Step 26800 Loss: 3.4882, lr 0.0005
Step 26900 Loss: 3.4745, lr 0.0005
Step 27000 Loss: 3.4586, lr 0.0005
Train Epoch: [4/200] Loss: 3.4542,lr 0.000625
Calling G2SDataset.batch()
Done, time:  1.98 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.61 s, total batches: 6755
Step 27100 Loss: 4.4551, lr 0.000625
Step 27200 Loss: 4.3518, lr 0.000625
Step 27300 Loss: 4.2587, lr 0.000625
Step 27400 Loss: 4.2134, lr 0.000625
Step 27500 Loss: 4.1877, lr 0.000625
Step 27600 Loss: 4.1505, lr 0.000625
Step 27700 Loss: 4.1151, lr 0.000625
Step 27800 Loss: 4.0891, lr 0.000625
Step 27900 Loss: 4.0687, lr 0.000625
Step 28000 Loss: 4.0524, lr 0.000625
Step 28100 Loss: 4.0388, lr 0.000625
Step 28200 Loss: 4.0213, lr 0.000625
Step 28300 Loss: 4.0047, lr 0.000625
Step 28400 Loss: 3.9903, lr 0.000625
Step 28500 Loss: 3.9780, lr 0.000625
Step 28600 Loss: 3.9672, lr 0.000625
Step 28700 Loss: 3.9576, lr 0.000625
Step 28800 Loss: 3.9490, lr 0.000625
Step 28900 Loss: 3.9385, lr 0.000625
Step 29000 Loss: 3.9203, lr 0.000625
Step 29100 Loss: 3.9038, lr 0.000625
Step 29200 Loss: 3.8890, lr 0.000625
Step 29300 Loss: 3.8753, lr 0.000625
Step 29400 Loss: 3.8628, lr 0.000625
Step 29500 Loss: 3.8512, lr 0.000625
Step 29600 Loss: 3.8407, lr 0.000625
Step 29700 Loss: 3.8308, lr 0.000625
Step 29800 Loss: 3.8201, lr 0.000625
Step 29900 Loss: 3.8068, lr 0.000625
Step 30000 Loss: 3.7947, lr 0.000625
Step 30100 Loss: 3.7832, lr 0.000625
Step 30200 Loss: 3.7724, lr 0.000625
Step 30300 Loss: 3.7623, lr 0.000625
Step 30400 Loss: 3.7528, lr 0.000625
Step 30500 Loss: 3.7438, lr 0.000625
Step 30600 Loss: 3.7353, lr 0.000625
Step 30700 Loss: 3.7272, lr 0.000625
Step 30800 Loss: 3.7196, lr 0.000625
Step 30900 Loss: 3.7124, lr 0.000625
Step 31000 Loss: 3.7056, lr 0.000625
Step 31100 Loss: 3.6992, lr 0.000625
Step 31200 Loss: 3.6930, lr 0.000625
Step 31300 Loss: 3.6866, lr 0.000625
Step 31400 Loss: 3.6775, lr 0.000625
Step 31500 Loss: 3.6687, lr 0.000625
Step 31600 Loss: 3.6603, lr 0.000625
Step 31700 Loss: 3.6522, lr 0.000625
Step 31800 Loss: 3.6446, lr 0.000625
Step 31900 Loss: 3.6373, lr 0.000625
Step 32000 Loss: 3.6305, lr 0.000625
Step 32100 Loss: 3.6237, lr 0.000625
Step 32200 Loss: 3.6173, lr 0.000625
Step 32300 Loss: 3.6090, lr 0.000625
Step 32400 Loss: 3.5997, lr 0.000625
Step 32500 Loss: 3.5906, lr 0.000625
Step 32600 Loss: 3.5819, lr 0.000625
Step 32700 Loss: 3.5735, lr 0.000625
Step 32800 Loss: 3.5655, lr 0.000625
Step 32900 Loss: 3.5578, lr 0.000625
Step 33000 Loss: 3.5481, lr 0.000625
Step 33100 Loss: 3.5371, lr 0.000625
Step 33200 Loss: 3.5267, lr 0.000625
Step 33300 Loss: 3.5165, lr 0.000625
Step 33400 Loss: 3.5057, lr 0.000625
Step 33500 Loss: 3.4916, lr 0.000625
Step 33600 Loss: 3.4778, lr 0.000625
Step 33700 Loss: 3.4642, lr 0.000625
Train Epoch: [5/200] Loss: 3.4500,lr 0.000750
Model Saving at epoch 5
Calling G2SDataset.batch()
Done, time:  2.45 s, total batches: 6755
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 6755
Step 33800 Loss: 4.7588, lr 0.00075
Step 33900 Loss: 4.4090, lr 0.00075
Step 34000 Loss: 4.3008, lr 0.00075
Step 34100 Loss: 4.2351, lr 0.00075
Step 34200 Loss: 4.2001, lr 0.00075
Step 34300 Loss: 4.1747, lr 0.00075
Step 34400 Loss: 4.1322, lr 0.00075
Step 34500 Loss: 4.1015, lr 0.00075
Step 34600 Loss: 4.0781, lr 0.00075
Step 34700 Loss: 4.0595, lr 0.00075
Step 34800 Loss: 4.0445, lr 0.00075
Step 34900 Loss: 4.0302, lr 0.00075
Step 35000 Loss: 4.0119, lr 0.00075
Step 35100 Loss: 3.9963, lr 0.00075
Step 35200 Loss: 3.9828, lr 0.00075
Step 35300 Loss: 3.9712, lr 0.00075
Step 35400 Loss: 3.9610, lr 0.00075
Step 35500 Loss: 3.9518, lr 0.00075
Step 35600 Loss: 3.9436, lr 0.00075
Step 35700 Loss: 3.9281, lr 0.00075
Step 35800 Loss: 3.9107, lr 0.00075
Step 35900 Loss: 3.8949, lr 0.00075
Step 36000 Loss: 3.8807, lr 0.00075
Step 36100 Loss: 3.8675, lr 0.00075
Step 36200 Loss: 3.8554, lr 0.00075
Step 36300 Loss: 3.8443, lr 0.00075
Step 36400 Loss: 3.8340, lr 0.00075
Step 36500 Loss: 3.8245, lr 0.00075
Step 36600 Loss: 3.8119, lr 0.00075
Step 36700 Loss: 3.7991, lr 0.00075
Step 36800 Loss: 3.7873, lr 0.00075
Step 36900 Loss: 3.7761, lr 0.00075
Step 37000 Loss: 3.7656, lr 0.00075
Step 37100 Loss: 3.7558, lr 0.00075
Step 37200 Loss: 3.7465, lr 0.00075
Step 37300 Loss: 3.7377, lr 0.00075
Step 37400 Loss: 3.7294, lr 0.00075
Step 37500 Loss: 3.7215, lr 0.00075
Step 37600 Loss: 3.7142, lr 0.00075
Step 37700 Loss: 3.7071, lr 0.00075
Step 37800 Loss: 3.7005, lr 0.00075
Step 37900 Loss: 3.6942, lr 0.00075
Step 38000 Loss: 3.6882, lr 0.00075
Step 38100 Loss: 3.6803, lr 0.00075
Step 38200 Loss: 3.6712, lr 0.00075
Step 38300 Loss: 3.6627, lr 0.00075
Step 38400 Loss: 3.6544, lr 0.00075
Step 38500 Loss: 3.6466, lr 0.00075
Step 38600 Loss: 3.6391, lr 0.00075
Step 38700 Loss: 3.6320, lr 0.00075
Step 38800 Loss: 3.6251, lr 0.00075
Step 38900 Loss: 3.6186, lr 0.00075
Step 39000 Loss: 3.6121, lr 0.00075
Step 39100 Loss: 3.6025, lr 0.00075
Step 39200 Loss: 3.5932, lr 0.00075
Step 39300 Loss: 3.5844, lr 0.00075
Step 39400 Loss: 3.5758, lr 0.00075
Step 39500 Loss: 3.5675, lr 0.00075
Step 39600 Loss: 3.5596, lr 0.00075
Step 39700 Loss: 3.5519, lr 0.00075
Step 39800 Loss: 3.5407, lr 0.00075
Step 39900 Loss: 3.5300, lr 0.00075
Step 40000 Loss: 3.5196, lr 0.00075
Step 40100 Loss: 3.5095, lr 0.00075
Step 40200 Loss: 3.4969, lr 0.00075
Step 40300 Loss: 3.4831, lr 0.00075
Step 40400 Loss: 3.4693, lr 0.00075
Step 40500 Loss: 3.4541, lr 0.00075
Train Epoch: [6/200] Loss: 3.4478,lr 0.000875
Calling G2SDataset.batch()
Done, time:  1.82 s, total batches: 6756
Calling G2SDataset.batch()
Done, time:  1.70 s, total batches: 6756
Step 40600 Loss: 4.4727, lr 0.000875
Step 40700 Loss: 4.3642, lr 0.000875
Step 40800 Loss: 4.2624, lr 0.000875
Step 40900 Loss: 4.2143, lr 0.000875
Step 41000 Loss: 4.1870, lr 0.000875
Step 41100 Loss: 4.1519, lr 0.000875
Step 41200 Loss: 4.1153, lr 0.000875
Step 41300 Loss: 4.0885, lr 0.000875
Step 41400 Loss: 4.0675, lr 0.000875
Step 41500 Loss: 4.0507, lr 0.000875
Step 41600 Loss: 4.0368, lr 0.000875
Step 41700 Loss: 4.0198, lr 0.000875
Step 41800 Loss: 4.0030, lr 0.000875
Step 41900 Loss: 3.9883, lr 0.000875
Step 42000 Loss: 3.9758, lr 0.000875
Step 42100 Loss: 3.9649, lr 0.000875
Step 42200 Loss: 3.9552, lr 0.000875
Step 42300 Loss: 3.9464, lr 0.000875
Step 42400 Loss: 3.9369, lr 0.000875
Step 42500 Loss: 3.9186, lr 0.000875
Step 42600 Loss: 3.9020, lr 0.000875
Step 42700 Loss: 3.8870, lr 0.000875
Step 42800 Loss: 3.8732, lr 0.000875
Step 42900 Loss: 3.8606, lr 0.000875
